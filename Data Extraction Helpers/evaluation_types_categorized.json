{
  "Accuracy & Core Capability": [
    "output quality / capability",
    "content accuracy",
    "factual accuracy",
    "information accuracy",
    "diagnostic accuracy",
    "accuracy",
    "completeness",
    "diagnostic agreement",
    "credibility assessment",
    "correlational validity",
    "capability (narrative)",
    "capability/accuracy",
    "performance",
    "factuality assessment",
    "medical accuracy",
    "output quality evaluation",
    "error analysis",
    "information quality assessment",
    "prognostic model performance",
    "capability demonstration",
    "clinical validation",
    "factuality/accuracy",
    "capability/benchmarking",
    "clinical relevance",
    "semantic similarity",
    "capability/quality",
    "quality",
    "capability evaluation",
    "capability/performance",
    "factual consistency",
    "reference accuracy",
    "diagnostic performance",
    "accuracy assessment",
    "capability benchmark",
    "information quality",
    "factual accuracy/quality",
    "knowledge assessment",
    "ablation",
    "capability assessment",
    "output quality",
    "response quality",
    "factuality",
    "capability"
  ],
  "Alignment & Guideline Concordance": [
    "alignment with expert decisions",
    "alignment with evidence-based guidelines",
    "alignment with expert consensus",
    "concordance with guidelines",
    "alignment",
    "alignment with guideline",
    "guideline concordance",
    "capability/alignment with clinical guidelines",
    "alignment with experts",
    "agreement with expert recommendations",
    "guideline adherence",
    "alignment with guidelines",
    "agreement with experts",
    "agreement",
    "alignment with clinicians",
    "alignment to guidelines",
    "capability (alignment with evidence-based guidelines)",
    "alignment with clinical guidelines",
    "agreement study",
    "comparison to guidelines"
  ],
  "Safety & Risk/Harm Assessment": [
    "safety/robustness",
    "safety/appropriateness",
    "hallucination assessment",
    "ethical assessment",
    "hallucination analysis",
    "safety/alignment",
    "safety/ethical",
    "risk assessment",
    "safety assessment",
    "safety",
    "safety/risk",
    "safety/harmfulness",
    "ethical analysis",
    "safety/misinformation",
    "hallucination",
    "potential harm",
    "appropriateness assessment"
  ],
  "Reliability, Robustness & Stability": [
    "drift",
    "reliability/repeatability",
    "robustness",
    "consistency/robustness",
    "response variability",
    "calibration",
    "stability",
    "reproducibility",
    "reliability assessment",
    "reliability",
    "external validation",
    "consistency",
    "response consistency",
    "calibration/metacognition",
    "content reliability",
    "repeatability",
    "internal consistency",
    "repeatability/reproducibility",
    "consistency analysis"
  ],
  "Bias & Fairness": [
    "fairness/bias evaluation",
    "fairness",
    "bias evaluation",
    "bias/fairness analysis",
    "bias/fairness",
    "bias assessment",
    "fairness/robustness",
    "cultural sensitivity",
    "bias/safety",
    "bias analysis",
    "fairness/bias",
    "bias robustness",
    "bias"
  ],
  "Explainability & Transparency": [
    "qualitative content analysis",
    "reasoning quality",
    "explanation quality preference",
    "reasoning consistency",
    "explanation consistency",
    "citation completeness",
    "confidence",
    "explainability",
    "reasoning",
    "alignment/interpretability",
    "source similarity analysis",
    "communication competency coverage",
    "source quality",
    "human subjective explanation rating"
  ],
  "Human Expert Evaluation & Comparison": [
    "human expert preference",
    "expert qualitative assessment",
    "expert review",
    "expert subjective rating",
    "human expert comparison",
    "human rating",
    "human expert assessment",
    "human preference judgment",
    "expert comparison",
    "expert validation",
    "human subjective evaluation",
    "agreement with human expert",
    "qualitative analysis",
    "human expert evaluation",
    "comparison to clinicians",
    "expert preference",
    "human preference/comparison",
    "human expert rating",
    "comparison to human baseline",
    "human judgment",
    "expert grading",
    "human expert review",
    "human-comparison",
    "expert comparison of report quality",
    "subjective human evaluation",
    "comparative human evaluation",
    "qualitative assessment",
    "comparison to physicians",
    "comparison with human performance",
    "human expert grading",
    "preference ranking",
    "comparison with clinicians",
    "qualitative thematic analysis",
    "alignment with human judgment",
    "human preference rating",
    "human preference/detection",
    "expert rating",
    "qualitative expert review",
    "human preference",
    "comparison against physicians",
    "human expert subjective rating",
    "human subjective rating",
    "expert evaluation",
    "human vs LLM comparison",
    "human evaluation",
    "comparison with human experts",
    "subjective quality assessment",
    "human preference study",
    "subjective assessment",
    "agreement with human experts",
    "qualitative feedback",
    "subjective preference",
    "subjective quality rating",
    "preference",
    "subjective human rating",
    "human qualitative rating",
    "human preference assessment",
    "human comparison",
    "comparison to humans"
  ],
  "Patient/End-User Outcomes & Satisfaction": [
    "patient-reported outcome",
    "patient rating",
    "patient-reported outcomes",
    "patient satisfaction",
    "clinical impact",
    "human preference survey",
    "human lay preference",
    "clinical outcomes",
    "human feedback",
    "clinical outcome",
    "user self-report"
  ],
  "User Experience, Usability & Trust": [
    "usability/acceptability",
    "user comprehension/usability",
    "user satisfaction",
    "human-computer interaction",
    "helpfulness",
    "user trust/cognitive load",
    "survey",
    "subjective satisfaction survey",
    "user perception",
    "acceptability",
    "engagement analysis",
    "trust survey",
    "satisfaction survey",
    "human user study",
    "user preference / usability",
    "human factors",
    "human satisfaction survey",
    "usability survey",
    "subjective comprehension survey",
    "user experience",
    "usefulness assessment",
    "Likert-scale quality assessment",
    "user preference/usability",
    "user acceptability",
    "usability",
    "trustworthiness",
    "user perception survey",
    "human-AI collaboration",
    "quality/usability assessment",
    "qualitative user experience",
    "qualitative usability analysis",
    "perception survey",
    "usability/user experience",
    "user preference",
    "usability/readability",
    "usability/quality assessment",
    "user satisfaction survey",
    "human-AI interaction",
    "user acceptance",
    "user study"
  ],
  "Education & Training Impact": [
    "education/training effectiveness",
    "human-rated educational value",
    "education impact",
    "educational outcome",
    "educational",
    "educational impact",
    "educational effectiveness"
  ],
  "Efficiency & Workflow Impact": [
    "utility",
    "clinical utility/efficiency",
    "energy efficiency",
    "quality improvement",
    "clinical utility assessment",
    "burnout",
    "cost analysis",
    "cost-effectiveness",
    "workflow efficiency",
    "cost/time efficiency",
    "utility assessment",
    "AI assistance effect",
    "efficiency",
    "efficiency/time"
  ],
  "Implementation & Feasibility": [
    "implementation",
    "functional testing",
    "real-world correlation",
    "feasibility",
    "real-world prospective deployment",
    "technical functionality",
    "feasibility study",
    "usability/feasibility",
    "prospective deployment",
    "real-world deployment",
    "feasibility/qualitative"
  ],
  "Regulatory & Ethical Compliance": [
    "regulatory compliance",
    "safety/ethics",
    "privacy robustness",
    "privacy",
    "ethical evaluation"
  ],
  "Benchmarking & Head-to-Head Evaluations": [
    "head-to-head comparison",
    "comparative human-AI",
    "benchmark",
    "randomized controlled trial",
    "reference standard comparison",
    "comparative performance",
    "comparison with prior model",
    "benchmarking",
    "comparative benchmark",
    "comparative",
    "AI detection",
    "Turing-like author identification",
    "head-to-head",
    "head-to-head model comparison",
    "capability benchmarking",
    "Turing test",
    "human-AI comparison"
  ],
  "Content Quality & Readability": [
    "readability metrics",
    "readability",
    "understandability/actionability assessment",
    "comprehensibility",
    "understandability assessment",
    "sentiment",
    "AI-generated content quality",
    "readability analysis",
    "sentiment analysis",
    "readability/usability",
    "content quality",
    "text readability",
    "quality evaluation",
    "comprehensibility/readability",
    "quality assessment",
    "sentiment/tone",
    "understandability",
    "content analysis",
    "usability/readability assessment",
    "capability (readability)",
    "presentation suitability",
    "comprehension",
    "readability assessment",
    "actionability",
    "empathy"
  ],
  "Other": [
    "prompt engineering",
    "instrument validation",
    "case study",
    "unsure",
    "method development",
    "clinical case study",
    "perception",
    "human",
    "demonstration",
    "conceptual discussion",
    "conceptual",
    "system description",
    "suitability",
    "originality"
  ]
}