Title,Abstract,Tiebreak,Tiebreaker,,
Evaluation of Generative Artificial Intelligence Models in Predicting Pediatric Emergency Severity Index Levels,"Objective: Evaluate the accuracy and reliability of various generative artificial intelligence (AI) models (ChatGPT-3.5, ChatGPT-4.0, T5, Llama-2, Mistral-Large, and Claude-3 Opus) in predicting Emergency Severity Index (ESI) levels for pediatric emergency department patients and assess the impact of medically oriented fine-tuning. Methods: Seventy pediatric clinical vignettes from the ESI Handbook version 4 were used as the gold standard. Each AI model predicted the ESI level for each vignette. Performance metrics, including sensitivity, specificity, and F1 score, were calculated. Reliability was assessed by repeating the tests and measuring the interrater reliability using Fleiss kappa. Paired t tests were used to compare the models before and after fine-tuning. Results: Claude-3 Opus achieved the highest performance amongst the untrained models with a sensitivity of 80.6% (95% confidence interval [CI]: 63.6-90.7), specificity of 91.3% (95% CI: 83.8-99), and an F1 score of 73.9% (95% CI: 58.9-90.7). After fine-tuning, the GPT-4.0 model showed statistically significant improvement with a sensitivity of 77.1% (95% CI: 60.1-86.5), specificity of 92.5% (95% CI: 89.5-97.4), and an F1 score of 74.6% (95% CI: 63.9-83.8, P < 0.04). Reliability analysis revealed high agreement for Claude-3 Opus (Fleiss κ: 0.85), followed by Mistral-Large (Fleiss κ: 0.79) and trained GPT-4.0 (Fleiss κ: 0.67). Training improved the reliability of GPT models (P < 0.001). Conclusions: Generative AI models demonstrate promising accuracy in predicting pediatric ESI levels, with fine-tuning significantly enhancing their performance and reliability. These findings suggest that AI could serve as a valuable tool in pediatric triage. © 2025 Lippincott Williams and Wilkins. All rights reserved.",III,Robert,,
Neurological Diagnosis: Artificial Intelligence Compared With Diagnostic Generator,"3308. Neurologist. 2024 May 1;29(3):143-145. doi: 10.1097/NRL.0000000000000560.

Neurological Diagnosis: Artificial Intelligence Compared With Diagnostic 
Generator.

Finelli PF(1).

Author information:
(1)Department of Neurology, Hartford Hospital and University of Connecticut 
School of Medicine, Hartford, CT.

OBJECTIVE: Artificial intelligence has recently become available for widespread 
use in medicine, including the interpretation of digitized information, big data 
for tracking disease trends and patterns, and clinical diagnosis. Comparative 
studies and expert opinion support the validity of imaging and data analysis, 
yet similar validation is lacking in clinical diagnosis. Artificial intelligence 
programs are here compared with a diagnostic generator program in clinical 
neurology.
METHODS: Using 4 nonrandomly selected case records from New England Journal of 
Medicine clinicopathologic conferences from 2017 to 2022, 2 artificial 
intelligence programs (ChatGPT-4 and GLASS AI) were compared with a neurological 
diagnostic generator program (NeurologicDx.com) for diagnostic capability and 
accuracy and source authentication.
RESULTS: Compared with NeurologicDx.com, the 2 AI programs showed results 
varying with order of key term entry and with repeat querying. The diagnostic 
generator yielded more differential diagnostic entities, with correct diagnoses 
in 4 of 4 test cases versus 0 of 4 for ChatGPT-4 and 1 of 4 for GLASS AI, 
respectively, and with authentication of diagnostic entities compared with the 
AI programs.
CONCLUSIONS: The diagnostic generator NeurologicDx yielded a more robust and 
reproducible differential diagnostic list with higher diagnostic accuracy and 
associated authentication compared with artificial intelligence programs.

Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.",III,Robert,,
Bridging the Gap in Neonatal Care: Evaluating AI Chatbots for Chronic Neonatal Lung Disease and Home Oxygen Therapy Management,"Objective: To evaluate the accuracy and comprehensiveness of eight free, publicly available large language model (LLM) chatbots in addressing common questions related to chronic neonatal lung disease (CNLD) and home oxygen therapy (HOT). Study Design: Twenty CNLD and HOT-related questions were curated across nine domains. Responses from ChatGPT-3.5, Google Bard, Bing Chat, Claude 3.5 Sonnet, ERNIE Bot 3.5, and GLM-4 were generated and evaluated by three experienced neonatologists using Likert scales for accuracy and comprehensiveness. Updated LLM models (ChatGPT-4o mini and Gemini 2.0 Flash Experimental) were incorporated to assess rapid technological advancement. Statistical analyses included ANOVA, Kruskal–Wallis tests, and intraclass correlation coefficients. Results: Bing Chat and Claude 3.5 Sonnet demonstrated superior performance, with the highest mean accuracy scores (5.78 ± 0.48 and 5.75 ± 0.54, respectively) and competence scores (2.65 ± 0.58 and 2.80 ± 0.41, respectively). In subsequent testing, Gemini 2.0 Flash Experimental and ChatGPT-4o mini achieved comparable high performance. Performance varied across domains, with all models excelling in “equipment and safety protocols” and “caregiver support.” ERNIE Bot 3.5 and GLM-4 showed self-correction capabilities when prompted. Conclusions: LLMs promise accurate CNLD/HOT information. However, performance variability and the risk of misinformation necessitate expert oversight and continued refinement before widespread clinical implementation. © 2025 Wiley Periodicals LLC.",III,Robert,,
Can We Trust the Information on Allergic Rhinitis From Chat Generated Pre-Trained Transformer?; [Chat Generated Pre-Trained Transformer를 통해 얻은 알레르기 비염 정보를 믿을 수 있을까?],"Background and Objectives Chat Generated Pre-Trained Transformer (ChatGPT) is a large language model, which allows consumers to get information with one simple question for free. There are few studies that reported the reliability and usefullness of ChatGPT in the field of otorhinolarynoglogy, so we would like to investigate the reliability of information about allergic rhinitis generated by ChatGPT. Materials and Method We asked ChatGPT 35 questions related to allergic rhinitis based on the textbook from Korean Society of Otorhinolaryngology-Head and Neck Surgery, the clinical guidelines of American Academy of Otolaryngology-Head and Neck Surgery, and the guidelines of Allergic Rhinitis and Its impact on Asthma, and recorded the generated answers. Five specialists from our department were shown the answers in a blind test to assess their reliability. Each question was rated either 'accurate' or 'inaccurate,' and if the contents were not found in the textbook or the guideline, it was rated as 'unreliable.' Results Of the 35 questions, 26 (74%) were identified as 'accurate,' 9 (26%) were 'inaccurate,' and 0 (0%) were 'unreliable.' Questions about epidemiology, causes, diagnosis, and prognosis were found to be more accurate than the average, whereas definitions and treatments were less accurate than the average. Conclusion The information about allergic rhinitis generated by ChatGPT was quite reliable, showing that ChatGPT can be helpful in understanding and treating the disease. It is necessary to use the developing medical artificial intelligence wisely. Copyright © 2024 Korean Society of Otorhinolaryngology-Head and Neck Surgery.",III,Robert,,
Evaluating ChatGPT on Orbital and Oculofacial Disorders: Accuracy and Readability Insights,"3136. Ophthalmic Plast Reconstr Surg. 2024 Mar-Apr 01;40(2):217-222. doi: 
10.1097/IOP.0000000000002552. Epub 2023 Nov 16.

Evaluating ChatGPT on Orbital and Oculofacial Disorders: Accuracy and 
Readability Insights.

Balas M(1), Janic A(1), Daigle P(2), Nijhawan N(2), Hussain A(3), Gill H(2), 
Lahaie GL(4), Belliveau MJ(5), Crawford SA(1)(6), Arjmand P(7), Ing EB(2)(8).

Author information:
(1)Temerty Faculty of Medicine.
(2)Department of Ophthalmology and Vision Sciences, University of Toronto, 
Toronto, Ontario, Canada.
(3)Department of Ophthalmology and Visual Sciences, Dalhousie University, 
Halifax, Nova Scotia, Canada.
(4)Department of Ophthalmology, Queen's University, Kingston, Ontario, Canada.
(5)Department of Ophthalmology, University of Ottawa and The Ottawa Hospital 
Research Institute, Ottawa, Ontario, Canada.
(6)Division of Vascular Surgery, Department of Surgery, University of Toronto, 
Toronto, Ontario, Canada.
(7)Mississauga Retina Institute, Toronto, Ontario, Canada.
(8)Department of Ophthalmology and Vision Sciences, University of Alberta, 
Edmonton, Alberta, Canada.

PURPOSE: To assess the accuracy and readability of responses generated by the 
artificial intelligence model, ChatGPT (version 4.0), to questions related to 10 
essential domains of orbital and oculofacial disease.
METHODS: A set of 100 questions related to the diagnosis, treatment, and 
interpretation of orbital and oculofacial diseases was posed to ChatGPT 4.0. 
Responses were evaluated by a panel of 7 experts based on appropriateness and 
accuracy, with performance scores measured on a 7-item Likert scale. Inter-rater 
reliability was determined via the intraclass correlation coefficient.
RESULTS: The artificial intelligence model demonstrated accurate and consistent 
performance across all 10 domains of orbital and oculofacial disease, with an 
average appropriateness score of 5.3/6.0 (""mostly appropriate"" to ""completely 
appropriate""). Domains of cavernous sinus fistula, retrobulbar hemorrhage, and 
blepharospasm had the highest domain scores (average scores of 5.5 to 5.6), 
while the proptosis domain had the lowest (average score of 5.0/6.0). The 
intraclass correlation coefficient was 0.64 (95% CI: 0.52 to 0.74), reflecting 
moderate inter-rater reliability. The responses exhibited a high reading-level 
complexity, representing the comprehension levels of a college or graduate 
education.
CONCLUSIONS: This study demonstrates the potential of ChatGPT 4.0 to provide 
accurate information in the field of ophthalmology, specifically orbital and 
oculofacial disease. However, challenges remain in ensuring accurate and 
comprehensive responses across all disease domains. Future improvements should 
focus on refining the model's correctness and eventually expanding the scope to 
visual data interpretation. Our results highlight the vast potential for 
artificial intelligence in educational and clinical ophthalmology contexts.

Copyright © 2023 The American Society of Ophthalmic Plastic and Reconstructive 
Surgery, Inc.",III,Robert,,
Performance of large language models (LLMs) in providing prostate cancer information,"Purpose: The diagnosis and management of prostate cancer (PCa), the second most common cancer in men worldwide, are highly complex. Hence, patients often seek knowledge through additional resources, including AI chatbots such as ChatGPT and Google Bard. This study aimed to evaluate the performance of LLMs in providing education on PCa. Methods: Common patient questions about PCa were collected from reliable educational websites and evaluated for accuracy, comprehensiveness, readability, and stability by two independent board-certified urologists, with a third resolving discrepancy. Accuracy was measured on a 3-point scale, comprehensiveness was measured on a 5-point Likert scale, and readability was measured using the Flesch Reading Ease (FRE) score and Flesch–Kincaid FK Grade Level. Results: A total of 52 questions on general knowledge, diagnosis, treatment, and prevention of PCa were provided to three LLMs. Although there was no significant difference in the overall accuracy of LLMs, ChatGPT-3.5 demonstrated superiority over the other LLMs in terms of general knowledge of PCa (p = 0.018). ChatGPT-4 achieved greater overall comprehensiveness than ChatGPT-3.5 and Bard (p = 0.028). For readability, Bard generated simpler sentences with the highest FRE score (54.7, p < 0.001) and lowest FK reading level (10.2, p < 0.001). Conclusion: ChatGPT-3.5, ChatGPT-4 and Bard generate accurate, comprehensive, and easily readable PCa material. These AI models might not replace healthcare professionals but can assist in patient education and guidance. © The Author(s) 2024.",III,Robert,,
Exploring the Role of a Large Language Model on Carpal Tunnel Syndrome Management: An Observation Study of ChatGPT,"Purpose: Recently, large language models, such as ChatGPT, have emerged as promising tools to facilitate scientific research and health care management. The present study aimed to explore the extent of knowledge possessed by ChatGPT concerning carpal tunnel syndrome (CTS), a compressive neuropathy that may lead to impaired hand function and that is frequently encountered in the field of hand surgery. Methods: Six questions pertaining to diagnosis and management of CTS were posed to ChatGPT. The responses were subsequently analyzed and evaluated based on their accuracy, coherence, and comprehensiveness. In addition, ChatGPT was requested to provide five high-level evidence references in support of its answers. A simulated doctor-patient consultation was also conducted to assess whether ChatGPT could offer safe medical advice. Results: ChatGPT supplied clinically relevant information regarding CTS, although at a relatively superficial level. In the context of doctor-patient interaction, ChatGPT suggested a diagnostic pathway that deviated from the widely accepted clinical consensus on CTS diagnosis. Nevertheless, it incorporated differential diagnoses and valuable management options for CTS. Although ChatGPT demonstrated the ability to retain and recall information from previous patient conversations, it infrequently produced pertinent references, many of which were either nonexistent or incorrect. Conclusions: ChatGPT displayed the capability to deliver validated medical information on CTS to nonmedical individuals. However, the generation of nonexistent and inaccurate references by ChatGPT presents a challenge to academic integrity. Clinical relevance: To increase their utility in medicine and academia, large language models must go through specialized reputable data set training and validation from experts. It is essential to note that at present, large language models cannot replace the expertise of health care professionals and may act as a supportive tool. © 2023",II,Robert,,
Neura: A specialized large language model solution in neurology,"Large language models’ (LLM) ability in natural language processing holds promise for diverse applications, yet their deployment in fields such as neurology faces domain-specific challenges. Hence, we introduce Neura: a scalable, explainable solution to specialize LLM. Blindly evaluated on a select set of five complex clinical cases compared to a cohort of 13 neurologists, Neura achieved normalized scores of 86.17% overall, 85% for differential diagnoses, and 88.24% for final diagnoses (55.11%, 46.15%, and 70.93% for neurologists) with rapid response times of 28.8 and 19 seconds (9 minutes and 37.2 seconds and 8 minutes and 51 seconds for neurologists) while consistently providing relevant, accurately cited information. These findings support the emerging role of LLM-driven applications to articulate human-acquired and integrated data with a vast corpus of knowledge, augmenting human experiential reasoning for clinical and research purposes.",III,Robert,,
"Evaluating the accuracy and reliability of AI chatbots in patient education on cardiovascular imaging: a comparative study of ChatGPT, gemini, and copilot","Background: The integration of artificial intelligence (AI) chatbots in medicine is expanding rapidly, with notable models like ChatGPT by OpenAI, Gemini by Google, and Copilot by Microsoft. These chatbots are increasingly used to provide medical information, yet their reliability in specific areas such as cardiovascular imaging remains underexplored. This study aims to evaluate the accuracy and reliability of ChatGPT (versions 3.5 and 4), Gemini, and Copilot in responding to patient inquiries about cardiovascular imaging. Methods: We sourced 30 patient-oriented questions on cardiovascular imaging. The questions were submitted to ChatGPT-4, ChatGPT-3.5, Copilot Balanced Mode, Copilot Precise Mode, and Gemini. Responses were evaluated by two cardiovascular radiologists based on accuracy, clarity, completeness, neutrality, and appropriateness using a structured rubric. Inter-rater reliability was assessed using Cohen’s Kappa. Results: ChatGPT-4 achieved the highest performance with 78.3% accuracy, 86.87% clarity and appropriateness, 81.7% completeness, and 100% neutrality. Gemini showed balanced performance, while Copilot Balanced Mode excelled in clarity and accuracy but lagged in completeness. Copilot Precise Mode had the lowest scores in completeness and accuracy. Penalty assessments revealed that ChatGPT-4 had the lowest incidence of missing or misleading information. Conclusion: ChatGPT-4 emerged as the most reliable AI model for providing accurate, clear, and comprehensive patient information on cardiovascular imaging. While other models showed potential, they require further refinement. This study underscores the value of integrating AI chatbots into clinical practice to enhance patient education and engagement. © The Author(s) 2025.",III,Robert,,
A comparative evaluation of ChatGPT 3.5 and ChatGPT 4 in responses to selected genetics questions,"Objectives: To evaluate the efficacy of ChatGPT 4 (GPT-4) in delivering genetic information about BRCA1, HFE, and MLH1, building on previous findings with ChatGPT 3.5 (GPT-3.5). To focus on assessing the utility, limitations, and ethical implications of using ChatGPT in medical settings. Materials and Methods: A structured survey was developed to assess GPT-4's clinical value. An expert panel of genetic counselors and clinical geneticists evaluated GPT-4's responses to these questions. We also performed comparative analysis with GPT-3.5, utilizing descriptive statistics and using Prism 9 for data analysis. Results: The findings indicate improved accuracy in GPT-4 over GPT-3.5 (P <. 0001). However, notable errors in accuracy remained. The relevance of responses varied in GPT-4, but was generally favorable, with a mean in the ""somewhat agree""range. There was no difference in performance by disease category. The 7-question subset of the Bot Usability Scale (BUS-15) showed no statistically significant difference between the groups but trended lower in the GPT-4 version. Discussion and Conclusion: The study underscores GPT-4's potential role in genetic education, showing notable progress yet facing challenges like outdated information and the necessity of ongoing refinement. Our results, while showing promise, emphasizes the importance of balancing technological innovation with ethical responsibility in healthcare information delivery.  © 2024 The Author(s).",III,Robert,,
Is ChatGPT an Accurate and Readable Patient Aid for Third Molar Extractions?,"Background: Artificial intelligence (AI) platforms such as Chat Generative Pre-Trained Transformer (ChatGPT) (Open AI, San Francisco, California, USA) have the capacity to answer health-related questions. It remains unknown whether AI can be a patient-friendly and accurate resource regarding third molar extraction. Purpose: The purpose was to determine the accuracy and readability of AI responses to common patient questions regarding third molar extraction. Study Design, Setting, Sample: This is a cross sectional in-silico assessment of readability and soundness of a computer-generated report. Independent Variable: Not applicable. Main Outcome Variables: Accuracy, or the ability to provide clinically correct and relevant information, was determined subjectively by 2 reviewers using a 5-point Likert scale, and objectively by comparing responses to American Association of Oral and Maxillofacial Surgeons (AAOMS) clinical consensus papers. Readability, or how easy a piece of text is to read, was assessed using the Flesch Kincaid Reading Ease (FKRE) and Flesch Kincaid Grade Level (FKGL). Both assess readability based on mean number of syllables per word, and words per sentence. To be deemed readable, FKRE should be >60 and FKGL should be <8. Covariates: Not applicable. Analyses: Descriptive statistics were used to analyze the findings of this study. Results: AI-generated responses above the recommended level for the average patient (FKRE: 52; FKGL: 10). The average Likert score was 4.36, suggesting that most responses were accurate with minor inaccuracies or missing information. AI correctly deferred to the provider in instances where no definitive answer exists. Of the responses that addressed content in AAOMS consensus papers, 18/19 responses closely aligned with them. All prompts did not provide citations or references. Conclusion and Relevance: AI was able to provide mostly accurate responses, and content was closely aligned with AAOMS guidelines. However, responses were too complex for the average third molar extraction patient, and were deficient in citations and references. It is important for providers to educate patients on the utility of AI, and to decide whether to recommend using it for information. Ultimately, the best resource for answers is from the practitioners themselves because the AI platform lacks clinical experience. © 2024 The Authors",III,Robert,,
Bridging Gaps with Generative AI: Enhancing Hypertension Monitoring Through Patient and Provider Insights,"3427. Stud Health Technol Inform. 2024 Aug 22;316:939-943. doi: 10.3233/SHTI240565.

Bridging Gaps with Generative AI: Enhancing Hypertension Monitoring Through 
Patient and Provider Insights.

Andreadis K(1), Rodriguez DV(1), Zakreuskaya A(1)(2), Chen J(3), Gonzalez J(3), 
Mann D(1)(3).

Author information:
(1)New York University Grossman School of Medicine, New York, USA.
(2)Machine Learning and Data Analytics Lab, Univerity Erlangen-Nuremberg, 
Germany.
(3)MCIT Department of Health Informatics, New York University Langone Health, 
New York, USA.

This study introduces a Generative Artificial Intelligence (GenAI) assistant 
designed to address key challenges in Remote Patient Monitoring (RPM) for 
hypertension. After a comprehensive needs assessment from clinicians and 
patients, we identified pivotal issues in RPM data management and patient 
engagement. The GenAI RPM assistant integrates a patient-facing chatbot, 
clinician-facing smart summaries, and automated draft portal messages to enhance 
communication and streamline data review. Validated through six rounds of 
testing and evaluations by ten participants, the initial prototype was 
positively received, highlighting the importance of personalized interactions. 
Our findings demonstrate GenAI's potential to improve RPM by optimizing data 
management and enhancing patient-provider communication.",I,Robert,,
A cross-sectional study to evaluate responses generated by two AI software programs for common patient queries about laparoscopic repair of inguinal hernia,"This study aimed to evaluate the quality and accuracy of responses provided by two user-interactive AI chatbots, namely ChatGPT and ChatSonic, in response to patient queries regarding laparoscopic repair of inguinal hernias, and additionally determine the suitability of these chatbots in addressing patient queries related to inguinal hernia repair. Ten questions regarding laparoscopic repair of inguinal hernias were developed and presented to ChatGPT 4.0 and ChatSonic. Responses were evaluated by two experienced surgeons blinded to the source, using the Global Quality Score (GQS) and modified DISCERN Score to gauge response quality and reliability. ChatGPT demonstrated high-quality responses (GQS = 4 & 5) for all ten questions according to one evaluator, and for seven out of ten questions according to the other. Similarly, ChatGPT showed high reliability (DISCERN = 4 & 5) for nine responses according to one evaluator, and for three responses according to the other, with only slight agreement between evaluators for both GQS (kappa = 0.20) and modified DISCERN scores (kappa = 0.08). ChatSonic also provided high-quality and reliable responses for a majority of questions, albeit to a lesser extent than ChatGPT, and both demonstrating limited concordance in responses (p > 0.05). Overall, Both ChatGPT and ChatSonic demonstrated potential utility in providing responses to patient queries about hernia surgery. However, due to inconsistencies in reliability and quality, ongoing refinement and validation of AI generated medical information remain necessary before widespread clinical adoption. © Italian Society of Surgery (SIC) 2025.",III,Robert,,
Assessing the performance of zero-shot visual question answering in multimodal large language models for 12-lead ECG image interpretation,"Large Language Models (LLM) are increasingly multimodal, and Zero-Shot Visual Question Answering (VQA) shows promise for image interpretation. If zero-shot VQA can be applied to a 12-lead electrocardiogram (ECG), a prevalent diagnostic tool in the medical field, the potential benefits to the field would be substantial. This study evaluated the diagnostic performance of zero-shot VQA with multimodal LLMs on 12-lead ECG images. The results revealed that multimodal LLM tended to make more errors in extracting and verbalizing image features than in describing preconditions and making logical inferences. Even when the answers were correct, erroneous descriptions of image features were common. These findings suggest a need for improved control over image hallucination and indicate that performance evaluation using the percentage of correct answers to multiple-choice questions may not be sufficient for performance assessment in VQA tasks. 2025 Seki, Kawazoe, Ito, Akagi, Takiguchi and Ohe.",III,Robert,,
A Comparison of ChatGPT and Human Questionnaire Evaluations of the Urological Cancer Videos Most Watched on YouTube,"Aim: To examine the reliability of ChatGPT in evaluating the quality of medical content of the most watched videos related to urological cancers on YouTube. Material and methods: In March 2024 a playlist was created of the first 20 videos watched on YouTube for each type of urological cancer. The video texts were evaluated by ChatGPT and by a urology specialist using the DISCERN-5 and Global Quality Scale (GQS) questionnaires. The results obtained were compared using the Kruskal-Wallis test. Results: For the prostate, bladder, renal, and testicular cancer videos, the median (IQR) DISCERN-5 scores given by the human evaluator and ChatGPT were (Human: 4 [1], 3 [0], 3 [2], 3 [1], P = .11; ChatGPT: 3 [1.75], 3 [1], 3 [2], 3 [0], P = .4, respectively) and the GQS scores were (Human: 4 [1.75], 3 [0.75], 3.5 [2], 3.5 [1], P = .12; ChatGPT: 4 [1], 3 [0.75], 3 [1], 3.5 [1], P = .1, respectively), with no significant difference determined between the scores. The repeatability of the ChatGPT responses was determined to be similar at 25 % for prostate cancer, 30 % for bladder cancer, 30 % for renal cancer, and 35 % for testicular cancer (P = .92). No statistically significant difference was determined between the median (IQR) DISCERN-5 and GQS scores given by humans and ChatGPT for the content of videos about prostate, bladder, renal, and testicular cancer (P > .05). Conclusion: Although ChatGPT is successful in evaluating the medical quality of video texts, the results should be evaluated with caution as the repeatability of the results is low. © 2024 Elsevier Inc.",III,Robert,,
ChatGPT M.D.: Is there any room for generative AI in neurology?,"ChatGPT, a general artificial intelligence, has been recognized as a powerful tool in scientific writing and programming but its use as a medical tool is largely overlooked. The general accessibility, rapid response time and comprehensive training database might enable ChatGPT to serve as a diagnostic augmentation tool in certain clinical settings. The diagnostic process in neurology is often challenging and complex. In certain time-sensitive scenarios, rapid evaluation and diagnostic decisions are needed, while in other cases clinicians are faced with rare disorders and atypical disease manifestations. Due to these factors, the diagnostic accuracy in neurology is often suboptimal. Here we evaluated whether ChatGPT can be utilized as a valuable and innovative diagnostic augmentation tool in various neurological settings. We used synthetic data generated by neurological experts to represent descriptive anamneses of patients with known neurology-related diseases, then the probability for an appropriate diagnosis made by ChatGPT was measured. To give clarity to the accuracy of the AI-determined diagnosis, all cases have been cross-validated by other experts and general medical doctors as well. We found that ChatGPT-determined diagnostic accuracy (ranging from 68.5% ± 3.28% to 83.83% ± 2.73%) can reach the accuracy of other experts (81.66% ± 2.02%), furthermore, it surpasses the probability of an appropriate diagnosis if the examiner is a general medical doctor (57.15% ± 2.64%). Our results showcase the efficacy of general artificial intelligence like ChatGPT as a diagnostic augmentation tool in medicine. In the future, AI-based supporting tools might be useful amendments in medical practice and help to improve the diagnostic process in neurology.",III,Robert,,
The RepVig framework for designing use-case specific representative vignettes and evaluating triage accuracy of laypeople and symptom assessment applications,"Most studies evaluating symptom-assessment applications (SAAs) rely on a common set of case vignettes that are authored by clinicians and devoid of context, which may be representative of clinical settings but not of situations where patients use SAAs. Assuming the use case of self-triage, we used representative design principles to sample case vignettes from online platforms where patients describe their symptoms to obtain professional advice and compared triage performance of laypeople, SAAs (e.g., WebMD or NHS 111), and Large Language Models (LLMs, e.g., GPT-4 or Claude) on representative versus standard vignettes. We found performance differences in all three groups depending on vignette type: When using representative vignettes, accuracy was higher (OR = 1.52 to 2.00, p < .001 to .03 in binary decisions, i.e., correct or incorrect), safety was higher (OR = 1.81 to 3.41, p < .001 to .002 in binary decisions, i.e., safe or unsafe), and the inclination to overtriage was also higher (OR = 1.80 to 2.66, p < .001 to p = .035 in binary decisions, overtriage or undertriage error). Additionally, we found changed rankings of best-performing SAAs and LLMs. Based on these results, we argue that our representative vignette sampling approach (that we call the RepVig Framework) should replace the practice of using a fixed vignette set as standard for SAA evaluation studies.",III,Robert,,
Comparing Large Language Model and Human Reader Accuracy with New England Journal of Medicine Image Challenge Case Image Inputs,"Background: Application of multimodal large language models (LLMs) with both textual and visual capabilities has been steadily increasing, but their ability to interpret radiologic images is still doubted. Purpose: To evaluate the accuracy of LLMs and compare it with that of human readers with varying levels of experience and to assess the factors affecting LLM accuracy in answering New England Journal of Medicine Image Challenge cases. Materials and Methods: Radiologic images of cases from October 13, 2005, to April 18, 2024, were retrospectively reviewed. Using text and image inputs, LLMs (Open AI’s GPT-4 Turbo with Vision [GPT-4V] and GPT-4 Omni [GPT-4o], Google’s DeepMind Gemini 1.5 Pro, and Anthropic’s Claude 3) provided answers. Human readers (seven junior faculty radiologists, two clinicians, one in-training radiologist, and one medical student), blinded to the published answers, also answered. LLM accuracy with and without image inputs and short (cases from 2005 to 2015) versus long text inputs (from 2016 to 2024) was evaluated in subgroup analysis to determine the effect of these factors. Factor analysis was assessed using multivariable logistic regression. Accuracy was compared with generalized estimating equations, with multiple comparisons adjusted by using Bonferroni correction. Results: A total of 272 cases were included. GPT-4o achieved the highest overall accuracy among LLMs (59.6%; 162 of 272), outperforming a medical student (47.1%; 128 of 272; P <.001) but not junior faculty (80.9%; 220 of 272; P <.001) or the in-training radiologist (70.2%; 191 of 272; P =.003). GPT-4o exhibited similar accuracy regardless of image inputs (without images vs with images, 54.0% [147 of 272] vs 59.6% [162 of 272], respectively; P =.59). Human reader accuracy was unaffected by text length, whereas LLMs demonstrated higher accuracy with long text inputs (all P <.001). Text input length affected LLM accuracy (odds ratio range, 3.2 [95% CI: 1.9, 5.5] to 6.6 [95% CI: 3.7, 12.0]). Conclusion: LLMs demonstrated substantial accuracy with text and image inputs, outperforming a medical student. However, their accuracy decreased with shorter text lengths, regardless of image input. © RSNA, 2024.",III,Robert,,
ChatGPT-4 Generates More Accurate and Complete Responses to Common Patient Questions About Anterior Cruciate Ligament Reconstruction Than Google's Search Engine,"Purpose: To replicate a patient's internet search to evaluate ChatGPT's appropriateness in answering common patient questions about anterior cruciate ligament reconstruction compared with a Google web search. Methods: A Google web search was performed by searching the term “anterior cruciate ligament reconstruction.” The top 20 frequently asked questions and responses were recorded. The prompt “What are the 20 most popular patient questions related to ‘anterior cruciate ligament reconstruction?’” was input into ChatGPT and questions and responses were recorded. Questions were classified based on the Rothwell system and responses assessed via Flesch-Kincaid Grade Level, correctness, and completeness were for both Google web search and ChatGPT. Results: Three of 20 (15%) questions were similar between Google web search and ChatGPT. The most common question types among the Google web search were value (8/20, 40%), fact (7/20, 35%), and policy (5/20, 25%). The most common question types amongst the ChatGPT search were fact (12/20, 60%), policy (6/20, 30%), and value (2/20, 10%). Mean Flesch-Kincaid Grade Level for Google web search responses was significantly lower (11.8 ± 3.8 vs 14.3 ± 2.2; P = .003) than for ChatGPT responses. The mean correctness for Google web search question answers was 1.47 ± 0.5, and mean completeness was 1.36 ± 0.5. Mean correctness for ChatGPT answers was 1.8 ± 0.4 and mean completeness was 1.9 ± 0.3, which were both significantly greater than Google web search answers (P = .03 and P = .0003). Conclusions: ChatGPT-4 generated more accurate and complete responses to common patient questions about anterior cruciate ligament reconstruction than Google's search engine. Clinical Relevance: The use of artificial intelligence such as ChatGPT is expanding. It is important to understand the quality of information as well as how the results of ChatGPT queries compare with those from Google web searches © 2024 The Authors",III,Robert,,
Comparative Performance of Anthropic Claude and OpenAI GPT Models in Basic Radiological Imaging Tasks,"Background: Publicly available artificial intelligence (AI) Vision Language Models (VLMs) are constantly improving. The advent of vision capabilities on these models could enhance radiology workflows. Evaluating their performance in radiological image interpretation is vital to their potential integration into practice. Aim: This study aims to evaluate the proficiency and consistency of the publicly available VLMs, Anthropic's Claude and OpenAI's GPT, across multiple iterations in basic image interpretation tasks. Method: Subsets from publicly available datasets, ROCOv2 and MURAv1.1, were used to evaluate 6 VLMs. A system prompt and image were input into each model three times. The outputs were compared to the dataset captions to evaluate each model's accuracy in recognising the modality, anatomy, and detecting fractures on radiographs. The consistency of the output across iterations was also analysed. Results: Evaluation of the ROCOv2 dataset showed high accuracy in modality recognition, with some models achieving 100%. Anatomical recognition ranged between 61% and 85% accuracy across all models tested. On the MURAv1.1 dataset, Claude-3.5-Sonnet had the highest anatomical recognition with 57% accuracy, while GPT-4o had the best fracture detection with 62% accuracy. Claude-3.5-Sonnet was the most consistent model, with 83% and 92% consistency in anatomy and fracture detection, respectively. Conclusion: Given Claude and GPT's current accuracy and reliability, the integration of these models into clinical settings is not yet feasible. This study highlights the need for ongoing development and establishment of standardised testing techniques to ensure these models achieve reliable performance. © 2025 Royal Australian and New Zealand College of Radiologists.",III,Robert,,
"GPT-4 generated psychological reports in psychodynamic perspective: a pilot study on quality, risk of hallucination and client satisfaction","Background: Recently, there have been active proposals on how to utilize large language models (LLMs) in the fields of psychiatry and counseling. It would be interesting to develop programs with LLMs that generate psychodynamic assessments to help individuals gain insights about themselves, and to evaluate the features of such services. However, studies on this subject are rare. This pilot study aims to evaluate quality, risk of hallucination (incorrect AI-generated information), and client satisfaction with psychodynamic psychological reports generated by GPT-4. Methods: The report comprised five components: psychodynamic formulation, psychopathology, parental influence, defense mechanisms, and client strengths. Participants were recruited from individuals distressed by repetitive interpersonal issues. The study was conducted in three steps: 1) Questions provided to participants, designed to create psychodynamic formulations: 14 questions were generated by GPT for inferring psychodynamic formulations, while 6 fixed questions focused on the participants’ relationship with their parents. A total of 20 questions were provided. Using participants’ responses to these questions, GPT-4 generated the psychological reports. 2) Seven professors of psychiatry from different university hospitals evaluated the quality and risk of hallucinations in the psychological reports by reading the reports only, without meeting the participants. This quality assessment compared the psychological reports generated by GPT-4 with those inferred by the experts. 3) Participants evaluated their satisfaction with the psychological reports. All assessments were conducted using self-report questionnaires based on a Likert scale developed for this study. Results: A total of 10 participants were recruited, and the average age was 32 years. The median response indicated that quality of all five components of the psychological report was similar to the level inferred by the experts. The risk of hallucination was assessed as ranging from unlikely to minor. According to the median response in the satisfaction evaluation, the participants agreed that the report is clearly understandable, insightful, credible, useful, satisfying, and recommendable. Conclusion: This study suggests the possibility that artificial intelligence could assist users by providing psychodynamic interpretations. Copyright © 2025 Kim, Lee, Park, On, Lee, Keum, Oh, Song, Lee, Won, Shin, Lho, Hwang and Kim.",III,Robert,,
Information Quality and Readability: ChatGPT's Responses to the Most Common Questions About Spinal Cord Injury,"Objective: This study aimed to assess the quality, readability, and comprehension of texts generated by ChatGPT in response to commonly asked questions about spinal cord injury (SCI). Methods: The study utilized Google Trends to identify the most frequently searched keywords related to SCI. The identified keywords were sequentially inputted into ChatGPT, and the resulting responses were assessed for quality using the Ensuring Quality Information for Patients (EQIP) tool. The readability of the texts was analyzed using the Flesch-Kincaid grade level and the Flesch-Kincaid reading ease parameters. Results: The mean EQIP score of the texts was determined to be 43.02 ± 6.37, the Flesch-Kincaid reading ease score to be 26.24 ± 13.81, and the Flesch-Kincaid grade level was determined to be 14.84 ± 1.79. The analysis revealed significant concerns regarding the quality of texts generated by ChatGPT, indicating serious problems with readability and comprehension. The mean EQIP score was low, suggesting a need for improvement in the accuracy and reliability of the information provided. The Flesch-Kincaid grade level indicated a high linguistic complexity, requiring a level of education equivalent to approximately 14 to 15 years of formal education for comprehension. Conclusions: The results of this study show heightened complexity in ChatGPT-generated SCI texts, surpassing optimal health communication readability. ChatGPT currently cannot substitute comprehensive medical consultations. Enhancing text quality could be attainable through dependence on credible sources, the establishment of a scientific board, and collaboration with expert teams. Addressing these concerns could improve text accessibility, empowering patients and facilitating informed decision-making in SCI. © 2023 Elsevier Inc.",III,Robert,,
Assessing the performance of ChatGPT's responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval,"Background: Epilepsy is a neurological condition marked by frequent seizures and various cognitive and psychological effects. Reliable information is essential for effective treatment. Natural language processing models like ChatGPT are increasingly used in healthcare for information access and data analysis, making it crucial to assess their accuracy. Objective: This study aimed to investigate the accuracy of ChatGPT in providing educational information related to epilepsy. Methods: We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common epilepsy questions based on the Korean Epilepsy Society's ""Epilepsy Patient and Caregiver Guide."" Two epileptologists reviewed the responses, with a third serving as an arbiter in cases of disagreement. Results: Out of 57 questions, 40 responses from ChatGPT-4 had ""sufficient educational value,"" 16 were ""correct but inadequate,"" and one was ""mixed with correct and incorrect"" information. No answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was often on par with or better than the official guide. Conclusions: ChatGPT-4 shows promise as a tool for delivering reliable epilepsy-related information and could help alleviate the educational burden on healthcare professionals. Further research is needed to explore the benefits and limitations of using such models in medical contexts. © 2023",III,Robert,,
Evaluating base and retrieval augmented LLMs with document or online support for evidence based neurology,"Effectively managing evidence-based information is increasingly challenging. This study tested large language models (LLMs), including document- and online-enabled retrieval-augmented generation (RAG) systems, using 13 recent neurology guidelines across 130 questions. Results showed substantial variability. RAG improved accuracy compared to base models but still produced potentially harmful answers. RAG-based systems performed worse on case-based than knowledge-based questions. Further refinement and improved regulation is needed for safe clinical integration of RAG-enhanced LLMs. © The Author(s) 2025.",III,Robert,,
"Assessing the readability, quality and reliability of responses produced by ChatGPT, Gemini, and Perplexity regarding most frequently asked keywords about low back pain","Background: Patients who are informed about the causes, pathophysiology, treatment and prevention of a disease are better able to participate in treatment procedures in the event of illness. Artificial intelligence (AI), which has gained popularity in recent years, is defined as the study of algorithms that provide machines with the ability to reason and perform cognitive functions, including object and word recognition, problem solving and decision making. This study aimed to examine the readability, reliability and quality of responses to frequently asked keywords about low back pain (LBP) given by three different AI-based chatbots (ChatGPT, Perplexity and Gemini), which are popular applications in online information presentation today. Methods: All three AI chatbots were asked the 25 most frequently used keywords related to LBP determined with the help of Google Trend. In order to prevent possible bias that could be created by the sequential processing of keywords in the answers given by the chatbots, the study was designed by providing input from different users (EO, VH) for each keyword. The readability of the responses given was determined with the Simple Measure of Gobbledygook (SMOG), Flesch Reading Ease Score (FRES) and Gunning Fog (GFG) readability scores. Quality was assessed using the Global Quality Score (GQS) and the Ensuring Quality Information for Patients (EQIP) score. Reliability was assessed by determining with DISCERN and Journal of American Medical Association (JAMA) scales. Results: The first three keywords detected as a result of Google Trend search were “Lower Back Pain”, “ICD 10 Low Back Pain”, and “Low Back Pain Symptoms”. It was determined that the readability of the responses given by all AI chatbots was higher than the recommended 6th grade readability level (p < 0.001). In the EQIP, JAMA, modified DISCERN and GQS score evaluation, Perplexity was found to have significantly higher scores than other chatbots (p < 0.001). Conclusion: It has been determined that the answers given by AI chatbots to keywords about LBP are difficult to read and have low reliability and quality assessment. It is clear that when new chatbots are introduced, they can provide better guidance to patients with increased clarity and text quality. This study can provide inspiration for future studies on improving the algorithms and responses of AI chatbots.",III,Robert,,
Let's chat about cervical cancer: Assessing the accuracy of ChatGPT responses to cervical cancer questions,"Objective: To quantify the accuracy of ChatGPT in answering commonly asked questions pertaining to cervical cancer prevention, diagnosis, treatment, and survivorship/quality-of-life (QOL). Methods: ChatGPT was queried with 64 questions adapted from professional society websites and the authors' clinical experiences. The answers were scored by two attending Gynecologic Oncologists according to the following scale: 1) correct and comprehensive, 2) correct but not comprehensive, 3) some correct, some incorrect, and 4) completely incorrect. Scoring discrepancies were resolved by additional reviewers as needed. The proportion of responses earning each score were calculated overall and within each question category. Results: ChatGPT provided correct and comprehensive answers to 34 (53.1%) questions, correct but not comprehensive answers to 19 (29.7%) questions, partially incorrect answers to 10 (15.6%) questions, and completely incorrect answers to 1 (1.6%) question. Prevention and survivorship/QOL had the highest proportion of “correct” scores (scores of 1 or 2) at 22/24 (91.7%) and 15/16 (93.8%), respectively. ChatGPT performed less well in the treatment category, with 15/21 (71.4%) correct scores. It performed the worst in the diagnosis category with only 1/3 (33.3%) correct scores. Conclusion: ChatGPT accurately answers questions about cervical cancer prevention, survivorship, and QOL. It performs less accurately for cervical cancer diagnosis and treatment. Further development of this immensely popular large language model should include physician input before it can be utilized as a tool for Gynecologists or recommended as a patient resource for information on cervical cancer diagnosis and treatment. © 2023 Elsevier Inc.",III,Robert,,
Perspectives on AI-based recommendations for mask-wearing and COVID-19 vaccination for transplant recipients in the post-COVID-19 era,"3558. Ren Fail. 2024 Dec;46(1):2337291. doi: 10.1080/0886022X.2024.2337291. Epub 2024 
Apr 7.

Perspectives on AI-based recommendations for mask-wearing and COVID-19 
vaccination for transplant recipients in the post-COVID-19 era.

Garcia Valencia OA(1), Thongprayoon C(1), Miao J(1), Bruminhent J(2)(3), Craici 
IM(1), Cheungpasitporn W(1).

Author information:
(1)Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
Rochester, MN, USA.
(2)Department of Medicine, Division of Infectious Diseases, Faculty of Medicine 
Ramathibodi Hospital, Mahidol University, Bangkok, Thailand.
(3)Ramathibodi Excellence Center for Organ Transplantation, Faculty of Medicine 
Ramathibodi Hospital, Mahidol University, Bangkok, Thailand.

In the aftermath of the COVID-19 pandemic, the ongoing necessity for preventive 
measures such as mask-wearing and vaccination remains particularly critical for 
organ transplant recipients, a group highly susceptible to infections due to 
immunosuppressive therapy. Given that many individuals nowadays increasingly 
utilize Artificial Intelligence (AI), understanding AI perspectives is 
important. Thus, this study utilizes AI, specifically ChatGPT 4.0, to assess its 
perspectives in offering precise health recommendations for mask-wearing and 
COVID-19 vaccination tailored to this vulnerable population. Through a series of 
scenarios reflecting diverse environmental settings and health statuses in 
December 2023, we evaluated the AI's responses to gauge its precision, 
adaptability, and potential biases in advising high-risk patient groups. Our 
findings reveal that ChatGPT 4.0 consistently recommends mask-wearing in crowded 
and indoor environments for transplant recipients, underscoring their elevated 
risk. In contrast, for settings with fewer transmission risks, such as outdoor 
areas where social distancing is possible, the AI suggests that mask-wearing 
might be less imperative. Regarding vaccination guidance, the AI strongly 
advocates for the COVID-19 vaccine across most scenarios for kidney transplant 
recipients. However, it recommends a personalized consultation with healthcare 
providers in cases where patients express concerns about vaccine-related side 
effects, demonstrating an ability to adapt recommendations based on individual 
health considerations. While this study provides valuable insights into the 
current AI perspective on these important topics, it is crucial to note that the 
findings do not directly reflect or influence health policy. Nevertheless, given 
the increasing utilization of AI in various domains, understanding AI's 
viewpoints on such critical matters is essential for informed decision-making 
and future research.",III,Robert,,
ICGA-GPT: Report generation and question answering for indocyanine green angiography images,"Background Indocyanine green angiography (ICGA) is vital for diagnosing chorioretinal diseases, but its interpretation and patient communication require extensive expertise and time-consuming efforts. We aim to develop a bilingual ICGA report generation and question-answering (QA) system. Methods Our dataset comprised 213 129 ICGA images from 2919 participants. The system comprised two stages: image-text alignment for report generation by a multimodal transformer architecture, and large language model (LLM)-based QA with ICGA text reports and human-input questions. Performance was assessed using both qualitative metrics (including Bilingual Evaluation Understudy (BLEU), Consensus-based Image Description Evaluation (CIDEr), Recall-Oriented Understudy for Gisting Evaluation-Longest Common Subsequence (ROUGE-L), Semantic Propositional Image Caption Evaluation (SPICE), accuracy, sensitivity, specificity, precision and F1 score) and subjective evaluation by three experienced ophthalmologists using 5-point scales (5 refers to high quality). Results We produced 8757 ICGA reports covering 39 disease-related conditions after bilingual translation (66.7% English, 33.3% Chinese). The ICGA-GPT model's report generation performance was evaluated with BLEU scores (1-4) of 0.48, 0.44, 0.40 and 0.37; CIDEr of 0.82; ROUGE of 0.41 and SPICE of 0.18. For disease-based metrics, the average specificity, accuracy, precision, sensitivity and F1 score were 0.98, 0.94, 0.70, 0.68 and 0.64, respectively. Assessing the quality of 50 images (100 reports), three ophthalmologists achieved substantial agreement (kappa=0.723 for completeness, kappa=0.738 for accuracy), yielding scores from 3.20 to 3.55. In an interactive QA scenario involving 100 generated answers, the ophthalmologists provided scores of 4.24, 4.22 and 4.10, displaying good consistency (kappa=0.779). Conclusion This pioneering study introduces the ICGA-GPT model for report generation and interactive QA for the first time, underscoring the potential of LLMs in assisting with automated ICGA image interpretation.  © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and permissions. Published by BMJ.",III,Robert,,
Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks,"Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application. © 2024 IEEE.",III,Robert,,
PAINe: An Artificial Intelligence-based Virtual Assistant to Aid in the Differentiation of Pain of Odontogenic versus Temporomandibular Origin,"INTRODUCTION: Pain associated with temporomandibular dysfunction (TMD) is often 
confused with odontogenic pain, which is a challenge in endodontic diagnosis. 
Validated screening questionnaires can aid in the identification and 
differentiation of the source of pain. Therefore, this study aimed to develop a 
virtual assistant based on artificial intelligence using natural language 
processing techniques to automate the initial screening of patients with tooth 
pain.
METHODS: The PAINe chatbot was developed in Python (Python Software Foundation, 
Beaverton, OR) language using the PyCharm (JetBrains, Prague, Czech Republic) 
environment and the openai library to integrate the ChatGPT 4 API (OpenAI, San 
Francisco, CA) and the Streamlit library (Snowflake Inc, San Francisco, CA) for 
interface construction. The validated TMD Pain Screener questionnaire and 1 
question regarding the current pain intensity were integrated into the chatbot 
to perform the differential diagnosis of TMD in patients with tooth pain. The 
accuracy of the responses was evaluated in 50 random scenarios to compare the 
chatbot with the validated questionnaire. The kappa coefficient was calculated 
to assess the agreement level between the chatbot responses and the validated 
questionnaire.
RESULTS: The chatbot achieved an accuracy rate of 86% and a substantial level of 
agreement (κ = 0.70). Most responses were clear and provided adequate 
information about the diagnosis.
CONCLUSIONS: The implementation of a virtual assistant using natural language 
processing based on large language models for initial differential diagnosis 
screening of patients with tooth pain demonstrated substantial agreement between 
validated questionnaires and the chatbot. This approach emerges as a practical 
and efficient option for screening these patients.

Copyright © 2024 American Association of Endodontists. Published by Elsevier 
Inc. All rights reserved.",III,Robert,,
GPT-4 accuracy and completeness against International Consensus Statement on Allergy and Rhinology: Rhinosinusitis,"19. Int Forum Allergy Rhinol. 2023 Dec;13(12):2231-2234. doi: 10.1002/alr.23201. 
Epub 2023 Jun 16.

GPT-4 accuracy and completeness against International Consensus Statement on 
Allergy and Rhinology: Rhinosinusitis.

Yoshiyasu Y(1), Wu F(2), Dhanda AK(2), Gorelik D(2), Takashima M(2), Ahmed 
OG(2).

Author information:
(1)Department of Otolaryngology-Head and Neck Surgery, University of Texas 
Medical Branch, Galveston, Texas, USA.
(2)Department of Otolaryngology-Head and Neck Surgery, Houston Methodist 
Hospital, Houston, Texas, USA.

GPT-4 is an AI language model that can answer basic questions about rhinologic 
disease. Vetting is needed before AI models can be safely integrated into 
otolarygologic patient care.

© 2023 ARS-AAOA, LLC.",III,Robert,,
TCM-GPT: Efficient pre-training of large language models for domain adaptation in Traditional Chinese Medicine,"Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retrieving from general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained model's weights and uses rank decomposition matrices to efficiently train specific dense layers for pre-training and fine-tuning, efficiently aligning the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted extensive experiments on two TCM tasks, including TCM examination and TCM diagnosis. TCM-GPT-7B archived the best performance across both datasets, outperforming other models by relative increments of 17% and 12% in accuracy, respectively. To the best of our knowledge, our study represents the pioneering validation of domain adaptation of a large language model with 7 billion parameters in TCM domain. We will release both TCM-Corpus-1B and TCM-GPT-7B model once accepted to facilitate interdisciplinary development in TCM and NLP, serving as the foundation for further study. © 2024 The Authors",III,Robert,,
"MedFrenchmark, a Small Set for Benchmarking Generative LLMs in Medical French","Generative Large Language Models (LLMs) have become ubiquitous in various fields, including healthcare and medicine. Consequently, there is growing interest in leveraging LLMs for medical applications, leading to the emergence of novel models daily. However, evaluation and benchmarking frameworks for LLMs are scarce, particularly those tailored for medical French. To address this gap, we introduce a minimal benchmark consisting of 114 open questions designed to assess the medical capabilities of LLMs in French. The proposed benchmark encompasses a wide range of medical domains, reflecting real-world clinical scenarios' complexity. A preliminary validation involved testing seven widely used LLMs with a parameter size of 7 billion. Results revealed significant variability in performance, emphasizing the importance of rigorous evaluation before deploying LLMs in medical settings. In conclusion, we present a novel and valuable resource for rapidly evaluating LLMs in medical French. By promoting greater accountability and standardization, this benchmark has the potential to enhance trustworthiness and utility in harnessing LLMs for medical applications. © 2024 The Authors.",III,Robert,,
Toward expert-level medical question answering with large language models,"8.

Toward expert-level medical question answering with large language models.

Singhal K(#)(1), Tu T(#)(1), Gottweis J(#)(1), Sayres R(#)(1), Wulczyn E(1), 
Amin M(1), Hou L(1), Clark K(2), Pfohl SR(1), Cole-Lewis H(1), Neal D(1), Rashid 
QM(1), Schaekermann M(1), Wang A(1), Dash D(3), Chen JH(4)(5)(6), Shah NH(7)(8), 
Lachgar S(1), Mansfield PA(1), Prakash S(1), Green B(1), Dominowska E(2), Agüera 
Y Arcas B(1), Tomašev N(2), Liu Y(1), Wong R(1), Semturs C(1), Mahdavi SS(2), 
Barral JK(2), Webster DR(1), Corrado GS(1), Matias Y(1), Azizi S(9), 
Karthikesalingam A(10), Natarajan V(11).

Author information:
(1)Google Research, Mountain View, CA, USA.
(2)Google DeepMind, Mountain View, CA, USA.
(3)Department of Emergency Medicine, Stanford University School of Medicine, 
Stanford, CA, USA.
(4)Stanford Center for Biomedical Informatics Research, Stanford University, 
Stanford, CA, USA.
(5)Division of Hospital Medicine, Stanford University, Stanford, CA, USA.
(6)Clinical Excellence Research Center, Stanford University, Stanford, CA, USA.
(7)Department of Medicine, Stanford University School of Medicine, Stanford, CA, 
USA.
(8)Technology and Digital Solutions, Stanford Healthcare, Palo Alto, CA, USA.
(9)Google DeepMind, Mountain View, CA, USA. shekazizi@google.com.
(10)Google Research, Mountain View, CA, USA. alankarthi@google.com.
(11)Google Research, Mountain View, CA, USA. natviv@google.com.
(#)Contributed equally

Large language models (LLMs) have shown promise in medical question answering, 
with Med-PaLM being the first to exceed a 'passing' score in United States 
Medical Licensing Examination style questions. However, challenges remain in 
long-form medical question answering and handling real-world workflows. Here, we 
present Med-PaLM 2, which bridges these gaps with a combination of base LLM 
improvements, medical domain fine-tuning and new strategies for improving 
reasoning and grounding through ensemble refinement and chain of retrieval. 
Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by 
over 19%, and demonstrates dramatic performance increases across MedMCQA, 
PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations 
framework shows that physicians prefer Med-PaLM 2 answers to those from other 
physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates 
significant improvements over its predecessor across all evaluation metrics, 
particularly on new adversarial datasets designed to probe LLM limitations 
(P < 0.001). In a pilot study using real-world medical questions, specialists 
preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. 
While specialist answers were still preferred overall, both specialists and 
generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating 
its growing potential in real-world medical applications.

© 2025. The Author(s).",III,Robert,,
Efficacy and safety of artificial intelligence-based large language models for decision making support in herniology: evaluation by experts and general surgeons,"Objective. To evaluate the quality of recommendations provided by ChatGPT regarding inguinal hernia repair. Material and methods. ChatGPT was asked 5 questions about surgical management of inguinal hernias. The chat-bot was assigned the role of expert in herniology and requested to search only specialized medical databases and provide information about references and evidence. Herniology experts and surgeons (non-experts) rated the quality of recommendations generated by ChatGPT using 4-point scale (from 0 to 3 points). Statistical correlations were explored between participants' ratings and their stance regarding artificial intelligence. Results. Experts scored the quality of ChatGPT responses lower than non-experts (2 (1-2) vs. 2 (2-3), p<0.001). The chat-bot failed to provide valid references and actual evidence, as well as falsified half of references. Respondents were optimistic about the future of neural networks for clinical decision-making support. Most of them were against restricting their use in healthcare. Conclusion. We would not recommend non-specialized large language models as a single or primary source of information for clinical decision making or virtual searching assistant. © 2024 Media Sphera Publishing Group. All rights reserved.",III,Robert,,
Postoperative Otoplasty Care With ChatGPT-4: A Study on Artificial Intelligence (AI)-Assisted Patient Concern and Education,"Background: Otoplasty is a cosmetic surgery that is performed to alter the size, shape, or position of the ear by using permanent stitches. Its main purpose is to correct protruding ears, a condition known as prominauris. After the surgery, it is crucial to provide proper care to ensure successful recovery. However, obtaining timely medical advice can be difficult, especially in remote areas or places with limited resources. To address this issue, incorporating advanced artificial intelligence (AI) tools like Chat Generative Pre-trained Transformer (ChatGPT)-4 into postsurgical care could help fill the gap in patient education and support. Aim: This study aims to assess whether ChatGPT4 can be a reliable, accurate, and effective method for answering the most common patient questions and concerns post-otoplasty. The main objective was to assess the AI chatbot’s capacity to deliver precise, concise, and pertinent information, especially in situations where health care professionals are limited in availability. Materials and Methods: In this study, over 50 patients were engaged, and ChatGPT4 was employed to present the same 5 common postoperative questions post-otoplasty surgery care. The AI chatbot’s responses were analyzed for accuracy, response time, clarity, and relevance. Results: The chatbot could potentially provide timely assistance, answer questions, and address concerns related to postsurgical care in otoplasty. The responses exhibited a perfect accuracy rate of 100%, closely corresponding to existing medical guidelines. Conclusion: This study explores the potential of AI-driven solutions to enhance patient education and support, especially in areas where access to health care professionals may be limited. However, professional medical advice is crucial in postoperative care and cannot be replaced by ChatGPT-4. By leveraging AI tools like chatbots, individuals in remote or resource-limited settings can potentially receive valuable information and guidance, contributing to successful rehabilitation and overall health care outcomes. Ethical considerations around the use of AI in health care must also be carefully addressed to ensure patient privacy, data security, and appropriate clinical oversight. Copyright © 2024 by Mutaz B. Habal, MD.",II,Anton,I read the paper but could not identify how the 50 patients were exactly engaged as questions were synthetic and asssesed by medical profesionals. Hence do not consider prospective. ,
Retrieval Augmented Therapy Suggestion for Molecular Tumor Boards: Algorithmic Development and Validation Study,"Background: Molecular tumor boards (MTBs) require intensive manual investigation to generate optimal treatment recommendations for patients. Large language models (LLMs) can catalyze MTB recommendations, decrease human error, improve accessibility to care, and enhance the efficiency of precision oncology. Objective: In this study, we aimed to investigate the efficacy of LLM-generated treatments for MTB patients. We specifically investigate the LLMs’ ability to generate evidence-based treatment recommendations using PubMed references. Methods: We built a retrieval augmented generation pipeline using PubMed data. We prompted the resulting LLM to generate treatment recommendations with PubMed references using a test set of patients from an MTB conference at a large comprehensive cancer center at a tertiary care institution. Members of the MTB manually assessed the relevancy and correctness of the generated responses. Results: A total of 75% of the referenced articles were properly cited from PubMed, while 17% of the referenced articles were hallucinations, and the remaining were not properly cited from PubMed. Clinician-generated LLM queries achieved higher accuracy through clinician evaluation than automated queries, with clinicians labeling 25% of LLM responses as equal to their recommendations and 37.5% as alternative plausible treatments. Conclusions: This study demonstrates how retrieval augmented generation–enhanced LLMs can be a powerful tool in accelerating MTB conferences, as LLMs are sometimes capable of achieving clinician-equal treatment recommendations. However, further investigation is required to achieve stable results with zero hallucinations. LLMs signify a scalable solution to the time-intensive process of MTB investigations. However, LLM performance demonstrates that they must be used with heavy clinician supervision, and cannot yet fully automate the MTB pipeline. ©Eliza Berman, Holly Sundberg Malek, Michael Bitzer, Nisar Malek, Carsten Eickhoff.",I,Anton,This was a disagreement between a reviewer who ranked it as I and III.,
Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes,"Background: ChatGPT has emerged as the most advanced and rapidly developing large language chatbot system. With its immense potential ranging from answering a simple query to cracking highly competitive medical exams, ChatGPT continues to impress the scientists and researchers worldwide giving room for more discussions regarding its utility in various fields. One such field of attention is Psychiatry. With suboptimal diagnosis and treatment, assuring mental health and well-being is a challenge in many countries, particularly developing nations. To this regard, we conducted an evaluation to assess the performance of ChatGPT 3.5 in Psychiatry using clinical cases to provide evidence-based information regarding the implication of ChatGPT 3.5 in enhancing mental health and well-being. Methods: ChatGPT 3.5 was used in this experimental study to initiate the conversations and collect responses to clinical vignettes in Psychiatry. Using 100 clinical case vignettes, the replies were assessed by expert faculties from the Department of Psychiatry. There were 100 different psychiatric illnesses represented in the cases. We recorded and assessed the initial ChatGPT 3.5 responses. The evaluation was conducted using the objective of questions that were put forth at the conclusion of the case, and the aim of the questions was divided into 10 categories. The grading was completed by taking the mean value of the scores provided by the evaluators. Graphs and tables were used to represent the grades. Results: The evaluation report suggests that ChatGPT 3.5 fared extremely well in Psychiatry by receiving “Grade A” ratings in 61 out of 100 cases, “Grade B” ratings in 31, and “Grade C” ratings in 8. Majority of the queries were concerned with the management strategies, which were followed by diagnosis, differential diagnosis, assessment, investigation, counselling, clinical reasoning, ethical reasoning, prognosis, and request acceptance. ChatGPT 3.5 performed extremely well, especially in generating management strategies followed by diagnoses for different psychiatric conditions. There were no responses which were graded “D” indicating that there were no errors in the diagnosis or response for clinical care. Only a few discrepancies and additional details were missed in a few responses that received a “Grade C” Conclusion: It is evident from our study that ChatGPT 3.5 has appreciable knowledge and interpretation skills in Psychiatry. Thus, ChatGPT 3.5 undoubtedly has the potential to transform the field of Medicine and we emphasize its utility in Psychiatry through the finding of our study. However, for any AI model to be successful, assuring the reliability, validation of information, proper guidelines and implementation framework are necessary. © 2023 Elsevier B.V.",III,Anton,"I can see this being I, but clinical vignettes seem more like III in the same land as board examination questions,",
Uncovering Language Disparity of ChatGPT in Healthcare: Non-English Clinical Environment for Retinal Vascular Disease Classification,"Objective: To evaluate the effectiveness and reasoning ability of ChatGPT in diagnosing retinal vascular diseases in the Chinese clinical environment. Materials and Methods: We collected 1226 fundus fluorescein angiography reports and corresponding diagnosis written in Chinese, and tested ChatGPT with four prompting strategies (direct diagnosis or diagnosis with explanation and in Chinese or English). Results: ChatGPT using English prompt for direct diagnosis achieved the best performance, with F1-score of 80.05%, which was inferior to ophthalmologists (89.35%) but close to ophthalmologist interns (82.69%). Although ChatGPT can derive reasoning process with a low error rate, mistakes such as misinformation (1.96%), and hallucination (0.59%) still exist. Discussion and Conclusions: ChatGPT can serve as a helpful medical assistant to provide diagnosis under non-English clinical environments, but there are still performance gaps, language disparity, and errors compared to professionals, which demonstrates the potential limitations and the desiration to continually explore more robust LLMs in ophthalmology practice.",I,Anton,"This feels like a prospectively collected data for a diagnosis based on a report, so a clinical task? I will say I.",
Development of dialogue and voice-call robot system for personalized dementia prevention and care,"Purpose The looming challenge of dementia, exacerbated by demographic shifts and an aging global population, places immense strain on society and our healthcare infrastructure. It is desirable for societies around the world to conquer this problem by developing effective care technologies for dementia patients and caregivers, as well as providing strategies for dementia prevention. In this project, we propose a robot system that is the basis for generating a method for effective communication with dementia patients and caregivers regarding dementia based on the aggregated diagnostic and living data of dementia patients. Method The system aims to integrate diagnostic and daily living data and the use of a robot. The output is a dialogue that is effective for helping the user to care about their health status. The data analysis/processing methods are constructed in two steps. In the first step, a table of effective robot dialogues for each user's diagnostic and living data is created in advance. Using the METIS Platform (Stock, 2022), we are able to integrate biometric data from wearable devices within the Internet of Medical Things (IoMT) along with supplementary patient-reported information, data-driven capabilities. In this study, as a method for establishing the needed dialogue capabilities, a scenario-based dialogue (Kumagai et al., 2022) has been adapted. In a scenario-based dialogue, the robot's utterances are designed so that a conversation can be established, regardless of the content of the user's responses. To create the lookup table, an appropriate dialogue theme depending on the user's state is determined in advance. This table allows the robot to generate simple utterances that are tailored to the user's status. In the second step, the robot conversation content is generated based on the user's data (diagnosis data and biometric data) by Chat GPT. Even if two users’ states are the same, from the perspective of getting the user to accept the robot's voice, the appropriate contents of the robot's utterances are assumed to differ depending on the individual user. For example, it may be necessary to consider how to directly point out the user's status, or whether to indirectly recommend appropriate actions. In our previous research, we used neural networks and reinforcement learning techniques to develop systems that individualize the parameterized robot's behavior by adjusting the parameters to individual users (Kumagai et al., 2018). In this project, we will use a similar approach to generate personalized robot dialogues for each user, based on both generalized knowledge and individual data. Feedback on the user's affective reactions to the robot's speech is thought to help personalize the way of the robot's speech. Results and Discussion This presentation will show the results of a simple survey to examine the robot's speech generated by the system we have built and the impressions of users’ dialogues with the robot. We also discuss challenges in connecting to robots. © (2024), (International Society for Gerontechnology). All rights reserved.",II,Anton,,
Performance Assessment of Large Language Models in Medical Consultation: Comparative Study,"Background: The recent introduction of generative artificial intelligence (AI) as an interactive consultant has sparked interest in evaluating its applicability in medical discussions and consultations, particularly within the domain of depression. Objective: This study evaluates the capability of large language models (LLMs) in AI to generate responses to depression-related queries. Methods: Using the PubMedQA and QuoraQA data sets, we compared various LLMs, including BioGPT, PMC-LLaMA, GPT-3.5, and Llama2, and measured the similarity between the generated and original answers. Results: The latest general LLMs, GPT-3.5 and Llama2, exhibited superior performance, particularly in generating responses to medical inquiries from the PubMedQA data set. Conclusions: Considering the rapid advancements in LLM development in recent years, it is hypothesized that version upgrades of general LLMs offer greater potential for enhancing their ability to generate “knowledge text” in the biomedical domain compared with fine-tuning for the biomedical field. These findings are expected to contribute significantly to the evolution of AI-based medical counseling systems. © 2025 JMIR Publications Inc.. All rights reserved.",II,Anton,,
ChatGPT efficacy for answering musculoskeletal anatomy questions: a study evaluating quality and consistency between raters and timepoints,"Purpose: There is increasing interest in the use of digital platforms such as ChatGPT for anatomy education. This study aims to evaluate the efficacy of ChatGPT in providing accurate and consistent responses to questions focusing on musculoskeletal anatomy across various time points (hours and days). Methods: A selection of 6 Anatomy-related questions were asked to ChatGPT 3.5 in 4 different timepoints. All answers were rated blindly by 3 expert raters for quality according to a 5 -point Likert Scale. Difference of 0 or 1 points in Likert scale scores between raters was considered as agreement and between different timepoints was considered as consistent indicating good reproducibility. Results: There was significant variation in the quality of the answers ranging from extremely good to very poor quality. There was also variation of consistency levels between different timepoints. Answers were rated as good quality (≥ 3 in Likert scale) in 50% of cases (3/6) and as consistent in 66.6% (4/6) of cases. In the low-quality answers, significant mistakes, conflicting data or lack of information were encountered. Conclusion: As of the time of this article, the quality and consistency of the ChatGPT v3.5 answers is variable, thus limiting its utility as independent and reliable resource of learning musculoskeletal anatomy. Validating information by reviewing the anatomical literature is highly recommended. © The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature 2024.",III,Robert,,Anton cannot participate
A pilot study on the efficacy of GPT-4 in providing orthopedic treatment recommendations from MRI reports,"754. Sci Rep. 2023 Nov 17;13(1):20159. doi: 10.1038/s41598-023-47500-2.

A pilot study on the efficacy of GPT-4 in providing orthopedic treatment 
recommendations from MRI reports.

Truhn D(1), Weber CD(2), Braun BJ(3), Bressem K(4), Kather JN(5)(6)(7)(8), Kuhl 
C(1), Nebelung S(9).

Author information:
(1)Department of Diagnostic and Interventional Radiology, University Hospital 
RWTH Aachen, Pauwels Street 30, 52074, Aachen, Germany.
(2)Department of Orthopaedics and Trauma Surgery, University Hospital RWTH 
Aachen, Aachen, Germany.
(3)University Hospital Tuebingen on Behalf of the Eberhard-Karls-University 
Tuebingen, BG Hospital, Schnarrenbergstr. 95, Tübingen, Germany.
(4)Department of Radiology, Charité - Universitätsmedizin Berlin, corporate 
member of Freie Universität Berlin and Humboldt-Universität zu Berlin, 
Hindenburgdamm 30, 12203, Berlin, Germany.
(5)Else Kroener Fresenius Center for Digital Health, Technical University 
Dresden, Dresden, Germany.
(6)Department of Medicine I, University Hospital Dresden, Dresden, Germany.
(7)Department of Medicine III, University Hospital RWTH Aachen, Aachen, Germany.
(8)Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany.
(9)Department of Diagnostic and Interventional Radiology, University Hospital 
RWTH Aachen, Pauwels Street 30, 52074, Aachen, Germany. snebelung@ukaachen.de.

Erratum in
    Sci Rep. 2024 Mar 5;14(1):5431. doi: 10.1038/s41598-024-56029-x.

Large language models (LLMs) have shown potential in various applications, 
including clinical practice. However, their accuracy and utility in providing 
treatment recommendations for orthopedic conditions remain to be investigated. 
Thus, this pilot study aims to evaluate the validity of treatment 
recommendations generated by GPT-4 for common knee and shoulder orthopedic 
conditions using anonymized clinical MRI reports. A retrospective analysis was 
conducted using 20 anonymized clinical MRI reports, with varying severity and 
complexity. Treatment recommendations were elicited from GPT-4 and evaluated by 
two board-certified specialty-trained senior orthopedic surgeons. Their 
evaluation focused on semiquantitative gradings of accuracy and clinical utility 
and potential limitations of the LLM-generated recommendations. GPT-4 provided 
treatment recommendations for 20 patients (mean age, 50 years ± 19 [standard 
deviation]; 12 men) with acute and chronic knee and shoulder conditions. The LLM 
produced largely accurate and clinically useful recommendations. However, 
limited awareness of a patient's overall situation, a tendency to incorrectly 
appreciate treatment urgency, and largely schematic and unspecific treatment 
recommendations were observed and may reduce its clinical usefulness. In 
conclusion, LLM-based treatment recommendations are largely adequate and not 
prone to 'hallucinations', yet inadequate in particular situations. Critical 
guidance by healthcare professionals is obligatory, and independent use by 
patients is discouraged, given the dependency on precise data input.

© 2023. The Author(s).",I,Robert,,Anton cannot participate
Comparative analysis of diabetes diagnosis: WE-LSTM networks and WizardLM-powered DiabeTalk chatbot,"Diabetes is a chronic metabolic disorder characterized by elevated blood glucose levels due to insufficient insulin production or insulin resistance. It primarily manifests in two forms: Type 1 diabetes, an autoimmune condition typically diagnosed in younger individuals, and Type 2 diabetes, which is more prevalent and often linked to lifestyle factors such as obesity and inactivity. This study evaluates the performance of Long Short-Term Memory networks in diagnosing the two types of diabetes from Italian medical text across four progressively refined pre-processing scenarios. Each scenario incrementally builds on the previous one to enhance text cleaning and data preparation, allowing for a more refined and effective data processing pipeline. In parallel, this study introduces DiabeTalk, a chatbot developed on the WizardLM model, designed to provide specialized advice and support for diabetes diagnosis. While the WE-long short term memory models were fine-tuned with clinical data, DiabeTalk was tested without prior training on clinical diaries, allowing us to evaluate its performance in a real-world context. The results indicate that, despite the lack of domain-specific pre-training, DiabeTalk effectively employs natural language understanding and decision-making algorithms to predict diabetes type and respond to user inquiries. However, the testing revealed limitations in accuracy (77.56% versus 97.80%), with the chatbot achieving a lower performance than the WE-long short term memory model, which was applied to minimally pre-processed raw data. The findings underscore the importance of training large language models on relevant clinical datasets to enhance their response capabilities. © 2024 IEEE.",III,Robert,,Anton cannot participate
Expert evaluation of large language models for clinical dialogue summarization,"We assessed the performance of large language models' summarizing clinical dialogues using computational metrics and human evaluations. The comparison was done between automatically generated and human-produced summaries. We conducted an exploratory evaluation of five language models: one general summarisation model, one fine-tuned for general dialogues, two fine-tuned with anonymized clinical dialogues, and one Large Language Model (ChatGPT). These models were assessed using ROUGE, UniEval metrics, and expert human evaluation was done by clinicians comparing the generated summaries against a clinician generated summary (gold standard). The fine-tuned transformer model scored the highest when evaluated with ROUGE, while ChatGPT scored the lowest overall. However, using UniEval, ChatGPT scored the highest across all the evaluated domains (coherence 0.957, consistency 0.7583, fluency 0.947, and relevance 0.947 and overall score 0.9891). Similar results were obtained when the systems were evaluated by clinicians, with ChatGPT scoring the highest in four domains (coherency 0.573, consistency 0.908, fluency 0.96 and overall clinical use 0.862). Statistical analyses showed differences between ChatGPT and human summaries vs. all other models. These exploratory results indicate that ChatGPT's performance in summarizing clinical dialogues approached the quality of human summaries. The study also found that the ROUGE metrics may not be reliable for evaluating clinical summary generation, whereas UniEval correlated well with human ratings. Large language models may provide a successful path for automating clinical dialogue summarization. Privacy concerns and the restricted nature of health records remain challenges for its integration. Further evaluations using diverse clinical dialogues and multiple initialization seeds are needed to verify the reliability and generalizability of automatically generated summaries.",I,Anton,"This is exactly ""Evaluations performed on real, never-before-seen patient data"" as they have 27 real cases, so data is prospectively collected I would say, so I.",
"Breaking Bones, Breaking Barriers: ChatGPT, DeepSeek, and Gemini in Hand Fracture Management","Background: Hand fracture management requires precise diagnostic accuracy and complex decision-making. Advances in artificial intelligence (AI) suggest that large language models (LLMs) may assist or even rival traditional clinical approaches. This study evaluates the effectiveness of ChatGPT-4o, DeepSeek-V3, and Gemini 1.5 in diagnosing and recommending treatment strategies for hand fractures compared to experienced surgeons. Methods: A retrospective analysis of 58 anonymized hand fracture cases was conducted. Clinical details, including fracture site, displacement, and soft-tissue involvement, were provided to the AI models, which generated management plans. Their recommendations were compared to actual surgeon decisions, assessing accuracy, precision, recall, and F1 score. Results: ChatGPT-4o demonstrated the highest accuracy (98.28%) and recall (91.74%), effectively identifying most correct interventions but occasionally proposing extraneous options (precision 58.48%). DeepSeek-V3 showed moderate accuracy (63.79%), with balanced precision (61.17%) and recall (57.89%), sometimes omitting correct treatments. Gemini 1.5 performed poorly (accuracy 18.97%), with low precision and recall, indicating substantial limitations in clinical decision support. Conclusions: AI models can enhance clinical workflows, particularly in radiographic interpretation and triage, but their limitations highlight the irreplaceable role of human expertise in complex hand trauma management. ChatGPT-4o demonstrated promising accuracy but requires refinement. Ethical concerns regarding AI-driven medical decisions, including bias and transparency, must be addressed before widespread clinical implementation. © 2025 by the authors.",I,Robert,,Anton cannot participate in tiebreak (one of reviewers)
Artificial Intelligence Promotes the Dunning Kruger Effect: Evaluating ChatGPT Answers to Frequently Asked Questions about Adolescent Idiopathic Scoliosis,"Introduction:Patients have long turned to the Internet for answers to common medical questions. As the ability to access information evolves beyond standard search engines, patients with adolescent idiopathic scoliosis (AIS) and their parents may use artificial intelligence chatbots such as ChatGPT as a new source of information.Methods:Ten frequently asked questions regarding AIS were posed to ChatGPT. The accuracy and adequacy of the responses were graded as excellent not requiring clarification, satisfactory requiring minimal clarification, satisfactory requiring moderate clarification, and unsatisfactory requiring substantial clarification.Results:ChatGPT gave one response that was excellent not requiring clarification, four responses that were satisfactory requiring minimal clarification, three responses that were satisfactory requiring moderate clarification, and two responses that were unsatisfactory requiring substantial clarification, with information about higher level, more complex areas of discussion such as surgical options being less accurate.Conclusion:ChatGPT provides answers to FAQs about AIS that were generally accurate, although correction was needed on specific surgical treatments. Patients may be at risk of developing a Dunning-Kruger effect by proxy from the superficial and sometimes inaccurate information provided by ChatGPT on more complex aspects of AIS. © American Academy of Orthopaedic Surgeons.",II,Anton,There wasn't a disagreement here lmao. One reviewer for got to capitalize the II (Ii) so the == gave an no..,
Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder,"Despite the promising capacity of large language model (LLM)-powered chatbots to diagnose diseases, they have not been tested for obsessive-compulsive disorder (OCD). We assessed the diagnostic accuracy of LLMs in OCD using vignettes and found that LLMs outperformed medical and mental health professionals. This highlights the potential benefit of LLMs in assisting in the timely and accurate diagnosis of OCD, which usually entails a long delay in diagnosis and treatment. © The Author(s) 2024.",II,Anton,"The task is real - diagnosis. So I fully agree with Eunice (as long as data is simulated, which it seems to be); disagree with Vivian saying it's fake task.",
The Large Language Model ChatGPT-4 Exhibits Excellent Triage Capabilities and Diagnostic Performance for Patients Presenting With Various Causes of Knee Pain,"Purpose: To provide a proof-of-concept analysis of the appropriateness and performance of ChatGPT-4 to triage, synthesize differential diagnoses, and generate treatment plans concerning common presentations of knee pain. Methods: Twenty knee complaints warranting triage and expanded scenarios were input into ChatGPT-4, with memory cleared prior to each new input to mitigate bias. For the 10 triage complaints, ChatGPT-4 was asked to generate a differential diagnosis that was graded for accuracy and suitability in comparison to a differential created by 2 orthopaedic sports medicine physicians. For the 10 clinical scenarios, ChatGPT-4 was prompted to provide treatment guidance for the patient, which was again graded. To test the higher-order capabilities of ChatGPT-4, further inquiry into these specific management recommendations was performed and graded. Results: All ChatGPT-4 diagnoses were deemed appropriate within the spectrum of potential pathologies on a differential. The top diagnosis on the differential was identical between surgeons and ChatGPT-4 for 70% of scenarios, and the top diagnosis provided by the surgeon appeared as either the first or second diagnosis in 90% of scenarios. Overall, 16 of 30 diagnoses (53.3%) in the differential were identical. When provided with 10 expanded vignettes with a single diagnosis, the accuracy of ChatGPT-4 increased to 100%, with the suitability of management graded as appropriate in 90% of cases. Specific information pertaining to conservative management, surgical approaches, and related treatments was appropriate and accurate in 100% of cases. Conclusions: ChatGPT-4 provided clinically reasonable diagnoses to triage patient complaints of knee pain due to various underlying conditions that were generally consistent with differentials provided by sports medicine physicians. Diagnostic performance was enhanced when providing additional information, allowing ChatGPT-4 to reach high predictive accuracy for recommendations concerning management and treatment options. However, ChatGPT-4 may show clinically important error rates for diagnosis depending on prompting strategy and information provided; therefore, further refinements are necessary prior to implementation into clinical workflows. Clinical Relevance: Although ChatGPT-4 is increasingly being used by patients for health information, the potential for ChatGPT-4 to serve as a clinical support tool is unclear. In this study, we found that ChatGPT-4 was frequently able to diagnose and triage knee complaints appropriately as rated by sports medicine surgeons, suggesting that it may eventually be a useful clinical support tool. © 2024 Arthroscopy Association of North America",II,Anton,"At least the way I read it - the data is simulated. Despite a good task this is exactly ""simulated clinical situations"".
Eunice's review confirmed scenarios are simulated per the paper. Thus for sure II; disagree with rater 1 (Vivian) who said it's real data.",
Large language model answers medical questions about standard pathology reports,"This study aims to evaluate the feasibility of large language model (LLM) in answering pathology questions based on pathology reports (PRs) of colorectal cancer (CRC). Four common questions (CQs) and corresponding answers about pathology were retrieved from public webpages. These questions were input as prompts for Chat Generative Pretrained Transformer (ChatGPT) (gpt-3.5-turbo). The quality indicators (understanding, scientificity, satisfaction) of all answers were evaluated by gastroenterologists. Standard PRs from 5 CRC patients who received radical surgeries in Shanghai Changzheng Hospital were selected. Six report questions (RQs) and corresponding answers were generated by a gastroenterologist and a pathologist. We developed an interactive PRs interpretation system which allows users to upload standard PRs as JPG images. Then the ChatGPT's responses to the RQs were generated. The quality indicators of all answers were evaluated by gastroenterologists and out-patients. As for CQs, gastroenterologists rated AI answers similarly to non-AI answers in understanding, scientificity, and satisfaction. As for RQ1-3, gastroenterologists and patients rated the AI mean scores higher than non-AI scores among the quality indicators. However, as for RQ4-6, gastroenterologists rated the AI mean scores lower than non-AI scores in understanding and satisfaction. In RQ4, gastroenterologists rated the AI scores lower than non-AI scores in scientificity (P = 0.011); patients rated the AI scores lower than non-AI scores in understanding (P = 0.004) and satisfaction (P = 0.011). In conclusion, LLM could generate credible answers to common pathology questions and conceptual questions on the PRs. It holds great potential in improving doctor-patient communication. Copyright © 2024 Wang, Zhou, Zhang, Cao, Xin, Xu and Zhou.",II,Anton,"Generally more of a report interpretation, so much closer to II than I. I could get behind an argument that this is like a consult to pathology asking to clarify something about a report and thus would be a real task, but gut feeling it is closer to II.",
Comparative Performance of Claude and GPT Models in Basic Radiological Imaging Tasks,"Background: Publicly available artificial intelligence (AI) Visual Language Models (VLMs) are constantly improving. The advent of vision capabilities on these models could enhance workflows in radiology. Evaluating their performance in radiological image interpretation is vital to their potential integration into practice. Aim: This study aims to evaluate the proficiency and consistency of the publicly available VLMs, Claude and GPT, across multiple iterations in basic image interpretation tasks. Method: Subsets from publicly available datasets, ROCOv2 and MURAv1.1, were used to evaluate 6 VLMs. A system prompt and image were inputted into each model thrice. The outputs were compared to the dataset captions to evaluate each model's accuracy in recognising the modality, anatomy, and detecting fractures on radiographs. The consistency of the output across iterations was also analysed. Results: Evaluation of the ROCOv2 dataset showed high accuracy in modality recognition, with some models achieving 100%. Anatomical recognition ranged between 61% and 85% accuracy across all models tested. On the MURAv1.1 dataset, Claude-3.5-Sonnet had the highest anatomical recognition with 57% accuracy, while GPT-4o had the best fracture detection with 62% accuracy. Claude-3.5-Sonnet was the most consistent model, with 83% and 92% consistency in anatomy and fracture detection, respectively. Conclusion: Given Claude and GPT's current accuracy and reliability, integration of these models into clinical settings is not yet feasible. This study highlights the need for ongoing development and establishment of standardised testing techniques to ensure these models achieve reliable performance.",II,Anton,The reviewer 2 also said it's II based on most criteria. Unsure what criteria Amelia is referring to.,
ChatGPT for Addressing Patient-centered Frequently Asked Questions in Glaucoma Clinical Practice,"I jumped through a lot of hoops and still was not able to obtain a full version of this article. Had to make a judgment call as the line between II and III is somewhat blurry here. If the questions were of the sort that a patient could ask - this is II, if they are more like exam-like questions this is a III. I concluded it is II as it just feels more the vibe if II (Likert score, etc.) I can see this being a judgement call.",II,Anton,In my conversation with Sully in Slack we concluded that a handful of patient-like questions is a II (vs a thousand of MC is a III). This appears to be doing patient conversation - II.,
"Comparison of the Audiological Knowledge of Three Chatbots: ChatGPT, Bing Chat, and Bard","Introduction: The purpose of this study was to evaluate three chatbots - OpenAI ChatGPT, Microsoft Bing Chat (currently Copilot), and Google Bard (currently Gemini) - in terms of their responses to a defined set of audiological questions. Methods: Each chatbot was presented with the same 10 questions. The authors rated the responses on a Likert scale ranging from 1 to 5. Additional features, such as the number of inaccuracies or errors and the provision of references, were also examined. Results: Most responses given by all three chatbots were rated as satisfactory or better. However, all chatbots generated at least a few errors or inaccuracies. ChatGPT achieved the highest overall score, while Bard was the worst. Bard was also the only chatbot unable to provide a response to one of the questions. ChatGPT was the only chatbot that did not provide information about its sources. Conclusions: Chatbots are an intriguing tool that can be used to access basic information in a specialized area like audiology. Nevertheless, one needs to be careful, as correct information is not infrequently mixed in with errors that are hard to pick up unless the user is well versed in the field. © 2024 S. Karger AG, Basel.",II,Anton,"I jumped through a lot of hoops and still was not able to obtain a full version of this article. Had to make a judgment call as the line between II and III is somewhat blurry here. If the questions were of the sort that a patient could ask - this is II, if they are more like exam-like questions this is a III. Given that there isnt a single mention of the source - I am going to conservatively say that this is III. Somewhat judgement call, but while unable to access it it's hard otherwise.",
Challenging the Chatbot: An Assessment of ChatGPT's Diagnoses and Recommendations for DBP Case Studies,"Objective:Chat Generative Pretrained Transformer-3.5 (ChatGPT) is a publicly available and free artificial intelligence chatbot that logs billions of visits per day; parents may rely on such tools for developmental and behavioral medical consultations. The objective of this study was to determine how ChatGPT evaluates developmental and behavioral pediatrics (DBP) case studies and makes recommendations and diagnoses.Methods:ChatGPT was asked to list treatment recommendations and a diagnosis for each of 97 DBP case studies. A panel of 3 DBP physicians evaluated ChatGPT's diagnostic accuracy and scored treatment recommendations on accuracy (5-point Likert scale) and completeness (3-point Likert scale). Physicians also assessed whether ChatGPT's treatment plan correctly addressed cultural and ethical issues for relevant cases. Scores were analyzed using Python, and descriptive statistics were computed.Results:The DBP panel agreed with ChatGPT's diagnosis for 66.2% of the case reports. The mean accuracy score of ChatGPT's treatment plan was deemed by physicians to be 4.6 (between entirely correct and more correct than incorrect), and the mean completeness was 2.6 (between complete and adequate). Physicians agreed that ChatGPT addressed relevant cultural issues in 10 out of the 11 appropriate cases and the ethical issues in the single ethical case.Conclusion:While ChatGPT can generate a comprehensive and adequate list of recommendations, the diagnosis accuracy rate is still low. Physicians must advise caution to patients when using such online sources. © 2024 Lippincott Williams and Wilkins. All rights reserved.",II,Anton,I do not see how this is prospective. ,