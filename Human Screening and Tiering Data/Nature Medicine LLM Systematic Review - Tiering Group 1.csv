Title,Abstract,Include? (GPT-4.1),Comments (GPT-4.1),DOI,Tier,Screener Name,Notes,,313
Large language models encode clinical knowledge,Large language models encode clinical knowledge,yes,"This study evaluates large language models (PaLM, Flan-PaLM, Med-PaLM) on a variety of medical question answering tasks, including US Medical Licensing Exam-style questions and other clinical benchmarks. It uses both automated and human evaluation frameworks to assess clinical knowledge, reasoning, and potential harm, directly addressing the clinical application of LLMs. The focus is on the clinical performance and limitations of LLMs, not just technical development.",10.1038/s41586-023-06291-2,III,Joanne,"Fake data
Fake tasks
not retro/prospective
no real world clinical deployment or RCT",,
Evaluation of Generative Artificial Intelligence Models in Predicting Pediatric Emergency Severity Index Levels,"Objective: Evaluate the accuracy and reliability of various generative artificial intelligence (AI) models (ChatGPT-3.5, ChatGPT-4.0, T5, Llama-2, Mistral-Large, and Claude-3 Opus) in predicting Emergency Severity Index (ESI) levels for pediatric emergency department patients and assess the impact of medically oriented fine-tuning. Methods: Seventy pediatric clinical vignettes from the ESI Handbook version 4 were used as the gold standard. Each AI model predicted the ESI level for each vignette. Performance metrics, including sensitivity, specificity, and F1 score, were calculated. Reliability was assessed by repeating the tests and measuring the interrater reliability using Fleiss kappa. Paired t tests were used to compare the models before and after fine-tuning. Results: Claude-3 Opus achieved the highest performance amongst the untrained models with a sensitivity of 80.6% (95% confidence interval [CI]: 63.6-90.7), specificity of 91.3% (95% CI: 83.8-99), and an F1 score of 73.9% (95% CI: 58.9-90.7). After fine-tuning, the GPT-4.0 model showed statistically significant improvement with a sensitivity of 77.1% (95% CI: 60.1-86.5), specificity of 92.5% (95% CI: 89.5-97.4), and an F1 score of 74.6% (95% CI: 63.9-83.8, P < 0.04). Reliability analysis revealed high agreement for Claude-3 Opus (Fleiss κ: 0.85), followed by Mistral-Large (Fleiss κ: 0.79) and trained GPT-4.0 (Fleiss κ: 0.67). Training improved the reliability of GPT models (P < 0.001). Conclusions: Generative AI models demonstrate promising accuracy in predicting pediatric ESI levels, with fine-tuning significantly enhancing their performance and reliability. These findings suggest that AI could serve as a valuable tool in pediatric triage. © 2025 Lippincott Williams and Wilkins. All rights reserved.",yes,"This study evaluates multiple generative LLMs (ChatGPT-3.5, ChatGPT-4.0, Llama-2, Claude-3 Opus, etc.) in the clinical context of predicting Emergency Severity Index (ESI) levels for pediatric emergency department patients. The models are tested on clinical vignettes, and their clinical decision-making performance is measured. This fits the inclusion criteria as it is a clinical evaluation of LLMs in healthcare.",10.1097/PEC.0000000000003315,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
The use of a ChatGPT-4-based chatbot in teledermatology: A retrospective exploratory study,,yes,"The title indicates that the study evaluates a ChatGPT-4-based chatbot in the context of teledermatology, which is a clinical application in healthcare. Although the abstract is missing, the title alone suggests a clinical evaluation of an LLM in medicine.",10.1111/ddg.15609_g,I,Sully,,,
Quality of Chatbot Responses to the Most Popular Questions Regarding Erectile Dysfunction,"Objective: Erectile dysfunction (ED) is a common cause of male sexual dysfunction. We aimed to evaluate the quality of ChatGPT and Gemini’s responses to the most frequently asked questions about ED. Materials and Methods: This study was conducted as a cross-sectional, observational study. Google Trends was used to determine the most frequently asked questions on the internet. ChatGPT-3.5 and Gemini were compared for these chatbots’ answers to the questions about ED. Two urologists with board certificates assessed the quality of responses using the Global Quality Score (GQS). Results: Fifteen questions about ED were included according to the Google Trends. ChatGPT was able to answer all the questions systematically, whereas Gemini could not answer two questions. Upon assessing the quality of the responses provided by both researchers with the GQS, it was observed that the frequency of low-quality responses from Gemini exceeded that of ChatGPT. The agreement between researchers was 92% for ChatGPT and 95% for Gemini. Conclusion: Despite the expeditious and comprehensive answers provided by chatbots, we identified inadequacies in their responses related to ED. In their current state, they cannot replace the patient-centered approach of healthcare professionals and require further development. © 2024, AVES. All rights reserved.",yes,"This study evaluates the quality of responses provided by generative LLMs (ChatGPT-3.5 and Gemini) to common patient questions about erectile dysfunction, with clinical experts rating the answers. This is a direct clinical application of LLMs in healthcare.",10.5152/tud.2024.24098,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
The Comparative Diagnostic Capability of Large Language Models in Otolaryngology,"Objectives: Evaluate and compare the ability of large language models (LLMs) to diagnose various ailments in otolaryngology. Methods: We collected all 100 clinical vignettes from the second edition of Otolaryngology Cases—The University of Cincinnati Clinical Portfolio by Pensak et al. With the addition of the prompt “Provide a diagnosis given the following history,” we prompted ChatGPT-3.5, Google Bard, and Bing-GPT4 to provide a diagnosis for each vignette. These diagnoses were compared to the portfolio for accuracy and recorded. All queries were run in June 2023. Results: ChatGPT-3.5 was the most accurate model (89% success rate), followed by Google Bard (82%) and Bing GPT (74%). A chi-squared test revealed a significant difference between the three LLMs in providing correct diagnoses (p = 0.023). Of the 100 vignettes, seven require additional testing results (i.e., biopsy, non-contrast CT) for accurate clinical diagnosis. When omitting these vignettes, the revised success rates were 95.7% for ChatGPT-3.5, 88.17% for Google Bard, and 78.72% for Bing-GPT4 (p = 0.002). Conclusions: ChatGPT-3.5 offers the most accurate diagnoses when given established clinical vignettes as compared to Google Bard and Bing-GPT4. LLMs may accurately offer assessments for common otolaryngology conditions but currently require detailed prompt information and critical supervision from clinicians. There is vast potential in the clinical applicability of LLMs; however, practitioners should be wary of possible “hallucinations” and misinformation in responses. Level of Evidence: 3 Laryngoscope, 134:3997–4002, 2024. © 2024 The Authors. The Laryngoscope published by Wiley Periodicals LLC on behalf of The American Laryngological, Rhinological and Otological Society, Inc.",yes,"This study directly evaluates the diagnostic performance of multiple LLMs (ChatGPT-3.5, Google Bard, Bing-GPT4) on clinical otolaryngology vignettes, comparing their accuracy in providing diagnoses. This is a clear clinical application of LLMs and fits the inclusion criteria.",10.1002/lary.31434,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings,"We tested the accuracy of ChatGPT, a large language model (LLM), in the ophthalmology question-answering space using two popular multiple choice question banks used for the high-stakes Ophthalmic Knowledge Assessment Program (OKAP) exam. The testing sets were of easy-to-moderate difficulty and were diversified, including recall, interpretation, practical and clinical decision-making problems. ChatGPT achieved 55.8% and 42.7% accuracy in the two 260-question simulated exams. Its performance varied across subspecialties, with the best results in general medicine and the worst in neuro-ophthalmology and ophthalmic pathology and intraocular tumors. These results are encouraging but suggest that specialising LLMs through domain-specific pretraining may be necessary to improve their performance in ophthalmic subspecialties.",yes,"This study evaluates ChatGPT, a large language model, on its ability to answer clinically relevant ophthalmology exam questions, including clinical decision-making problems. This fits the inclusion criteria as it assesses LLM performance in a clinical/medical context.",10.1101/2023.01.22.23284882,III,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Neurological Diagnosis: Artificial Intelligence Compared With Diagnostic Generator,"3308. Neurologist. 2024 May 1;29(3):143-145. doi: 10.1097/NRL.0000000000000560.

Neurological Diagnosis: Artificial Intelligence Compared With Diagnostic 
Generator.

Finelli PF(1).

Author information:
(1)Department of Neurology, Hartford Hospital and University of Connecticut 
School of Medicine, Hartford, CT.

OBJECTIVE: Artificial intelligence has recently become available for widespread 
use in medicine, including the interpretation of digitized information, big data 
for tracking disease trends and patterns, and clinical diagnosis. Comparative 
studies and expert opinion support the validity of imaging and data analysis, 
yet similar validation is lacking in clinical diagnosis. Artificial intelligence 
programs are here compared with a diagnostic generator program in clinical 
neurology.
METHODS: Using 4 nonrandomly selected case records from New England Journal of 
Medicine clinicopathologic conferences from 2017 to 2022, 2 artificial 
intelligence programs (ChatGPT-4 and GLASS AI) were compared with a neurological 
diagnostic generator program (NeurologicDx.com) for diagnostic capability and 
accuracy and source authentication.
RESULTS: Compared with NeurologicDx.com, the 2 AI programs showed results 
varying with order of key term entry and with repeat querying. The diagnostic 
generator yielded more differential diagnostic entities, with correct diagnoses 
in 4 of 4 test cases versus 0 of 4 for ChatGPT-4 and 1 of 4 for GLASS AI, 
respectively, and with authentication of diagnostic entities compared with the 
AI programs.
CONCLUSIONS: The diagnostic generator NeurologicDx yielded a more robust and 
reproducible differential diagnostic list with higher diagnostic accuracy and 
associated authentication compared with artificial intelligence programs.

Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.",yes,"This study directly evaluates the clinical diagnostic performance of two LLMs (ChatGPT-4 and GLASS AI) in neurology, comparing them to a traditional diagnostic generator. The focus is on clinical diagnosis, which fits the inclusion criteria.",10.1097/NRL.0000000000000560,I,Joanne,"Real data
Real tasks
Retrospective
no real world clinical deployment or RCT",,
The Large Language Model Improves the Diagnostic Performance of Suspicious Breast Lesions by Radiologists Using Grayscale Ultrasound: A Multicenter Cohort Study,"PurposeImproving diagnostic performance and reducing unnecessary biopsy has been a major challenge for management of suspicious breast lesions. The purpose was to investigate the potential of three publicly available large language models (LLMs) in assisting radiologists with different experiences for diagnosis of suspicious breast lesions.MethodsFrom January 2020 to August 2022, this study prospectively evaluated a dataset of ultrasound (US) images in 1054 patients with 1062 suspicious breast lesions which were classified to BI-RADS 4 category from three tertiary referral hospitals. The diagnostic performance and unnecessary biopsy rate of LLM with radiologists, two types of radiomic approaches [machine learning (ML)-based radiomic approach and deep learning (DL)-based radiomic approach], and shear wave elastography (SWE) or contrast-enhanced ultrasound (CEUS) assessment were assessed and compared.ResultsAmong three LLMs, Gemini performed slightly better than ChatGPT 4.0 (p > 0.05), while achieved better performance than ChatGPT 3.5 (p < 0.05). For identifying malignancies in suspicious breast lesions, Gemini with senior radiologist demonstrated comparable diagnostic performance to DL-based radiomics approach with accuracies (ACCs) of 90.06-90.53% and 85.08-88.42%, respectively. Gemini with junior radiologist achieved similar diagnostic performance to ML-based radiomics approach with ACCs of 73.68-74.59% and 73.48-75.79%, respectively. With the assistance of Gemini, radiologist achieved superior performance than SWE and CEUS assessment. The unnecessary biopsy rates were reduced from 49.17-51.56% to 33.09-34.78% for junior radiologist aided with Gemini, and to 16.36-16.36% for senior radiologist aided with Gemini.ConclusionGemini showed potential in enhancing radiologists with different experiences using grayscale US for suspicious breast lesion.",yes,"This study evaluates the use of three publicly available LLMs (including Gemini and ChatGPT) to assist radiologists in diagnosing suspicious breast lesions using ultrasound images, comparing their performance to other diagnostic approaches. The LLMs are directly involved in a clinical diagnostic task, making this a clear clinical evaluation of LLMs in healthcare.",10.2139/ssrn.5116563,I,Joanne,"Real data
Real tasks
Prospective
no real world clinical deployment or RCT",,
Bridging the Gap in Neonatal Care: Evaluating AI Chatbots for Chronic Neonatal Lung Disease and Home Oxygen Therapy Management,"Objective: To evaluate the accuracy and comprehensiveness of eight free, publicly available large language model (LLM) chatbots in addressing common questions related to chronic neonatal lung disease (CNLD) and home oxygen therapy (HOT). Study Design: Twenty CNLD and HOT-related questions were curated across nine domains. Responses from ChatGPT-3.5, Google Bard, Bing Chat, Claude 3.5 Sonnet, ERNIE Bot 3.5, and GLM-4 were generated and evaluated by three experienced neonatologists using Likert scales for accuracy and comprehensiveness. Updated LLM models (ChatGPT-4o mini and Gemini 2.0 Flash Experimental) were incorporated to assess rapid technological advancement. Statistical analyses included ANOVA, Kruskal–Wallis tests, and intraclass correlation coefficients. Results: Bing Chat and Claude 3.5 Sonnet demonstrated superior performance, with the highest mean accuracy scores (5.78 ± 0.48 and 5.75 ± 0.54, respectively) and competence scores (2.65 ± 0.58 and 2.80 ± 0.41, respectively). In subsequent testing, Gemini 2.0 Flash Experimental and ChatGPT-4o mini achieved comparable high performance. Performance varied across domains, with all models excelling in “equipment and safety protocols” and “caregiver support.” ERNIE Bot 3.5 and GLM-4 showed self-correction capabilities when prompted. Conclusions: LLMs promise accurate CNLD/HOT information. However, performance variability and the risk of misinformation necessitate expert oversight and continued refinement before widespread clinical implementation. © 2025 Wiley Periodicals LLC.",yes,"This study evaluates multiple large language model (LLM) chatbots (including ChatGPT, Bard, Claude, Gemini, etc.) on their ability to answer clinical questions related to chronic neonatal lung disease and home oxygen therapy. The evaluation is based on accuracy and comprehensiveness as judged by neonatologists, which is a direct clinical application relevant to patient care.",10.1002/ppul.71020,III,Joanne,"Fake data
Real tasks
Retrospective
no real world clinical deployment or RCT",,
Can We Trust the Information on Allergic Rhinitis From Chat Generated Pre-Trained Transformer?; [Chat Generated Pre-Trained Transformer를 통해 얻은 알레르기 비염 정보를 믿을 수 있을까?],"Background and Objectives Chat Generated Pre-Trained Transformer (ChatGPT) is a large language model, which allows consumers to get information with one simple question for free. There are few studies that reported the reliability and usefullness of ChatGPT in the field of otorhinolarynoglogy, so we would like to investigate the reliability of information about allergic rhinitis generated by ChatGPT. Materials and Method We asked ChatGPT 35 questions related to allergic rhinitis based on the textbook from Korean Society of Otorhinolaryngology-Head and Neck Surgery, the clinical guidelines of American Academy of Otolaryngology-Head and Neck Surgery, and the guidelines of Allergic Rhinitis and Its impact on Asthma, and recorded the generated answers. Five specialists from our department were shown the answers in a blind test to assess their reliability. Each question was rated either 'accurate' or 'inaccurate,' and if the contents were not found in the textbook or the guideline, it was rated as 'unreliable.' Results Of the 35 questions, 26 (74%) were identified as 'accurate,' 9 (26%) were 'inaccurate,' and 0 (0%) were 'unreliable.' Questions about epidemiology, causes, diagnosis, and prognosis were found to be more accurate than the average, whereas definitions and treatments were less accurate than the average. Conclusion The information about allergic rhinitis generated by ChatGPT was quite reliable, showing that ChatGPT can be helpful in understanding and treating the disease. It is necessary to use the developing medical artificial intelligence wisely. Copyright © 2024 Korean Society of Otorhinolaryngology-Head and Neck Surgery.",yes,"This study evaluates the reliability of ChatGPT's answers to clinical questions about allergic rhinitis, as judged by medical specialists. It directly assesses the clinical utility and accuracy of a generative LLM (ChatGPT) in a healthcare context.",10.3342/kjorl-hns.2023.00451,III,Joanne,"Fake data
Fake tasks
Retrospective
no real world clinical deployment or RCT",,
Evaluation of LLMs accuracy and consistency in the registered dietitian exam through prompt engineering and knowledge retrieval,"Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses. Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots.",yes,"This study evaluates the clinical performance of multiple frontier LLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) on the Registered Dietitian exam, which is a clinically relevant, healthcare-focused assessment. The study tests LLMs' ability to answer nutrition-related questions, which is within the scope of clinical evaluation. The focus is not on technical architecture or non-clinical tasks, but on clinical knowledge and decision-making relevant to healthcare.",10.1038/s41598-024-85003-w,III,Joanne,"Fake data
Fake tasks
not retro/prospective
no real world clinical deployment or RCT",,
Assessing the accuracy and consistency of answers by ChatGPT to questions regarding carbon monoxide poisoning,"Background ChatGPT, developed by OpenAI, is an artificial intelligence software designed to generate text-based responses. The objective of this study is to evaluate the accuracy and consistency of ChatGPT’s responses to single-choice questions pertaining to carbon monoxide poisoning. This evaluation will contribute to our understanding of the reliability of ChatGPT-generated information in the medical field. Methods The questions utilized in this study were selected from the ""Medical Exam Assistant (Yi Kao Bang)"" application and encompassed a range of topics related to carbon monoxide poisoning. A total of 44 single-choice questions were included in the study following a screening process. Each question was entered into ChatGPT ten times in Chinese, followed by a translation into English, where it was also entered ten times. The responses generated by ChatGPT were subjected to statistical analysis with the objective of assessing their accuracy and consistency in both languages. In this assessment process, the ""Medical Exam Assistant (Yi Kao Bang)"" reference responses were employed as benchmarks. The data analysis was conducted using the Python. Results In approximately 50% of the cases, the responses generated by ChatGPT exhibited a high degree of consistency, whereas in approximately one-third of the cases, the responses exhibited unacceptable blurring of the answers. Meanwhile, the accuracy of these responses was less favorable, with an accuracy rate of 61.1% in Chinese and 57% in English. This indicates that ChatGPT could be enhanced with respect to both consistency and accuracy in responding to queries pertaining to carbon monoxide poisoning. Conclusions It is currently evident that the consistency and accuracy of responses generated by ChatGPT regarding carbon monoxide poisoning is inadequate. Although it offers significant insights, it should not supersede the role of healthcare professionals in making clinical decisions.",yes,"This study evaluates ChatGPT's accuracy and consistency in answering clinically relevant questions about carbon monoxide poisoning, using medical exam questions as the benchmark. The focus is on ChatGPT's performance in a clinical knowledge context, which aligns with the inclusion criteria.",10.1371/journal.pone.0311937,III,Joanne,"Fake data
Fake tasks
Retrospective
no real world clinical deployment or RCT",,
Evaluating ChatGPT on Orbital and Oculofacial Disorders: Accuracy and Readability Insights,"3136. Ophthalmic Plast Reconstr Surg. 2024 Mar-Apr 01;40(2):217-222. doi: 
10.1097/IOP.0000000000002552. Epub 2023 Nov 16.

Evaluating ChatGPT on Orbital and Oculofacial Disorders: Accuracy and 
Readability Insights.

Balas M(1), Janic A(1), Daigle P(2), Nijhawan N(2), Hussain A(3), Gill H(2), 
Lahaie GL(4), Belliveau MJ(5), Crawford SA(1)(6), Arjmand P(7), Ing EB(2)(8).

Author information:
(1)Temerty Faculty of Medicine.
(2)Department of Ophthalmology and Vision Sciences, University of Toronto, 
Toronto, Ontario, Canada.
(3)Department of Ophthalmology and Visual Sciences, Dalhousie University, 
Halifax, Nova Scotia, Canada.
(4)Department of Ophthalmology, Queen's University, Kingston, Ontario, Canada.
(5)Department of Ophthalmology, University of Ottawa and The Ottawa Hospital 
Research Institute, Ottawa, Ontario, Canada.
(6)Division of Vascular Surgery, Department of Surgery, University of Toronto, 
Toronto, Ontario, Canada.
(7)Mississauga Retina Institute, Toronto, Ontario, Canada.
(8)Department of Ophthalmology and Vision Sciences, University of Alberta, 
Edmonton, Alberta, Canada.

PURPOSE: To assess the accuracy and readability of responses generated by the 
artificial intelligence model, ChatGPT (version 4.0), to questions related to 10 
essential domains of orbital and oculofacial disease.
METHODS: A set of 100 questions related to the diagnosis, treatment, and 
interpretation of orbital and oculofacial diseases was posed to ChatGPT 4.0. 
Responses were evaluated by a panel of 7 experts based on appropriateness and 
accuracy, with performance scores measured on a 7-item Likert scale. Inter-rater 
reliability was determined via the intraclass correlation coefficient.
RESULTS: The artificial intelligence model demonstrated accurate and consistent 
performance across all 10 domains of orbital and oculofacial disease, with an 
average appropriateness score of 5.3/6.0 (""mostly appropriate"" to ""completely 
appropriate""). Domains of cavernous sinus fistula, retrobulbar hemorrhage, and 
blepharospasm had the highest domain scores (average scores of 5.5 to 5.6), 
while the proptosis domain had the lowest (average score of 5.0/6.0). The 
intraclass correlation coefficient was 0.64 (95% CI: 0.52 to 0.74), reflecting 
moderate inter-rater reliability. The responses exhibited a high reading-level 
complexity, representing the comprehension levels of a college or graduate 
education.
CONCLUSIONS: This study demonstrates the potential of ChatGPT 4.0 to provide 
accurate information in the field of ophthalmology, specifically orbital and 
oculofacial disease. However, challenges remain in ensuring accurate and 
comprehensive responses across all disease domains. Future improvements should 
focus on refining the model's correctness and eventually expanding the scope to 
visual data interpretation. Our results highlight the vast potential for 
artificial intelligence in educational and clinical ophthalmology contexts.

Copyright © 2023 The American Society of Ophthalmic Plastic and Reconstructive 
Surgery, Inc.",yes,"This study evaluates ChatGPT-4.0's accuracy and readability in answering clinical questions about orbital and oculofacial disorders, covering diagnosis, treatment, and interpretation. The evaluation is based on expert review of clinical content, making it a direct assessment of an LLM in a clinical context.",10.1097/IOP.0000000000002552,III,Joanne,"Fake data
Fake tasks
Retrospective
no real world clinical deployment or RCT",,
Performance of large language models (LLMs) in providing prostate cancer information,"Purpose: The diagnosis and management of prostate cancer (PCa), the second most common cancer in men worldwide, are highly complex. Hence, patients often seek knowledge through additional resources, including AI chatbots such as ChatGPT and Google Bard. This study aimed to evaluate the performance of LLMs in providing education on PCa. Methods: Common patient questions about PCa were collected from reliable educational websites and evaluated for accuracy, comprehensiveness, readability, and stability by two independent board-certified urologists, with a third resolving discrepancy. Accuracy was measured on a 3-point scale, comprehensiveness was measured on a 5-point Likert scale, and readability was measured using the Flesch Reading Ease (FRE) score and Flesch–Kincaid FK Grade Level. Results: A total of 52 questions on general knowledge, diagnosis, treatment, and prevention of PCa were provided to three LLMs. Although there was no significant difference in the overall accuracy of LLMs, ChatGPT-3.5 demonstrated superiority over the other LLMs in terms of general knowledge of PCa (p = 0.018). ChatGPT-4 achieved greater overall comprehensiveness than ChatGPT-3.5 and Bard (p = 0.028). For readability, Bard generated simpler sentences with the highest FRE score (54.7, p < 0.001) and lowest FK reading level (10.2, p < 0.001). Conclusion: ChatGPT-3.5, ChatGPT-4 and Bard generate accurate, comprehensive, and easily readable PCa material. These AI models might not replace healthcare professionals but can assist in patient education and guidance. © The Author(s) 2024.",yes,"This study evaluates the performance of LLMs (ChatGPT-3.5, ChatGPT-4, Bard) in providing patient education about prostate cancer by assessing their answers to common patient questions for accuracy, comprehensiveness, and readability. The evaluation is clinical in nature, focusing on the LLMs' ability to provide reliable medical information to patients.",10.1186/s12894-024-01570-0,III,Joanne,"Fake data
Fake tasks
Retrospective
no real world clinical deployment or RCT",,
Exploring the Role of a Large Language Model on Carpal Tunnel Syndrome Management: An Observation Study of ChatGPT,"Purpose: Recently, large language models, such as ChatGPT, have emerged as promising tools to facilitate scientific research and health care management. The present study aimed to explore the extent of knowledge possessed by ChatGPT concerning carpal tunnel syndrome (CTS), a compressive neuropathy that may lead to impaired hand function and that is frequently encountered in the field of hand surgery. Methods: Six questions pertaining to diagnosis and management of CTS were posed to ChatGPT. The responses were subsequently analyzed and evaluated based on their accuracy, coherence, and comprehensiveness. In addition, ChatGPT was requested to provide five high-level evidence references in support of its answers. A simulated doctor-patient consultation was also conducted to assess whether ChatGPT could offer safe medical advice. Results: ChatGPT supplied clinically relevant information regarding CTS, although at a relatively superficial level. In the context of doctor-patient interaction, ChatGPT suggested a diagnostic pathway that deviated from the widely accepted clinical consensus on CTS diagnosis. Nevertheless, it incorporated differential diagnoses and valuable management options for CTS. Although ChatGPT demonstrated the ability to retain and recall information from previous patient conversations, it infrequently produced pertinent references, many of which were either nonexistent or incorrect. Conclusions: ChatGPT displayed the capability to deliver validated medical information on CTS to nonmedical individuals. However, the generation of nonexistent and inaccurate references by ChatGPT presents a challenge to academic integrity. Clinical relevance: To increase their utility in medicine and academia, large language models must go through specialized reputable data set training and validation from experts. It is essential to note that at present, large language models cannot replace the expertise of health care professionals and may act as a supportive tool. © 2023",yes,"This study evaluates ChatGPT's ability to answer clinical questions about carpal tunnel syndrome, including diagnosis, management, and simulated doctor-patient interactions. It assesses the clinical relevance, accuracy, and safety of the information provided, which aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.jhsa.2023.07.003,III,Joanne,"Fake data
Fake tasks 
Retrospective
no real world clinical deployment or RCT",,
Recommendations for initial diabetic retinopathy screening of diabetic patients using large language model-based artificial intelligence in real-life case scenarios,"Purpose: To study the role of artificial intelligence (AI) to identify key risk factors for diabetic retinopathy (DR) screening and develop recommendations based on clinician and large language model (LLM) based AI platform opinions for newly detected diabetes mellitus (DM) cases. Methods: Five clinicians and three AI applications were given 20 AI-generated hypothetical case scenarios to assess DR screening timing. We calculated inter-rater agreements between clinicians, AI-platforms, and the “majority clinician response” (defined as the maximum number of identical responses provided by the clinicians) and “majority AI-platform” (defined as the maximum number of identical responses among the 3 distinct AI). Scoring was used to identify risk factors of different severity. Three, two, and one points were given to risk factors requiring screening immediately, within a year, and within five years, respectively. After calculating a cumulative screening score, categories were assigned. Results: Clinicians, AI platforms, and the “majority clinician response” and “majority AI response” had fair inter-rater reliability (k value: 0.21–0.40). Uncontrolled DM and systemic co-morbidities required immediate screening, while family history of DM and a co-existing pregnancy required screening within a year. The absence of these risk factors required screening within 5 years of DM diagnosis. Screening scores in this study were between 0 and 10. Cases with screening scores of 0–2 needed screening within 5 years, 3–5 within 1 year, and 6–12 immediately. Conclusion: Based on the findings of this study, AI could play a critical role in DR screening of newly diagnosed DM patients by developing a novel DR screening score. Future studies would be required to validate the DR screening score before it could be used as a reference in real-life clinical situations. Clinical trial registration: Not applicable. © 2024, The Author(s).",yes,"This study evaluates large language model-based AI platforms in making clinical recommendations for diabetic retinopathy screening in hypothetical patient scenarios, comparing their performance to clinicians. The LLMs are used in a clinical decision support context, which fits the inclusion criteria.",10.1186/s40942-024-00533-9,II,Joanne,"Fake data
Real tasks 
not retro/prospective
no real world clinical deployment or RCT",,
A chat about bipolar disorder,"28.

A chat about bipolar disorder.

Parker G(1), Spoelma MJ(1)(2).

Author information:
(1)Discipline of Psychiatry and Mental Health, School of Clinical Medicine, 
Faculty of Medicine and Health, University of New South Wales, Sydney, New South 
Wales, Australia.
(2)Black Dog Institute, Sydney, New South Wales, Australia.

OBJECTIVES: This study aimed to assess the capabilities of ChatGPT (Chat 
Generative Pre-Trained Transformer) in generating informative content related to 
bipolar disorders. The objectives were to evaluate its ability to provide 
accurate information on symptoms, classification, causes, and management of 
bipolar disorder and to explore its creativity in generating topic-related 
songs.
METHODS: ChatGPT3 was used for the study, and a series of clinically relevant 
questions were asked to test its knowledge and creativity. Questions ranged from 
common symptom descriptions to more artistic requests for songs related to 
bipolar disorder.
RESULTS: ChatGPT demonstrated the capacity to provide basic and informative 
material on bipolar disorders, including descriptions of symptoms, 
classification types, causes, and treatment options. It also showed creativity 
in generating songs that capture the nuances of bipolar symptoms, both during 
high and low states.
CONCLUSIONS: While ChatGPT3 can offer superficial information on psychiatric 
topics like bipolar disorder, its inability to provide accurate and up-to-date 
references limits its utility for creating a comprehensive review article for 
scientific journals. However, it may be helpful in generating educational 
material and assisting in component tasks for those with bipolar disorder or 
other psychiatric conditions. As newer versions of AI models are continually 
developed, their capabilities in producing more accurate and advanced content 
will need further evaluation.

© 2023 The Authors. Bipolar Disorders published by John Wiley & Sons Ltd.",yes,"This study evaluates ChatGPT-3's ability to answer clinically relevant questions about bipolar disorder, including symptoms, classification, causes, and management. It assesses the LLM's performance in providing clinical information, which fits the inclusion criteria. The additional evaluation of creative tasks (song generation) does not detract from the clinical focus.",10.1111/bdi.13379,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Exploring ChatGPT for information of cardiopulmonary resuscitation,"2304. Resuscitation. 2023 Apr;185:109729. doi: 10.1016/j.resuscitation.2023.109729. 
Epub 2023 Feb 10.

Exploring ChatGPT for information of cardiopulmonary resuscitation.

Ahn C(1).

Author information:
(1)Department of Emergency Medicine, College of Medicine, Chung-Ang University, 
Seoul, South Korea.",yes,"The study appears to evaluate ChatGPT's ability to provide information about cardiopulmonary resuscitation, which is a clinical topic. This suggests a clinical application of an LLM in healthcare, likely assessing its accuracy or usefulness in a medical context.",10.1016/j.resuscitation.2023.109729,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Evaluation of ChatGPT's responses to information needs and information seeking of dementia patients,"Many people in the advanced stages of dementia require full-time caregivers, most of whom are family members who provide informal (non-specialized) care. It is important to provide these caregivers with high-quality information to help them understand and manage the symptoms and behaviors of dementia patients. This study aims to evaluate ChatGPT, a chatbot built using the Generative Pre-trained Transformer (GPT) large language model, in responding to information needs and information seeking of such informal caregivers. We identified the information needs of dementia patients based on the relevant literature (22 articles were selected from 2442 retrieved articles). From this analysis, we created a list of 31 items that describe these information needs, and used them to formulate 118 relevant questions. We then asked these questions to ChatGPT and investigated its responses. In the next phase, we asked 15 informal and 15 formal dementia-patient caregivers to analyze and evaluate these ChatGPT responses, using both quantitative (questionnaire) and qualitative (interview) approaches. In the interviews conducted, informal caregivers were more positive towards the use of ChatGPT to obtain non-specialized information about dementia compared to formal caregivers. However, ChatGPT struggled to provide satisfactory responses to more specialized (clinical) inquiries. In the questionnaire study, informal caregivers gave higher ratings to ChatGPT's responsiveness on the 31 items describing information needs, giving an overall mean score of 3.77 (SD 0.98) out of 5; the mean score among formal caregivers was 3.13 (SD 0.65), indicating that formal caregivers showed less trust in ChatGPT's responses compared to informal caregivers. ChatGPT's responses to non-clinical information needs related to dementia patients were generally satisfactory at this stage. As this tool is still under heavy development, it holds promise for providing even higher-quality information in response to information needs, particularly when developed in collaboration with healthcare professionals. Thus, large language models such as ChatGPT can serve as valuable sources of information for informal caregivers, although they may not fully meet the needs of formal caregivers who seek specialized (clinical) answers. Nevertheless, even in its current state, ChatGPT was able to provide responses to some of the clinical questions related to dementia that were asked.",yes,"This study evaluates ChatGPT, a generative LLM, in responding to information needs of dementia patients' caregivers, including both non-clinical and some clinical questions. The study involves real caregivers assessing the quality of ChatGPT's responses to questions derived from literature on dementia information needs. Although much of the focus is on non-specialized information, the study also assesses ChatGPT's ability to answer some clinical questions. This fits the inclusion criteria as it evaluates an LLM in a healthcare context, including clinical information provision.",10.1038/s41598-024-61068-5,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
The Potential of GPT-4 as a Personalized Virtual Assistant for Bariatric Surgery Patients,"3.

The Potential of GPT-4 as a Personalized Virtual Assistant for Bariatric Surgery 
Patients.

Ali H(1).

Author information:
(1)Department of Internal Medicine, ECU Health/Brody School of Medicine, 
Greenville, NC, 27834, USA. alih20@ecu.edu.",yes,"The title suggests the study evaluates GPT-4 as a personalized virtual assistant for bariatric surgery patients, which implies a clinical application of an LLM in patient care. Although the abstract is minimal, the context fits the inclusion criteria.",10.1007/s11695-023-06576-5,II,Joanne,"Fake data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Predicting dementia from spontaneous speech using large language models,"Language impairment is an important biomarker of neurodegenerative disorders such as Alzheimer’s disease (AD). Artificial intelligence (AI), particularly natural language processing (NLP), has recently been increasingly used for early prediction of AD through speech. Yet, relatively few studies exist on using large language models, especially GPT-3, to aid in the early diagnosis of dementia. In this work, we show for the first time that GPT-3 can be utilized to predict dementia from spontaneous speech. Specifically, we leverage the vast semantic knowledge encoded in the GPT-3 model to generate text embedding, a vector representation of the transcribed text from speech, that captures the semantic meaning of the input. We demonstrate that the text embedding can be reliably used to (1) distinguish individuals with AD from healthy controls, and (2) infer the subject’s cognitive testing score, both solely based on speech data. We further show that text embedding considerably outperforms the conventional acoustic feature-based approach and even performs competitively with prevailing fine-tuned models. Together, our results suggest that GPT-3 based text embedding is a viable approach for AD assessment directly from speech and has the potential to improve early diagnosis of dementia. © 2022 Agbavor, Liang. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",yes,"This study uses GPT-3, a large language model, to generate text embeddings from patient speech and uses these embeddings to predict dementia and cognitive scores. The application is clinical (early diagnosis of dementia), and the LLM is used in a way that directly impacts clinical decision-making. This fits the inclusion criteria.",10.1371/journal.pdig.0000168,I,Joanne,"Real data
Real tasks
not retro/prospective
no real world clinical deployment or RCT",,
Accuracy of ChatGPT-4o in Text and Video Analysis of Laryngeal Malignant and Premalignant Diseases,"Introduction: Chatbot Generative Pretrained Transformer (ChatGPT), a multimodal generative AI, has been studied for potential applications in healthcare, including otolaryngology-head and neck surgery. In this study, authors investigates the consistency of ChatGPT-4o in analyzing clinical fiberoptic videos of suspected laryngeal malignancies compared to expert clinicians. Methods: This experimental study involved twenty patients with primary laryngeal disease consulting at a tertiary academic center. Data, including laryngeal fiberoptic video examinations, were retrospectively analyzed using the ChatGPT-4o application programming interface. Responses were assessed for diagnostic accuracy, consistency, and clinical recommendations. Three otolaryngology-head and neck consultants independently evaluated ChatGPT-4o's performance using the Artificial Intelligence Performance Instrument and a five-point Likert scale for complexity and consistency. Results: ChatGPT-4o identified malignant diagnoses as the primary diagnosis in 30% of cases, while proposing malignancies as one of the top three diagnoses in 90% of cases. Despite high sensitivity, specificity was limited. The mean consistency score for image analysis was 2.36 ± 1.13, with an intraclass correlation coefficient of 0.890 (P = 0.03). The model showed a tendency to prioritize text over visual data, limiting the improvement in diagnostic accuracy from video input. Conclusion: While ChatGPT-4o demonstrates potential in analyzing laryngeal pathologies through multimodal data, current limitations in specificity and image interpretation indicate the need for further refinement. Ongoing advancements could enhance its integration into clinical workflows, supporting accurate diagnoses and decision-making in otolaryngology. © 2025 The Voice Foundation",yes,"This study evaluates ChatGPT-4o, a frontier LLM, in a direct clinical application: analyzing fiberoptic laryngeal videos for diagnosis and clinical recommendations. The study assesses diagnostic accuracy, consistency, and compares LLM performance to expert clinicians, which fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.jvoice.2025.03.006,I,Joanne,"Real data
Real task
Retrospective
no real world clinical deployment or RCT",,
ChatGPT-o1 and the Pitfalls of Familiar Reasoning in Medical Ethics,"Large language models (LLMs) like ChatGPT often exhibit Type 1 thinking—fast, intuitive reasoning that relies on familiar patterns—which can be dangerously simplistic in complex medical or ethical scenarios requiring more deliberate analysis. In our recent explorations, we observed that LLMs frequently default to well-known answers, failing to recognize nuances or twists in presented situations. For instance, when faced with modified versions of the classic ""Surgeon's Dilemma"" or medical ethics cases where typical dilemmas were resolved, LLMs still reverted to standard responses, overlooking critical details. Even models designed for enhanced analytical reasoning, such as ChatGPT-o1, did not consistently overcome these limitations. This suggests that despite advancements toward fostering Type 2 thinking, LLMs remain heavily influenced by familiar patterns ingrained during training. As LLMs are increasingly integrated into clinical practice, it is crucial to acknowledge and address these shortcomings to ensure reliable and contextually appropriate AI assistance in medical decision-making.",yes,"This study evaluates the reasoning patterns of ChatGPT-o1 and other LLMs in the context of medical ethics scenarios, specifically examining their performance and limitations in clinical decision-making situations. The focus is on how LLMs handle complex ethical cases, which is a clinical application relevant to healthcare. Therefore, it meets the inclusion criteria.",10.1101/2024.09.25.24314342,III,Joanne,"Fake data
Fake tasks
not retro/prospective
no real world clinical deployment or RCT",,
Protocol For Human Evaluation of Artificial Intelligence Chatbots in Clinical Consultations,"Background Generative artificial intelligence (AI) technology has the revolutionary potentials to augment clinical practice and telemedicine. The nuances of real-life patient scenarios and complex clinical environments demand a rigorous, evidence-based approach to ensure safe and effective application. Methods We present a protocol for the systematic evaluation of generative AI large language models (LLMs) as chatbots within the context of clinical microbiology and infectious disease consultations. We aim to critically assess the clinical accuracy, comprehensiveness, coherence, and safety of recommendations produced by leading generative AI models, including Claude 2, Gemini Pro, GPT-4.0, and a GPT-4.0-based custom AI chatbot. Discussion A standardised healthcare-specific prompt template is employed to elicit clinically impactful AI responses. Generated responses will be graded by a panel of human evaluators, encompassing a wide spectrum of domain expertise in clinical microbiology and virology and clinical infectious diseases. Evaluations are performed using a 5-point Likert scale across four clinical domains: factual consistency, comprehensiveness, coherence, and medical harmfulness. Our study will offer insights into the feasibility, limitations, and boundaries of generative AI in healthcare, providing guidance for future research and clinical implementation. Ethical guidelines and safety guardrails should be developed to uphold patient safety and clinical standards.",yes,"Although this is a protocol paper and not a completed study, it describes a planned human evaluation of generative LLMs (Claude 2, Gemini Pro, GPT-4.0) in the context of clinical consultations in microbiology and infectious diseases. The focus is on clinical accuracy, safety, and comprehensiveness of LLM-generated recommendations, which aligns with the inclusion criteria. Protocols are not explicitly listed as excluded (unlike reviews or editorials), so this should be included.",10.1101/2024.03.01.24303593,X,Sully,,,
Social workers’ evaluation of ChatGPT for solving ethical dilemmas within the limits of confidentiality,"This study examined how social workers evaluate the use of ChatGPT in solving ethical dilemmas related to confidentiality in social work. The study employed qualitative document analysis based on a phenomenological approach, analysing assignments of 70 master’s degree students in social work. Participants compared their own solutions to ethical dilemmas with those proposed by ChatGPT. The analysis revealed three main issues: disparities in analysing complex professional dilemmas, differences in reference to ethical codes and laws, and disparities in emotional aspects and empathy. Study participants perceived their responses as more comprehensive, legally based, and empathetic compared to ChatGPT’s solutions, which were perceived as superficial and lacking emotional understanding. While ChatGPT offers potential as a supportive tool, it cannot replace social workers’ ethical judgement and empathy. Recommendations include developing AI training workshops and examining AI tools for ethical training while maintaining the profession’s human element. © 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",yes,"This study evaluates ChatGPT's ability to solve ethical dilemmas in a clinical context (social work), specifically regarding confidentiality. The comparison of ChatGPT's responses to those of social workers on real ethical scenarios is a clinical application relevant to healthcare. The focus is on ChatGPT's performance in a professional, client-facing context, not just technical or non-clinical tasks.",10.1080/02650533.2025.2480092,I,Joanne,"Real data
Real tasks
Retrospective
no real world clinical deployment or RCT",,
Exploring the Performance of ChatGPT in an Orthopaedic Setting and Its Potential Use as an Educational Tool,"Introduction:We assessed ChatGPT-4 vision (GPT-4V)'s performance for image interpretation, diagnosis formulation, and patient management capabilities. We aim to shed light on its potential as an educational tool addressing real-life cases for medical students.Methods:Ten of the most popular orthopaedic trauma cases from OrthoBullets were selected. GPT-4V interpreted medical imaging and patient information, providing diagnoses, and guiding responses to OrthoBullets questions. Four fellowship-trained orthopaedic trauma surgeons rated GPT-4V responses using a 5-point Likert scale (strongly disagree to strongly agree). Each of GPT-4V's answers was assessed for alignment with current medical knowledge (accuracy), rationale and whether it is logical (rationale), relevancy to the specific case (relevance), and whether surgeons would trust the answers (trustworthiness). Mean scores from surgeon ratings were calculated.Results:In total, 10 clinical cases, comprising 97 questions, were analyzed (10 imaging, 35 management, and 52 treatment). The surgeons assigned a mean overall rating of 3.46/5.00 to GPT-4V's imaging response (accuracy 3.28, rationale 3.68, relevance 3.75, and trustworthiness 3.15). Management questions received an overall score of 3.76 (accuracy 3.61, rationale 3.84, relevance 4.01, and trustworthiness 3.58), while treatment questions had an average overall score of 4.04 (accuracy 3.99, rationale 4.08, relevance 4.15, and trustworthiness 3.93).Conclusion:This is the first study evaluating GPT-4V's imaging interpretation, personalized management, and treatment approaches as a medical educational tool. Surgeon ratings indicate overall fair agreement in GPT-4V reasoning behind decision-making. GPT-4V performed less favorably in imaging interpretation compared with its management and treatment approach performance. The performance of GPT-4V falls below our fellowship-trained orthopaedic trauma surgeon's standards as a standalone tool for medical education. © 2024 The Authors. Published by The Journal of Bone and Joint Surgery, Incorporated. All rights reserved.",yes,"This study evaluates GPT-4V (a frontier LLM) in interpreting medical images, formulating diagnoses, and providing management/treatment recommendations in real-life orthopedic trauma cases. The LLM's clinical reasoning and decision-making are assessed by expert surgeons, and its potential as an educational tool is discussed. The focus is on clinical application (diagnosis, management, treatment) in a healthcare context.",10.2106/JBJS.OA.24.00081,I,Joanne,"Real data
Real tasks
Retrospective
no real world clinical deployment or RCT",,
Neura: A specialized large language model solution in neurology,"Large language models’ (LLM) ability in natural language processing holds promise for diverse applications, yet their deployment in fields such as neurology faces domain-specific challenges. Hence, we introduce Neura: a scalable, explainable solution to specialize LLM. Blindly evaluated on a select set of five complex clinical cases compared to a cohort of 13 neurologists, Neura achieved normalized scores of 86.17% overall, 85% for differential diagnoses, and 88.24% for final diagnoses (55.11%, 46.15%, and 70.93% for neurologists) with rapid response times of 28.8 and 19 seconds (9 minutes and 37.2 seconds and 8 minutes and 51 seconds for neurologists) while consistently providing relevant, accurately cited information. These findings support the emerging role of LLM-driven applications to articulate human-acquired and integrated data with a vast corpus of knowledge, augmenting human experiential reasoning for clinical and research purposes.",yes,"This study evaluates a specialized LLM (Neura) on complex clinical neurology cases, directly comparing its diagnostic performance to human neurologists. The focus is on clinical application (differential and final diagnosis), which fits the inclusion criteria.",10.1101/2024.02.11.24302658,I,Joanne,"Real data
Real tasks
Retrospective
no real world clinical deployment or RCT",,
Research Design Protocol: Assessing the Impact of Using ChatGPT in Radiology Reporting in an Emergency Setting in Egypt,"This research design protocol outlines a study conducted in Egypt as an example of a developing country with limited resources. The objective is to assess the impact of using ChatGPT, a language model, in radiology reporting in the context of an emergency setting where reporting is traditionally done manually. The study aims to evaluate the effectiveness of ChatGPT in helping format structured reports, drawing impressions from the reports, and improving the efficiency of communication between radiologists and treating clinicians. A standardized survey will be utilized to compare the differences between the older subjective reporting format and ChatGPT-assisted reports.",yes,"This is a research protocol (not a review or opinion) that proposes to evaluate ChatGPT, a large language model, in the clinical context of radiology reporting in emergency settings. The study aims to assess ChatGPT's impact on report structuring, drawing clinical impressions, and communication efficiency, which are all relevant clinical applications.",10.1101/2023.06.28.23291928,X,,,,
Evaluating the performance of ChatGPT-4 on the United Kingdom Medical Licensing Assessment,"Introduction: Recent developments in artificial intelligence large language models (LLMs), such as ChatGPT, have allowed for the understanding and generation of human-like text. Studies have found LLMs abilities to perform well in various examinations including law, business and medicine. This study aims to evaluate the performance of ChatGPT in the United Kingdom Medical Licensing Assessment (UKMLA). Methods: Two publicly available UKMLA papers consisting of 200 single-best-answer (SBA) questions were screened. Nine SBAs were omitted as they contained images that were not suitable for input. Each question was assigned a specialty based on the UKMLA content map published by the General Medical Council. A total of 191 SBAs were inputted in ChatGPT-4 through three attempts over the course of 3 weeks (once per week). Results: ChatGPT scored 74.9% (143/191), 78.0% (149/191) and 75.6% (145/191) on three attempts, respectively. The average of all three attempts was 76.3% (437/573) with a 95% confidence interval of (74.46% and 78.08%). ChatGPT answered 129 SBAs correctly and 32 SBAs incorrectly on all three attempts. On three attempts, ChatGPT performed well in mental health (8/9 SBAs), cancer (11/14 SBAs) and cardiovascular (10/13 SBAs). On three attempts, ChatGPT did not perform well in clinical haematology (3/7 SBAs), endocrine and metabolic (2/5 SBAs) and gastrointestinal including liver (3/10 SBAs). Regarding to response consistency, ChatGPT provided correct answers consistently in 67.5% (129/191) of SBAs but provided incorrect answers consistently in 12.6% (24/191) and inconsistent response in 19.9% (38/191) of SBAs, respectively. Discussion and conclusion: This study suggests ChatGPT performs well in the UKMLA. There may be a potential correlation between specialty performance. LLMs ability to correctly answer SBAs suggests that it could be utilised as a supplementary learning tool in medical education with appropriate medical educator supervision. Copyright © 2023 Lai, Wu, Hsu and Kan.",yes,"This study evaluates the performance of ChatGPT-4, a generative LLM, on the United Kingdom Medical Licensing Assessment (UKMLA), which is a clinically relevant exam. The study directly assesses the LLM's ability to answer clinical questions, which fits the inclusion criteria.",10.3389/fmed.2023.1240915,III,Joanne,"Fake data
Fake tasks
not retro/prospective
no real world clinical deployment or RCT",,
"Evaluating the accuracy and reliability of AI chatbots in patient education on cardiovascular imaging: a comparative study of ChatGPT, gemini, and copilot","Background: The integration of artificial intelligence (AI) chatbots in medicine is expanding rapidly, with notable models like ChatGPT by OpenAI, Gemini by Google, and Copilot by Microsoft. These chatbots are increasingly used to provide medical information, yet their reliability in specific areas such as cardiovascular imaging remains underexplored. This study aims to evaluate the accuracy and reliability of ChatGPT (versions 3.5 and 4), Gemini, and Copilot in responding to patient inquiries about cardiovascular imaging. Methods: We sourced 30 patient-oriented questions on cardiovascular imaging. The questions were submitted to ChatGPT-4, ChatGPT-3.5, Copilot Balanced Mode, Copilot Precise Mode, and Gemini. Responses were evaluated by two cardiovascular radiologists based on accuracy, clarity, completeness, neutrality, and appropriateness using a structured rubric. Inter-rater reliability was assessed using Cohen’s Kappa. Results: ChatGPT-4 achieved the highest performance with 78.3% accuracy, 86.87% clarity and appropriateness, 81.7% completeness, and 100% neutrality. Gemini showed balanced performance, while Copilot Balanced Mode excelled in clarity and accuracy but lagged in completeness. Copilot Precise Mode had the lowest scores in completeness and accuracy. Penalty assessments revealed that ChatGPT-4 had the lowest incidence of missing or misleading information. Conclusion: ChatGPT-4 emerged as the most reliable AI model for providing accurate, clear, and comprehensive patient information on cardiovascular imaging. While other models showed potential, they require further refinement. This study underscores the value of integrating AI chatbots into clinical practice to enhance patient education and engagement. © The Author(s) 2025.",yes,"This study directly evaluates the clinical performance of several generative LLMs (ChatGPT-3.5, ChatGPT-4, Gemini, Copilot) in answering patient questions about cardiovascular imaging. The focus is on the accuracy and reliability of LLM-generated responses in a healthcare context, specifically patient education, which is a clinical application.",10.1186/s43055-025-01452-x,I,Sully,,,
Artificial intelligence (ChatGPT) ready to evaluate ECG in real life? Not yet!,"Objective: This study aims at evaluating if ChatGPT-based artificial intelligence (AI) models are effective in interpreting electrocardiograms (ECGs) and determine their accuracy as compared to those of cardiologists. The purpose is therefore to explore if ChatGPT can be employed for clinical setting, particularly where there are no available cardiologists. Methods: A total of 107 ECG cases classified according to difficulty (simple, intermediate, complex) were analyzed using three AI models (GPT-ECGReader, GPT-ECGAnalyzer, GPT-ECGInterpreter) and compared with the performance of two cardiologists. The statistical analysis was conducted using chi-square and Fisher exact tests using scikit-learn library in Python 3.8. Results: Cardiologists demonstrated superior accuracy (92.52%) compared to ChatGPT-based models (GPT-ECGReader: 57.94%, GPT-ECGInterpreter: 62.62%, GPT-ECGAnalyzer: 62.62%). Statistically significant differences were observed between cardiologists and AI models (p < 0.05). ChatGPT models exhibited enhanced performance with female patients; however, the differences found were not statistically significant. Cardiologists significantly outperformed AI models across all difficulty levels. When it comes to diagnosing patients with arrhythmia (A) and cardiac structural disease ECG patterns, cardiologists gave the best results though there was no statistical difference between them and AI models in diagnosing people with normal (N) ECG patterns. Conclusions: ChatGPT-based models have potential in ECG interpretation; however, they currently lack adequate reliability beyond oversight from a doctor. Additionally, further studies that would improve the accuracy of these models, especially in intricate diagnoses are needed. © The Author(s) 2025.",yes,"This study directly evaluates ChatGPT-based LLMs for the clinical task of ECG interpretation, comparing their diagnostic accuracy to that of cardiologists. The focus is on a clinical application (ECG diagnosis), not technical development or non-clinical tasks. Therefore, it meets the inclusion criteria.",10.1177/20552076251325279,I,Sully,,,
Matching Human Expertise: ChatGPT’s Performance on Hand Surgery Examinations,"Background: The integration of artificial intelligence (AI) into health care witnessed significant advancements, particularly with AI-driven tools like ChatGPT. Initial evaluations indicated that ChatGPT 3.5 did not perform as well as humans on specialized hand surgery self-assessment examinations. The purpose of this study is to evaluate the performance of ChatGPT 4o on American Society for Surgery of the Hand (ASSH) self-assessment questions and whether using enhanced techniques such as better prompts and file search improve accuracy. Methods: Using data from the ASSH self-assessment examinations (2008-2013), we explored the impact of ChatGPT model version, prompt, and file search on the accuracy of AI-generated responses. We used OpenAI’s application programming interface to automate question input and response scoring. Statistical analysis was conducted using one-way analysis of variance. KR-20 was used to assess the reliability of the test. Results: Results indicate that the latest AI models, particularly ChatGPT 4o with enhanced prompting and access to peer-reviewed literature, can achieve performance levels comparable to human examinees, particularly on text-based questions. ChatGPT 4o performed significantly better than ChatGPT 3.5 and showed marked improvement with better prompts and file search capabilities. The KR-20 for the 2013 examination was 0.946, indicating a very reliable test. Conclusions: These findings highlight AI’s potential to support medical education and practice, demonstrating that ChatGPT can perform at a human-equivalent level on hand surgery self-assessment examinations. Our results suggest potential utility as a supplementary tool in educational settings and as a supportive resource in clinical practice. © The Author(s) 2025.",yes,"This study evaluates the performance of ChatGPT (specifically GPT-3.5 and GPT-4o) on hand surgery self-assessment examinations, which are clinically relevant board-style exams. The study directly assesses the LLM's ability to answer clinical questions, aligning with the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1177/15589447251322914,III,Sully,,,
Accuracy of GPT's artificial intelligence on emergency medicine board recertification exam,"2874. Am J Emerg Med. 2024 Feb;76:254-255. doi: 10.1016/j.ajem.2023.11.061. Epub 2023 
Dec 2.

Accuracy of GPT's artificial intelligence on emergency medicine board 
recertification exam.

Akhter M(1).

Author information:
(1)Department of Emergency Medicine, Penn State Hershey Medical Center, 500 
University Dr., Hershey, PA 17033, United States of America; Department of 
Emergency Medicine, HCA Florida, Miami, FL, United States of America. Electronic 
address: murtazaakhter@gmail.com.",yes,"The study evaluates the accuracy of GPT's artificial intelligence on an emergency medicine board recertification exam, which is a clinical evaluation of an LLM's performance in a medical context.",10.1016/j.ajem.2023.11.061,III,Sully,,,
Transforming healthcare documentation: harnessing the potential of AI to generate discharge summaries,"Background: Hospital discharge summaries play an essential role in informing GPs of recent admissions to ensure excellent continuity of care and prevent adverse events; however, they are notoriously poorly written, time-consuming, and can result in delayed discharge. Aim: To evaluate the potential of artificial intelligence (AI) to produce high-quality discharge summaries equivalent to the level of a doctor who has completed the UK Foundation Programme. Design & setting: Feasibility study using 25 mock patient vignettes. Method: Twenty-five mock patient vignettes were written by the authors. Five junior doctors wrote discharge summaries from the case vignettes (five each). The same case vignettes were input into ChatGPT. In total, 50 discharge summaries were generated; 25 by Al and 25 by junior doctors. Quality and suitability were determined through both independent GP evaluators and adherence to a minimum dataset. Results: Of the 25 AI-written discharge summaries 100% were deemed by GPs to be of an acceptable quality compared with 92% of the junior doctor summaries. They both showed a mean compliance of 97% with the minimum dataset. In addition, the ability of GPs to determine if the summary was written by ChatGPT was poor, with only a 60% accuracy of detection. Similarly, when run through an AI-detection tool all were recognised as being very unlikely to be written by AI. Conclusion: AI has proven to produce discharge summaries of equivalent quality to a junior doctor who has completed the UK Foundation Programme; however, larger studies with real-world patient data with NHS-approved AI tools will need to be conducted. © 2024, The Authors. All Rights Reserved.",yes,"This study evaluates ChatGPT, a generative LLM, for generating hospital discharge summaries—a clinical documentation task that directly impacts patient care and continuity. The summaries are assessed for quality and suitability by GPs, making this a clinical application of LLMs in healthcare.",10.3399/BJGPO.2023.0116,II,Sully,,,
Evaluating the Performance of Different Large Language Models on Health Consultation and Patient Education in Urolithiasis,"Objectives: To evaluate the effectiveness of four large language models (LLMs) (Claude, Bard, ChatGPT4, and New Bing) that have large user bases and significant social attention, in the context of medical consultation and patient education in urolithiasis. Materials and methods: In this study, we developed a questionnaire consisting of 21 questions and 2 clinical scenarios related to urolithiasis. Subsequently, clinical consultations were simulated for each of the four models to assess their responses to the questions. Urolithiasis experts then evaluated the model responses in terms of accuracy, comprehensiveness, ease of understanding, human care, and clinical case analysis ability based on a predesigned 5-point Likert scale. Visualization and statistical analyses were then employed to compare the four models and evaluate their performance. Results: All models yielded satisfying performance, except for Bard, who failed to provide a valid response to Question 13. Claude consistently scored the highest in all dimensions compared with the other three models. ChatGPT4 ranked second in accuracy, with a relatively stable output across multiple tests, but shortcomings were observed in empathy and human caring. Bard exhibited the lowest accuracy and overall performance. Claude and ChatGPT4 both had a high capacity to analyze clinical cases of urolithiasis. Overall, Claude emerged as the best performer in urolithiasis consultations and education. Conclusion: Claude demonstrated superior performance compared with the other three in urolithiasis consultation and education. This study highlights the remarkable potential of LLMs in medical health consultations and patient education, although professional review, further evaluation, and modifications are still required. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",yes,"This study evaluates the clinical performance of four LLMs (Claude, Bard, ChatGPT4, New Bing) in the context of health consultation and patient education for urolithiasis. The LLMs are tested on clinical scenarios and patient questions, and their responses are assessed by experts for clinical accuracy and other relevant dimensions. This fits the inclusion criteria as it directly assesses LLMs in a clinical context.",10.1007/s10916-023-02021-3,II,Sully,,,
AI in Hand Surgery: Assessing Large Language Models in the Classification and Management of Hand Injuries,"Background: OpenAI’s ChatGPT (San Francisco, CA, USA) and Google’s Gemini (Mountain View, CA, USA) are two large language models that show promise in improving and expediting medical decision making in hand surgery. Evaluating the applications of these models within the field of hand surgery is warranted. This study aims to evaluate ChatGPT-4 and Gemini in classifying hand injuries and recommending treatment. Methods: Gemini and ChatGPT were given 68 fictionalized clinical vignettes of hand injuries twice. The models were asked to use a specific classification system and recommend surgical or nonsurgical treatment. Classifications were scored based on correctness. Results were analyzed using descriptive statistics, a paired two-tailed t-test, and sensitivity testing. Results: Gemini, correctly classifying 70.6% hand injuries, demonstrated superior classification ability over ChatGPT (mean score 1.46 vs. 0.87, p-value < 0.001). For management, ChatGPT demonstrated higher sensitivity in recommending surgical intervention compared to Gemini (98.0% vs. 88.8%), but lower specificity (68.4% vs. 94.7%). When compared to ChatGPT, Gemini demonstrated greater response replicability. Conclusions: Large language models like ChatGPT and Gemini show promise in assisting medical decision making, particularly in hand surgery, with Gemini generally outperforming ChatGPT. These findings emphasize the importance of considering the strengths and limitations of different models when integrating them into clinical practice. © 2024 by the authors.",yes,"This study directly evaluates the clinical performance of two LLMs (ChatGPT-4 and Gemini) in classifying hand injuries and recommending treatment based on clinical vignettes. It assesses their accuracy and decision-making in a medical context, which fits the inclusion criteria.",10.3390/jcm13102832,II,Sully,,,
"Comparing the performance of ChatGPT-3.5-Turbo, ChatGPT-4, and Google Bard with Iranian students in pre-internship comprehensive exams","This study aims to measure the performance of different AI-language models in three sets of pre-internship medical exams and to compare their performance with Iranian medical students. Three sets of Persian pre-internship exams were used, along with their English translation (six sets in total). In late September 2023, we sent requests to ChatGPT-3.5-Turbo-0613, GPT-4-0613, and Google Bard in both Persian and English languages (excluding questions with any visual content) with each query in a new session and reviewed their responses. GPT models produced responses at varying levels of randomness. In both Persian and English tests, GPT-4 ranked first and obtained the highest score in all exams and different levels of randomness. While Google Bard scored below average on the Persian exams (still in an acceptable range), ChatGPT-3.5 failed all exams. There was a significant difference between the Large Language Models (LLMs) in Persian exams. While GPT-4 yielded the best scores on the English exams, the distinction between all LLMs and students was not statistically significant. The GPT-4 model outperformed students and other LLMs in medical exams, highlighting its potential application in the medical field. However, more research is needed to fully understand and address the limitations of using these models.",yes,"This study evaluates the clinical performance of multiple LLMs (ChatGPT-3.5-Turbo, GPT-4, Google Bard) by comparing their results on pre-internship medical exams to those of medical students. The focus is on clinical knowledge and exam performance, which is a direct clinical application relevant to medical education.",10.1038/s41598-024-79335-w,III,Sully,,,
ChatGPT 4.0's efficacy in the self-diagnosis of non-traumatic hand conditions,"Background: With advancements in artificial intelligence, patients increasingly turn to generative AI models like ChatGPT for medical advice. This study explores the utility of ChatGPT 4.0 (GPT-4.0), the most recent version of ChatGPT, as an interim diagnostician for common hand conditions. Secondarily, the study evaluates the terminology GPT-4.0 associates with each condition by assessing its ability to generate condition-specific questions from a patient's perspective. Methods: Five common hand conditions were identified: trigger finger (TF), Dupuytren's Contracture (DC), carpal tunnel syndrome (CTS), de Quervain's tenosynovitis (DQT), and thumb carpometacarpal osteoarthritis (CMC). GPT-4.0 was queried with author-generated questions. The frequency of correct diagnoses, differential diagnoses, and recommendations were recorded. Chi-squared and pairwise Fisher's exact tests were used to compare response accuracy between conditions. GPT-4.0 was prompted to produce its own questions. Common terms in responses were recorded. Results: GPT-4.0's diagnostic accuracy significantly differed between conditions (p < 0.005). While GPT-4.0 diagnosed CTS, TF, DQT, and DC with >95 % accuracy, 60 % (n = 15) of CMC queries were correctly diagnosed. Additionally, there were significant differences in providing of differential diagnoses (p < 0.005), diagnostic tests (p < 0.005), and risk factors (p < 0.05). GPT-4.0 recommended visiting a healthcare provider for 97 % (n = 121) of the questions. Analysis of ChatGPT-generated questions showed four of the ten most used terms were shared between DQT and CMC. Conclusions: The results suggest that GPT-4.0 has potential preliminary diagnostic utility. Future studies should further investigate factors that improve or worsen AI's diagnostic power and consider the implications of patient utilization. © 2025 Society for Indian Hand Surgery and Micro Surgeons",yes,"This study directly evaluates the clinical diagnostic performance of ChatGPT-4.0 for common hand conditions, including its ability to generate relevant questions and provide recommendations. It uses clinical scenarios and measures diagnostic accuracy, which fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.jham.2025.100217,II,Sully,,,
"Complications Following Body Contouring: Performance Validation of Bard, a Novel AI Large Language Model, in Triaging and Managing Postoperative Patient Concerns","Introduction: Large language models (LLM) have revolutionized the way humans interact with artificial intelligence (AI) technology, with marked potential for applications in esthetic surgery. The present study evaluates the performance of Bard, a novel LLM, in identifying and managing postoperative patient concerns for complications following body contouring surgery. Methods: The American Society of Plastic Surgeons’ website was queried to identify and simulate all potential postoperative complications following body contouring across different acuities and severity. Bard’s accuracy was assessed in providing a differential diagnosis, soliciting a history, suggesting a most-likely diagnosis, appropriate disposition, treatments/interventions to begin from home, and red-flag signs/symptoms indicating deterioration, or requiring urgent emergency department (ED) presentation. Results: Twenty-two simulated body contouring complications were examined. Overall, Bard demonstrated a 59% accuracy in listing relevant diagnoses on its differentials, with a 52% incidence of incorrect or misleading diagnoses. Following history-taking, Bard demonstrated an overall accuracy of 44% in identifying the most-likely diagnosis, and a 55% accuracy in suggesting the indicated medical dispositions. Helpful treatments/interventions to begin from home were suggested with a 40% accuracy, whereas red-flag signs/symptoms, indicating deterioration, were shared with a 48% accuracy. A detailed analysis of performance, stratified according to latency of postoperative presentation (<48hours, 48hours–1month, or >1month postoperatively), and according to acuity and indicated medical disposition, is presented herein. Conclusions: Despite promising potential of LLMs and AI in healthcare-related applications, Bard’s performance in the present study significantly falls short of accepted clinical standards, thus indicating a need for further research and development prior to adoption. Level of Evidence IV: This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266. © Springer Science+Business Media, LLC, part of Springer Nature and International Society of Aesthetic Plastic Surgery 2024.",yes,"This study evaluates the clinical performance of the Bard LLM in triaging and managing postoperative complications in body contouring surgery, including tasks such as differential diagnosis, history taking, and treatment recommendations. The evaluation is directly related to clinical decision-making and patient management using a frontier LLM.",10.1007/s00266-023-03819-9,II,Sully,,,
From Answers to Insights: Unveiling the Strengths and Limitations of ChatGPT and Biomedical Knowledge Graphs,"Large Language Models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, utilizing their language generation capabilities and knowledge acquisition potential from unstructured text. However, when applied to the biomedical domain, LLMs encounter limitations, resulting in erroneous and inconsistent answers. Knowledge Graphs (KGs) have emerged as valuable resources for structured information representation and organization. Specifically, Biomedical Knowledge Graphs (BKGs) have attracted significant interest in managing large-scale and heterogeneous biomedical knowledge. This study evaluates the capabilities of ChatGPT and existing BKGs in question answering, knowledge discovery, and reasoning. Results indicate that while ChatGPT with GPT-4.0 surpasses both GPT-3.5 and BKGs in providing existing information, BKGs demonstrate superior information reliability. Additionally, ChatGPT exhibits limitations in performing novel discoveries and reasoning, particularly in establishing structured links between entities compared to BKGs. To overcome these limitations, future research should focus on integrating LLMs and BKGs to leverage their respective strengths. Such an integrated approach would optimize task performance and mitigate potential risks, thereby advancing knowledge in the biomedical field and contributing to overall well-being.",yes,"This study evaluates ChatGPT (GPT-3.5 and GPT-4.0) in biomedical question answering, knowledge discovery, and reasoning, directly comparing its clinical/biomedical performance to knowledge graphs. The focus is on the LLM's ability to answer biomedical questions, which is a clinical application relevant to healthcare.",10.1101/2023.06.09.23291208,III,Sully,,,
Accuracy and concordance of DMIND AI application with a renowned depression assessment tool in Thai adults,"Background: In Thailand, the high prevalence of depression, particularly in rural areas with limited mental health services, poses significant challenges. Advanced technologies like digital phenotyping and artificial intelligence (AI), particularly natural language processing (NLP) and Large Language Models (LLMs), offer promising solutions by analyzing digital data to detect early signs of depression. Objectives: This study evaluated the performance of the ""Detection and Monitoring Intelligence Network of Depression (DMIND)"" application, an AI-powered screening tool for detecting depression in Thai adults that analyzes behavioral data from participant responses using machine learning algorithms, such as NLP models and LLMs. The study aimed to determine the agreement of the DMIND AI model and the Thai version of the Hamilton depression rating scale (HDRS-17 Thai) in classifying depression severity. Methods: This cross-sectional study recruited 388 participants from one tertiary care hospital and two psychiatric hospitals. Initially, participants used the DMIND application, where they were asked to answer a series of questions. Their response in the application was recorded and a pre-trained AI model predicted their depression severity. Subsequently, a trained nurse or psychologist then assessed participants using the HDRS-17 Thai to establish a baseline measure of depression severity. Statistical analysis involved comparing the depression severity classifications from the DMIND AI model with the HDRS-17 Thai. Cohen's kappa coefficient, sensitivity, specificity, and predictive values were used to evaluate the agreement between the two assessments. Results: Our DMIND application demonstrated moderate agreement with the HDRS-17 Thai, indicating substantial consistency in depression severity classification. The tool showed high sensitivity (87.3%) and moderate specificity (59.5%), with strong negative predictive values for detecting depression. Conclusion: The AI-powered DMIND application effectively screens for depression by analyzing digital data from participants' responses. Its moderate agreement with a traditional clinical assessment and strong diagnostic performance highlights its potential as a scalable, accessible tool for mental health management in Thailand. Integrating AI tools like the DMIND into the public health infrastructure could significantly enhance the accessibility, accuracy, and responsiveness of mental health services, particularly in underserved regions, potentially revolutionizing the management and treatment of depression across the country. © 2024, Chulalongkorn University Printing House. All rights reserved.",yes,"The study evaluates the clinical performance of an AI-powered application (DMIND) that uses NLP and LLMs to assess depression severity in Thai adults, comparing its results to a standard clinical tool (HDRS-17 Thai). The focus is on the clinical application of LLMs for mental health screening, which fits the inclusion criteria.",10.56808/2673-060X.5495,I,Sully,,,
A comparative evaluation of ChatGPT 3.5 and ChatGPT 4 in responses to selected genetics questions,"Objectives: To evaluate the efficacy of ChatGPT 4 (GPT-4) in delivering genetic information about BRCA1, HFE, and MLH1, building on previous findings with ChatGPT 3.5 (GPT-3.5). To focus on assessing the utility, limitations, and ethical implications of using ChatGPT in medical settings. Materials and Methods: A structured survey was developed to assess GPT-4's clinical value. An expert panel of genetic counselors and clinical geneticists evaluated GPT-4's responses to these questions. We also performed comparative analysis with GPT-3.5, utilizing descriptive statistics and using Prism 9 for data analysis. Results: The findings indicate improved accuracy in GPT-4 over GPT-3.5 (P <. 0001). However, notable errors in accuracy remained. The relevance of responses varied in GPT-4, but was generally favorable, with a mean in the ""somewhat agree""range. There was no difference in performance by disease category. The 7-question subset of the Bot Usability Scale (BUS-15) showed no statistically significant difference between the groups but trended lower in the GPT-4 version. Discussion and Conclusion: The study underscores GPT-4's potential role in genetic education, showing notable progress yet facing challenges like outdated information and the necessity of ongoing refinement. Our results, while showing promise, emphasizes the importance of balancing technological innovation with ethical responsibility in healthcare information delivery.  © 2024 The Author(s).",yes,"This study directly evaluates the clinical utility and accuracy of ChatGPT-3.5 and ChatGPT-4 in responding to genetics questions relevant to healthcare, with expert assessment by genetic counselors and clinical geneticists. The focus is on the models' ability to deliver medical information, which is a clinical application.",10.1093/jamia/ocae128,III,Sully,,,
Performance of Generative Artificial Intelligence in Dental Licensing Examinations,"OBJECTIVES: Generative artificial intelligence (GenAI), including large language models (LLMs), has vast potential applications in health care and education. However, it is unclear how proficient LLMs are in interpreting written input and providing accurate answers in dentistry. This study aims to investigate the accuracy of GenAI in answering questions from dental licensing examinations. METHODS: A total of 1461 multiple-choice questions from question books for the US and the UK dental licensing examinations were input into 2 versions of ChatGPT 3.5 and 4.0. The passing rates of the US and UK dental examinations were 75.0% and 50.0%, respectively. The performance of the 2 versions of GenAI in individual examinations and dental subjects was analysed and compared. RESULTS: ChatGPT 3.5 correctly answered 68.3% (n = 509) and 43.3% (n = 296) of questions from the US and UK dental licensing examinations, respectively. The scores for ChatGPT 4.0 were 80.7% (n = 601) and 62.7% (n = 429), respectively. ChatGPT 4.0 passed both written dental licensing examinations, whilst ChatGPT 3.5 failed. ChatGPT 4.0 answered 327 more questions correctly and 102 incorrectly compared to ChatGPT 3.5 when comparing the 2 versions. CONCLUSIONS: The newer version of GenAI has shown good proficiency in answering multiple-choice questions from dental licensing examinations. Whilst the more recent version of GenAI generally performed better, this observation may not hold true in all scenarios, and further improvements are necessary. The use of GenAI in dentistry will have significant implications for dentist-patient communication and the training of dental professionals.",yes,"This study evaluates the performance of ChatGPT (GPT-3.5 and GPT-4.0) on dental licensing examination questions, which are clinically relevant and used for professional certification. The study directly assesses the clinical knowledge and reasoning capabilities of LLMs in a healthcare context.",10.1016/j.identj.2023.12.007,III,Sully,,,
Is ChatGPT an Accurate and Readable Patient Aid for Third Molar Extractions?,"Background: Artificial intelligence (AI) platforms such as Chat Generative Pre-Trained Transformer (ChatGPT) (Open AI, San Francisco, California, USA) have the capacity to answer health-related questions. It remains unknown whether AI can be a patient-friendly and accurate resource regarding third molar extraction. Purpose: The purpose was to determine the accuracy and readability of AI responses to common patient questions regarding third molar extraction. Study Design, Setting, Sample: This is a cross sectional in-silico assessment of readability and soundness of a computer-generated report. Independent Variable: Not applicable. Main Outcome Variables: Accuracy, or the ability to provide clinically correct and relevant information, was determined subjectively by 2 reviewers using a 5-point Likert scale, and objectively by comparing responses to American Association of Oral and Maxillofacial Surgeons (AAOMS) clinical consensus papers. Readability, or how easy a piece of text is to read, was assessed using the Flesch Kincaid Reading Ease (FKRE) and Flesch Kincaid Grade Level (FKGL). Both assess readability based on mean number of syllables per word, and words per sentence. To be deemed readable, FKRE should be >60 and FKGL should be <8. Covariates: Not applicable. Analyses: Descriptive statistics were used to analyze the findings of this study. Results: AI-generated responses above the recommended level for the average patient (FKRE: 52; FKGL: 10). The average Likert score was 4.36, suggesting that most responses were accurate with minor inaccuracies or missing information. AI correctly deferred to the provider in instances where no definitive answer exists. Of the responses that addressed content in AAOMS consensus papers, 18/19 responses closely aligned with them. All prompts did not provide citations or references. Conclusion and Relevance: AI was able to provide mostly accurate responses, and content was closely aligned with AAOMS guidelines. However, responses were too complex for the average third molar extraction patient, and were deficient in citations and references. It is important for providers to educate patients on the utility of AI, and to decide whether to recommend using it for information. Ultimately, the best resource for answers is from the practitioners themselves because the AI platform lacks clinical experience. © 2024 The Authors",yes,"This study evaluates ChatGPT's ability to answer patient questions about third molar extraction, focusing on the accuracy and readability of its responses. It directly assesses the clinical utility of a generative LLM (ChatGPT) in a healthcare context, specifically for patient education and information provision.",10.1016/j.joms.2024.06.177,I,Sully,,,
Efficacy of ChatGPT in Educating Patients and Clinicians About Skin Toxicities Associated With Cancer Treatment,"1737. JMIR Dermatol. 2024 Nov 20;7:e54919. doi: 10.2196/54919.

Efficacy of ChatGPT in Educating Patients and Clinicians About Skin Toxicities 
Associated With Cancer Treatment.

Chang A(1), Young J(1), Para A(1), Lamb A(1), Gulati N(1).

Author information:
(1)Department of Dermatology, Icahn School of Medicine at Mount Sinai, 5 East 
98th St, 5th Floor, New York, NY, 10029, United States, 1 212-241-9728.

This study investigates the application of ChatGPT, an artificial intelligence 
tool, in providing information on skin toxicities associated with cancer 
treatments, highlighting that while ChatGPT can serve as a valuable resource for 
clinicians, its use for patient education requires careful consideration due to 
the complex nature of the information provided.

© Annie Chang, Jade Young, Andrew Para, Angela Lamb, Nicholas Gulati. Originally 
published in JMIR Dermatology (http://derma.jmir.org).",yes,"This study evaluates ChatGPT's effectiveness in educating both patients and clinicians about skin toxicities related to cancer treatment, which is a clinical application relevant to patient and provider education in a healthcare context.",10.2196/54919,II,Sully,,,
The inclusion of social determinants of health into evaluations of quality and appropriateness of AI assistant-ChatGPT,"2890. Prostate Cancer Prostatic Dis. 2024 Mar;27(1):157. doi: 
10.1038/s41391-023-00720-z. Epub 2023 Sep 13.

The inclusion of social determinants of health into evaluations of quality and 
appropriateness of AI assistant-ChatGPT.

Hswen Y(1), Nguyen TT(2).

Author information:
(1)Department of Epidemiology and Biostatistics, University of California San 
Francisco, San Francisco, CA, USA. yulin.hswen@ucsf.edu.
(2)Department of Epidemiology and Biostatistics, University of Maryland School 
of Public Health, College Park, MD, USA.",yes,"The title suggests the study evaluates ChatGPT (an LLM) in the context of healthcare, specifically focusing on how social determinants of health are incorporated into assessments of the quality and appropriateness of its responses. This aligns with the inclusion criteria, as it involves a clinical evaluation of an LLM. The abstract does not indicate that this is a review, editorial, or non-clinical application.",10.1038/s41391-023-00720-z,X,Sully,,,
Quality of ChatGPT Responses to Questions Related To Liver Transplantation,"1694. J Gastrointest Surg. 2023 Aug;27(8):1716-1719. doi: 10.1007/s11605-023-05714-9. 
Epub 2023 May 30.

Quality of ChatGPT Responses to Questions Related To Liver Transplantation.

Endo Y(1), Sasaki K(2), Moazzam Z(1), Lima HA(1), Schenk A(1), Limkemann A(1), 
Washburn K(1), Pawlik TM(3).

Author information:
(1)Department of Surgery, The Ohio State University Wexner Medical Center, 
Columbus, OH, USA.
(2)Department of Surgery, Stanford University, Stanford, CA, USA.
(3)Department of Surgery, The Ohio State University, Wexner Medical Center, 395 
W. 12th Ave., Suite 670 614 293 8701, Columbus, USA. Tim.Pawlik@osumc.edu.",yes,"The study evaluates the quality of ChatGPT's responses to questions related to liver transplantation, which is a clinical application of a generative large language model in a healthcare context.",10.1007/s11605-023-05714-9,II,Sully,,,
Performance of trauma-trained large language models on surgical assessment questions: A new approach in resource identification,"Background: Large language models have successfully navigated simulated medical board examination questions. However, whether and how language models can be used in surgical education is less understood. Our study evaluates the efficacy of domain-specific large language models in curating study materials for surgical board style questions. Methods: We developed EAST-GPT and ACS-GPT, custom large language models with domain-specific knowledge from published guidelines from the Eastern Association of the Surgery of Trauma and the American College of Surgeons Trauma Quality Programs. EAST-GPT, ACS-GPT, and an untrained GPT-4 performance were assessed trauma-related questions from Surgical Education and Self-Assessment Program (18th edition). Large language models were asked to choose answers and provide answer rationales. Rationales were assessed against an educational framework with 5 domains: accuracy, relevance, comprehensiveness, evidence-base, and clarity. Results: Ninety guidelines trained EAST-GPT and 10 trained ACS-GPT. All large language models were tested on 62 trauma questions. EAST-GPT correctly answered 76%, whereas ACS-GPT answered 68% correctly. Both models outperformed ChatGPT-4 (P < .05), which answered 45% correctly. For reasoning, EAST-GPT achieved the gratest mean scores across all 5 educational framework metrics. ACS-GPT scored lower than ChatGPT-4 in comprehensiveness and evidence-base; however, these differences were not statistically significant. Conclusion: Our study presents a novel methodology in identifying test-preparation resources by training a large language model to answer board-style multiple choice questions. Both trained models outperformed ChatGPT-4, demonstrating its answers were accurate, relevant, and evidence-based. Potential implications of such AI integration into surgical education must be explored. © 2024",yes,"This study evaluates the performance of both custom-trained and general LLMs (including GPT-4) on surgical board-style questions, focusing on their ability to answer and provide rationales for trauma-related clinical questions. The evaluation is in the context of surgical education and clinical knowledge assessment, which fits the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1016/j.surg.2024.08.026,III,Sully,,,
Bridging Gaps with Generative AI: Enhancing Hypertension Monitoring Through Patient and Provider Insights,"3427. Stud Health Technol Inform. 2024 Aug 22;316:939-943. doi: 10.3233/SHTI240565.

Bridging Gaps with Generative AI: Enhancing Hypertension Monitoring Through 
Patient and Provider Insights.

Andreadis K(1), Rodriguez DV(1), Zakreuskaya A(1)(2), Chen J(3), Gonzalez J(3), 
Mann D(1)(3).

Author information:
(1)New York University Grossman School of Medicine, New York, USA.
(2)Machine Learning and Data Analytics Lab, Univerity Erlangen-Nuremberg, 
Germany.
(3)MCIT Department of Health Informatics, New York University Langone Health, 
New York, USA.

This study introduces a Generative Artificial Intelligence (GenAI) assistant 
designed to address key challenges in Remote Patient Monitoring (RPM) for 
hypertension. After a comprehensive needs assessment from clinicians and 
patients, we identified pivotal issues in RPM data management and patient 
engagement. The GenAI RPM assistant integrates a patient-facing chatbot, 
clinician-facing smart summaries, and automated draft portal messages to enhance 
communication and streamline data review. Validated through six rounds of 
testing and evaluations by ten participants, the initial prototype was 
positively received, highlighting the importance of personalized interactions. 
Our findings demonstrate GenAI's potential to improve RPM by optimizing data 
management and enhancing patient-provider communication.",yes,"The study describes the development and evaluation of a generative AI assistant (likely an LLM-based chatbot) for hypertension remote patient monitoring, including patient-facing chatbot interactions and clinician-facing summaries. It involves clinical evaluation with participants and focuses on clinical communication and data management, which are relevant clinical applications.",10.3233/SHTI240565,II,Sully,"They actually only did the validation through a contrived survey, though they will do more in the future which is exciting!",,
ChatGPT's learning and reasoning capacity in anesthesiology; [Capacidaddeaprendizaje yrazonamientode ChatGPTentemasdeanestesiología],"Introduction: Over the past few months, ChatGPT has raised a lot of interest given its ability to perform complex tasks through natural language and conversation. However, its use in clinical decision-making is limited and its application in the field of anesthesiology is unknown. Objective: To assess ChatGPT’s basic and clinical reasoning and its learning ability in a performance test on general and specific anesthesia topics. Methods: A three-phase assessment was conducted. Basic knowledge of anesthesia was assessed in the first phase, followed by a review of difficult airway management and, finally, measurement of decision-making ability in ten clinical cases. The second and the third phases were conducted before and after feeding ChatGPT with the 2022 guidelines of the American Society of Anesthesiologists on difficult airway management. Results: On average, ChatGPT succeded 65% of the time in the first phase and 48% of the time in the second phase. Agreement in clinical cases was 20%, with 90% relevance and 10% error rate. After learning, ChatGPT improved in the second phase, and was correct 59% of the time, with agreement in clinical cases also increasing to 40%. Conclusions: ChatGPT showed acceptable accuracy in the basic knowledge test, high relevance in the management of specific difficult airway clinical cases, and the ability to improve after learning. Copyright © 2023 Sociedad Colombiana de Anestesiología y Reanimación (S.C.A.R.E.).",yes,"This study evaluates ChatGPT's clinical reasoning and learning ability in anesthesiology, including performance on knowledge tests and clinical decision-making in specific cases. It directly assesses the LLM's clinical application and decision support capabilities.",10.5554/22562087.e1092,II,Sully,,,
A comparison of drug information question responses by a drug information center and by ChatGPT,"DISCLAIMER: In an effort to expedite the publication of articles, AJHP is posting manuscripts online as soon as possible after acceptance. Accepted manuscripts have been peer-reviewed and copyedited, but are posted online before technical formatting and author proofing. These manuscripts are not the final version of record and will be replaced with the final article (formatted per AJHP style and proofed by the authors) at a later time. PURPOSE: A study was conducted to assess the accuracy and ability of Chat Generative Pre-trained Transformer (ChatGPT) to systematically respond to drug information inquiries relative to responses of a drug information center (DIC). METHODS: Ten drug information questions answered by the DIC in 2022 or 2023 were selected for analysis. Three pharmacists created new ChatGPT accounts and submitted each question to ChatGPT at the same time. Each question was submitted twice to identify consistency in responses. Two days later, the same process was conducted by a fourth pharmacist. Phase 1 of data analysis consisted of a drug information pharmacist assessing all 84 ChatGPT responses for accuracy relative to the DIC responses. In phase 2, 10 ChatGPT responses were selected to be assessed by 3 blinded reviewers. Reviewers utilized an 8-question predetermined rubric to evaluate the ChatGPT and DIC responses. RESULTS: When comparing the ChatGPT responses (n = 84) to the DIC responses, ChatGPT had an overall accuracy rate of 50%. Accuracy across the different question types varied. In regards to the overall blinded score, ChatGPT responses scored higher than the responses by the DIC according to the rubric (overall scores of 67.5% and 55.0%, respectively). The DIC responses scored higher in the categories of references mentioned and references identified. CONCLUSION: Responses generated by ChatGPT have been found to be better than those created by a DIC in clarity and readability; however, the accuracy of ChatGPT responses was lacking. ChatGPT responses to drug information questions would need to be carefully reviewed for accuracy and completeness.",yes,"This study directly evaluates ChatGPT's ability to answer drug information questions, comparing its responses to those of a drug information center. The focus is on clinical information provision, which is a relevant healthcare application of an LLM.",10.1093/ajhp/zxae316,II,Sully,,,
ChatGPT-4 generates orthopedic discharge documents faster than humans maintaining comparable quality: a pilot study of 6 cases,"Background and purpose — Large language models like ChatGPT-4 have emerged. They hold the potential to reduce the administrative burden by generating everyday clinical documents, thus allowing the physician to spend more time with the patient. We aimed to assess both the quality and efficiency of discharge documents generated by ChatGPT-4 in comparison with those produced by physicians. Patients and methods — To emulate real-world situations, the health records of 6 fictional orthopedic cases were created. Discharge documents for each case were generated by a junior attending orthopedic surgeon and an advanced orthopedic resident. ChatGPT-4 was then prompted to generate the discharge documents using the same health record information. The quality assessment was performed by an expert panel (n = 15) blinded to the source of the documents. As secondary outcome, the time required to generate the documents was compared, logging the duration of the cre-ation of the discharge documents by the physician and by ChatGPT-4. Results — Overall, both ChatGPT-4 and physician-generated notes were comparable in quality. Notably, Chat-GPT-4 generated discharge documents 10 times faster than the traditional method. 4 events of hallucinations were found in the ChatGPT-4-generated content, compared with 6 events in the human/physician produced notes. Conclusion — ChatGPT-4 creates orthopedic discharge notes faster than physicians, with comparable quality. This shows it has great potential for making these documents more efficient in orthopedic care. ChatGPT-4 has the potential to significantly reduce the administrative burden on healthcare professionals. © 2024 The Author(s).",yes,"This study evaluates ChatGPT-4's ability to generate clinical discharge documents in an orthopedic context, comparing its quality and efficiency to that of human physicians. The task is directly related to clinical documentation, which is a core part of patient care and healthcare delivery. The study uses a frontier LLM (ChatGPT-4) and assesses its performance in a real-world clinical workflow.",10.2340/17453674.2024.40182,II,Sully,,,
Can Chatbot Artificial Intelligence Replace Infectious Diseases Physicians in the Management of Bloodstream Infections? A Prospective Cohort Study,"BACKGROUND: The development of chatbot artificial intelligence (AI) has raised 
major questions about their use in healthcare. We assessed the quality and 
safety of the management suggested by Chat Generative Pre-training Transformer 4 
(ChatGPT-4) in real-life practice for patients with positive blood cultures.
METHODS: Over a 4-week period in a tertiary care hospital, data from consecutive 
infectious diseases (ID) consultations for a first positive blood culture were 
prospectively provided to ChatGPT-4. Data were requested to propose a 
comprehensive management plan (suspected/confirmed diagnosis, workup, antibiotic 
therapy, source control, follow-up). We compared the management plan suggested 
by ChatGPT-4 with the plan suggested by ID consultants based on literature and 
guidelines. Comparisons were performed by 2 ID physicians not involved in 
patient management.
RESULTS: Forty-four cases with a first episode of positive blood culture were 
included. ChatGPT-4 provided detailed and well-written responses in all cases. 
AI's diagnoses were identical to those of the consultant in 26 (59%) cases. 
Suggested diagnostic workups were satisfactory (ie, no missing important 
diagnostic tests) in 35 (80%) cases; empirical antimicrobial therapies were 
adequate in 28 (64%) cases and harmful in 1 (2%). Source control plans were 
inadequate in 4 (9%) cases. Definitive antibiotic therapies were optimal in 16 
(36%) patients and harmful in 2 (5%). Overall, management plans were considered 
optimal in only 1 patient, as satisfactory in 17 (39%), and as harmful in 7 
(16%).
CONCLUSIONS: The use of ChatGPT-4 without consultant input remains hazardous 
when seeking expert medical advice in 2023, especially for severe IDs.

© The Author(s) 2023. Published by Oxford University Press on behalf of 
Infectious Diseases Society of America. All rights reserved. For permissions, 
please e-mail: journals.permissions@oup.com.",yes,"This study prospectively evaluates ChatGPT-4's ability to provide clinical management plans for patients with bloodstream infections, directly comparing its recommendations to those of infectious diseases physicians. The focus is on clinical decision-making and patient management, which fits the inclusion criteria.",10.1093/cid/ciad632,I,Sully,,,
Assessing the medical reasoning skills of GPT-4 in complex ophthalmology cases,"BACKGROUND/AIMS: This study assesses the proficiency of Generative Pre-trained 
Transformer (GPT)-4 in answering questions about complex clinical ophthalmology 
cases.
METHODS: We tested GPT-4 on 422 Journal of the American Medical Association 
Ophthalmology Clinical Challenges, and prompted the model to determine the 
diagnosis (open-ended question) and identify the next-step (multiple-choice 
question). We generated responses using two zero-shot prompting strategies, 
including zero-shot plan-and-solve+ (PS+), to improve the reasoning of the 
model. We compared the best-performing model to human graders in a benchmarking 
effort.
RESULTS: Using PS+ prompting, GPT-4 achieved mean accuracies of 48.0% (95% CI 
(43.1% to 52.9%)) and 63.0% (95% CI (58.2% to 67.6%)) in diagnosis and next 
step, respectively. Next-step accuracy did not significantly differ by 
subspecialty (p=0.44). However, diagnostic accuracy in pathology and tumours was 
significantly higher than in uveitis (p=0.027). When the diagnosis was accurate, 
75.2% (95% CI (68.6% to 80.9%)) of the next steps were correct. Conversely, when 
the diagnosis was incorrect, 50.2% (95% CI (43.8% to 56.6%)) of the next steps 
were accurate. The next step was three times more likely to be accurate when the 
initial diagnosis was correct (p<0.001). No significant differences were 
observed in diagnostic accuracy and decision-making between board-certified 
ophthalmologists and GPT-4. Among trainees, senior residents outperformed GPT-4 
in diagnostic accuracy (p≤0.001 and 0.049) and in accuracy of next step 
(p=0.002 and 0.020).
CONCLUSION: Improved prompting enhances GPT-4's performance in complex clinical 
situations, although it does not surpass ophthalmology trainees in our context. 
Specialised large language models hold promise for future assistance in medical 
decision-making and diagnosis.

© Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and 
permissions. Published by BMJ.",yes,"This study directly evaluates GPT-4, a generative large language model, on its ability to answer clinical questions (diagnosis and next-step management) in complex ophthalmology cases. The evaluation is clinical in nature, comparing LLM performance to human experts and trainees, and focuses on medical reasoning and decision-making.",10.1136/bjo-2023-325053,II,Sully,,,
A cross-sectional study to evaluate responses generated by two AI software programs for common patient queries about laparoscopic repair of inguinal hernia,"This study aimed to evaluate the quality and accuracy of responses provided by two user-interactive AI chatbots, namely ChatGPT and ChatSonic, in response to patient queries regarding laparoscopic repair of inguinal hernias, and additionally determine the suitability of these chatbots in addressing patient queries related to inguinal hernia repair. Ten questions regarding laparoscopic repair of inguinal hernias were developed and presented to ChatGPT 4.0 and ChatSonic. Responses were evaluated by two experienced surgeons blinded to the source, using the Global Quality Score (GQS) and modified DISCERN Score to gauge response quality and reliability. ChatGPT demonstrated high-quality responses (GQS = 4 & 5) for all ten questions according to one evaluator, and for seven out of ten questions according to the other. Similarly, ChatGPT showed high reliability (DISCERN = 4 & 5) for nine responses according to one evaluator, and for three responses according to the other, with only slight agreement between evaluators for both GQS (kappa = 0.20) and modified DISCERN scores (kappa = 0.08). ChatSonic also provided high-quality and reliable responses for a majority of questions, albeit to a lesser extent than ChatGPT, and both demonstrating limited concordance in responses (p > 0.05). Overall, Both ChatGPT and ChatSonic demonstrated potential utility in providing responses to patient queries about hernia surgery. However, due to inconsistencies in reliability and quality, ongoing refinement and validation of AI generated medical information remain necessary before widespread clinical adoption. © Italian Society of Surgery (SIC) 2025.",yes,"This study evaluates the clinical utility and accuracy of ChatGPT (an LLM) and ChatSonic in answering patient queries about a specific surgical procedure. The focus is on the quality and reliability of LLM-generated responses to real patient questions, which is a direct clinical application.",10.1007/s13304-025-02158-5,I,Sully,,,
ChatGPT's Performance on Portuguese Medical Examination Questions: Comparative Analysis of ChatGPT-3.5 Turbo and ChatGPT-4o Mini,"BACKGROUND: Advancements in ChatGPT are transforming medical education by 
providing new tools for assessment and learning, potentially enhancing 
evaluations for doctors and improving instructional effectiveness.
OBJECTIVE: This study evaluates the performance and consistency of ChatGPT-3.5 
Turbo and ChatGPT-4o mini in solving European Portuguese medical examination 
questions (2023 National Examination for Access to Specialized Training; Prova 
Nacional de Acesso à Formação Especializada [PNA]) and compares their 
performance to human candidates.
METHODS: ChatGPT-3.5 Turbo was tested on the first part of the examination (74 
questions) on July 18, 2024, and ChatGPT-4o mini on the second part (74 
questions) on July 19, 2024. Each model generated an answer using its natural 
language processing capabilities. To test consistency, each model was asked, 
""Are you sure?"" after providing an answer. Differences between the first and 
second responses of each model were analyzed using the McNemar test with 
continuity correction. A single-parameter t test compared the models' 
performance to human candidates. Frequencies and percentages were used for 
categorical variables, and means and CIs for numerical variables. Statistical 
significance was set at P<.05.
RESULTS: ChatGPT-4o mini achieved an accuracy rate of 65% (48/74) on the 2023 
PNA examination, surpassing ChatGPT-3.5 Turbo. ChatGPT-4o mini outperformed 
medical candidates, while ChatGPT-3.5 Turbo had a more moderate performance.
CONCLUSIONS: This study highlights the advancements and potential of ChatGPT 
models in medical education, emphasizing the need for careful implementation 
with teacher oversight and further research.

© Filipe Prazeres. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).",yes,"This study evaluates the performance of two generative LLMs (ChatGPT-3.5 Turbo and ChatGPT-4o mini) on a real medical board examination, directly comparing their results to human candidates. This is a clinical evaluation of LLMs in a medical context, specifically in medical education and assessment.",10.2196/65108,III,Sully,,,
TCM-KLLaMA: Intelligent generation model for Traditional Chinese Medicine Prescriptions based on knowledge graph and large language model,"Traditional Chinese medicine (TCM) prescriptions are a basic component of TCM treatment, developed by assessing patient symptoms and prescribing a mix of herbs. Accurate prescription generation is critical for enhancing treatment outcomes and maintaining patient safety. However, conventional methods based on Large Language Models (LLMs) focus mainly on symptom information, neglecting other TCM diagnostic expertise, such as tongue and pulse diagnosis, and are prone to hallucination, which is unacceptable in medical applications. To address these challenges, the paper proposes an effective prescription generation model enriched by a TCM knowledge graph (KG) called the TCM-KLLaMA model. In this model, the Chinese-LLaMA2-7 B model is provided with a new output layer and loss function to suppress hallucinations and increase recommendation accuracy. A TCM KG including symptoms, tongue diagnosis, and pulse diagnosis was developed, and the model was fine-tuned utilizing the suggested synonym and matching knowledge injection (SMKI) mechanism. Extensive experiments demonstrate that the TCM- KLLaMA outperforms baseline models in both Precision and F1 Score, proving its superior performance in prescription generation tasks. © 2025 Elsevier Ltd",yes,"The study evaluates a large language model (Chinese-LLaMA2-7B) for the clinical task of generating Traditional Chinese Medicine prescriptions based on patient diagnostic information. The model is applied directly to a clinical decision-making process (prescription generation), which is a relevant healthcare application. The focus is not on technical architecture alone, but on clinical utility and performance.",10.1016/j.compbiomed.2025.109887,II,Sully,,,
Exploring community pharmacists’ attitudes in Thailand towards ChatGPT usage: A pilot qualitative investigation,"Background: ChatGPT has recently emerged as a disruptive technology, potentially impacting various societal dimensions, including pharmacy practices. In Thailand, community pharmacists are navigating transitions as patients increasingly rely on digital tools for healthcare recommendations. This study explores the attitudes of community pharmacists in Hatyai, one of Thailand's most populated cities, towards the integration of ChatGPT in pharmacy services. Method: ChatGPT-3.5 was used to generate responses to three questions concerning the use of medicine in special populations in the Thai language. These responses were then incorporated into a questionnaire and evaluated using a Likert scale from 1 to 5. Participants who consented were asked to rate the responses and participate in an in-depth interview. Results: The majority of participants rated the responses favorably, with scores of 4 and 5 accounting for at least 60% of the ratings. Only a small proportion of responses received doubtful ratings (score of 3) or was in disagreement, ranging from 20% to 40%. Moreover, open opinions extracted from the interviews suggested that participants viewed ChatGPT as a capable assistant, as it provided fast yet reasonably accurate information in the Thai language. Conclusion: The findings indicate that community pharmacists view ChatGPT as a capable assistant, albeit noting the need for further refinements. The study underscores the importance for pharmacists to proactively adapt to technological advancements, particularly those affecting patient safety, to enhance healthcare delivery and optimize treatment outcomes. © The Author(s) 2024.",yes,"This study evaluates ChatGPT-3.5's ability to answer clinical questions about medicine use in special populations, with community pharmacists rating the clinical relevance and accuracy of its responses. The focus is on ChatGPT's clinical application in pharmacy practice, not just attitudes or opinions. Therefore, it meets the inclusion criteria.",10.1177/20552076241283256,II,Sully,,,
Evaluating the Performance of Artificial Intelligence in Generating Differential Diagnoses for Infectious Diseases Cases: A Comparative Study of Large Language Models,"Background: Artificial Intelligence (AI) has potential to transform healthcare including the field of infectious diseases diagnostics. This study assesses the capability of three large language models (LLMs), GPT 4, Llama 3, and Gemini 1.5 to generate differential diagnoses, comparing their outputs against those of medical experts to evaluate AI’s potential in augmenting clinical decision-making. Methods: This study evaluates the differential diagnosis capabilities of three LLMs, GPT 4, Llama 3, and Gemini 1.5, using 50 simulated infectious disease cases. The cases were diverse, complex, and reflective of common clinical scenarios, including detailed histories, symptoms, lab results, and imaging findings. Each model received standardized case information and produced differential diagnoses, which were then compared to reference differential diagnosis lists created by medical experts. The analysis utilized the Jaccard index and Kendall’s Tau to assess similarity and order accuracy, summarizing findings with mean, standard deviation, and combined p-values. Results: The mean numbers of differential diagnoses generated by GPT 4, Llama 3, and Gemini 1.5 were 6.22, 5.06, and 10.02 respectively which was significantly different (p<0.001) from the medical experts. The mean Jaccard index of GPT 4, Llama 3, and Gemini 1.5 were 0.3, 0.21, and 0.24 while the mean Kendall’s Tau were 0.4, 0.7, and 0.33 respectively. The combined p-value of GPT 4, Llama 3, and Gemini 1.5 were 1, 1, 0.979 respectively indicating no significant association between the differential diagnosis generated by the LLMs and the medical experts. Conclusion: Although LLMs like GPT 4, Llama 3, and Gemini 1.5 exhibit varying effectiveness, none align significantly with expert-level diagnostic accuracy, emphasizing the need for further development and refinement. The findings highlight the importance of rigorous validation, ethical considerations, and seamless integration into clinical workflows to ensure AI tools enhance healthcare delivery and patient outcomes effectively.",yes,"This study directly evaluates the clinical performance of three frontier LLMs (GPT-4, Llama 3, Gemini 1.5) in generating differential diagnoses for infectious disease cases, comparing their outputs to those of medical experts. The focus is on a core clinical task (differential diagnosis), and the models assessed are all generative LLMs. This fits the inclusion criteria perfectly.",10.1101/2024.06.28.24309694,II,Sully,,,
Bots in white coats: are large language models the future of patient education? A multicenter cross-sectional analysis,"OBJECTIVES: Every year, around 300 million surgeries are conducted worldwide, with an estimated 4.2 million deaths occurring within 30 days after surgery. Adequate patient education is crucial, but often falls short due to the stress patients experience before surgery. Large language models (LLMs) can significantly enhance this process by delivering thorough information and addressing patient concerns that might otherwise go unnoticed. MATERIAL AND METHODS: This cross-sectional study evaluated Chat Generative Pretrained Transformer-4o's audio-based responses to frequently asked questions (FAQs) regarding six general surgical procedures. Three experienced surgeons and two senior residents formulated seven general and three procedure-specific FAQs for both preoperative and postoperative situations, covering six surgical scenarios (major: pancreatic head resection, rectal resection, total gastrectomy; minor: cholecystectomy, Lichtenstein procedure, hemithyroidectomy). In total, 120 audio responses were generated, transcribed, and assessed by 11 surgeons from 6 different German university hospitals. RESULTS: ChatGPT-4o demonstrated strong performance, achieving an average score of 4.12/5 for accuracy, 4.46/5 for relevance, and 0.22/5 for potential harm across 120 questions. Postoperative responses surpassed preoperative ones in both accuracy and relevance, while also exhibiting lower potential for harm. Additionally, responses related to minor surgeries were minimal, but significantly more accurate compared to those for major surgeries. CONCLUSIONS: This study underscores GPT-4o's potential to enhance patient education both before and after surgery by delivering accurate and relevant responses to FAQs about various surgical procedures. Responses regarding the postoperative course proved to be more accurate and less harmful than those addressing preoperative ones. Although a few responses carried moderate risks, the overall performance was robust, indicating GPT-4o's value in patient education. The study suggests the development of hospital-specific applications or the integration of GPT-4o into interactive robotic systems to provide patients with reliable, immediate answers, thereby improving patient satisfaction and informed decision-making. Copyright © 2025 The Author(s). Published by Wolters Kluwer Health, Inc.",yes,"This study evaluates ChatGPT-4o's performance in answering patient FAQs about surgical procedures, with clinical experts rating the accuracy, relevance, and potential harm of its responses. The focus is on the LLM's ability to provide patient education in a clinical context, which is a direct clinical application relevant to healthcare.",10.1097/JS9.0000000000002250,II,Sully,,,
Assessing the Potential Role of Artificial Intelligence in Medication-Related Osteonecrosis of the Jaw Information Sharing,"BACKGROUND: Artificial Intelligence, by answering questions about disease 
prevention strategies, can contribute to making diseases more treatable in their 
early stages.
PURPOSE: This study aims to evaluate the quality of patient information by 
assessing the responses of the Chat Generative Pretrained Transformer (ChatGPT, 
Open AI, USA) artificial intelligence model to questions related to 
medication-related osteonecrosis of the jaw (MRONJ).
STUDY DESIGN, SETTING, SAMPLE: The study was prospective cross-sectional design. 
The study was conducted within the Department of Oral and Maxillofacial Surgery. 
The study's questions were prepared by an experienced oral and maxillofacial 
surgeon and directed to the artificial intelligence platform. The responses were 
evaluated by oral and maxillofacial surgeons using the Global Quality Scale 
(GQS).
PREDICTOR VARIABLE: The predictor variable is question type. A total of 120 
questions were categorized into six groups, which encompassed general 
information about MRONJ (Group 1), queries from patients about to initiate 
medication therapy (Group 2), questions from patients currently undergoing 
medication treatment (Group 3), inquiries from patients who had completed 
medication usage (Group 4), general treatment-related information (Group 5), and 
case scenarios (Group 6).
MAIN OUTCOME VARIABLES: The main variable is the GQS score. The GQS rates the 
quality of information and its utility for the patients. The scores are as 
follows: Score 1: Poor quality, Score 2: Generally poor quality, Score 3: 
Moderate quality, Score 4: Good quality, Score 5: Excellent quality.
COVARIATES: Not applicable.
ANALYSES: Kruskal-Wallis and Mann-Whitney U tests were applied for intragroup 
and intergroup analyses. The statistical significance level was determined as 
P < .05 and P < .01.
RESULTS: The average score for all questions was calculated to be 3.9 ± 0.8, 
which is above the ""moderate quality"" threshold. Group 1 had a mean score of 
3.4 ± 1.1; group 2 had 4.1 ± 0.7; group 3 had 3.8 ± 0.8; group 4 had 4.3 ± 0.6; 
group 5 had 4.2 ± 0.7; and group 6 had 4.1 ± 0.5. The variations in mean scores 
among these groups did not exhibit statistical significance (P > .05).
CONCLUSION AND RELEVANCE: The artificial intelligence model has generated 
responses of moderate quality to questions about MRONJ. The use of the 
artificial intelligence platform may assist in patients gaining a fundamental 
understanding of MRONJ.

Copyright © 2024 American Association of Oral and Maxillofacial Surgeons. 
Published by Elsevier Inc. All rights reserved.",yes,"This study evaluates ChatGPT's ability to answer patient questions about medication-related osteonecrosis of the jaw, assessing the clinical quality of its responses. It directly tests an LLM in a clinical information-sharing context relevant to patient care.",10.1016/j.joms.2024.03.001,II,Sully,,,
Automated Evaluation of Antibiotic Prescribing Guideline Concordance in Pediatric Sinusitis Clinical Notes,"Background: Ensuring antibiotics are prescribed only when necessary is crucial for maintaining their effectiveness and is a key focus of public health initiatives worldwide. In cases of sinusitis, among the most common reasons for antibiotic prescriptions in children, healthcare providers must distinguish between bacterial and viral causes based on clinical signs and symptoms. However, due to the overlap between symptoms of acute sinusitis and viral upper respiratory infections, antibiotics are often over-prescribed. Objectives: Currently, there are no electronic health record (EHR)-based methods, such as lab tests or ICD-10 codes, to retroactively assess the appropriateness of these prescriptions, making manual chart reviews the only available method for evaluation, which is time-intensive and not feasible at a large scale. In this study, we propose using natural language processing to automate this assessment. Methods: We developed, trained, and evaluated generative models to classify the appropriateness of antibiotic prescriptions in 300 clinical notes from pediatric patients with sinusitis seen at a primary care practice in the Children’s Hospital of Philadelphia network. We utilized standard prompt engineering techniques, including few-shot learning and chain-of-thought prompting, to refine an initial prompt. Additionally, we employed Parameter-Efficient Fine-Tuning to train a medium-sized generative model Llama 3 70B-instruct. Results: While parameter-efficient fine-tuning did not enhance performance, the combination of few-shot learning and chain-of-thought prompting proved beneficial. Our best results were achieved using the largest generative model publicly available to date, the Llama 3.1 405B-instruct. On our test set, the model correctly identified 91.4% of the 35 notes where antibiotic prescription was appropriate and 71.4% of the 14 notes where it was not appropriate. However, notes that were insufficiently, vaguely, or ambiguously documented by physicians posed a challenge to our model, as none evaluation sets were accurately classified. Conclusion: Our generative model demonstrated strong performance in the challenging task of chart review. This level of performance may be sufficient for deploying the model within the EHR, where it can assist physicians in real-time to prescribe antibiotics in concordance with the guidelines, or for monitoring antibiotic stewardship on a large scale.",yes,"This study evaluates generative LLMs (specifically Llama 3 and Llama 3.1) for a clinical application: assessing the appropriateness of antibiotic prescriptions in pediatric sinusitis based on clinical notes. The LLM is used to automate chart review, a task directly relevant to clinical decision support and antibiotic stewardship. The study is not a review, editorial, or technical architecture paper, and the LLM is applied in a clinical context.",10.1101/2024.08.09.24311714,I,Sully,,,
Performance evaluation of ChatGPT-4.0 and Gemini on image-based neurosurgery board practice questions: A comparative analysis,"Introduction: Artificial intelligence (AI) has gained significant attention in medicine, particularly in neurosurgery, where its potential is frequently discussed and occasionally feared. Large language models (LLMs), such as ChatGPT-4.0 (OpenAI) and Gemini (Google DeepMind), have shown promise in text-based tasks but remain underexplored in image-based domains, which are essential for neurosurgery. This study evaluates the performance of ChatGPT-4.0 and Gemini on image-based neurosurgery board practice questions, focusing on their ability to interpret visual data, a critical aspect of neurosurgical decision-making. Methods: A total of 250 image-based questions selected from two neurosurgical review textbooks were obtained. Each question was presented to both ChatGPT-4.0 and Gemini in its original format, including images such as MRI scans, pathology slides, and surgical visuals. The models were tasked with answering the questions, and their accuracy was determined based on the number of correct responses. Results: ChatGPT-4.0 correctly answered 84 questions (33.6 %), significantly outperforming Gemini, which answered only 1 question correctly (0.4 %) (p < 0.0001). ChatGPT-4.0 provided correct answers for 17.7 % of questions from The Comprehensive Neurosurgery Board Preparation Book and 50.0 % from Neurosurgery Board Review. Gemini exhibited a 17.8 % “inability response” rate, explicitly stating it could not interpret images. The performance gap between the two models was significant (p < 0.0001), highlighting their limitations in handling complex visual data. Conclusions: While ChatGPT-4.0 demonstrated some capacity to interpret image-based neurosurgery board questions, both models exhibited significant limitations, particularly in processing and analyzing complex visual data. These findings emphasize the need for targeted advancements in AI to improve visual interpretation in neurosurgical education and practice. © 2025 Elsevier Ltd",yes,"This study evaluates the clinical performance of two frontier LLMs (ChatGPT-4.0 and Gemini) on image-based neurosurgery board practice questions, which are directly relevant to clinical decision-making and medical education. The focus is on their ability to interpret visual data in a clinical context, making it a clinical evaluation of LLMs.",10.1016/j.jocn.2025.111097,III,Sully,,,
Optimizing athletic performance through advanced nutrition strategies: can AI and digital platforms have a role in ultra-endurance sports?,"Nutrition is vital for athletic performance, especially in ultra-endurance sports, which pose unique nutritional challenges. Despite its importance, there exist gaps in the nutrition knowledge among athletes, and emerging digital tools could potentially bridge this gap. The ULTRA-Q, a sports nutrition questionnaire adapted for ultra-endurance athletes, was used to assess the nutritional knowledge of ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Copilot. Their performance was compared with experienced ultra-endurance athletes, registered sports nutritionists and dietitians, and the general population. ChatGPT-4 demonstrated the highest accuracy (93%), followed by Microsoft Copilot (92%), Bard (84%), and ChatGPT-3.5 (83%). The averaged AI model achieved an overall score of 88%, with the highest score in Body Composition (94%) and the lowest in Nutrients (84%). The averaged AI model outperformed the general population by 31% points and ultra-endurance athletes by 20% points in overall knowledge. The AI model exhibited superior knowledge in Fluids, outperforming registered dietitians by 49% points, the general population by 42% points, and ultra-endurance athletes by 32% points. In Body Composition, the AI model surpassed the general population by 31% points and ultra-endurance athletes by 24% points. In Supplements, it outperformed registered dietitians by 58% points and the general population by 55% points. Finally, in Nutrients and in Recovery, it outperformed the general population only, by 24% and 29% points, respectively. AI models show high proficiency in sports nutrition knowledge, potentially serving as valuable tools for nutritional education and advice. AI-generated insights could be integrated with expert human judgment for effective athlete performance optimization. © 2024 Institute of Sport. All rights reserved.",yes,"This study evaluates the performance of several LLMs (ChatGPT-3.5, ChatGPT-4, Bard, Copilot) on a sports nutrition questionnaire relevant to ultra-endurance athletes, comparing their knowledge to that of athletes, dietitians, and the general population. The focus is on the LLMs' ability to answer nutrition-related questions, which could inform clinical or practical advice for athletes. While the primary task is knowledge assessment rather than direct clinical decision-making, the context is healthcare-related (nutrition advice for athletes), and the LLMs are being evaluated for their potential to provide clinical or educational support.",10.5114/biolsport.2024.141063,III,Sully,,,
Using artificial intelligence to generate medical literature for urology patients: a comparison of three different large language models,"Purpose: Large language models (LLMs) are a form of artificial intelligence (AI) that uses deep learning techniques to understand, summarize and generate content. The potential benefits of LLMs in healthcare is predicted to be immense. The objective of this study was to examine the quality of patient information leaflets (PILs) produced by 3 LLMs on urological topics. Methods: Prompts were created to generate PILs from 3 LLMs: ChatGPT-4, PaLM 2 (Google Bard) and Llama 2 (Meta) across four urology topics (circumcision, nephrectomy, overactive bladder syndrome, and transurethral resection of the prostate). PILs were evaluated using a quality assessment checklist. PIL readability was assessed by the Average Reading Level Consensus Calculator. Results: PILs generated by PaLM 2 had the highest overall average quality score (3.58), followed by Llama 2 (3.34) and ChatGPT-4 (3.08). PaLM 2 generated PILs were of the highest quality in all topics except TURP and was the only LLM to include images. Medical inaccuracies were present in all generated content including instances of significant error. Readability analysis identified PaLM 2 generated PILs as the simplest (age 14–15 average reading level). Llama 2 PILs were the most difficult (age 16–17 average). Conclusion: While LLMs can generate PILs that may help reduce healthcare professional workload, generated content requires clinician input for accuracy and inclusion of health literacy aids, such as images. LLM-generated PILs were above the average reading level for adults, necessitating improvement in LLM algorithms and/or prompt design. How satisfied patients are to LLM-generated PILs remains to be evaluated. © Crown 2024.",yes,"This study evaluates the quality and readability of patient information leaflets (PILs) generated by three frontier LLMs (ChatGPT-4, PaLM 2/Bard, Llama 2) on urological topics. The focus is on the clinical utility of LLM-generated materials for patient education, which is a direct clinical application relevant to healthcare. The study does not merely assess technical aspects or non-clinical tasks, but rather the potential of LLMs to support patient understanding and communication in a medical context.",10.1007/s00345-024-05146-3,II,Sully,,,
Do ChatGPT and Google differ in answers to commonly asked patient questions regarding total shoulder and total elbow arthroplasty?,"BACKGROUND: Artificial intelligence (AI) and large language models (LLMs) offer 
a new potential resource for patient education. The answers by Chat Generative 
Pre-Trained Transformer (ChatGPT), a LLM AI text bot, to frequently asked 
questions (FAQs) were compared to answers provided by a contemporary Google 
search to determine the reliability of information provided by these sources for 
patient education in upper extremity arthroplasty.
METHODS: ""Total shoulder arthroplasty"" (TSA) and ""total elbow arthroplasty"" 
(TEA) were entered into Google Search and ChatGPT 3.0 to determine the ten most 
FAQs. On Google, the FAQs were obtained through the ""people also ask"" section, 
while ChatGPT was asked to provide the ten most FAQs. Each question, answer, and 
reference(s) cited were recorded. A modified version of the Rothwell system was 
used to categorize questions into 10 subtopics: special activities, timeline of 
recovery, restrictions, technical details, cost, indications/management, risks 
and complications, pain, longevity, and evaluation of surgery. Each reference 
was categorized into the following groups: commercial, academic, medical 
practice, single surgeon personal, or social media. Questions for TSA and TEA 
were combined for analysis and compared between Google and ChatGPT with a 2 
sample Z-test for proportions.
RESULTS: Overall, most questions were related to procedural indications or 
management (17.5%). There were no significant differences between Google and 
ChatGPT between question categories. The majority of references were from 
academic websites (65%). ChatGPT produced a greater number of academic 
references compared to Google (80% vs. 50%; P = .047), while Google more 
commonly provided medical practice references (25% vs. 0%; P = .017).
CONCLUSION: In conjunction with patient-physician discussions, AI LLMs may 
provide a reliable resource for patients. By providing information based on 
academic references, these tools have the potential to improve health literacy 
and improved shared decision making for patients searching for information about 
TSA and TEA.
CLINICAL SIGNIFICANCE: With the rising prevalence of AI programs, it is 
essential to understand how these applications affect patient education in 
medicine.

Copyright © 2024 Journal of Shoulder and Elbow Surgery Board of Trustees. 
Published by Elsevier Inc. All rights reserved.",yes,"This study evaluates ChatGPT (an LLM) in the context of answering patient questions about total shoulder and elbow arthroplasty, directly assessing its clinical utility for patient education. The comparison to Google is secondary; the primary focus is on the reliability and content of ChatGPT's answers to clinical FAQs. This fits the inclusion criteria as it tests an LLM in a healthcare context.",10.1016/j.jse.2023.11.014,II,Sully,,,
Can people with epilepsy trust AI chatbots for information on physical exercise?,"Purpose: This study aims to evaluate the similarity, readability, and alignment with current scientific knowledge of responses from AI-based chatbots to common questions about epilepsy and physical exercise. Methods: Four AI chatbots (ChatGPT-3.5,ChatGPT 4, Google Gemini, and Microsoft Copilot) were evaluated. Fourteen questions on epilepsy and physical exercise were designed to compare the platforms. Lexical similarity, response patterns, and thematic content were analyzed. Readability was measured using the Flesch Reading Ease and Flesch–Kincaid Grade Level scores. Seven experts rated the quality of responses on a Likert scale from “very poor” to “very good.” Results: The responses showed lexical similarity, with approaches to physical exercise ranging from conservative to holistic. Microsoft Copilot scored the highest on the Flesch Reading Ease scale (48.42 ± 13.71), while ChatGPT-3.5 scored the lowest (23.84 ± 8.19). All responses were generally rated as difficult to read. Quality ratings ranged from “Good” to “Acceptable,” with ChatGPT 4 being the preferred platform, chosen by 48.98 % of reviewers. Conclusion: The findings highlight the potential of AI chatbots as useful sources of information on epilepsy and physical exercise. However, simplifying language and tailoring content to user's needs is essential to enhance their effectiveness. © 2024 The Author(s)",yes,"This study evaluates multiple LLM-based chatbots (ChatGPT-3.5, ChatGPT-4, Gemini, Copilot) in their ability to answer patient questions about epilepsy and physical exercise, including expert assessment of answer quality. This is a clinical application relevant to patient information and decision support.",10.1016/j.yebeh.2024.110193,II,Sully,,,
A Multidisciplinary Assessment of ChatGPT’s Knowledge of Amyloidosis: Observational Study,"Background: Amyloidosis, a rare multisystem condition, often requires complex, multidisciplinary care. Its low prevalence underscores the importance of efforts to ensure the availability of high-quality patient education materials for better outcomes. ChatGPT (OpenAI) is a large language model powered by artificial intelligence that offers a potential avenue for disseminating accurate, reliable, and accessible educational resources for both patients and providers. Its user-friendly interface, engaging conversational responses, and the capability for users to ask follow-up questions make it a promising future tool in delivering accurate and tailored information to patients. Objective: We performed a multidisciplinary assessment of the accuracy, reproducibility, and readability of ChatGPT in answering questions related to amyloidosis. Methods: In total, 98 amyloidosis questions related to cardiology, gastroenterology, and neurology were curated from medical societies, institutions, and amyloidosis Facebook support groups and inputted into ChatGPT-3.5 and ChatGPT-4. Cardiology- and gastroenterology-related responses were independently graded by a board-certified cardiologist and gastroenterologist, respectively, who specialize in amyloidosis. These 2 reviewers (RG and DCK) also graded general questions for which disagreements were resolved with discussion. Neurology-related responses were graded by a board-certified neurologist (AAH) who specializes in amyloidosis. Reviewers used the following grading scale: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and (4) completely incorrect. Questions were stratified by categories for further analysis. Reproducibility was assessed by inputting each question twice into each model. The readability of ChatGPT-4 responses was also evaluated using the Textstat library in Python (Python Software Foundation) and the Textstat readability package in R software (R Foundation for Statistical Computing). Results: ChatGPT-4 (n=98) provided 93 (95%) responses with accurate information, and 82 (84%) were comprehensive. ChatGPT-3.5 (n=83) provided 74 (89%) responses with accurate information, and 66 (79%) were comprehensive. When examined by question category, ChatGTP-4 and ChatGPT-3.5 provided 53 (95%) and 48 (86%) comprehensive responses, respectively, to “general questions” (n=56). When examined by subject, ChatGPT-4 and ChatGPT-3.5 performed best in response to cardiology questions (n=12) with both models producing 10 (83%) comprehensive responses. For gastroenterology (n=15), ChatGPT-4 received comprehensive grades for 9 (60%) responses, and ChatGPT-3.5 provided 8 (53%) responses. Overall, 96 of 98 (98%) responses for ChatGPT-4 and 73 of 83 (88%) for ChatGPT-3.5 were reproducible. The readability of ChatGPT-4’s responses ranged from 10th to beyond graduate US grade levels with an average of 15.5 (SD 1.9). Conclusions: Large language models are a promising tool for accurate and reliable health information for patients living with amyloidosis. However, ChatGPT’s responses exceeded the American Medical Association’s recommended fifth- to sixth-grade reading level. Future studies focusing on improving response accuracy and readability are warranted. Prior to widespread implementation, the technology’s limitations and ethical implications must be further explored to ensure patient safety and equitable implementation. © Ryan C King, Jamil S Samaan, Yee Hui Yeo, Yuxin Peng, David C Kunkel, Ali A Habib, Roxana Ghashghaei.",yes,"This study evaluates ChatGPT-3.5 and ChatGPT-4's ability to answer clinical questions about amyloidosis, assessing accuracy, reproducibility, and readability. The questions are clinically relevant and responses are graded by medical specialists, making this a direct clinical evaluation of LLMs in a healthcare context.",10.2196/53421,II,Sully,,,
Testing and Validation of a Custom Retrained Large Language Model for the Supportive Care of HN Patients with External Knowledge Base,"Purpose: This study aimed to develop a retrained large language model (LLM) tailored to the needs of HN cancer patients treated with radiotherapy, with emphasis on symptom management and survivorship care. Methods: A comprehensive external database was curated for training ChatGPT-4, integrating expert-identified consensus guidelines on supportive care for HN patients and correspondences from physicians and nurses within our institution’s electronic medical records for 90 HN patients. The performance of our model was evaluated using 20 patient post-treatment inquiries that were then assessed by three Board certified radiation oncologists (RadOncs). The rating of the model was assessed on a scale of 1 (strongly disagree) to 5 (strongly agree) based on accuracy, clarity of response, completeness s, and relevance. Results: The average scores for the 20 tested questions were 4.25 for accuracy, 4.35 for clarity, 4.22 for completeness, and 4.32 for relevance, on a 5-point scale. Overall, 91.67% (220 out of 240) of assessments received scores of 3 or higher, and 83.33% (200 out of 240) received scores of 4 or higher. Conclusion: The custom-trained model demonstrates high accuracy in providing support to HN patients offering evidence-based information and guidance on their symptom management and survivorship care. © 2024 by the authors.",yes,"This study evaluates a retrained ChatGPT-4 LLM for providing clinical supportive care and survivorship guidance to head and neck cancer patients, with performance assessed by board-certified oncologists on real patient inquiries. This is a direct clinical application of an LLM in healthcare.",10.3390/cancers16132311,I,Sully,,,
Leveraging generative AI to prioritize drug repurposing candidates for Alzheimer’s disease with real-world clinical validation,"Drug repurposing represents an attractive alternative to the costly and time-consuming process of new drug development, particularly for serious, widespread conditions with limited effective treatments, such as Alzheimer’s disease (AD). Emerging generative artificial intelligence (GAI) technologies like ChatGPT offer the promise of expediting the review and summary of scientific knowledge. To examine the feasibility of using GAI for identifying drug repurposing candidates, we iteratively tasked ChatGPT with proposing the twenty most promising drugs for repurposing in AD, and tested the top ten for risk of incident AD in exposed and unexposed individuals over age 65 in two large clinical datasets: (1) Vanderbilt University Medical Center and (2) the All of Us Research Program. Among the candidates suggested by ChatGPT, metformin, simvastatin, and losartan were associated with lower AD risk in meta-analysis. These findings suggest GAI technologies can assimilate scientific insights from an extensive Internet-based search space, helping to prioritize drug repurposing candidates and facilitate the treatment of diseases. © The Author(s) 2024.",yes,"This study evaluates ChatGPT (a generative LLM) in a clinical context by tasking it to propose drug repurposing candidates for Alzheimer's disease, and then tests these suggestions in real-world clinical datasets. The LLM is used for a clinically relevant task (prioritizing drugs for disease treatment), and the results are validated with clinical data, fitting the inclusion criteria.",10.1038/s41746-024-01038-3,I,Sully,,,
Artificial Intelligence Chatbot Performance in Triage of Ophthalmic Conditions,"Importance Access to human expertise for affordable and efficient triage of ophthalmic conditions is inconsistent. With recent advancements in publicly available artificial intelligence (AI) chatbots, individuals may turn to these tools for triage of ophthalmic complaints. Objective To evaluate the triage performance of AI chatbots for ophthalmic conditions Design Cross-sectional study Setting Single center Participants Ophthalmology trainees, OpenAI ChatGPT (GPT-4), Bing Chat, and WebMD Symptom Checker Methods Forty-four clinical vignettes representing common ophthalmic complaints were developed, and a standardized pathway of prompts was presented to each tool in March 2023. Primary outcomes were proportion of responses with correct diagnosis listed in the top three possible diagnoses and proportion with correct triage urgency. Ancillary outcomes included presence of grossly inaccurate statements, mean reading grade level, mean response word count, proportion with attribution, most common sources cited, and proportion with a disclaimer regarding chatbot limitations in dispensing medical advice. Results The physician respondents, ChatGPT, Bing Chat, and WebMD listed the appropriate diagnosis among the top three suggestions in 42 (95%), 41 (93%), 34 (77%), and 8 (33%) cases, respectively. Triage urgency was appropriate in 38 (86%), 43 (98%), and 37 (84%) cases for the physicians, ChatGPT, and Bing Chat, correspondingly. Conclusions and Relevance ChatGPT using the GPT-4 model offered high diagnostic and triage accuracy that was comparable to the physician respondents, with no grossly inaccurate statements. Bing Chat had lower accuracy, some instances of grossly inaccurate statements, and a tendency to overestimate triage urgency.",yes,"This study evaluates the clinical performance of LLM-based chatbots (ChatGPT GPT-4 and Bing Chat) in triaging ophthalmic conditions using clinical vignettes, comparing their diagnostic and triage accuracy to physicians. This is a direct clinical application of LLMs in healthcare.",10.1101/2023.06.11.23291247,II,Sully,,,
Assessing the performance of zero-shot visual question answering in multimodal large language models for 12-lead ECG image interpretation,"Large Language Models (LLM) are increasingly multimodal, and Zero-Shot Visual Question Answering (VQA) shows promise for image interpretation. If zero-shot VQA can be applied to a 12-lead electrocardiogram (ECG), a prevalent diagnostic tool in the medical field, the potential benefits to the field would be substantial. This study evaluated the diagnostic performance of zero-shot VQA with multimodal LLMs on 12-lead ECG images. The results revealed that multimodal LLM tended to make more errors in extracting and verbalizing image features than in describing preconditions and making logical inferences. Even when the answers were correct, erroneous descriptions of image features were common. These findings suggest a need for improved control over image hallucination and indicate that performance evaluation using the percentage of correct answers to multiple-choice questions may not be sufficient for performance assessment in VQA tasks. 2025 Seki, Kawazoe, Ito, Akagi, Takiguchi and Ohe.",yes,"This study evaluates the diagnostic performance of multimodal LLMs (with visual question answering capabilities) on interpreting 12-lead ECG images, a direct clinical application. It assesses the LLMs' ability to extract and verbalize image features relevant to diagnosis, which is a clinical task.",10.3389/fcvm.2025.1458289,II,Sully,,,
Artificial Intelligence-Generated Draft Replies to Patient Inbox Messages,"3573. JAMA Netw Open. 2024 Mar 4;7(3):e243201. doi: 10.1001/jamanetworkopen.2024.3201.

Artificial Intelligence-Generated Draft Replies to Patient Inbox Messages.

Garcia P(1), Ma SP(1), Shah S(1)(2), Smith M(2), Jeong Y(2), Devon-Sand A(2), 
Tai-Seale M(3), Takazawa K(4), Clutter D(4), Vogt K(4), Lugtu C(5), Rojo M(4), 
Lin S(1)(2), Shanafelt T(1)(6), Pfeffer MA(1)(4), Sharp C(1).

Author information:
(1)Department of Medicine, Stanford University School of Medicine, Stanford, 
California.
(2)Stanford Healthcare AI Applied Research Team, Division of Primary Care and 
Population Health, Stanford University School of Medicine, Stanford, California.
(3)Department of Family Medicine, University of California San Diego School of 
Medicine, La Jolla.
(4)Technology and Digital Solutions, Stanford Medicine, Stanford, California.
(5)Nursing Informatics & Innovation, Stanford Healthcare, Stanford, California.
(6)WellMD Center, Stanford University School of Medicine, Stanford, California.

IMPORTANCE: The emergence and promise of generative artificial intelligence (AI) 
represent a turning point for health care. Rigorous evaluation of generative AI 
deployment in clinical practice is needed to inform strategic decision-making.
OBJECTIVE: To evaluate the implementation of a large language model used to 
draft responses to patient messages in the electronic inbox.
DESIGN, SETTING, AND PARTICIPANTS: A 5-week, prospective, single-group quality 
improvement study was conducted from July 10 through August 13, 2023, at a 
single academic medical center (Stanford Health Care). All attending physicians, 
advanced practice practitioners, clinic nurses, and clinical pharmacists from 
the Divisions of Primary Care and Gastroenterology and Hepatology were enrolled 
in the pilot.
INTERVENTION: Draft replies to patient portal messages generated by a Health 
Insurance Portability and Accountability Act-compliant electronic health 
record-integrated large language model.
MAIN OUTCOMES AND MEASURES: The primary outcome was AI-generated draft reply 
utilization as a percentage of total patient message replies. Secondary outcomes 
included changes in time measures and clinician experience as assessed by 
survey.
RESULTS: A total of 197 clinicians were enrolled in the pilot; 35 clinicians who 
were prepilot beta users, out of office, or not tied to a specific ambulatory 
clinic were excluded, leaving 162 clinicians included in the analysis. The 
survey analysis cohort consisted of 73 participants (45.1%) who completed both 
the presurvey and postsurvey. In gastroenterology and hepatology, there were 58 
physicians and APPs and 10 nurses. In primary care, there were 83 physicians and 
APPs, 4 nurses, and 8 clinical pharmacists. The mean AI-generated draft response 
utilization rate across clinicians was 20%. There was no change in reply action 
time, write time, or read time between the prepilot and pilot periods. There 
were statistically significant reductions in the 4-item physician task load 
score derivative (mean [SD], 61.31 [17.23] presurvey vs 47.26 [17.11] 
postsurvey; paired difference, -13.87; 95% CI, -17.38 to -9.50; P < .001) and 
work exhaustion scores (mean [SD], 1.95 [0.79] presurvey vs 1.62 [0.68] 
postsurvey; paired difference, -0.33; 95% CI, -0.50 to -0.17; P < .001).
CONCLUSIONS AND RELEVANCE: In this quality improvement study of an early 
implementation of generative AI, there was notable adoption, usability, and 
improvement in assessments of burden and burnout. There was no improvement in 
time. Further code-to-bedside testing is needed to guide future development and 
organizational strategy.",yes,"This study evaluates the implementation of a large language model (LLM) to draft responses to patient messages in a clinical setting, measuring adoption, usability, and clinician experience. The LLM is used for a clinical communication task (replying to patient messages), and the outcomes are relevant to clinical workflow and patient care. This fits the inclusion criteria as it assesses a generative LLM in a healthcare context.",10.1001/jamanetworkopen.2024.3201,I,Sully,,,
Text dialogue analysis Based ChatGPT for Primary Screening of Mild Cognitive Impairment,"Background: AI models tailored to diagnose cognitive impairment have shown excellent results. However, it is unclear whether large linguistic models can rival specialized models by text alone. Objectives: We would explore the effectiveness of ChatGPT for primary screening of mild cognitive impairment (MCI) and standardize the design steps and components of the prompt. Methods: We obtained 174 participants from the DementiaBank screening and classified 70% of them into the training set and 30% into the test set. Three dimensions of variables were incorporated, including: vocabulary, syntax and grammar, and semantics. These variables were generated from published studies and statistical analyses. We used R 4.3.0. for the analysis of variables and diagnostic indicators. Results: The final variables included by published studies included: word frequency and word ratio, phrase frequency and phrase ratio, lexical complexity, syntactic complexity, grammatical components, semantic density, and semantic coherence; variables included in the analysis included: tip-of-the-tongue phenomenon (P < 0.001), difficulty with complex ideas (P < 0.001), and memory issues (P < 0.001). The final GPT4 model achieved the sensitivity (SEN) of 0.8636, specificity (SPE) of 0.9487 and area under the curve (AUC) of 0.9062 on the training set; on the test set, the SEN, SPE and AUC reached 0.7727, 0.8333 and 0.8030, respectively. The prompt consisted of five main parts, including character setting, scoring system setting, indicator setting, output setting, and explanatory information setting. Conclusion: ChatGPT was effective in primary screening of participants with possible MCI. Improved standardization of prompts by professional clinicians would also improve the performance of the model. It is important to note that ChatGPT is not a substitute for a clinician making a diagnosis.",yes,"This study evaluates ChatGPT (an LLM) for the clinical task of primary screening for mild cognitive impairment (MCI) using text dialogue analysis. The model's diagnostic performance is assessed, and the application is clearly clinical. The study is not a review, editorial, or technical architecture paper, and it does not focus on non-clinical tasks.",10.1101/2023.06.27.23291884,I,Sully,,,
Comparing generative and retrieval-based chatbots in answering patient questions regarding age-related macular degeneration and diabetic retinopathy,"Background/aims To compare the performance of generative versus retrieval-based chatbots in answering patient inquiries regarding age-related macular degeneration (AMD) and diabetic retinopathy (DR). Methods We evaluated four chatbots: generative models (ChatGPT-4, ChatGPT-3.5 and Google Bard) and a retrieval-based model (OcularBERT) in a cross-sectional study. Their response accuracy to 45 questions (15 AMD, 15 DR and 15 others) was evaluated and compared. Three masked retinal specialists graded the responses using a three-point Likert scale: either 2 (good, error-free), 1 (borderline) or 0 (poor with significant inaccuracies). The scores were aggregated, ranging from 0 to 6. Based on majority consensus among the graders, the responses were also classified as'Good','Borderline' or'Poor' quality. Results Overall, ChatGPT-4 and ChatGPT-3.5 outperformed the other chatbots, both achieving median scores (IQR) of 6 (1), compared with 4.5 (2) in Google Bard, and 2 (1) in OcularBERT (all p ≤8.4×10−3). Based on the consensus approach, 83.3% of ChatGPT-4's responses and 86.7% of ChatGPT-3.5's were rated as'Good', surpassing Google Bard (50%) and OcularBERT (10%) (all p ≤1.4×10−2). ChatGPT-4 and ChatGPT-3.5 had no'Poor' rated responses. Google Bard produced 6.7% Poor responses, and OcularBERT produced 20%. Across question types, ChatGPT-4 outperformed Google Bard only for AMD, and ChatGPT-3.5 outperformed Google Bard for DR and others. Conclusion ChatGPT-4 and ChatGPT-3.5 demonstrated superior performance, followed by Google Bard and OcularBERT. Generative chatbots are potentially capable of answering domain-specific questions outside their original training. Further validation studies are still required prior to real-world implementation. © Author(s) (or their employer(s)) 2024.",yes,"This study directly evaluates generative LLMs (ChatGPT-4, ChatGPT-3.5, Google Bard) in answering patient questions about specific medical conditions (AMD and DR), comparing their clinical response quality to a retrieval-based model. The focus is on clinical information delivery to patients, which is a relevant healthcare application.",10.1136/bjo-2023-324533,II,Sully,,,
Evaluating AI in medicine: a comparative analysis of expert and ChatGPT responses to colorectal cancer questions,"Colorectal cancer (CRC) is a global health challenge, and patient education plays a crucial role in its early detection and treatment. Despite progress in AI technology, as exemplified by transformer-like models such as ChatGPT, there remains a lack of in-depth understanding of their efficacy for medical purposes. We aimed to assess the proficiency of ChatGPT in the field of popular science, specifically in answering questions related to CRC diagnosis and treatment, using the book ""Colorectal Cancer: Your Questions Answered"" as a reference. In general, 131 valid questions from the book were manually input into ChatGPT. Responses were evaluated by clinical physicians in the relevant fields based on comprehensiveness and accuracy of information, and scores were standardized for comparison. Not surprisingly, ChatGPT showed high reproducibility in its responses, with high uniformity in comprehensiveness, accuracy, and final scores. However, the mean scores of ChatGPT's responses were significantly lower than the benchmarks, indicating it has not reached an expert level of competence in CRC. While it could provide accurate information, it lacked in comprehensiveness. Notably, ChatGPT performed well in domains of radiation therapy, interventional therapy, stoma care, venous care, and pain control, almost rivaling the benchmarks, but fell short in basic information, surgery, and internal medicine domains. While ChatGPT demonstrated promise in specific domains, its general efficiency in providing CRC information falls short of expert standards, indicating the need for further advancements and improvements in AI technology for patient education in healthcare.",yes,"This study evaluates ChatGPT's ability to answer patient questions about colorectal cancer diagnosis and treatment, with responses assessed by clinical experts for accuracy and comprehensiveness. This is a direct clinical application relevant to patient education in healthcare.",10.1038/s41598-024-52853-3,II,Sully,,,
Performance of Generative Large Language Models on Ophthalmology Board-Style Questions,"2268. Am J Ophthalmol. 2023 Oct;254:141-149. doi: 10.1016/j.ajo.2023.05.024. Epub 2023 
Jun 18.

Performance of Generative Large Language Models on Ophthalmology Board-Style 
Questions.

Cai LZ(1), Shaheen A(2), Jin A(3), Fukui R(4), Yi JS(2), Yannuzzi N(2), Alabiad 
C(2).

Author information:
(1)From the Bascom Palmer Eye Institute, Miami, Florida, USA (L.Z.C., A.S., 
J.S.Y., N.Y., C.A.). Electronic address: louiezcai@gmail.com.
(2)From the Bascom Palmer Eye Institute, Miami, Florida, USA (L.Z.C., A.S., 
J.S.Y., N.Y., C.A.).
(3)Yale Eye Center, New Haven, Connecticut, USA (A.J.).
(4)Houston Rehabilitation Group, Houston, Texas, USA (R.F.).

Comment in
    Am J Ophthalmol. 2023 Dec;256:200. doi: 10.1016/j.ajo.2023.07.029.

PURPOSE: To investigate the ability of generative artificial intelligence models 
to answer ophthalmology board-style questions.
DESIGN: Experimental study.
METHODS: This study evaluated 3 large language models (LLMs) with chat 
interfaces, Bing Chat (Microsoft) and ChatGPT 3.5 and 4.0 (OpenAI), using 250 
questions from the Basic Science and Clinical Science Self-Assessment Program. 
Although ChatGPT is trained on information last updated in 2021, Bing Chat 
incorporates a more recently indexed internet search to generate its answers. 
Performance was compared with human respondents. Questions were categorized by 
complexity and patient care phase, and instances of information fabrication or 
nonlogical reasoning were documented.
MAIN OUTCOME MEASURES: Primary outcome was response accuracy. Secondary outcomes 
were performance in question subcategories and hallucination frequency.
RESULTS: Human respondents had an average accuracy of 72.2%. ChatGPT-3.5 scored 
the lowest (58.8%), whereas ChatGPT-4.0 (71.6%) and Bing Chat (71.2%) performed 
comparably. ChatGPT-4.0 excelled in workup-type questions (odds ratio [OR], 
3.89, 95% CI, 1.19-14.73, P = .03) compared with diagnostic questions, but 
struggled with image interpretation (OR, 0.14, 95% CI, 0.05-0.33, P < .01) when 
compared with single-step reasoning questions. Against single-step questions, 
Bing Chat also faced difficulties with image interpretation (OR, 0.18, 95% CI, 
0.08-0.44, P < .01) and multi-step reasoning (OR, 0.30, 95% CI, 0.11-0.84, 
P = .02). ChatGPT-3.5 had the highest rate of hallucinations and nonlogical 
reasoning (42.4%), followed by ChatGPT-4.0 (18.0%) and Bing Chat (25.6%).
CONCLUSIONS: LLMs (particularly ChatGPT-4.0 and Bing Chat) can perform similarly 
with human respondents answering questions from the Basic Science and Clinical 
Science Self-Assessment Program. The frequency of hallucinations and nonlogical 
reasoning suggests room for improvement in the performance of conversational 
agents in the medical domain.

Copyright © 2023 Elsevier Inc. All rights reserved.",yes,"This study evaluates the performance of generative LLMs (ChatGPT-3.5, ChatGPT-4.0, Bing Chat) on ophthalmology board-style questions, directly assessing their clinical knowledge and reasoning in a medical context. This fits the inclusion criteria as it is a clinical evaluation of LLMs' performance on board exams.",10.1016/j.ajo.2023.05.024,III,Sully,,,
Feasibility of large language models for CEUS LI-RADS categorization of small liver nodules in patients at risk for hepatocellular carcinoma,"Background: Large language models (LLMs) offer opportunities to enhance radiological applications, but their performance in handling complex tasks remains insufficiently investigated Purpose: To evaluate the performance of LLMs integrated with Contrast-enhanced Ultrasound Liver Imaging Reporting and Data System (CEUS LI-RADS) in diagnosing small (≤20mm) hepatocellular carcinoma (sHCC) in high-risk patients. Materials and Methods: From November 2014 to December 2023, high-risk HCC patients with untreated small (≤20mm) focal liver lesions (sFLLs), were included in this retrospective study. ChatGPT-4.0, ChatGPT-4o, ChatGPT-4o mini, and Google Gemini were integrated with imaging features from structured CEUS LI-RADS reports to assess their diagnostic performance for sHCC. The diagnostic efficacy of LLMs for small HCC were compared using McNemar test. Results: The final population consisted of 403 high-risk patients (52 years ± 11, 323 men). ChatGPT-4.0 and ChatGPT-4o demonstrated substantial to almost perfect intra-agreement for CEUS LI-RADS categorization (κ values: 0.76-1.0 and 0.7-0.94, respectively), outperforming ChatGPT-4o mini (κ values: 0.51-0.72) and Google Gemini (κ values: -0.04-0.47). ChatGPT-4.0 had higher sensitivity in detecting sHCC than ChatGPT-4o (83%-89% vs. 70%-78%, p < 0.02) with comparable specificity (76%-90% vs. 83%-86%, p > 0.05). Compared to human readers, ChatGPT-4.0 showed superior sensitivity (83%-89% vs. 63%-78%, p < 0.004) and comparable specificity (76%-90% vs. 90%-95%, p > 0.05) in diagnosing sHCC. Conclusion: LLM integrated with CEUS LI-RADS offers potential tool in diagnosing sHCC for high-risk patients. ChatGPT-4.0 demonstrated satisfactory consistency in CEUS LI-RADS categorization, offering higher sensitivity in diagnosing sHCC while maintaining comparable specificity to that of human readers. Copyright © 2024 Huang, Yang, Huang, Zeng, Liu, Luo, Lyshchik and Lu.",yes,"This study evaluates the clinical performance of several LLMs (ChatGPT-4.0, ChatGPT-4o, ChatGPT-4o mini, and Google Gemini) in categorizing liver nodules using CEUS LI-RADS and diagnosing small hepatocellular carcinoma in high-risk patients. The LLMs' diagnostic accuracy is compared to human readers, which is a direct clinical application.",10.3389/fonc.2024.1513608,I,Sully,,,
Enhancing puncture skills training with generative AI and digital technologies: a parallel cohort study,"BACKGROUND: Traditional puncture skills training for refresher doctors faces limitations in effectiveness and efficiency. This study explored the application of generative AI (ChatGPT), templates, and digital imaging to enhance puncture skills training. METHODS: 90 refresher doctors were enrolled sequentially into 3 groups: traditional training; template and digital imaging training; and ChatGPT, template and digital imaging training. Outcomes included theoretical knowledge, technical skills, and trainee satisfaction measured at baseline, post-training, and 3-month follow-up. RESULTS: The ChatGPT group increased theoretical knowledge scores by 17-21% over traditional training at post-training (81.6 ± 4.56 vs. 69.6 ± 4.58, p < 0.001) and follow-up (86.5 ± 4.08 vs. 71.3 ± 4.83, p < 0.001). It also outperformed template training by 4-5% at post-training (81.6 ± 4.56 vs. 78.5 ± 4.65, p = 0.032) and follow-up (86.5 ± 4.08 vs. 82.7 ± 4.68, p = 0.004). For technical skills, the ChatGPT (4.0 ± 0.32) and template (4.0 ± 0.18) groups showed similar scores at post-training, outperforming traditional training (3.6 ± 0.50) by 11% (p < 0.001). At follow-up, ChatGPT (4.0 ± 0.18) and template (4.0 ± 0.32) still exceeded traditional training (3.8 ± 0.43) by 5% (p = 0.071, p = 0.026). Learning curve analysis revealed fastest knowledge (slope 13.02) and skill (slope 0.62) acquisition for ChatGPT group over template (slope 11.28, 0.38) and traditional (slope 5.17, 0.53). ChatGPT responses showed 100% relevance, 50% completeness, 60% accuracy, with 15.9 s response time. For training satisfaction, ChatGPT group had highest scores (4.2 ± 0.73), over template (3.8 ± 0.68) and traditional groups (2.6 ± 0.94) (p < 0.01). CONCLUSION: Integrating AI, templates and digital imaging significantly improved puncture knowledge and skills over traditional training. Combining technological innovations and AI shows promise for streamlining complex medical competency mastery.",yes,"This study evaluates the use of ChatGPT (a generative LLM) as part of a training intervention for improving puncture skills among doctors. The LLM is used to enhance theoretical knowledge and technical skills in a clinical procedure context. The focus is on clinical education and skill acquisition, which is a valid clinical application of LLMs in medicine.",10.1186/s12909-024-06217-0,X,Sully,,,
A Comparison of ChatGPT and Human Questionnaire Evaluations of the Urological Cancer Videos Most Watched on YouTube,"Aim: To examine the reliability of ChatGPT in evaluating the quality of medical content of the most watched videos related to urological cancers on YouTube. Material and methods: In March 2024 a playlist was created of the first 20 videos watched on YouTube for each type of urological cancer. The video texts were evaluated by ChatGPT and by a urology specialist using the DISCERN-5 and Global Quality Scale (GQS) questionnaires. The results obtained were compared using the Kruskal-Wallis test. Results: For the prostate, bladder, renal, and testicular cancer videos, the median (IQR) DISCERN-5 scores given by the human evaluator and ChatGPT were (Human: 4 [1], 3 [0], 3 [2], 3 [1], P = .11; ChatGPT: 3 [1.75], 3 [1], 3 [2], 3 [0], P = .4, respectively) and the GQS scores were (Human: 4 [1.75], 3 [0.75], 3.5 [2], 3.5 [1], P = .12; ChatGPT: 4 [1], 3 [0.75], 3 [1], 3.5 [1], P = .1, respectively), with no significant difference determined between the scores. The repeatability of the ChatGPT responses was determined to be similar at 25 % for prostate cancer, 30 % for bladder cancer, 30 % for renal cancer, and 35 % for testicular cancer (P = .92). No statistically significant difference was determined between the median (IQR) DISCERN-5 and GQS scores given by humans and ChatGPT for the content of videos about prostate, bladder, renal, and testicular cancer (P > .05). Conclusion: Although ChatGPT is successful in evaluating the medical quality of video texts, the results should be evaluated with caution as the repeatability of the results is low. © 2024 Elsevier Inc.",yes,"This study evaluates ChatGPT's ability to assess the quality of medical content in urological cancer videos using established clinical quality scales, comparing its performance to that of a human specialist. The task is related to clinical information quality assessment, which is relevant to patient education and clinical communication. The use of ChatGPT is directly evaluated in a healthcare context.",10.1016/j.clgc.2024.102145,III,Sully,,,
"Critical analysis of information provided by ChatGPT on lactate, exercise, fatigue, and muscle pain: current insights and future prospects for enhancement","2289. Adv Physiol Educ. 2024 Dec 1;48(4):898-903. doi: 10.1152/advan.00073.2024. Epub 
2024 Sep 12.

Critical analysis of information provided by ChatGPT on lactate, exercise, 
fatigue, and muscle pain: current insights and future prospects for enhancement.

Rocha-Silva R(1), Rodrigues MAM(1), Viana RB(2), Nakamoto FP(3), Vancini RL(4), 
Andrade MS(5), Rosemann T(6), Weiss K(6), Knechtle B(6)(7), de Lira CAB(1).

Author information:
(1)Faculty of Physical Education and Dance, Federal University of Goiás, 
Goiânia, Brazil.
(2)Institute of Physical Education and Sports, Federal University of Ceará, 
Fortaleza, Brazil.
(3)Department of Nutrition, São Camilo University Center, São Paulo, Brazil.
(4)Center for Physical Education and Sports, Federal University of Espírito 
Santo, Vitória, Brazil.
(5)Department of Physiology, Federal University of São Paulo, São Paulo, Brazil.
(6)Institute of Primary Care, University of Zurich, Zurich, Switzerland.
(7)Medbase St. Gallen Am Vadianplatz, St. Gallen, Switzerland.

This study aimed to critically evaluate the information provided by ChatGPT on 
the role of lactate in fatigue and muscle pain during physical exercise. We 
inserted the prompt ""What is the cause of fatigue and pain during exercise?"" 
using ChatGPT versions 3.5 and 4o. In both versions, ChatGPT associated muscle 
fatigue with glycogen depletion and ""lactic acid"" accumulation, whereas pain was 
linked to processes such as inflammation and microtrauma. We deepened the 
investigation with ChatGPT 3.5, implementing user feedback to question the 
accuracy of the information about lactate. The response was then reformulated, 
involving a scientific debate about the true role of lactate in physical 
exercise and debunking the idea that it is the primary cause of muscle fatigue 
and pain. We also utilized the creation of a ""well-crafted prompt,"" which 
included persona identification and thematic characterization, resulting in much 
more accurate information in both the ChatGPT 3.5 and 4o models, presenting a 
range of information from the physiological process of lactate to its true role 
in physical exercise. The results indicated that the accuracy of the responses 
provided by ChatGPT can vary depending on the data available in its database 
and, more importantly, on how the question is formulated. Therefore, it is 
indispensable that educators guide their students in the processes of managing 
the AI tool to mitigate risks of misinformation.NEW & NOTEWORTHY Generative 
artificial intelligence (AI), exemplified by ChatGPT, provides immediate and 
easily accessible answers about lactate and exercise. However, the reliability 
of this information may fluctuate, contingent upon the scope and intricacy of 
the knowledge derived from the training process before most recent update. 
Furthermore, a deep understanding of the basic principles of human physiology 
becomes crucial for the effective correction and safe use of this technology.",yes,"This study evaluates ChatGPT (versions 3.5 and 4o) in its ability to answer questions about the physiological causes of fatigue and muscle pain during exercise, including the impact of prompt engineering and user feedback. The focus is on the accuracy and reliability of ChatGPT's clinical/scientific information relevant to exercise physiology. While the context is not direct patient care, it does assess the LLM's ability to provide medically relevant information that could impact health education and potentially clinical advice. The study is not a review, editorial, or technical report, and it directly tests LLM output in a health-related context.",10.1152/advan.00073.2024,II,Sully,,,
An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial Results,"3557. Sci Data. 2025 Mar 11;12(1):424. doi: 10.1038/s41597-025-04718-1.

An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial 
Results.

Yazdani A(1), Bornet A(2), Khlebnikov P(3), Zhang B(2), Rouhizadeh H(2), Amini 
P(3), Teodoro D(4).

Author information:
(1)Department of Radiology and Medical Informatics, Faculty of Medicine, 
University of Geneva, Geneva, Switzerland. anthony.yazdani@unige.ch.
(2)Department of Radiology and Medical Informatics, Faculty of Medicine, 
University of Geneva, Geneva, Switzerland.
(3)Risklick AG, Bern, Switzerland.
(4)Department of Radiology and Medical Informatics, Faculty of Medicine, 
University of Geneva, Geneva, Switzerland. douglas.teodoro@unige.ch.

Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus, 
predicting ADEs is key to developing safer medications and enhancing patient 
outcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel 
ADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and 
168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA 
ontology. Unlike existing resources, CT-ADE integrates treatment and target 
population data, enabling comparative analyses under varying conditions, such as 
dosage, administration route, and demographics. In addition, CT-ADE 
systematically collects all ADEs in the study population, including positive and 
negative cases. To provide a baseline for ADE prediction performance using the 
CT-ADE dataset, we conducted analyses using large language models (LLMs). The 
best LLM achieved an F1-score of 56%, with models incorporating treatment and 
patient information outperforming by 21%-38% those relying solely on the 
chemical structure. These findings underscore the importance of contextual 
information in ADE prediction and establish CT-ADE as a robust resource for 
safety risk assessment in pharmaceutical research and development.

© 2025. The Author(s).",yes,"This study evaluates large language models (LLMs) for the prediction of adverse drug events (ADEs) using clinical trial data. The LLMs are applied to a clinical prediction task (ADE prediction), which is relevant to healthcare and medicine. The study provides performance metrics for LLMs in this context, meeting the inclusion criteria.",10.1038/s41597-025-04718-1,I,Sully,,,
Assessing ChatGPT's performance in national nuclear medicine specialty examination: An evaluative analysis,"Introduction: The rapid development of artificial intelligence (AI) has sparked a desire to analyse its potential applications in medicine. The aim of this article is to present the effectiveness of the ChatGPT advanced language model in the context of the pass rate of the polish National Specialty Examination (PES) in nuclear medicine. It also aims to identify its strengths and limitations through an in-depth analysis of the issues raised in the exam questions. Methods: The PES exam provided by the Centre for Medical Examinations in Łódź, consisting of 120 questions, was used for the study. The questions were asked using the openai.com platform, through which free access to the GPT-3.5 model is available. All questions were classified according to Bloom's taxonomy to determine their complexity and difficulty, and according to two authors' subcategories. To assess the model's confidence in the validity of the answers, each questions was asked five times in independent sessions. Results: ChatGPT achieved 56%, which means it did not pass the exam. The pass rate is 60%. Of the 117 questions asked, 66 were answered correctly. In the percentage of each type and subtype of questions answered correctly, there were no statistically significant differences. Conclusion: Further testing is needed using the questions provided by Centre for Medical Examinations from the nuclear medicine specialty exam to evaluate the utility of the ChatGPT model. This opens the door for further research on upcoming improved versions of the ChatGPT. Copyright © 2024 The Authors. Published by Tehran University of Medical Sciences.",yes,"This study evaluates the performance of ChatGPT (GPT-3.5) on a national specialty medical board exam in nuclear medicine, which is a direct clinical application relevant to medical education and assessment of LLMs in a healthcare context.",10.22034/IRJNM.2023.129434.1580,III,Sully,,,
Are ChatGPT’s knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology examination?: a descriptive study,"This study aimed to compare the knowledge and interpretation ability of ChatGPT, a language model of artificial general intelligence, with those of medical students in Korea by administering a parasitology examination to both ChatGPT and medical students. The examination consisted of 79 items and was administered to ChatGPT on January 1, 2023. The examination results were analyzed in terms of ChatGPT’s overall performance score, its correct answer rate by the items’ knowledge level, and the acceptability of its explanations of the items. ChatGPT’s performance was lower than that of the medical students, and ChatGPT’s correct answer rate was not related to the items’ knowledge level. However, there was a relationship between acceptable explanations and correct answers. In conclusion, ChatGPT’s knowledge and interpretation ability for this parasitology examination were not yet comparable to those of medical students in Korea. 2023 Korea Health Personnel Licensing Examination Institute.",yes,"This study evaluates ChatGPT's performance on a medical parasitology exam and compares it to medical students, including analysis of its explanations. This is a direct clinical/medical evaluation of an LLM's capabilities relevant to healthcare education.",10.3352/jeehp.2023.20.1,III,Sully,,,
Large language models to help appeal denied radiotherapy services,"Background Large language model (LLM) artificial intelligences have potential to perform myriad healthcare tasks but should be validated in specific clinical use cases before deployment. One use case is to help physicians appeal insurer denials of prescribed medical services, a task that delays patient care and contributes to burnout. We evaluated LLM performance at this task for denials of radiotherapy services. Methods We evaluated generative pre-trained transformer 3.5 (GPT-3.5) (OpenAI, San Francisco, CA), GPT-4, GPT-4 with internet search functionality (GPT-4web), and GPT-3.5ft. The latter was developed by fine-tuning GPT-3.5 via an OpenAI application programming interface with 53 examples of appeal letters written by radiation oncologists. Twenty test prompts with simulated patient histories were programmatically presented to the LLMs, and output appeal letters were scored by three blinded radiation oncologists for language representation, clinical detail inclusion, clinical reasoning validity, literature citations, and overall readiness for insurer submission. Results Interobserver agreement between radiation oncologists’ scores was moderate or better for all domains (Cohen’s kappa coefficients: 0.41 – 0.91). GPT-3.5, GPT-4, and GPT-4web wrote letters that were on average linguistically clear, summarized provided clinical histories without confabulation, reasoned appropriately, and were scored useful to expedite the insurance appeal process. GPT-4 and GPT-4web letters demonstrated superior clinical reasoning and were readier for submission than GPT-3.5 letters (p < 0.001). Fine-tuning increased GPT-3.5ft confabulation and compromised performance compared to other LLMs across all domains (p < 0.001). All LLMs, including GPT-4web, were poor at supporting clinical assertions with existing, relevant, and appropriately cited primary literature. Conclusions When prompted appropriately, three commercially available LLMs drafted letters that physicians deemed would expedite appealing insurer denials of radiotherapy services. LLMs may decrease this task’s clerical workload on providers. However, LLM performance worsened when fine-tuned with a task-specific, small training dataset.",yes,"This study evaluates the performance of several LLMs (GPT-3.5, GPT-4, GPT-4web, and a fine-tuned GPT-3.5) in generating appeal letters for denied radiotherapy services, a task directly related to clinical workflow and patient care. The LLMs' outputs were assessed by clinicians for clinical reasoning, detail, and readiness for submission, making this a clinical application of LLMs in healthcare.",10.1101/2024.04.30.24306634,II,Sully,,,
ChatGPT M.D.: Is there any room for generative AI in neurology?,"ChatGPT, a general artificial intelligence, has been recognized as a powerful tool in scientific writing and programming but its use as a medical tool is largely overlooked. The general accessibility, rapid response time and comprehensive training database might enable ChatGPT to serve as a diagnostic augmentation tool in certain clinical settings. The diagnostic process in neurology is often challenging and complex. In certain time-sensitive scenarios, rapid evaluation and diagnostic decisions are needed, while in other cases clinicians are faced with rare disorders and atypical disease manifestations. Due to these factors, the diagnostic accuracy in neurology is often suboptimal. Here we evaluated whether ChatGPT can be utilized as a valuable and innovative diagnostic augmentation tool in various neurological settings. We used synthetic data generated by neurological experts to represent descriptive anamneses of patients with known neurology-related diseases, then the probability for an appropriate diagnosis made by ChatGPT was measured. To give clarity to the accuracy of the AI-determined diagnosis, all cases have been cross-validated by other experts and general medical doctors as well. We found that ChatGPT-determined diagnostic accuracy (ranging from 68.5% ± 3.28% to 83.83% ± 2.73%) can reach the accuracy of other experts (81.66% ± 2.02%), furthermore, it surpasses the probability of an appropriate diagnosis if the examiner is a general medical doctor (57.15% ± 2.64%). Our results showcase the efficacy of general artificial intelligence like ChatGPT as a diagnostic augmentation tool in medicine. In the future, AI-based supporting tools might be useful amendments in medical practice and help to improve the diagnostic process in neurology.",yes,This study evaluates ChatGPT's performance as a diagnostic augmentation tool in neurology using synthetic patient cases and compares its diagnostic accuracy to that of experts and general medical doctors. The focus is on clinical application (diagnosis) of a generative LLM in a medical context.,10.1371/journal.pone.0310028,III,Sully,,,
Quality and Dependability of ChatGPT and DingXiangYuan Forums for Remote Orthopedic Consultations: Comparative Analysis,"BACKGROUND: The widespread use of artificial intelligence, such as ChatGPT 
(OpenAI), is transforming sectors, including health care, while separate 
advancements of the internet have enabled platforms such as China's 
DingXiangYuan to offer remote medical services.
OBJECTIVE: This study evaluates ChatGPT-4's responses against those of 
professional health care providers in telemedicine, assessing artificial 
intelligence's capability to support the surge in remote medical consultations 
and its impact on health care delivery.
METHODS: We sourced remote orthopedic consultations from ""Doctor DingXiang,"" 
with responses from its certified physicians as the control and ChatGPT's 
responses as the experimental group. In all, 3 blindfolded, experienced 
orthopedic surgeons assessed responses against 7 criteria: ""logical reasoning,"" 
""internal information,"" ""external information,"" ""guiding function,"" ""therapeutic 
effect,"" ""medical knowledge popularization education,"" and ""overall 
satisfaction."" We used Fleiss κ to measure agreement among multiple raters.
RESULTS: Initially, consultation records for a cumulative count of 8 maladies 
(equivalent to 800 cases) were gathered. We ultimately included 73 consultation 
records by May 2023, following primary and rescreening, in which no 
communication records containing private information, images, or voice messages 
were transmitted. After statistical scoring, we discovered that ChatGPT's 
""internal information"" score (mean 4.61, SD 0.52 points vs mean 4.66, SD 0.49 
points; P=.43) and ""therapeutic effect"" score (mean 4.43, SD 0.75 points vs mean 
4.55, SD 0.62 points; P=.32) were lower than those of the control group, but the 
differences were not statistically significant. ChatGPT showed better 
performance with a higher ""logical reasoning"" score (mean 4.81, SD 0.36 points 
vs mean 4.75, SD 0.39 points; P=.38), ""external information"" score (mean 4.06, 
SD 0.72 points vs mean 3.92, SD 0.77 points; P=.25), and ""guiding function"" 
score (mean 4.73, SD 0.51 points vs mean 4.72, SD 0.54 points; P=.96), although 
the differences were not statistically significant. Meanwhile, the ""medical 
knowledge popularization education"" score of ChatGPT was better than that of the 
control group (mean 4.49, SD 0.67 points vs mean 3.87, SD 1.01 points; P<.001), 
and the difference was statistically significant. In terms of ""overall 
satisfaction,"" the difference was not statistically significant between the 
groups (mean 8.35, SD 1.38 points vs mean 8.37, SD 1.24 points; P=.92). 
According to how Fleiss κ values were interpreted, 6 of the control group's 
score points were classified as displaying ""fair agreement"" (P<.001), and 1 was 
classified as showing ""substantial agreement"" (P<.001). In the experimental 
group, 3 points were classified as indicating ""fair agreement,"" while 4 
suggested ""moderate agreement"" (P<.001).
CONCLUSIONS: ChatGPT-4 matches the expertise found in DingXiangYuan forums' paid 
consultations, excelling particularly in scientific education. It presents a 
promising alternative for remote health advice. For health care professionals, 
it could act as an aid in patient education, while patients may use it as a 
convenient tool for health inquiries.

©Zhaowen Xue, Yiming Zhang, Wenyi Gan, Huajun Wang, Guorong She, Xiaofei Zheng. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 14.03.2024.",yes,"This study directly evaluates ChatGPT-4's performance in providing remote orthopedic consultations, comparing its responses to those of certified physicians in a clinical telemedicine context. The assessment includes clinical criteria such as logical reasoning, therapeutic effect, and patient education, making it a clear clinical application of an LLM.",10.2196/50882,I,Sully,,,
The RepVig framework for designing use-case specific representative vignettes and evaluating triage accuracy of laypeople and symptom assessment applications,"Most studies evaluating symptom-assessment applications (SAAs) rely on a common set of case vignettes that are authored by clinicians and devoid of context, which may be representative of clinical settings but not of situations where patients use SAAs. Assuming the use case of self-triage, we used representative design principles to sample case vignettes from online platforms where patients describe their symptoms to obtain professional advice and compared triage performance of laypeople, SAAs (e.g., WebMD or NHS 111), and Large Language Models (LLMs, e.g., GPT-4 or Claude) on representative versus standard vignettes. We found performance differences in all three groups depending on vignette type: When using representative vignettes, accuracy was higher (OR = 1.52 to 2.00, p < .001 to .03 in binary decisions, i.e., correct or incorrect), safety was higher (OR = 1.81 to 3.41, p < .001 to .002 in binary decisions, i.e., safe or unsafe), and the inclination to overtriage was also higher (OR = 1.80 to 2.66, p < .001 to p = .035 in binary decisions, overtriage or undertriage error). Additionally, we found changed rankings of best-performing SAAs and LLMs. Based on these results, we argue that our representative vignette sampling approach (that we call the RepVig Framework) should replace the practice of using a fixed vignette set as standard for SAA evaluation studies.",yes,"This study evaluates the triage accuracy of large language models (LLMs, e.g., GPT-4, Claude) in a clinical context (self-triage) and compares their performance to laypeople and symptom assessment applications using representative vignettes. The focus is on clinical decision-making (triage), which fits the inclusion criteria.",10.1038/s41598-024-83844-z,II,Sully,,,
Enhancing Hand Fracture Care: A Prospective Study of Artificial Intelligence Application With ChatGPT,"Purpose: The integration of artificial intelligence and machine learning technologies into the medical field has brought about remarkable advancements, particularly in the domain of clinical decision support systems. However, it is uncertain how they will perform as clinical decision-makers. Methods: This prospective cohort study evaluates the potential of incorporating ChatGPT-4 plus into the management of subcapital fifth metacarpal fractures. The treatment recommendations provided by ChatGPT-4 plus were compared with those of the two control groups—the attending clinic plastic surgeon and an independent expert panel. The primary outcome measures, operative or conservative, were compared between the groups. Intraclass correlation of 0.61 infers moderate reliability in the consistency of recommended management plans across all groups. Results: Key predictors for opting for operative management, regardless of the decision-maker, included clinical signs of scissoring, extension deficit, and radiographic evidence of intra-articular extension. Conclusions: These findings support the potential for artificial intelligence applications in enhancing diagnostic and treatment decisions. Type of study/level of evidence: Therapeutic IV. © 2024 The Authors",yes,"This study prospectively evaluates ChatGPT-4 plus in making clinical management decisions for hand fractures, directly comparing its recommendations to those of clinicians. It assesses the LLM's performance in a real clinical decision-making context.",10.1016/j.jhsg.2024.03.014,I,Sully,,,
Use of Large Language Models to Assess the Likelihood of Epidemics From the Content of Tweets: Infodemiology Study,"Background: Previous work suggests that Google searches could be useful in identifying conjunctivitis epidemics. Content-based assessment of social media content may provide additional value in serving as early indicators of conjunctivitis and other systemic infectious diseases. Objective: We investigated whether large language models, specifically GPT-3.5 and GPT-4 (OpenAI), can provide probabilistic assessments of whether social media posts about conjunctivitis could indicate a regional outbreak. Methods: A total of 12,194 conjunctivitis-related tweets were obtained using a targeted Boolean search in multiple languages from India, Guam (United States), Martinique (France), the Philippines, American Samoa (United States), Fiji, Costa Rica, Haiti, and the Bahamas, covering the time frame from January 1, 2012, to March 13, 2023. By providing these tweets via prompts to GPT-3.5 and GPT-4, we obtained probabilistic assessments that were validated by 2 human raters. We then calculated Pearson correlations of these time series with tweet volume and the occurrence of known outbreaks in these 9 locations, with time series bootstrap used to compute CIs. Results: Probabilistic assessments derived from GPT-3.5 showed correlations of 0.60 (95% CI 0.47-0.70) and 0.53 (95% CI 0.40-0.65) with the 2 human raters, with higher results for GPT-4. The weekly averages of GPT-3.5 probabilities showed substantial correlations with weekly tweet volume for 44% (4/9) of the countries, with correlations ranging from 0.10 (95% CI 0.0-0.29) to 0.53 (95% CI 0.39-0.89), with larger correlations for GPT-4. More modest correlations were found for correlation with known epidemics, with substantial correlation only in American Samoa (0.40, 95% CI 0.16-0.81). Conclusions: These findings suggest that GPT prompting can efficiently assess the content of social media posts and indicate possible disease outbreaks to a degree of accuracy comparable to that of humans. Furthermore, we found that automated content analysis of tweets is related to tweet volume for conjunctivitis-related posts in some locations and to the occurrence of actual epidemics. Future work may improve the sensitivity and specificity of these methods for disease outbreak detection. © 2024 JMIR Publications Inc.. All rights reserved.",yes,"This study evaluates GPT-3.5 and GPT-4 LLMs for their ability to assess the likelihood of disease outbreaks (epidemics) from the content of social media posts (tweets). The LLMs are used to make probabilistic clinical assessments (potential outbreak detection), and their outputs are compared to human raters and real-world epidemic data. This is a clinical application relevant to public health surveillance.",10.2196/49139,I,Sully,,,
"Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study","BACKGROUND: Regular physical activity is critical for health and disease 
prevention. Yet, health care providers and patients face barriers to implement 
evidence-based lifestyle recommendations. The potential to augment care with the 
increased availability of artificial intelligence (AI) technologies is 
limitless; however, the suitability of AI-generated exercise recommendations has 
yet to be explored.
OBJECTIVE: The purpose of this study was to assess the comprehensiveness, 
accuracy, and readability of individualized exercise recommendations generated 
by a novel AI chatbot.
METHODS: A coding scheme was developed to score AI-generated exercise 
recommendations across ten categories informed by gold-standard exercise 
recommendations, including (1) health condition-specific benefits of exercise, 
(2) exercise preparticipation health screening, (3) frequency, (4) intensity, 
(5) time, (6) type, (7) volume, (8) progression, (9) special considerations, and 
(10) references to the primary literature. The AI chatbot was prompted to 
provide individualized exercise recommendations for 26 clinical populations 
using an open-source application programming interface. Two independent 
reviewers coded AI-generated content for each category and calculated 
comprehensiveness (%) and factual accuracy (%) on a scale of 0%-100%. 
Readability was assessed using the Flesch-Kincaid formula. Qualitative analysis 
identified and categorized themes from AI-generated output.
RESULTS: AI-generated exercise recommendations were 41.2% (107/260) 
comprehensive and 90.7% (146/161) accurate, with the majority (8/15, 53%) of 
inaccuracy related to the need for exercise preparticipation medical clearance. 
Average readability level of AI-generated exercise recommendations was at the 
college level (mean 13.7, SD 1.7), with an average Flesch reading ease score of 
31.1 (SD 7.7). Several recurring themes and observations of AI-generated output 
included concern for liability and safety, preference for aerobic exercise, and 
potential bias and direct discrimination against certain age-based populations 
and individuals with disabilities.
CONCLUSIONS: There were notable gaps in the comprehensiveness, accuracy, and 
readability of AI-generated exercise recommendations. Exercise and health care 
professionals should be aware of these limitations when using and endorsing 
AI-based technologies as a tool to support lifestyle change involving exercise.

©Amanda L Zaleski, Rachel Berkowsky, Kelly Jean Thomas Craig, Linda S 
Pescatello. Originally published in JMIR Medical Education 
(https://mededu.jmir.org), 11.01.2024.",yes,"This study evaluates an AI-based chatbot's ability to provide individualized exercise recommendations for various clinical populations, assessing comprehensiveness, accuracy, and readability. The chatbot is used to generate clinical recommendations, which are then evaluated. Although the specific LLM is not named, the use of an 'AI chatbot' to generate individualized clinical recommendations for patients fits the inclusion criteria, as it is a clinical application of a generative LLM or similar model.",10.2196/51308,II,Sully,,,
Examining the Accuracy and Reproducibility of Responses to Nutrition Questions Related to Inflammatory Bowel Disease by Generative Pre-trained Transformer-4,"Background: Generative pre-trained transformer-4 (GPT-4) is a large language model (LLM) trained on a vast corpus of data, including the medical literature. Nutrition plays an important role in managing inflammatory bowel disease (IBD), with an unmet need for nutrition-related patient education resources. This study examines the accuracy, comprehensiveness, and reproducibility of responses by GPT-4 to patient nutrition questions related to IBD. Methods: Questions were obtained from adult IBD clinic visits, Facebook, and Reddit. Two IBD-focused registered dieticians independently graded the accuracy and reproducibility of GPT-4’s responses while a third senior IBD-focused registered dietitian arbitrated. Each question was inputted twice into the model. Results: 88 questions were selected. The model correctly responded to 73/88 questions (83.0%), with 61 (69.0%) graded as comprehensive. 15/88 (17%) responses were graded as mixed with correct and incorrect/outdated data. The model comprehensively responded to 10 (62.5%) questions related to “Nutrition and diet needs for surgery,” 12 (92.3%) “Tube feeding and parenteral nutrition,” 11 (64.7%) “General diet questions,” 10 (50%) “Diet for reducing symptoms/inflammation,” and 18 (81.8%) “Micronutrients/supplementation needs.” The model provided reproducible responses to 81/88 (92.0%) questions. Conclusions: GPT-4 comprehensively answered most questions, demonstrating the promising potential of LLMs as supplementary tools for IBD patients seeking nutrition-related information. However, 17% of responses contained incorrect information, highlighting the need for continuous refinement prior to incorporation into clinical practice. Future studies should emphasize leveraging LLMs to enhance patient outcomes and promoting patient and healthcare professional proficiency in using LLMs to maximize their efficacy. © The Author(s) 2025. Published by Oxford University Press on behalf of Crohn's & Colitis Foundation.",yes,"This study evaluates GPT-4, a large language model, in its ability to answer patient nutrition questions related to inflammatory bowel disease (IBD). The evaluation is clinical in nature, focusing on the accuracy, comprehensiveness, and reproducibility of responses to real patient questions, which aligns with the inclusion criteria.",10.1093/crocol/otae077,I,Sully,,,
Future Perspective of Risk Prediction in Aesthetic Surgery: Is Artificial Intelligence Reliable?,"Background: Artificial intelligence (AI) techniques are showing significant potential in the medical field. The rapid advancement in artificial intelligence methods suggests their soon-to-be essential role in physicians’ practices. Objectives: In this study, we sought to assess and compare the readability, clarity, and precision of medical knowledge responses provided by 3 large language models (LLMs) and informed consent forms for 14 common aesthetic surgical procedures, as prepared by the American Society of Plastic Surgeons (ASPS). Methods: The efficacy, readability, and accuracy of 3 leading LLMs, ChatGPT-4 (OpenAI, San Francisco, CA), Gemini (Google, Mountain View, CA), and Copilot (Microsoft, Redmond, WA), was systematically evaluated with 14 different prompts related to the risks of 14 common aesthetic procedures. Alongside these LLM responses, risk sections from the informed consent forms for these procedures, provided by the ASPS, were also reviewed. Results: The risk factor segments of the combined general and specific operation consent forms were rated highest for medical knowledge accuracy (P < .05). Regarding readability and clarity, the procedure-specific informed consent forms, including LLMs, scored highest scores (P < .05). However, these same forms received the lowest score for medical knowledge accuracy (P < .05). Interestingly, surgeons preferred patient-facing materials created by ChatGPT-4, citing superior accuracy and medical information compared to other AI tools. Conclusions: Physicians prefer patient-facing materials created by ChatGPT-4 over other AI tools due to their precise and comprehensive medical knowledge. Importantly, adherence to the strong recommendation of ASPS for signing both the procedure-specific and the general informed consent forms can avoid potential future complications and ethical concerns, thereby ensuring patients receive adequate information. © The Author(s) 2024.",yes,"This study evaluates the performance of three LLMs (ChatGPT-4, Gemini, Copilot) in generating patient-facing medical information about risks of aesthetic surgery, comparing their outputs to official informed consent forms. The focus is on the clinical accuracy, readability, and clarity of LLM-generated responses to patient-relevant prompts. This is a direct clinical application relevant to patient education and risk communication.",10.1093/asj/sjae140,II,Sully,,,
Comparing Large Language Model and Human Reader Accuracy with New England Journal of Medicine Image Challenge Case Image Inputs,"Background: Application of multimodal large language models (LLMs) with both textual and visual capabilities has been steadily increasing, but their ability to interpret radiologic images is still doubted. Purpose: To evaluate the accuracy of LLMs and compare it with that of human readers with varying levels of experience and to assess the factors affecting LLM accuracy in answering New England Journal of Medicine Image Challenge cases. Materials and Methods: Radiologic images of cases from October 13, 2005, to April 18, 2024, were retrospectively reviewed. Using text and image inputs, LLMs (Open AI’s GPT-4 Turbo with Vision [GPT-4V] and GPT-4 Omni [GPT-4o], Google’s DeepMind Gemini 1.5 Pro, and Anthropic’s Claude 3) provided answers. Human readers (seven junior faculty radiologists, two clinicians, one in-training radiologist, and one medical student), blinded to the published answers, also answered. LLM accuracy with and without image inputs and short (cases from 2005 to 2015) versus long text inputs (from 2016 to 2024) was evaluated in subgroup analysis to determine the effect of these factors. Factor analysis was assessed using multivariable logistic regression. Accuracy was compared with generalized estimating equations, with multiple comparisons adjusted by using Bonferroni correction. Results: A total of 272 cases were included. GPT-4o achieved the highest overall accuracy among LLMs (59.6%; 162 of 272), outperforming a medical student (47.1%; 128 of 272; P <.001) but not junior faculty (80.9%; 220 of 272; P <.001) or the in-training radiologist (70.2%; 191 of 272; P =.003). GPT-4o exhibited similar accuracy regardless of image inputs (without images vs with images, 54.0% [147 of 272] vs 59.6% [162 of 272], respectively; P =.59). Human reader accuracy was unaffected by text length, whereas LLMs demonstrated higher accuracy with long text inputs (all P <.001). Text input length affected LLM accuracy (odds ratio range, 3.2 [95% CI: 1.9, 5.5] to 6.6 [95% CI: 3.7, 12.0]). Conclusion: LLMs demonstrated substantial accuracy with text and image inputs, outperforming a medical student. However, their accuracy decreased with shorter text lengths, regardless of image input. © RSNA, 2024.",yes,"This study evaluates the clinical performance of several frontier LLMs (GPT-4V, GPT-4o, Gemini, Claude 3) in interpreting radiologic images and answering clinical cases, directly comparing their accuracy to human readers. The task is a clinical one (diagnosis/interpretation of radiologic images), and the LLMs are used in a clinical context.",10.1148/radiol.241668,III,Sully,,,
The Prediction of Stress in Radiation Therapy: Integrating Artificial Intelligence with Biological Signals,"This study aimed to predict stress in patients using artificial intelligence (AI) from biological signals and verify the effect of stress on respiratory irregularity. We measured 123 cases in 41 patients and calculated stress scores with seven stress-related features derived from heart-rate variability. The distribution and trends of stress scores across the treatment period were analyzed. Before-treatment information was used to predict the stress features during treatment. AI models included both non-pretrained (decision tree, random forest, support vector machine, long short-term memory (LSTM), and transformer) and pretrained (ChatGPT) models. Performance was evaluated using 10-fold cross-validation, exact match ratio, accuracy, recall, precision, and F1 score. Respiratory irregularities were calculated in phase and amplitude and analyzed for correlation with stress score. Over 90% of the patients experienced stress during radiation therapy. LSTM and prompt engineering GPT4.0 had the highest accuracy (feature classification, LSTM: 0.703, GPT4.0: 0.659; stress classification, LSTM: 0.846, GPT4.0: 0.769). A 10% increase in stress score was associated with a 0.286 higher phase irregularity (p < 0.025). Our research pioneers the use of AI and biological signals for stress prediction in patients undergoing radiation therapy, potentially identifying those needing psychological support and suggesting methods to improve radiotherapy effectiveness through stress management. © 2024 by the authors.",yes,"This study evaluates the use of ChatGPT (GPT-4.0) as a pretrained LLM for predicting stress in patients undergoing radiation therapy, comparing its performance to other AI models. The clinical context is clear, as the LLM is used to predict a clinically relevant outcome (patient stress) that could impact treatment. The study is not a review or opinion piece, and the LLM is used for a clinical prediction task.",10.3390/cancers16111964,I,Sully,,,
How accurately can ChatGPT 3.5 answer frequently asked questions by patients on glenohumeral osteoarthritis?; [Wie genau kann ChatGPT 3.5 häufig gestellte Fragen von Patienten zur glenohumeralen Osteoarthritis beantworten?],"Background: Conversational artificial intelligence (AI) systems like ChatGPT have emerged as valuable assets in providing accessible information across various domains, including the healthcare system. The use of ChatGPT may contribute to better patient education and better general healthcare knowledge. However, there is a paucity of data on the reliability of responses generated by ChatGPT in the context of specific medical diagnoses. Methods: We identified 12 frequently asked questions by patients about glenohumeral osteoarthritis. These questions were formulated in both English and German, using common and medical terms for the condition, thus creating four groups for evaluation. The questions were then presented to ChatGPT 3.5. The generated responses were evaluated for accuracy by four independent orthopedic and trauma surgery consultants using a Likert scale (0 = fully inaccurate to 4 = fully accurate). Results: Although there were two questions in two groups, all questions across all versions were answered with good accuracy by ChatGPT 3.5. The highest score on the Likert scale was 3.9 for the group where questions were posed in English using the medical term “glenohumeral osteoarthritis.” The lowest score of 3.2 was for the group where questions were posed in English using the common term “shoulder arthrosis.” On average, questions in English received a score of 3.5 on the Likert scale, slightly higher than those in German, which received a score of 3.4. Conclusion: ChatGPT 3.5 can already provide accurate responses to patients’ frequently asked questions on glenohumeral osteoarthritis. ChatGPT can therefore be a valuable tool for patient communication and education in the field of orthopedics. Further studies, however, have to be performed in order to fully understand the mechanisms and impact of ChatGPT in the field. © The Author(s) 2024.",yes,"This study evaluates ChatGPT 3.5's ability to answer patient questions about a specific medical condition (glenohumeral osteoarthritis), with clinical experts rating the accuracy of its responses. This is a direct clinical application of an LLM in patient education and communication.",10.1007/s11678-024-00836-1,II,Sully,,,
Performance of a Large Language Model in the Generation of Clinical Guidelines for Antibiotic Prophylaxis in Spine Surgery,"Objective: Large language models, such as chat generative pre-trained transformer (ChatGPT), have great potential for streamlining medical processes and assisting physicians in clinical decision-making. This study aimed to assess the potential of ChatGPT’s 2 models (GPT-3. 5 and GPT-4. 0) to support clinical decision-making by comparing its responses for antibiotic prophylaxis in spine surgery to accepted clinical guidelines. Methods: ChatGPT models were prompted with questions from the North American Spine Society (NASS) Evidence-based Clinical Guidelines for Multidisciplinary Spine Care for Antibiotic Prophylaxis in Spine Surgery (2013). Its responses were then compared and assessed for accuracy. Results: Of the 16 NASS guideline questions concerning antibiotic prophylaxis, 10 responses (62. 5%) were accurate in ChatGPT’s GPT-3. 5 model and 13 (81%) were accurate in GPT-4. 0. Twenty-five percent of GPT-3. 5 answers were deemed as overly confident while 62. 5% of GPT-4. 0 answers directly used the NASS guideline as evidence for its response. Conclusion: ChatGPT demonstrated an impressive ability to accurately answer clinical questions. GPT-3. 5 model’s performance was limited by its tendency to give overly confident responses and its inability to identify the most significant elements in its responses. GPT-4. 0 model’s responses had higher accuracy and cited the NASS guideline as direct evidence many times. While GPT-4. 0 is still far from perfect, it has shown an exceptional ability to extract the most relevant research available compared to GPT-3.5. Thus, while ChatGPT has shown far-reaching potential, scrutiny should still be exercised regarding its clinical use at this time. © 2024 by the Korean Spinal Neurosurgery Society.",yes,"This study evaluates the performance of ChatGPT (GPT-3.5 and GPT-4.0) in generating clinical guideline-based recommendations for antibiotic prophylaxis in spine surgery, directly comparing its responses to established clinical guidelines. This is a clinical application relevant to decision support in medicine.",10.14245/ns.2347310.655,II,Sully,,,
Application of ChatGPT in multilingual medical education: How does ChatGPT fare in 2023's Iranian residency entrance examination,"Background: ChatGPT is a large language model (LLM) artificial intelligence instrument trained on massive amounts of text data extracted from the internet and/or user input. In the present article, we aim to apply the latest version of ChatGPT to the Iranian Medical Residency Examination. Methods: The Iranian Medical Residency Examination is composed of 200 multichoice questions covering all domains of medicine. We used ChatGPT to translate questions into English, French, and Spanish. We fed the questions as multiple-choice questions and allowed ChatGPT to provide comprehensive answers and justifications for its choices. Results: ChatGPT was able to answer 161 (81.3% = 161/198) questions correctly when the Persian language was used. When the questions were translated into English, French, and Spanish, ChatGPT answered six, one, and five additional questions correctly, respectively. When comparing the different languages, there was no significant difference in the functioning of ChatGPT in different languages using either the McNemar test or the Binomial test. Conclusion: ChatGPT can deliver above-average performance in the Iranian Medical Residency Examination, demonstrating its potential for using language models in medicine. © 2023 The Authors",yes,"This study evaluates ChatGPT's performance on a clinically relevant medical board exam (Iranian Medical Residency Examination) in multiple languages, which is a direct clinical evaluation of an LLM. The focus is on the model's ability to answer medical questions, which aligns with the inclusion criteria.",10.1016/j.imu.2023.101314,III,Sully,,,
Can off-the-shelf visual large language models detect and diagnose ocular diseases from retinal photographs?,"BACKGROUND: The advent of generative artificial intelligence has led to the 
emergence of multiple vision large language models (VLLMs). This study aimed to 
evaluate the capabilities of commonly available VLLMs, such as OpenAI's GPT-4V 
and Google's Gemini, in detecting and diagnosing ocular diseases from retinal 
images.
METHODS AND ANALYSIS: From the Singapore Epidemiology of Eye Diseases (SEED) 
study, we selected 44 representative retinal photographs, including 10 healthy 
and 34 representing six eye diseases (age-related macular degeneration, diabetic 
retinopathy, glaucoma, visually significant cataract, myopic macular 
degeneration and retinal vein occlusion). OpenAI's GPT-4V (both default and data 
analyst modes) and Google Gemini were prompted with each image to determine if 
the retina was normal or abnormal and to provide diagnostic descriptions if 
deemed abnormal. The outputs from the VLLMs were evaluated for accuracy by three 
attending-level ophthalmologists using a three-point scale (poor, borderline, 
good).
RESULTS: GPT-4V default mode demonstrated the highest detection rate, correctly 
identifying 33 out of 34 detected correctly (97.1%), outperforming its data 
analyst mode (61.8%) and Google Gemini (41.2%). Despite the relatively high 
detection rates, the quality of diagnostic descriptions was generally 
suboptimal-with only 21.2% of GPT-4V's (default) responses, 4.8% of GPT-4V's 
(data analyst) responses and 28.6% for Google Gemini's responses rated as good.
CONCLUSIONS: Although GPT-4V default mode showed generally high sensitivity in 
abnormality detection, all evaluated VLLMs were inadequate in providing accurate 
diagnoses for ocular diseases. These findings emphasise the need for 
domain-customised VLLMs and suggest the continued need for human oversight in 
clinical ophthalmology.

© Author(s) (or their employer(s)) 2025. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ Group.",yes,"This study evaluates the clinical performance of frontier visual large language models (GPT-4V and Gemini) in detecting and diagnosing ocular diseases from retinal photographs. The models are assessed on their ability to identify abnormalities and provide diagnostic descriptions, with results reviewed by ophthalmologists. This is a direct clinical application of LLMs in medicine.",10.1136/bmjophth-2024-002076,I,Sully,,,
A Study on Evaluation of Capability of ChatGPT in management of Drug Related Problems in treatments of Metabolic Disorders.,"Introduction: The application of ChatGPT in managing drug-related problems offers a best approach to enhancing policies of healthcare support in decision-making. Aim and Objectives: The present study mainly involves in Evaluation of Capability of ChatGPT in management of Drug Related Problems and drug counselling among patients suffering with Metabolic Disorders. Methodology: The present study was a community based interventional study conducted for a period of 4 months from June 2024 to September 2024 among 109 patients suffering with metabolic disorders residing in yelahanka region of Bangalore, Karnataka, India. Cases related to metabolic disorders (Hypertension, Diabetes, Thyroid, PCOD) were taken from the population residing within Yelahanka, Bangalore, Karnataka, India. All the prescriptions were analyzed in detail for the identification of drug-related problems (drug interactions, medical errors, wrong dose, therapeutic duplications, drug misuse, wrong prescribing patterns) by using ChatGPT. The information related to adverse drug reactions was collected through patient personal interviews and medication history review and further analyzed by using the Naranjo scale and Hartwig scale which intern verified and confirmed by using ChatGPT to analyze and evaluate the capability of ChatGPT in the management of drug-related problems Statistical Analysis: Microsoft Excel was used for recording and analyzing the data of recruited subjects and by calculating mean, standard deviation, etc. Prism Graph Pad software version 10 will be used for Descriptive statistics, P value was calculated for the present study for statistical significance. Conclusion: By leveraging and monitoring the natural language processing, ChatGPT provides real-time assistance in identifying potential drug interactions, suggesting therapeutic alternatives approaches, and offering drug dosage and dosing recommendations. It also aids both healthcare professionals as well as patients by delivering real clear, accessible information on patient drug safety, side effects, and medication adherence to treatment protocols. Furthermore, it contributes to patient education, improving understanding and medication compliance.",yes,"This study evaluates ChatGPT's capability in managing drug-related problems and providing drug counseling for patients with metabolic disorders. It involves direct clinical application, including identification of drug interactions, medical errors, and providing recommendations, which fits the inclusion criteria.",10.48047/jcdr.2024.15.10.168,I,Sully,,,
"Chatbot Performance in Defining and Differentiating Palliative Care, Supportive Care, Hospice Care","Context: Artificial intelligence (AI) chatbot platforms are increasingly used by patients as sources of information. However, there is limited data on the performance of these platforms, especially regarding palliative care terms. Objectives: We evaluated the accuracy, comprehensiveness, reliability, and readability of three AI platforms in defining and differentiating “palliative care,” “supportive care,” and “hospice care.” Methods: We asked ChatGPT, Microsoft Bing Chat, Google Bard to define and differentiate “palliative care,” “supportive care,” and “hospice care” and provide three references. Outputs were randomized and assessed by six blinded palliative care physicians using 0–10 scales (10 = best) for accuracy, comprehensiveness, and reliability. Readability was assessed using Flesch Kincaid Grade Level and Flesch Reading Ease scores. Results: The mean (SD) accuracy scores for ChatGPT, Bard, and Bing Chat were 9.1 (1.3), 8.7 (1.5), and 8.2 (1.7), respectively; for comprehensiveness, the scores for the three platforms were 8.7 (1.5), 8.1 (1.9), and 5.6 (2.0), respectively; for reliability, the scores were 6.3 (2.5), 3.2 (3.1), and 7.1 (2.4), respectively. Despite generally high accuracy, we identified some major errors (e.g., Bard stated that supportive care had “the goal of prolonging life or even achieving a cure”). We found several major omissions, particularly with Bing Chat (e.g., no mention of interdisciplinary teams in palliative care or hospice care). References were often unreliable. Readability scores did not meet recommended levels for patient educational materials. Conclusion: We identified important concerns regarding the accuracy, comprehensiveness, reliability, and readability of outputs from AI platforms. Further research is needed to improve their performance. © 2024 American Academy of Hospice and Palliative Medicine",yes,"This study evaluates the performance of generative LLM-based chatbots (ChatGPT, Bing Chat, Bard) in providing clinical information about palliative care, supportive care, and hospice care. The evaluation focuses on the accuracy, comprehensiveness, reliability, and readability of their responses to clinical questions, which is a direct clinical application relevant to patient education and information. The study does not focus on technical aspects or non-clinical tasks.",10.1016/j.jpainsymman.2024.01.008,III,Sully,,,
ChatGPT-4 Generates More Accurate and Complete Responses to Common Patient Questions About Anterior Cruciate Ligament Reconstruction Than Google's Search Engine,"Purpose: To replicate a patient's internet search to evaluate ChatGPT's appropriateness in answering common patient questions about anterior cruciate ligament reconstruction compared with a Google web search. Methods: A Google web search was performed by searching the term “anterior cruciate ligament reconstruction.” The top 20 frequently asked questions and responses were recorded. The prompt “What are the 20 most popular patient questions related to ‘anterior cruciate ligament reconstruction?’” was input into ChatGPT and questions and responses were recorded. Questions were classified based on the Rothwell system and responses assessed via Flesch-Kincaid Grade Level, correctness, and completeness were for both Google web search and ChatGPT. Results: Three of 20 (15%) questions were similar between Google web search and ChatGPT. The most common question types among the Google web search were value (8/20, 40%), fact (7/20, 35%), and policy (5/20, 25%). The most common question types amongst the ChatGPT search were fact (12/20, 60%), policy (6/20, 30%), and value (2/20, 10%). Mean Flesch-Kincaid Grade Level for Google web search responses was significantly lower (11.8 ± 3.8 vs 14.3 ± 2.2; P = .003) than for ChatGPT responses. The mean correctness for Google web search question answers was 1.47 ± 0.5, and mean completeness was 1.36 ± 0.5. Mean correctness for ChatGPT answers was 1.8 ± 0.4 and mean completeness was 1.9 ± 0.3, which were both significantly greater than Google web search answers (P = .03 and P = .0003). Conclusions: ChatGPT-4 generated more accurate and complete responses to common patient questions about anterior cruciate ligament reconstruction than Google's search engine. Clinical Relevance: The use of artificial intelligence such as ChatGPT is expanding. It is important to understand the quality of information as well as how the results of ChatGPT queries compare with those from Google web searches © 2024 The Authors",yes,"This study evaluates ChatGPT-4's ability to answer common patient questions about anterior cruciate ligament reconstruction, comparing its responses to those from Google search. The focus is on the clinical appropriateness, correctness, and completeness of LLM-generated answers to patient questions, which is a direct clinical application.",10.1016/j.asmr.2024.100939,II,Joanne ,,,
Transforming orthopaedics with AI: Insights from a custom ChatGPT on ESSKA osteotomy consensus,,yes,"The title suggests the use of a custom ChatGPT model to provide insights on an orthopaedic consensus (ESSKA osteotomy). While the abstract is missing, the title implies a clinical application of an LLM (ChatGPT) in orthopaedics, likely evaluating its performance or utility in a clinical context. This fits the inclusion criteria.",10.1002/ksa.12653,II,Joanne,,,
The interaction of structured data using openEHR and large Language models for clinical decision support in prostate cancer,"Background: Multidisciplinary teams (MDTs) are essential for cancer care but are resource-intensive. Decision-making processes within MDTs, while critical, contribute to increased healthcare costs due to the need for specialist time and coordination. The recent emergence of large language models (LLMs) offers the potential to improve the efficiency and accuracy of clinical decision-making processes, potentially reducing costs associated with traditional MDT models. Methods: We conducted a retrospective study of 171 consecutively treated patients with newly diagnosed prostate cancer. Relevant structured clinical data and the European Association of Urology (EAU) pocket guidelines were provided to two LLMs (chatGPT-4, Claude-3-Opus). LLM treatment recommendations were compared to actual treatment recommendations of the MDT meeting (MDM). Results: Both LLMs demonstrated an overall adherence of 93% with the MDT treatment recommendations. Discrepancies between LLM and MDT recommendations were observed in 15 cases (9%), primarily due to lack of clinical information that could be provided to the LLMs. In 5 cases (3%), the LLM recommendations were not in line with EAU guidelines despite having access to all relevant information. Conclusions: Our findings provide evidence that LLMs can provide accurate treatment recommendations for newly diagnosed prostate cancer patients. LLMs have the potential to streamline MDT workflows, enabling specialists to focus on complex cases and patient-centered discussions. Patient Summary: In this study, we explored the potential of artificial intelligence models called large language models (LLMs) to assist in treatment decision-making for prostate cancer patients. We found that LLMs, when provided with patient information and clinical guidelines, can recommend treatments that closely match those made by a team of cancer specialists, suggesting that LLMs could help streamline the decision-making process and potentially reduce healthcare costs. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",yes,"This study evaluates the use of large language models (ChatGPT-4 and Claude-3-Opus) for making clinical treatment recommendations for prostate cancer patients, comparing their outputs to multidisciplinary team decisions. It directly assesses LLMs in a clinical decision support context, which fits the inclusion criteria.",10.1007/s00345-024-05423-1,I,Joanne,,,
ChatGPT4 Outperforms Endoscopists for Determination of Postcolonoscopy Rescreening and Surveillance Recommendations,"Background & Aims: Large language models including Chat Generative Pretrained Transformers version 4 (ChatGPT4) improve access to artificial intelligence, but their impact on the clinical practice of gastroenterology is undefined. This study compared the accuracy, concordance, and reliability of ChatGPT4 colonoscopy recommendations for colorectal cancer rescreening and surveillance with contemporary guidelines and real-world gastroenterology practice. Methods: History of present illness, colonoscopy data, and pathology reports from patients undergoing procedures at 2 large academic centers were entered into ChatGPT4 and it was queried for the next recommended colonoscopy follow-up interval. Using the McNemar test and inter-rater reliability, we compared the recommendations made by ChatGPT4 with the actual surveillance interval provided in the endoscopist's procedure report (gastroenterology practice) and the appropriate US Multisociety Task Force (USMSTF) guidance. The latter was generated for each case by an expert panel using the clinical information and guideline documents as reference. Results: Text input of de-identified data into ChatGPT4 from 505 consecutive patients undergoing colonoscopy between January 1 and April 30, 2023, elicited a successful follow-up recommendation in 99.2% of the queries. ChatGPT4 recommendations were in closer agreement with the USMSTF Panel (85.7%) than gastroenterology practice recommendations with the USMSTF Panel (75.4%) (P < .001). Of the 14.3% discordant recommendations between ChatGPT4 and the USMSTF Panel, recommendations were for later screening in 26 (5.1%) and for earlier screening in 44 (8.7%) cases. The inter-rater reliability was good for ChatGPT4 vs USMSTF Panel (Fleiss κ, 0.786; 95% CI, 0.734–0.838; P < .001). Conclusions: Initial real-world results suggest that ChatGPT4 can define routine colonoscopy screening intervals accurately based on verbatim input of clinical data. Large language models have potential for clinical applications, but further training is needed for broad use. © 2024 AGA Institute",yes,"This study directly evaluates ChatGPT-4's performance in making clinical recommendations for postcolonoscopy rescreening and surveillance, comparing its outputs to both expert guidelines and real-world clinical practice. The use of real patient data and the focus on a clinical decision task (screening interval recommendations) make it a clear example of LLM evaluation in a healthcare context.",10.1016/j.cgh.2024.04.022,I,Joanne,,,
Comparative Performance of Anthropic Claude and OpenAI GPT Models in Basic Radiological Imaging Tasks,"Background: Publicly available artificial intelligence (AI) Vision Language Models (VLMs) are constantly improving. The advent of vision capabilities on these models could enhance radiology workflows. Evaluating their performance in radiological image interpretation is vital to their potential integration into practice. Aim: This study aims to evaluate the proficiency and consistency of the publicly available VLMs, Anthropic's Claude and OpenAI's GPT, across multiple iterations in basic image interpretation tasks. Method: Subsets from publicly available datasets, ROCOv2 and MURAv1.1, were used to evaluate 6 VLMs. A system prompt and image were input into each model three times. The outputs were compared to the dataset captions to evaluate each model's accuracy in recognising the modality, anatomy, and detecting fractures on radiographs. The consistency of the output across iterations was also analysed. Results: Evaluation of the ROCOv2 dataset showed high accuracy in modality recognition, with some models achieving 100%. Anatomical recognition ranged between 61% and 85% accuracy across all models tested. On the MURAv1.1 dataset, Claude-3.5-Sonnet had the highest anatomical recognition with 57% accuracy, while GPT-4o had the best fracture detection with 62% accuracy. Claude-3.5-Sonnet was the most consistent model, with 83% and 92% consistency in anatomy and fracture detection, respectively. Conclusion: Given Claude and GPT's current accuracy and reliability, the integration of these models into clinical settings is not yet feasible. This study highlights the need for ongoing development and establishment of standardised testing techniques to ensure these models achieve reliable performance. © 2025 Royal Australian and New Zealand College of Radiologists.",yes,"This study evaluates the clinical performance of generative LLMs (Claude and GPT models with vision capabilities) in interpreting radiological images, including tasks such as modality recognition, anatomical identification, and fracture detection. The focus is on clinical image interpretation, which is a direct clinical application. The models tested (Claude, GPT-4o) are frontier LLMs with vision capabilities, and the study assesses their accuracy and consistency in a healthcare context.",10.1111/1754-9485.13858,II,Joanne,,,
"Performance of GPT-4 on the American College of Radiology In-training Examination: Evaluating Accuracy, Model Drift, and Fine-tuning","2292. Acad Radiol. 2024 Jul;31(7):3046-3054. doi: 10.1016/j.acra.2024.04.006. Epub 
2024 Apr 22.

Performance of GPT-4 on the American College of Radiology In-training 
Examination: Evaluating Accuracy, Model Drift, and Fine-tuning.

Payne DL(1), Purohit K(2), Borrero WM(2), Chung K(2), Hao M(2), Mpoy M(2), Jin 
M(2), Prasanna P(3), Hill V(4).

Author information:
(1)Stony Brook University Hospital Department of Radiology, 101 Nicolls Road, 
Stony Brook, New York 11794, USA (D.L.P., K.P., W.M.B., K.C., M.H., M.M., M.J.); 
Stony Brook University Department of Biomedical Informatics, 1 Lauterbur Drive, 
Stony Brook, New York 11794, USA (D.L.P., P.P.). Electronic address: 
david.payne@stonybrookmedicine.edu.
(2)Stony Brook University Hospital Department of Radiology, 101 Nicolls Road, 
Stony Brook, New York 11794, USA (D.L.P., K.P., W.M.B., K.C., M.H., M.M., M.J.).
(3)Stony Brook University Department of Biomedical Informatics, 1 Lauterbur 
Drive, Stony Brook, New York 11794, USA (D.L.P., P.P.).
(4)Northwestern University Feinberg School of Medicine Department of Radiology, 
676 North Clair Street, Chicago, Illinois 60611, USA (V.H.).

Comment in
    Acad Radiol. 2024 Aug;31(8):3446-3447. doi: 10.1016/j.acra.2024.07.018.

RATIONALE AND OBJECTIVES: In our study, we evaluate GPT-4's performance on the 
American College of Radiology (ACR) 2022 Diagnostic Radiology In-Training 
Examination (DXIT). We perform multiple experiments across time points to assess 
for model drift, as well as after fine-tuning to assess for differences in 
accuracy.
MATERIALS AND METHODS: Questions were sequentially input into GPT-4 with a 
standardized prompt. Each answer was recorded and overall accuracy was 
calculated, as was logic-adjusted accuracy, and accuracy on image-based 
questions. This experiment was repeated several months later to assess for model 
drift, then again after the performance of fine-tuning to assess for changes in 
GPT's performance.
RESULTS: GPT-4 achieved 58.5% overall accuracy, lower than the PGY-3 average 
(61.9%) but higher than the PGY-2 average (52.8%). Adjusted accuracy was 52.8%. 
GPT-4 showed significantly higher (p = 0.012) confidence for correct answers 
(87.1%) compared to incorrect (84.0%). Performance on image-based questions was 
significantly poorer (p < 0.001) at 45.4% compared to text-only questions 
(80.0%), with adjusted accuracy for image-based questions of 36.4%. When the 
questions were repeated, GPT-4 chose a different answer 25.5% of the time and 
there was no change in accuracy. Fine-tuning did not improve accuracy.
CONCLUSION: GPT-4 performed between PGY-2 and PGY-3 levels on the 2022 DXIT, 
significantly poorer on image-based questions, and with large variability in 
answer choices across time points. Exploratory experiments in fine-tuning did 
not improve performance. This study underscores the potential and risks of using 
minimally-prompted general AI models in interpreting radiologic images as a 
diagnostic tool. Implementers of general AI radiology systems should exercise 
caution given the possibility of spurious yet confident responses.

Copyright © 2024 The Association of University Radiologists. Published by 
Elsevier Inc. All rights reserved.",yes,"This study evaluates GPT-4's performance on a clinically relevant board examination (the ACR Diagnostic Radiology In-Training Examination), including its accuracy, model drift, and fine-tuning effects. The focus is on clinical knowledge and diagnostic reasoning, which aligns with the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1016/j.acra.2024.04.006,III,Joanne,,,
Performance of ChatGPT on American Board of Surgery In-Training Examination Preparation Questions,"Introduction: Chat Generative Pretrained Transformer (ChatGPT) is a large language model capable of generating human-like text. This study sought to evaluate ChatGPT's performance on Surgical Council on Resident Education (SCORE) self-assessment questions. Methods: General surgery multiple choice questions were randomly selected from the SCORE question bank. ChatGPT (GPT-3.5, April-May 2023) evaluated questions and responses were recorded. Results: ChatGPT correctly answered 123 of 200 questions (62%). ChatGPT scored lowest on biliary (2/8 questions correct, 25%), surgical critical care (3/10, 30%), general abdomen (1/3, 33%), and pancreas (1/3, 33%) topics. ChatGPT scored higher on biostatistics (4/4 correct, 100%), fluid/electrolytes/acid-base (4/4, 100%), and small intestine (8/9, 89%) questions. ChatGPT answered questions with thorough and structured support for its answers. It scored 56% on ethics questions and provided coherent explanations regarding end-of-life discussions, communication with coworkers and patients, and informed consent. For many questions answered incorrectly, ChatGPT provided cogent, yet factually incorrect descriptions, including anatomy and steps of operations. In two instances, it gave a correct explanation but chose the wrong answer. It did not answer two questions, stating it needed additional information to determine the next best step in treatment. Conclusions: ChatGPT answered 62% of SCORE questions correctly. It performed better at questions requiring standard recall but struggled with higher-level questions that required complex clinical decision making, despite providing detailed responses behind its rationale. Due to its mediocre performance on this question set and sometimes confidently-worded, yet factually inaccurate responses, caution should be used when interpreting ChatGPT's answers to general surgery questions. © 2024 Elsevier Inc.",yes,"This study evaluates ChatGPT's (GPT-3.5) performance on clinically relevant board exam preparation questions for general surgery, which is a direct clinical application of an LLM in a healthcare context. The study assesses the model's ability to answer questions that are used for clinical training and decision-making.",10.1016/j.jss.2024.04.060,III,Joanne,,,
Chat GPT as a Neuro-Score Calculator: Analysis of a Large Language Model's Performance on Various Neurological Exam Grading Scales,"Background: ChatGPT is a large language model artificial intelligence chatbot that has been applied to different aspects of the medical field. Our study aims to assess the quality of chatGPT to evaluate patients based on their exams for different scores including Glasgow Coma Scale (GCS), intracranial hemorrhage score (ICH), and Hunt & Hess (H&H) classification. Methods: We created batches of patient test cases with detailed neurological exams, totaling 20 cases and created variants of increasing complex phrasing of the test cases. Using ChatGPT, we assessed repeatability and quantified the errors, including the average error rate (AER) and magnitude of errors (AME). We repeated this process for the H&H and the ICH score using base cases. Specific prompts were created for each calculator. Results: The GCS calculator on 10 base test cases had an AER/AME of 10%/0.150. The accuracy of ChatGPT decreased with increasing complexity; for example, in a variation where crucial information was missing, the AER was 45% for 20 cases. For H&H, AER/AME was 13%/0.13 and for ICH, AER/AME was 27.5%/0.325. Using a simple prompt resulted in a significantly higher error rate of 70%. Conclusions: ChatGPT demonstrates ability in this proof-of-concept experiment in evaluating neuroexams using established assessment scales including GCS, ICH, and H&H. However, it has limitations in accuracy and may “hallucinate” with complex or vague descriptions. Nonetheless, ChatGPT, has promising potential in medicine. © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's performance in calculating clinical neurological scores (GCS, ICH, H&H) based on patient exam scenarios, which is a direct clinical application relevant to decision support and assessment in medicine.",10.1016/j.wneu.2023.08.088,II,Joanne,,,
"GPT-4 generated psychological reports in psychodynamic perspective: a pilot study on quality, risk of hallucination and client satisfaction","Background: Recently, there have been active proposals on how to utilize large language models (LLMs) in the fields of psychiatry and counseling. It would be interesting to develop programs with LLMs that generate psychodynamic assessments to help individuals gain insights about themselves, and to evaluate the features of such services. However, studies on this subject are rare. This pilot study aims to evaluate quality, risk of hallucination (incorrect AI-generated information), and client satisfaction with psychodynamic psychological reports generated by GPT-4. Methods: The report comprised five components: psychodynamic formulation, psychopathology, parental influence, defense mechanisms, and client strengths. Participants were recruited from individuals distressed by repetitive interpersonal issues. The study was conducted in three steps: 1) Questions provided to participants, designed to create psychodynamic formulations: 14 questions were generated by GPT for inferring psychodynamic formulations, while 6 fixed questions focused on the participants’ relationship with their parents. A total of 20 questions were provided. Using participants’ responses to these questions, GPT-4 generated the psychological reports. 2) Seven professors of psychiatry from different university hospitals evaluated the quality and risk of hallucinations in the psychological reports by reading the reports only, without meeting the participants. This quality assessment compared the psychological reports generated by GPT-4 with those inferred by the experts. 3) Participants evaluated their satisfaction with the psychological reports. All assessments were conducted using self-report questionnaires based on a Likert scale developed for this study. Results: A total of 10 participants were recruited, and the average age was 32 years. The median response indicated that quality of all five components of the psychological report was similar to the level inferred by the experts. The risk of hallucination was assessed as ranging from unlikely to minor. According to the median response in the satisfaction evaluation, the participants agreed that the report is clearly understandable, insightful, credible, useful, satisfying, and recommendable. Conclusion: This study suggests the possibility that artificial intelligence could assist users by providing psychodynamic interpretations. Copyright © 2025 Kim, Lee, Park, On, Lee, Keum, Oh, Song, Lee, Won, Shin, Lho, Hwang and Kim.",yes,"This study evaluates GPT-4, a large language model, in generating psychodynamic psychological reports for individuals with interpersonal issues. The study assesses the quality of these reports, risk of hallucination, and client satisfaction, all of which are relevant to clinical application in psychiatry and counseling. The evaluation involves both expert comparison and patient feedback, making it a clinical use case.",10.3389/fpsyt.2025.1473614,II,Joanne,,,
Information Quality and Readability: ChatGPT's Responses to the Most Common Questions About Spinal Cord Injury,"Objective: This study aimed to assess the quality, readability, and comprehension of texts generated by ChatGPT in response to commonly asked questions about spinal cord injury (SCI). Methods: The study utilized Google Trends to identify the most frequently searched keywords related to SCI. The identified keywords were sequentially inputted into ChatGPT, and the resulting responses were assessed for quality using the Ensuring Quality Information for Patients (EQIP) tool. The readability of the texts was analyzed using the Flesch-Kincaid grade level and the Flesch-Kincaid reading ease parameters. Results: The mean EQIP score of the texts was determined to be 43.02 ± 6.37, the Flesch-Kincaid reading ease score to be 26.24 ± 13.81, and the Flesch-Kincaid grade level was determined to be 14.84 ± 1.79. The analysis revealed significant concerns regarding the quality of texts generated by ChatGPT, indicating serious problems with readability and comprehension. The mean EQIP score was low, suggesting a need for improvement in the accuracy and reliability of the information provided. The Flesch-Kincaid grade level indicated a high linguistic complexity, requiring a level of education equivalent to approximately 14 to 15 years of formal education for comprehension. Conclusions: The results of this study show heightened complexity in ChatGPT-generated SCI texts, surpassing optimal health communication readability. ChatGPT currently cannot substitute comprehensive medical consultations. Enhancing text quality could be attainable through dependence on credible sources, the establishment of a scientific board, and collaboration with expert teams. Addressing these concerns could improve text accessibility, empowering patients and facilitating informed decision-making in SCI. © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's responses to common patient questions about spinal cord injury, specifically assessing the quality and readability of its answers. The focus is on the clinical application of an LLM (ChatGPT) in providing health information to patients, which fits the inclusion criteria.",10.1016/j.wneu.2023.11.062,II,Joanne,,,
Assessing the performance of ChatGPT's responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval,"Background: Epilepsy is a neurological condition marked by frequent seizures and various cognitive and psychological effects. Reliable information is essential for effective treatment. Natural language processing models like ChatGPT are increasingly used in healthcare for information access and data analysis, making it crucial to assess their accuracy. Objective: This study aimed to investigate the accuracy of ChatGPT in providing educational information related to epilepsy. Methods: We compared the answers from ChatGPT-4 and ChatGPT-3.5 to 57 common epilepsy questions based on the Korean Epilepsy Society's ""Epilepsy Patient and Caregiver Guide."" Two epileptologists reviewed the responses, with a third serving as an arbiter in cases of disagreement. Results: Out of 57 questions, 40 responses from ChatGPT-4 had ""sufficient educational value,"" 16 were ""correct but inadequate,"" and one was ""mixed with correct and incorrect"" information. No answers were entirely incorrect. GPT-4 generally outperformed GPT-3.5 and was often on par with or better than the official guide. Conclusions: ChatGPT-4 shows promise as a tool for delivering reliable epilepsy-related information and could help alleviate the educational burden on healthcare professionals. Further research is needed to explore the benefits and limitations of using such models in medical contexts. © 2023",yes,"This study evaluates the accuracy of ChatGPT-3.5 and ChatGPT-4 in answering patient and caregiver questions about epilepsy, with responses reviewed by clinical experts. The focus is on the clinical utility of LLMs in providing medical information, which aligns with the inclusion criteria.",10.1016/j.seizure.2023.11.013,II,Joanne,,,
Evaluating base and retrieval augmented LLMs with document or online support for evidence based neurology,"Effectively managing evidence-based information is increasingly challenging. This study tested large language models (LLMs), including document- and online-enabled retrieval-augmented generation (RAG) systems, using 13 recent neurology guidelines across 130 questions. Results showed substantial variability. RAG improved accuracy compared to base models but still produced potentially harmful answers. RAG-based systems performed worse on case-based than knowledge-based questions. Further refinement and improved regulation is needed for safe clinical integration of RAG-enhanced LLMs. © The Author(s) 2025.",yes,"This study evaluates large language models, including retrieval-augmented LLMs, on their ability to answer clinical questions based on neurology guidelines. The focus is on clinical application (evidence-based neurology questions), and the models tested are LLMs. This fits the inclusion criteria.",10.1038/s41746-025-01536-y,II,Joanne,,,
"ChatGPT v4 outperforming v3.5 on cancer treatment recommendations in quality, clinical guideline, and expert opinion concordance","Objectives: To assess the quality and alignment of ChatGPT's cancer treatment recommendations (RECs) with National Comprehensive Cancer Network (NCCN) guidelines and expert opinions. Methods: Three urologists performed quantitative and qualitative assessments in October 2023 analyzing responses from ChatGPT-4 and ChatGPT-3.5 to 108 prostate, kidney, and bladder cancer prompts using two zero-shot prompt templates. Performance evaluation involved calculating five ratios: expert-approved/expert-disagreed and NCCN-aligned RECs against total ChatGPT RECs plus coverage and adherence rates to NCCN. Experts rated the response's quality on a 1-5 scale considering correctness, comprehensiveness, specificity, and appropriateness. Results: ChatGPT-4 outperformed ChatGPT-3.5 in prostate cancer inquiries, with an average word count of 317.3 versus 124.4 (p < 0.001) and 6.1 versus 3.9 RECs (p < 0.001). Its rater-approved REC ratio (96.1% vs. 89.4%) and alignment with NCCN guidelines (76.8% vs. 49.1%, p = 0.001) were superior and scored significantly better on all quality dimensions. Across 108 prompts covering three cancers, ChatGPT-4 produced an average of 6.0 RECs per case, with an 88.5% approval rate from raters, 86.7% NCCN concordance, and only a 9.5% disagreement rate. It achieved high marks in correctness (4.5), comprehensiveness (4.4), specificity (4.0), and appropriateness (4.4). Subgroup analyses across cancer types, disease statuses, and different prompt templates were reported. Conclusions: ChatGPT-4 demonstrated significant improvement in providing accurate and detailed treatment recommendations for urological cancers in line with clinical guidelines and expert opinion. However, it is vital to recognize that AI tools are not without flaws and should be utilized with caution. ChatGPT could supplement, but not replace, personalized advice from healthcare professionals. © The Author(s) 2024.",yes,"This study directly evaluates the clinical performance of two versions of ChatGPT (3.5 and 4) in generating cancer treatment recommendations, comparing their outputs to clinical guidelines (NCCN) and expert opinion. The focus is on clinical decision support, guideline adherence, and quality of recommendations, which fits the inclusion criteria.",10.1177/20552076241269538,II,Joanne,,,
ChatGPT and Rhinoplasty Recovery: An Exploration of AI's Role in Postoperative Guidance,"2291. Facial Plast Surg. 2024 Oct;40(5):628-631. doi: 10.1055/a-2219-4901. Epub 2023 
Nov 29.

ChatGPT and Rhinoplasty Recovery: An Exploration of AI's Role in Postoperative 
Guidance.

Capelleras M(1), Soto-Galindo GA(1), Cruellas M(2), Apaydin F(3).

Author information:
(1)Department of Otolaryngology Head & Neck Surgery, Ege University, Izmir, 
Türkiye.
(2)Department of Otorhinolaryngology, Hospital Universitari de Bellvitge, 
Barcelona, Spain.
(3)Department of Otorhinolaryngology, Ege University Medical Faculty, Izmir, 
Türkiye.

The potential applications of artificial intelligence (AI) in health care have 
garnered significant interest in recent years. This study presents the first 
published exploration of ChatGPT, an AI language model, as a tool for providing 
postoperative guidance during rhinoplasty recovery. The primary objective was to 
shed light on the role of ChatGPT in augmenting patient care during the critical 
postoperative phase. Using the Rhinobase database, standardized questions were 
formulated to evaluate AI-generated responses addressing pain management, 
swelling, bruising, and potential asymmetries. Results demonstrated that ChatGPT 
has the potential to enhance patient education and alleviate emotional distress 
by providing general information and reassurance during the recovery process. 
However, the study emphasized that AI should not replace personalized advice 
from qualified health care professionals. This pioneering investigation offers 
valuable insights into the integration of AI and human expertise, paving the way 
for optimized postrhinoplasty recovery care.

Thieme. All rights reserved.",yes,"This study evaluates ChatGPT, a generative LLM, in the context of providing postoperative guidance to rhinoplasty patients. It assesses ChatGPT's responses to standardized clinical questions about recovery, which is a direct clinical application relevant to patient care.",10.1055/a-2219-4901,II,Joanne,,,
"Assessing the readability, quality and reliability of responses produced by ChatGPT, Gemini, and Perplexity regarding most frequently asked keywords about low back pain","Background: Patients who are informed about the causes, pathophysiology, treatment and prevention of a disease are better able to participate in treatment procedures in the event of illness. Artificial intelligence (AI), which has gained popularity in recent years, is defined as the study of algorithms that provide machines with the ability to reason and perform cognitive functions, including object and word recognition, problem solving and decision making. This study aimed to examine the readability, reliability and quality of responses to frequently asked keywords about low back pain (LBP) given by three different AI-based chatbots (ChatGPT, Perplexity and Gemini), which are popular applications in online information presentation today. Methods: All three AI chatbots were asked the 25 most frequently used keywords related to LBP determined with the help of Google Trend. In order to prevent possible bias that could be created by the sequential processing of keywords in the answers given by the chatbots, the study was designed by providing input from different users (EO, VH) for each keyword. The readability of the responses given was determined with the Simple Measure of Gobbledygook (SMOG), Flesch Reading Ease Score (FRES) and Gunning Fog (GFG) readability scores. Quality was assessed using the Global Quality Score (GQS) and the Ensuring Quality Information for Patients (EQIP) score. Reliability was assessed by determining with DISCERN and Journal of American Medical Association (JAMA) scales. Results: The first three keywords detected as a result of Google Trend search were “Lower Back Pain”, “ICD 10 Low Back Pain”, and “Low Back Pain Symptoms”. It was determined that the readability of the responses given by all AI chatbots was higher than the recommended 6th grade readability level (p < 0.001). In the EQIP, JAMA, modified DISCERN and GQS score evaluation, Perplexity was found to have significantly higher scores than other chatbots (p < 0.001). Conclusion: It has been determined that the answers given by AI chatbots to keywords about LBP are difficult to read and have low reliability and quality assessment. It is clear that when new chatbots are introduced, they can provide better guidance to patients with increased clarity and text quality. This study can provide inspiration for future studies on improving the algorithms and responses of AI chatbots.",yes,"This study evaluates the clinical utility of LLMs (ChatGPT, Gemini, and Perplexity) by assessing the readability, quality, and reliability of their responses to common patient questions about low back pain. The focus is on the ability of these LLMs to provide patient-facing clinical information, which is a relevant clinical application. The study does not focus on technical architecture or non-clinical tasks.",10.7717/peerj.18847,II,Joanne,,,
Leveraging artificial intelligence to detect ethical concerns in medical research: A case study,"Background Institutional review boards (IRBs) have been criticised for delays in approvals for research proposals due to inadequate or inexperienced IRB staff. Artificial intelligence (AI), particularly large language models (LLMs), has significant potential to assist IRB members in a prompt and efficient reviewing process. Methods Four LLMs were evaluated on whether they could identify potential ethical issues in seven validated case studies. The LLMs were prompted with queries related to the proposed eligibility criteria of the study participants, vulnerability issues, information to be disclosed in the informed consent document (ICD), risk-benefit assessment and justification of the use of a placebo. Another query was issued to the LLMs to generate ICDs for these case scenarios. Results All four LLMs were able to provide answers to the queries related to all seven cases. In general, the responses were homogeneous with respect to most elements. LLMs performed suboptimally in identifying the suitability of the placebo arm, risk mitigation strategies and potential risks to study participants in certain case studies with a single prompt. However, multiple prompts led to better outputs in all of these domains. Each of the LLMs included all of the fundamental elements of the ICD for all case scenarios. Use of jargon, understatement of benefits and failure to state potential risks were the key observations in the AI-generated ICD. Conclusion It is likely that LLMs can enhance the identification of potential ethical issues in clinical research, and they can be used as an adjunct tool to prescreen research proposals and enhance the efficiency of an IRB. © Author(s) (or their employer(s)) 2025.",yes,"This study evaluates the use of LLMs to identify ethical issues in clinical research proposals and to generate informed consent documents, both of which are directly related to clinical research processes. The LLMs are assessed for their ability to support IRB decision-making, which is a clinical application relevant to healthcare. The focus is not on technical aspects or non-clinical tasks, but on a real-world clinical workflow (IRB review and consent documentation).",10.1136/jme-2023-109767,II,Joanne,,,
"ChatGPT and Clinical Decision Support: Scope, Application, and Limitations","This study examines ChatGPT’s role in clinical decision support, by analyzing its scope, application, and limitations. By analyzing patient data and providing evidence-based recommendations, ChatGPT, an AI language model, can help healthcare professionals make well-informed decisions. This study examines ChatGPT’s use in clinical decision support, including diagnosis and treatment planning. However, it acknowledges limitations like biases, lack of contextual understanding, and human oversight and also proposes a framework for the future clinical decision support system. Understanding these factors will allow healthcare professionals to utilize ChatGPT effectively and make accurate clinical decisions. Further research is needed to understand the implications of using ChatGPT in healthcare settings and to develop safeguards for responsible use.",yes,"This study evaluates ChatGPT's application in clinical decision support, including diagnosis and treatment planning, which is a direct clinical use of an LLM. It discusses the model's scope, application, and limitations in a healthcare context, fitting the inclusion criteria.",10.1007/s10439-023-03329-4,X,Sully,,,
Let's chat about cervical cancer: Assessing the accuracy of ChatGPT responses to cervical cancer questions,"Objective: To quantify the accuracy of ChatGPT in answering commonly asked questions pertaining to cervical cancer prevention, diagnosis, treatment, and survivorship/quality-of-life (QOL). Methods: ChatGPT was queried with 64 questions adapted from professional society websites and the authors' clinical experiences. The answers were scored by two attending Gynecologic Oncologists according to the following scale: 1) correct and comprehensive, 2) correct but not comprehensive, 3) some correct, some incorrect, and 4) completely incorrect. Scoring discrepancies were resolved by additional reviewers as needed. The proportion of responses earning each score were calculated overall and within each question category. Results: ChatGPT provided correct and comprehensive answers to 34 (53.1%) questions, correct but not comprehensive answers to 19 (29.7%) questions, partially incorrect answers to 10 (15.6%) questions, and completely incorrect answers to 1 (1.6%) question. Prevention and survivorship/QOL had the highest proportion of “correct” scores (scores of 1 or 2) at 22/24 (91.7%) and 15/16 (93.8%), respectively. ChatGPT performed less well in the treatment category, with 15/21 (71.4%) correct scores. It performed the worst in the diagnosis category with only 1/3 (33.3%) correct scores. Conclusion: ChatGPT accurately answers questions about cervical cancer prevention, survivorship, and QOL. It performs less accurately for cervical cancer diagnosis and treatment. Further development of this immensely popular large language model should include physician input before it can be utilized as a tool for Gynecologists or recommended as a patient resource for information on cervical cancer diagnosis and treatment. © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's accuracy in answering clinical questions about cervical cancer, including prevention, diagnosis, treatment, and survivorship. The evaluation is performed by clinicians and focuses on the model's ability to provide correct and comprehensive clinical information, which is a direct clinical application.",10.1016/j.ygyno.2023.11.008,II,Joanne,,,
The Transformative Potential of Large Language Models in Mining Electronic Health Records Data,"Introduction: In this study, we evaluate the accuracy, efficiency, and cost-effectiveness of Large Language Models (LLMs) in extracting and structuring information from free-text clinical reports, particularly in identifying and classifying patient comorbidities within oncology electronic health records. We specifically compare the performance of gpt-3.5-turbo-1106 and gpt-4-1106-preview models against that of specialized human evaluators. Methods: We implemented a script using the OpenAI API to extract structured information in JSON format from comorbidities reported in 250 personal history reports. These reports were manually reviewed in batches of 50 by five specialists in radiation oncology. We compared the results using metrics such as Sensitivity, Specificity, Precision, Accuracy, F-value, Kappa index, and the McNemar test, in addition to examining the common causes of errors in both humans and GPT models. Results: The GPT-3.5 model exhibited slightly lower performance compared to physicians across all metrics, though the differences were not statistically significant (McNemar’s test p = 0.79). GPT-4 demonstrated clear superiority in several key metrics (McNemar’s test p < 0.001). Notably, it achieved a sensitivity of 96.8%, compared to 88.2% for GPT-3.5 and 88.8% for physicians. However, physicians marginally outperformed GPT-4 in precision (97.7% vs. 96.8%). GPT-4 showed greater consistency, replicating the exact same results in 76% of the reports across 10 repeated analyses, compared to 59% for GPT-3.5, indicating more stable and reliable performance. Physicians were more likely to miss explicit comorbidities, while the GPT models more frequently inferred non-explicit comorbidities, sometimes correctly, though this also resulted in more false positives. Conclusion: This study demonstrates that, with well-designed prompts, the LLMs examined can match or even surpass medical specialists in extracting information from complex clinical reports. Their superior efficiency in time and costs, along with easy integration with databases, makes them a valuable tool for large-scale data mining and real-world evidence generation.",yes,"The study evaluates LLMs (GPT-3.5 and GPT-4) in extracting and structuring clinical information (comorbidities) from free-text electronic health records, comparing their performance to human specialists. Although the primary task is data extraction/structuring, the context is clinical (identifying comorbidities in oncology EHRs), and the LLMs' outputs are directly compared to clinical experts. This is a borderline case, but since the extraction of comorbidities from clinical notes is a clinically relevant task (with implications for patient care and research), and the LLMs are evaluated in a real-world clinical context, it fits the inclusion criteria.",10.1101/2024.03.07.24303588,X,Joanne,,,
Fact Check: Assessing the Response of ChatGPT to Alzheimer’s Disease Statements with Varying Degrees of Misinformation,"Background There are many myths regarding Alzheimer’s disease (AD) that have been circulated on the Internet, each exhibiting varying degrees of accuracy, inaccuracy, and misinformation. Large language models such as ChatGPT, may be a useful tool to help assess these myths for veracity and inaccuracy. However, they can induce misinformation as well. The objective of this study is to assess ChatGPT's ability to identify and address AD myths with reliable information. Methods We conducted a cross-sectional study of clinicians’ evaluation of ChatGPT (GPT 4.0)’s responses to 20 selected AD myths. We prompted ChatGPT to express its opinion on each myth and then requested it to rephrase its explanation using a simplified language that could be more readily understood by individuals with a middle school education. We implemented a survey using Redcap to determine the degree to which clinicians agreed with the accuracy of each ChatGPT’s explanation and the degree to which the simplified rewriting was readable and retained the message of the original. We also collected their explanation on any disagreement with ChatGPT’s responses. We used five Likert-type scale with a score ranging from -2 to 2 to quantify clinicians’ agreement in each aspect of the evaluation. Results The clinicians (n=11) were generally satisfied with ChatGPT’s explanations, with a mean (SD) score of 1.0(±0.3) across the 20 myths. While ChatGPT correctly identified that all the 20 myths were inaccurate, some clinicians disagreed with its explanations on 7 of the myths. Overall, 9 of the 11 professionals either agreed or strongly agreed that ChatGPT has the potential to provide meaningful explanations of certain myths. Conclusions The majority of surveyed healthcare professionals acknowledged the potential value of ChatGPT in mitigating AD misinformation. However, the need for more refined and detailed explanations of the disease’s mechanisms and treatments was highlighted.",yes,"This study evaluates ChatGPT (GPT-4.0) in a clinical context by assessing its ability to address and correct misinformation about Alzheimer's disease, with clinicians rating the accuracy and readability of its responses. The focus is on ChatGPT's performance in providing reliable health information, which is a clinical application relevant to patient education and misinformation mitigation.",10.1101/2023.09.04.23294917,X,Vivian,"data=myths about AD circulating on Internet (technically, ""real"" not synthesized data but not patient/clinical data and also not a reliable source)",,
"ChatGPT-3.5 and Bing Chat in ophthalmology: an updated evaluation of performance, readability, and informative sources","BACKGROUND/OBJECTIVES: Experimental investigation. Bing Chat (Microsoft) 
integration with ChatGPT-4 (OpenAI) integration has conferred the capability of 
accessing online data past 2021. We investigate its performance against 
ChatGPT-3.5 on a multiple-choice question ophthalmology exam.
SUBJECTS/METHODS: In August 2023, ChatGPT-3.5 and Bing Chat were evaluated 
against 913 questions derived from the Academy's Basic and Clinical Science 
Collection collection. For each response, the sub-topic, performance, Simple 
Measure of Gobbledygook readability score (measuring years of required education 
to understand a given passage), and cited resources were collected. The primary 
outcomes were the comparative scores between models, and qualitatively, the 
resources referenced by Bing Chat. Secondary outcomes included performance 
stratified by response readability, question type (explicit or situational), and 
BCSC sub-topic.
RESULTS: Across 913 questions, ChatGPT-3.5 scored 59.69% [95% CI 56.45,62.94] 
while Bing Chat scored 73.60% [95% CI 70.69,76.52]. Both models performed 
significantly better in explicit than clinical reasoning questions. Both models 
performed best on general medicine questions than ophthalmology subsections. 
Bing Chat referenced 927 online entities and provided at-least one citation to 
836 of the 913 questions. The use of more reliable (peer-reviewed) sources was 
associated with higher likelihood of correct response. The most-cited resources 
were eyewiki.aao.org, aao.org, wikipedia.org, and ncbi.nlm.nih.gov. Bing Chat 
showed significantly better readability than ChatGPT-3.5, averaging a reading 
level of grade 11.4 [95% CI 7.14, 15.7] versus 12.4 [95% CI 8.77, 16.1], 
respectively (p-value < 0.0001, ρ = 0.25).
CONCLUSIONS: The online access, improved readability, and citation feature of 
Bing Chat confers additional utility for ophthalmology learners. We recommend 
critical appraisal of cited sources during response interpretation.

© 2024. The Author(s), under exclusive licence to The Royal College of 
Ophthalmologists.",yes,"This study evaluates the clinical performance of ChatGPT-3.5 and Bing Chat (integrated with ChatGPT-4) on a multiple-choice ophthalmology exam, directly assessing their ability to answer clinically relevant questions. The focus is on their accuracy, readability, and citation of sources in a medical context, which aligns with the inclusion criteria for clinical evaluation of LLMs.",10.1038/s41433-024-03037-w,III,Vivian,"sythesized data, fake task, not clinical study",,
Perspectives on AI-based recommendations for mask-wearing and COVID-19 vaccination for transplant recipients in the post-COVID-19 era,"3558. Ren Fail. 2024 Dec;46(1):2337291. doi: 10.1080/0886022X.2024.2337291. Epub 2024 
Apr 7.

Perspectives on AI-based recommendations for mask-wearing and COVID-19 
vaccination for transplant recipients in the post-COVID-19 era.

Garcia Valencia OA(1), Thongprayoon C(1), Miao J(1), Bruminhent J(2)(3), Craici 
IM(1), Cheungpasitporn W(1).

Author information:
(1)Division of Nephrology and Hypertension, Department of Medicine, Mayo Clinic, 
Rochester, MN, USA.
(2)Department of Medicine, Division of Infectious Diseases, Faculty of Medicine 
Ramathibodi Hospital, Mahidol University, Bangkok, Thailand.
(3)Ramathibodi Excellence Center for Organ Transplantation, Faculty of Medicine 
Ramathibodi Hospital, Mahidol University, Bangkok, Thailand.

In the aftermath of the COVID-19 pandemic, the ongoing necessity for preventive 
measures such as mask-wearing and vaccination remains particularly critical for 
organ transplant recipients, a group highly susceptible to infections due to 
immunosuppressive therapy. Given that many individuals nowadays increasingly 
utilize Artificial Intelligence (AI), understanding AI perspectives is 
important. Thus, this study utilizes AI, specifically ChatGPT 4.0, to assess its 
perspectives in offering precise health recommendations for mask-wearing and 
COVID-19 vaccination tailored to this vulnerable population. Through a series of 
scenarios reflecting diverse environmental settings and health statuses in 
December 2023, we evaluated the AI's responses to gauge its precision, 
adaptability, and potential biases in advising high-risk patient groups. Our 
findings reveal that ChatGPT 4.0 consistently recommends mask-wearing in crowded 
and indoor environments for transplant recipients, underscoring their elevated 
risk. In contrast, for settings with fewer transmission risks, such as outdoor 
areas where social distancing is possible, the AI suggests that mask-wearing 
might be less imperative. Regarding vaccination guidance, the AI strongly 
advocates for the COVID-19 vaccine across most scenarios for kidney transplant 
recipients. However, it recommends a personalized consultation with healthcare 
providers in cases where patients express concerns about vaccine-related side 
effects, demonstrating an ability to adapt recommendations based on individual 
health considerations. While this study provides valuable insights into the 
current AI perspective on these important topics, it is crucial to note that the 
findings do not directly reflect or influence health policy. Nevertheless, given 
the increasing utilization of AI in various domains, understanding AI's 
viewpoints on such critical matters is essential for informed decision-making 
and future research.",yes,"This study evaluates ChatGPT 4.0's ability to provide clinical recommendations (mask-wearing and vaccination) for transplant recipients, using scenario-based prompts and analyzing the precision and adaptability of its responses. The focus is on the LLM's clinical advice in a specific patient population, which fits the inclusion criteria.",10.1080/0886022X.2024.2337291,II ,Joanne,,,
Diagnostic accuracy of a large language model in rheumatology: comparison of physician and ChatGPT-4,"Pre-clinical studies suggest that large language models (i.e., ChatGPT) could be used in the diagnostic process to distinguish inflammatory rheumatic (IRD) from other diseases. We therefore aimed to assess the diagnostic accuracy of ChatGPT-4 in comparison to rheumatologists. For the analysis, the data set of Gräf et al. (2022) was used. Previous patient assessments were analyzed using ChatGPT-4 and compared to rheumatologists’ assessments. ChatGPT-4 listed the correct diagnosis comparable often to rheumatologists as the top diagnosis 35% vs 39% (p = 0.30); as well as among the top 3 diagnoses, 60% vs 55%, (p = 0.38). In IRD-positive cases, ChatGPT-4 provided the top diagnosis in 71% vs 62% in the rheumatologists’ analysis. Correct diagnosis was among the top 3 in 86% (ChatGPT-4) vs 74% (rheumatologists). In non-IRD cases, ChatGPT-4 provided the correct top diagnosis in 15% vs 27% in the rheumatologists’ analysis. Correct diagnosis was among the top 3 in non-IRD cases in 46% of the ChatGPT-4 group vs 45% in the rheumatologists group. If only the first suggestion for diagnosis was considered, ChatGPT-4 correctly classified 58% of cases as IRD compared to 56% of the rheumatologists (p = 0.52). ChatGPT-4 showed a slightly higher accuracy for the top 3 overall diagnoses compared to rheumatologist’s assessment. ChatGPT-4 was able to provide the correct differential diagnosis in a relevant number of cases and achieved better sensitivity to detect IRDs than rheumatologist, at the cost of lower specificity. The pilot results highlight the potential of this new technology as a triage tool for the diagnosis of IRD. © 2023, The Author(s).",yes,"This study directly evaluates the diagnostic accuracy of ChatGPT-4 in a clinical context (rheumatology), comparing its performance to that of human rheumatologists using real patient assessments. The focus is on clinical diagnosis and triage, which fits the inclusion criteria.",10.1007/s00296-023-05464-6,I,Joanne,,,
Evaluating the reliability of ChatGPT as a tool for imaging test referral: a comparative study with a clinical decision support system,"Objectives: As the technology continues to evolve and advance, we can expect to see artificial intelligence (AI) being used in increasingly sophisticated ways to make a diagnosis and decisions such as suggesting the most appropriate imaging referrals. We aim to explore whether Chat Generative Pretrained Transformer (ChatGPT) can provide accurate imaging referrals for clinical use that are at least as good as the ESR iGuide. Methods: A comparative study was conducted in a tertiary hospital. Data was collected from 97 consecutive cases that were admitted to the emergency department with abdominal complaints. We compared the imaging test referral recommendations suggested by the ESR iGuide and the ChatGPT and analyzed cases of disagreement. In addition, we selected cases where ChatGPT recommended a chest abdominal pelvis (CAP) CT (n = 66), and asked four specialists to grade the appropriateness of the referral. Results: ChatGPT recommendations were consistent with the recommendations provided by the ESR iGuide. No statistical differences were found between the appropriateness of referrals by age or gender. Using a sub-analysis of CAP cases, a high agreement between ChatGPT and the specialists was found. Cases of disagreement (12.4%) were further analyzed and presented themes of vague recommendations such as “it would be advisable” and “this would help to rule out.” Conclusions: ChatGPT’s ability to guide the selection of appropriate tests may be comparable to some degree with the ESR iGuide. Features such as the clinical, ethical, and regulatory implications are still warranted and need to be addressed prior to clinical implementation. Further studies are needed to confirm these findings. Clinical relevance statement: The article explores the potential of using advanced language models, such as ChatGPT, in healthcare as a CDS for selecting appropriate imaging tests. Using ChatGPT can improve the efficiency of the decision-making process Key Points: • ChatGPT recommendations were highly consistent with the recommendations provided by the ESR iGuide. • ChatGPT’s ability in guiding the selection of appropriate tests may be comparable to some degree with ESR iGuide’s. © The Author(s), under exclusive licence to European Society of Radiology 2023.",yes,"This study directly evaluates ChatGPT's performance as a clinical decision support tool for imaging test referrals, comparing its recommendations to an established CDS system and specialist opinions. The focus is on a clinical application (imaging referral decision-making), which fits the inclusion criteria.",10.1007/s00330-023-10230-0,I,Joanne,,,
The Operating and Anesthetic Reference Assistant (OARA): A fine-tuned large language model for resident teaching,"Objective: This study aimed to fine-tune a large language model (LLM) for domain-specific text generation in surgical and anesthesia residency education. Summary background data: With growing interest in artificial intelligence (AI) for medical training, the potential of LLMs to transform residency education is explored. Methods: The 7-billion parameter base model “Vicuna v1.5"" was trained on 266,342 lines of text from 821 peer-reviewed documents. We evaluated the model with 150 surgical or anesthesia queries and assessed accuracy, token count, and inference speed across various reasoning tasks. Tests of significance were conducted using ANOVA and chi-square analysis. Results: Our model achieved 65.3% accuracy, excelling in surgical case-based tasks. We found no significant difference in accuracy between knowledge domains (P=0.081), though longer response generation demonstrated poorer accuracy, with significant accuracy variation based on output length (P ​= ​0.002). Conclusions: LLMs show potential in enhancing residency education. Our model's efficiency and task-specific accuracy highlights such promise, though limits in parameter count diminishes accuracy of longer response generation. Our findings showcase how AI may be integrated effectively within future residency training. © 2024 Elsevier Inc.",yes,"This study evaluates a fine-tuned LLM (Vicuna v1.5) for its ability to answer surgical and anesthesia queries relevant to residency education, measuring accuracy and performance on clinical reasoning tasks. The focus is on the LLM's clinical knowledge and reasoning in a medical education context, which fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.amjsurg.2024.02.016,II,Joanne,,,
A Comparative assessment of ChatGPT vs. Google Translate for the translation of patient instructions,"Background: An estimated 30 million people in the United States demonstrate a non-English language preference (NELP). Professional interpreter services and NELP patient advocates can mitigate some barriers to quality care but are not readily available at all institutions. Many studies have explored the use of Google Translate (GT) for medical communication with mixed results, most concluding that GT alone should not be used in the translation of medical documents for patients. Studies of medical translation with newer artificial intelligence (AI) language models, particularly ChatGPT, do not exist to date. This study’s purpose is to compare ChatGPT vs. GT in its ability to accurately translate instructional and educational medical documents across multiple languages. Methods: ChatGPT-3.5 and GT were prompted to translate two English language documents: post-operative discharge instructions for care after circumcision and patient information for undescended testicles (UDT). The instructions were copied and pasted into ChatGPT and GT, and translated into Spanish, Vietnamese, and Russian. Reviewers were blinded as to which documents corresponded to ChatGPT and GT. Members of the Language Services department independently reviewed the translations and assessed them based on meaning, expression, and technical errors. Each translation error was recorded with the reviewer’s comment and reviewer’s preferred translation. The errors were categorized as meaning, flow, language, form, style guide, or terminology errors, and were labeled as minor, major, or critical. Primary outcome assessed was accuracy of the translation, largely based on retention of meaning. Results: There were 132 sentences between the two documents. In Spanish, ChatGPT incorrectly translated 3.8% of all sentences, while GT incorrectly translated 18.1% of sentences. In Russian, ChatGPT and GT incorrectly translated 35.6% and 41.6% of all sentences, respectively. In Vietnamese, ChatGPT and GT incorrectly translated 24.2% and 10.6% of sentences, respectively. Conclusions: ChatGPT excelled considerably compared to GT for Spanish translation. However, it was outperformed by GT for the Vietnamese translations, and both ChatGPT and GT produced low quality translations in Russian. ChatGPT has an unacceptably high rate of translation error in Vietnamese and Russian and should not be used alone to translate medical documents from English to these languages. While it shows promise in the translation of Spanish materials, its utility for additional languages requires further study and it remains unreliable for use without human oversight. © Journal of Medical Artificial Intelligence. All rights reserved.",yes,"This study evaluates ChatGPT (an LLM) for the translation of patient instructions, which are clinical documents intended for patient care. The focus is on the accuracy of translation for medical communication, which directly impacts patient understanding and safety. While the primary task is translation, the context is clinical communication with patients, not general or administrative translation. The study does not focus on technical architecture or non-clinical tasks, and it is not a review or editorial.",10.21037/jmai-24-24,II,Joanne,,,
ICGA-GPT: Report generation and question answering for indocyanine green angiography images,"Background Indocyanine green angiography (ICGA) is vital for diagnosing chorioretinal diseases, but its interpretation and patient communication require extensive expertise and time-consuming efforts. We aim to develop a bilingual ICGA report generation and question-answering (QA) system. Methods Our dataset comprised 213 129 ICGA images from 2919 participants. The system comprised two stages: image-text alignment for report generation by a multimodal transformer architecture, and large language model (LLM)-based QA with ICGA text reports and human-input questions. Performance was assessed using both qualitative metrics (including Bilingual Evaluation Understudy (BLEU), Consensus-based Image Description Evaluation (CIDEr), Recall-Oriented Understudy for Gisting Evaluation-Longest Common Subsequence (ROUGE-L), Semantic Propositional Image Caption Evaluation (SPICE), accuracy, sensitivity, specificity, precision and F1 score) and subjective evaluation by three experienced ophthalmologists using 5-point scales (5 refers to high quality). Results We produced 8757 ICGA reports covering 39 disease-related conditions after bilingual translation (66.7% English, 33.3% Chinese). The ICGA-GPT model's report generation performance was evaluated with BLEU scores (1-4) of 0.48, 0.44, 0.40 and 0.37; CIDEr of 0.82; ROUGE of 0.41 and SPICE of 0.18. For disease-based metrics, the average specificity, accuracy, precision, sensitivity and F1 score were 0.98, 0.94, 0.70, 0.68 and 0.64, respectively. Assessing the quality of 50 images (100 reports), three ophthalmologists achieved substantial agreement (kappa=0.723 for completeness, kappa=0.738 for accuracy), yielding scores from 3.20 to 3.55. In an interactive QA scenario involving 100 generated answers, the ophthalmologists provided scores of 4.24, 4.22 and 4.10, displaying good consistency (kappa=0.779). Conclusion This pioneering study introduces the ICGA-GPT model for report generation and interactive QA for the first time, underscoring the potential of LLMs in assisting with automated ICGA image interpretation.  © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study evaluates a system (ICGA-GPT) that uses a multimodal transformer for report generation from medical images and a large language model (LLM) for answering clinical questions based on those reports. The LLM is directly assessed for its ability to answer clinical questions and generate reports relevant to patient care, with evaluation by clinicians. This fits the inclusion criteria as it assesses LLMs in a clinical context.",10.1136/bjo-2023-324446,II,Joanne,,,
Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks,"Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application. © 2024 IEEE.",yes,"This study evaluates the clinical performance of multimodal LLMs (LLaVA and GPT-4-based ChatGPT) in medical image classification tasks (tumor classification on brain MRI, breast ultrasound, and kidney CT). The focus is on the LLMs' ability to perform a clinical diagnostic task, directly comparing them to a classic AI model. This fits the inclusion criteria as it assesses LLMs in a clinical context.",10.1109/ICHI61247.2024.00080,III,Joanne,,,
ChatGPT: Friend or Foe? – Utility in Trauma Triage,"Artificial intelligence (AI) has been growing rapidly in the health and production industry. The most recent AI program which has caught everyone’s eye is, Chat Generative Pre-trained Transformer (ChatGPT). It is a program based on Open AI and was released in November 2022 to the public. It is a chatbot that helps with analyzing and learning data from the net, based on human literature and data. © The Author(s).",yes,"The title and abstract suggest the study discusses the utility of ChatGPT in trauma triage, which is a clinical application. Although the abstract is brief and lacks methodological detail, the focus on ChatGPT's use in trauma triage aligns with the inclusion criteria.",10.5005/jp-journals-10071-24498,II,Sully,,,
Evaluating the performance of Generative Pre-trained Transformer-4 (GPT-4) in standardizing radiology reports,"Objective: Radiology reporting is an essential component of clinical diagnosis and decision-making. With the advent of advanced artificial intelligence (AI) models like GPT-4 (Generative Pre-trained Transformer 4), there is growing interest in evaluating their potential for optimizing or generating radiology reports. This study aimed to compare the quality and content of radiologist-generated and GPT-4 AI-generated radiology reports. Methods: A comparative study design was employed in the study, where a total of 100 anonymized radiology reports were randomly selected and analyzed. Each report was processed by GPT-4, resulting in the generation of a corresponding AI-generated report. Quantitative and qualitative analysis techniques were utilized to assess similarities and differences between the two sets of reports. Results: The AI-generated reports showed comparable quality to radiologist-generated reports in most categories. Significant differences were observed in clarity (p = 0.027), ease of understanding (p = 0.023), and structure (p = 0.050), favoring the AI-generated reports. AI-generated reports were more concise, with 34.53 fewer words and 174.22 fewer characters on average, but had greater variability in sentence length. Content similarity was high, with an average Cosine Similarity of 0.85, Sequence Matcher Similarity of 0.52, BLEU Score of 0.5008, and BERTScore F1 of 0.8775. Conclusion: The results of this proof-of-concept study suggest that GPT-4 can be a reliable tool for generating standardized radiology reports, offering potential benefits such as improved efficiency, better communication, and simplified data extraction and analysis. However, limitations and ethical implications must be addressed to ensure the safe and effective implementation of this technology in clinical practice. Clinical relevance statement: The findings of this study suggest that GPT-4 (Generative Pre-trained Transformer 4), an advanced AI model, has the potential to significantly contribute to the standardization and optimization of radiology reporting, offering improved efficiency and communication in clinical practice. Key Points: • Large language model–generated radiology reports exhibited high content similarity and moderate structural resemblance to radiologist-generated reports. • Performance metrics highlighted the strong matching of word selection and order, as well as high semantic similarity between AI and radiologist-generated reports. • Large language model demonstrated potential for generating standardized radiology reports, improving efficiency and communication in clinical settings. © This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2023.",yes,"This study evaluates GPT-4, a generative large language model, in the clinical context of generating and standardizing radiology reports. The focus is on comparing the quality and content of LLM-generated reports to those written by radiologists, which is a direct clinical application relevant to healthcare practice.",10.1007/s00330-023-10384-x,I,Sully,,,
Evaluating the performance of ChatGPT in medical physiology university examination of phase I MBBS,,yes,"The study evaluates ChatGPT's performance on a medical physiology university examination for phase I MBBS students. This is a clinical evaluation of an LLM (ChatGPT) in a medical education context, which fits the inclusion criteria.",10.1152/advan.00036.2023,III,Sully,,,
Philippine Region-based Food and Nutrition Cross-reference using Fine-tuned Generative ChatBot,"This paper describes the development of a Large Language Model Chatbot for Philippine Food and Nutrition knowledge base. The objective is to associate the region or province of origin of any available food item in the country to the nutrients contained and the nutritional benefits to the human body, considering the build of the knowledge base coming from 30 web sites or sources. Three models have been used to test and evaluate the performance in accomplishing the task. In the training and fine-tuning efficiency, the Meta Llama 2 7B HF model is the most efficient when varying the training epochs from 1 through 5; but based on evaluation metrics of BERTScore, ROUGE Score and GLEU, the Meta Llama 3 7B Instruct model obtained the highest scores, and the highest acceptance score when tested on 100 pairs of question-and-answer with ground truths. Further demonstration of the chatbot revealed that the objective has been achieved in most query attempts during the testing phase of the development. © 2024 IEEE.",yes,"The study describes the development and evaluation of a fine-tuned LLM chatbot (Llama 2 and Llama 3) for providing region-based food and nutrition information in the Philippines. It evaluates the chatbot's ability to answer questions about food origin, nutrients, and nutritional benefits, using clinical/health-related queries and ground truth comparisons. This fits the inclusion criteria as it assesses an LLM's performance in a health-related context.",10.1109/WCONF61366.2024.10692031,II,Sully,,,
Evaluating AI Competence in Specialized Medicine: Comparative Analysis of ChatGPT and Neurologists in a Neurology Specialist Examination in Spain,"BACKGROUND: With the rapid advancement of artificial intelligence (AI) in 
various fields, evaluating its application in specialized medical contexts 
becomes crucial. ChatGPT, a large language model developed by OpenAI, has shown 
potential in diverse applications, including medicine.
OBJECTIVE: This study aims to compare the performance of ChatGPT with that of 
attending neurologists in a real neurology specialist examination conducted in 
the Valencian Community, Spain, assessing the AI's capabilities and limitations 
in medical knowledge.
METHODS: We conducted a comparative analysis using the 2022 neurology specialist 
examination results from 120 neurologists and responses generated by ChatGPT 
versions 3.5 and 4. The examination consisted of 80 multiple-choice questions, 
with a focus on clinical neurology and health legislation. Questions were 
classified according to Bloom's Taxonomy. Statistical analysis of performance, 
including the κ coefficient for response consistency, was performed.
RESULTS: Human participants exhibited a median score of 5.91 (IQR: 4.93-6.76), 
with 32 neurologists failing to pass. ChatGPT-3.5 ranked 116th out of 122, 
answering 54.5% of questions correctly (score 3.94). ChatGPT-4 showed marked 
improvement, ranking 17th with 81.8% of correct answers (score 7.57), surpassing 
several human specialists. No significant variations were observed in the 
performance on lower-order questions versus higher-order questions. 
Additionally, ChatGPT-4 demonstrated increased interrater reliability, as 
reflected by a higher κ coefficient of 0.73, compared to ChatGPT-3.5's 
coefficient of 0.69.
CONCLUSIONS: This study underscores the evolving capabilities of AI in medical 
knowledge assessment, particularly in specialized fields. ChatGPT-4's 
performance, outperforming the median score of human participants in a rigorous 
neurology examination, represents a significant milestone in AI development, 
suggesting its potential as an effective tool in specialized medical education 
and assessment.

© Pablo Ros-Arlanzón, Angel Perez-Sempere. Originally published in JMIR Medical 
Education (https://mededu.jmir.org).",yes,"This study directly evaluates the performance of ChatGPT (versions 3.5 and 4) on a real neurology specialist examination, comparing its results to those of practicing neurologists. The focus is on clinical knowledge assessment in a specialized medical context, which aligns with the inclusion criteria for clinical evaluation of LLMs in medicine.",10.2196/56762,III,Sully,,,
Evaluating the Efficacy of AI Chatbots as Tutors in Urology: A Comparative Analysis of Responses to the 2022 In-Service Assessment of the European Board of Urology,"Introduction: This study assessed the potential of large language models (LLMs) as educational tools by evaluating their accuracy in answering questions across urological subtopics. Methods: Three LLMs (ChatGPT-3.5, ChatGPT-4, and Bing AI) were examined in two testing rounds, separated by 48 h, using 100 Multiple-Choice Questions (MCQs) from the 2022 European Board of Urology (EBU) In-Service Assessment (ISA), covering five different subtopics. The correct answer was defined as “formal accuracy” (FA) representing the designated single best answer (SBA) among four options. Alternative answers selected from LLMs, which may not necessarily be the SBA but are still deemed correct, were labeled as “extended accuracy” (EA). Their capacity to enhance the overall accuracy rate when combined with FA was examined. Results: In two rounds of testing, the FA scores were achieved as follows: ChatGPT-3.5: 58% and 62%, ChatGPT-4: 63% and 77%, and BING AI: 81% and 73%. The incorporation of EA did not yield a significant enhancement in overall performance. The achieved gains for ChatGPT-3.5, ChatGPT-4, and BING AI were as a result 7% and 5%, 5% and 2%, and 3% and 1%, respectively (p > 0.3). Within urological subtopics, LLMs showcased best performance in Pediatrics/Congenital and comparatively less effectiveness in Functional/BPS/Incontinence. Conclusion: LLMs exhibit suboptimal urology knowledge and unsatisfactory proficiency for educational purposes. The overall accuracy did not significantly improve when combining EA to FA. The error rates remained high ranging from 16 to 35%. Proficiency levels vary substantially across subtopics. Further development of medicine-specific LLMs is required before integration into urological training programs. © 2024 The Author(s). Published by S. Karger AG, Basel.",yes,"This study evaluates the performance of multiple LLMs (ChatGPT-3.5, ChatGPT-4, Bing AI) on a clinically relevant board exam (European Board of Urology In-Service Assessment), specifically measuring their accuracy in answering medical MCQs. The focus is on the LLMs' ability to answer clinical questions, which is a direct clinical/medical application relevant to medical education and assessment.",10.1159/000537854,III,Sully,,,
Multimodal Large Language Model Passes Specialty Board Examination and Surpasses Human Test-Taker Scores: A Comparative Analysis Examining the Stepwise Impact of Model Prompting Strategies on Performance,"Background: Large language models (LLMs) have shown promise in answering medical licensing examination-style questions. However, there is limited research on the performance of multimodal LLMs on subspecialty medical examinations. Our study benchmarks the performance of multimodal LLM’s enhanced by model prompting strategies on gastroenterology subspeciality examination-style questions and examines how these prompting strategies incrementally improve overall performance. Methods: We used the 2022 American College of Gastroenterology (ACG) self-assessment examination (N=300). This test is typically completed by gastroenterology fellows and established gastroenterologists preparing for the gastroenterology subspeciality board examination. We employed a sequential implementation of model prompting strategies: prompt engineering, retrieval augmented generation (RAG), five-shot learning, and an LLM-powered answer validation revision model (AVRM). GPT-4 and Gemini Pro were tested. Results: Implementing all prompting strategies improved the overall score of GPT-4 from 60.3% to 80.7% and Gemini Pro’s from 48.0% to 54.3%. GPT-4’s score surpassed the 70% passing threshold and 75% average human test-taker scores unlike Gemini Pro. Stratification of questions by difficulty showed the accuracy of both LLMs mirrored that of human examinees, demonstrating higher accuracy as human test-taker accuracy increased. The addition of the AVRM to prompt, RAG and 5-shot increased GPT-4’s accuracy by 4.4%. The incremental addition of model prompting strategies improved accuracy for both non-image (57.2% to 80.4%) and image-based (63.0% to 80.9%) questions for GPT-4, but not Gemini Pro. Conclusions: Our results underscore the value of model prompting strategies in improving LLM performance on subspecialty-level licensing exam questions. We also present a novel implementation of an LLM-powered reviewer model in the context of subspecialty medicine which further improved model performance when combined with other prompting strategies. Our findings highlight the potential future role of multimodal LLMs, particularly with the implementation of multiple model prompting strategies, as clinical decision support systems in subspecialty care for healthcare providers.",yes,"This study evaluates the performance of multimodal LLMs (GPT-4 and Gemini Pro) on a subspecialty board examination (gastroenterology), including both text and image-based questions. It directly assesses LLMs in a clinical education context, which is within scope. The study also discusses implications for clinical decision support, further supporting inclusion.",10.1101/2024.07.27.24310809,III,Sully,,,
PAINe: An Artificial Intelligence-based Virtual Assistant to Aid in the Differentiation of Pain of Odontogenic versus Temporomandibular Origin,"INTRODUCTION: Pain associated with temporomandibular dysfunction (TMD) is often 
confused with odontogenic pain, which is a challenge in endodontic diagnosis. 
Validated screening questionnaires can aid in the identification and 
differentiation of the source of pain. Therefore, this study aimed to develop a 
virtual assistant based on artificial intelligence using natural language 
processing techniques to automate the initial screening of patients with tooth 
pain.
METHODS: The PAINe chatbot was developed in Python (Python Software Foundation, 
Beaverton, OR) language using the PyCharm (JetBrains, Prague, Czech Republic) 
environment and the openai library to integrate the ChatGPT 4 API (OpenAI, San 
Francisco, CA) and the Streamlit library (Snowflake Inc, San Francisco, CA) for 
interface construction. The validated TMD Pain Screener questionnaire and 1 
question regarding the current pain intensity were integrated into the chatbot 
to perform the differential diagnosis of TMD in patients with tooth pain. The 
accuracy of the responses was evaluated in 50 random scenarios to compare the 
chatbot with the validated questionnaire. The kappa coefficient was calculated 
to assess the agreement level between the chatbot responses and the validated 
questionnaire.
RESULTS: The chatbot achieved an accuracy rate of 86% and a substantial level of 
agreement (κ = 0.70). Most responses were clear and provided adequate 
information about the diagnosis.
CONCLUSIONS: The implementation of a virtual assistant using natural language 
processing based on large language models for initial differential diagnosis 
screening of patients with tooth pain demonstrated substantial agreement between 
validated questionnaires and the chatbot. This approach emerges as a practical 
and efficient option for screening these patients.

Copyright © 2024 American Association of Endodontists. Published by Elsevier 
Inc. All rights reserved.",yes,"This study evaluates a virtual assistant built using the ChatGPT-4 API to aid in the differential diagnosis of odontogenic versus temporomandibular pain. The LLM is used in a clinical context to screen and differentiate sources of pain, and its performance is compared to a validated questionnaire. This fits the inclusion criteria as it assesses a generative LLM in a clinical application.",10.1016/j.joen.2024.09.008,I,Sully,,,
Evaluation of Multilingual Simplifications of IR Procedural Reports Using GPT-4,"2449. J Vasc Interv Radiol. 2025 Apr;36(4):696-703.e1. doi: 
10.1016/j.jvir.2025.01.002. Epub 2025 Jan 9.

Evaluation of Multilingual Simplifications of IR Procedural Reports Using GPT-4.

Li HH(1), Moon JT(2), Kumar S(3), Ricci J(4), Sim N(5), Bercu ZL(6), Newsome 
J(2), Trivedi HM(7), Gichoya JW(2).

Author information:
(1)Division of Interventional Radiology and Image-Guided Medicine, Department of 
Radiology and Imaging Science, Emory University School of Medicine, Atlanta, 
Georgia. Electronic address: Hli277@emory.edu.
(2)Division of Interventional Radiology and Image-Guided Medicine, Department of 
Radiology and Imaging Science, Emory University School of Medicine, Atlanta, 
Georgia.
(3)Temple University Lewis Katz School of Medicine, Philadelphia, Pennsylvania.
(4)Georgetown University School of Medicine, Washington, D.C.
(5)Wayne State University School of Medicine, Detroit, Michigan.
(6)Division of Interventional Radiology and Image-Guided Medicine, Department of 
Radiology and Imaging Science, Emory University School of Medicine, Atlanta, 
Georgia; Wallace H. Coulter Department of Biomedical Engineering, Georgia 
Institute of Technology, Atlanta, Georgia.
(7)Division of Emergency Radiology, Emory University School of Medicine, 
Atlanta, Georgia.

This study assessed the feasibility of large language models such as GPT-4 
(OpenAI, San Francisco, California) to summarize interventional radiology 
procedural reports to improve layperson understanding and translate medical 
texts into multiple languages. Two hundred reports from 8 categories were 
summarized using GPT-4. Readability was assessed with Flesch-Kincaid reading 
level (FKRL) and Flesch reading ease score (FRES). Accuracy was assessed by 8 
interventional radiologists. Summaries were translated into Spanish, Korean, 
Chinese, and Swahili, and their accuracy were assessed by 8 bilingual 
interventional radiologists. The original reports' FKRL of 10.7 and FRES of 41.9 
improved to 7.0 and 73.0, respectively. Summaries were mostly accurate, with 
minimal misinformation. Translations introduced an increase in number of 
misinformation but no significant increase in critically wrong information. 
Layperson comprehension scores improved significantly from 2.5 to 4.3 out of 5 
after summarization. Overall, GPT-4 enhanced report readability and 
comprehension, suggesting potential for broader application in improving patient 
communication.

Copyright © 2025 SIR. Published by Elsevier Inc. All rights reserved.",yes,"This study evaluates GPT-4's ability to simplify and translate interventional radiology procedural reports to improve layperson understanding, with clinical experts assessing accuracy and readability. The primary focus is on patient communication and comprehension of clinical documents, which is a relevant clinical application of LLMs in healthcare.",10.1016/j.jvir.2025.01.002,I,Sully,,,
Evaluating the effectiveness of ChatGPT4 in the diagnosis and workup of dermatologic conditions,"3277. Dermatol Online J. 2024 Aug 15;30(4). doi: 10.5070/D330464104.

Evaluating the effectiveness of ChatGPT4 in the diagnosis and workup of 
dermatologic conditions.

Greif C(1), Mpunga N, Koopman IV, Pye A, Hivnor CM, Owen JL.

Author information:
(1)Department of Medicine, University of Texas Health Science Center at San 
Antonio, San Antonio, Texas, USA.

ChatGPT is a publicly available chatbot released by OpenAI. Its usefulness in 
responding to medical questions has been assessed in several specialties, but 
there is limited literature in dermatology. This study seeks to understand how 
well ChatGPT4 can provide accurate diagnoses and appropriate workup suggestions 
for clinical vignettes describing common dermatologic conditions. Ten vignettes 
were input into ChatGPT4 representing presentations of common dermatologic 
conditions, written from the perspective of a physician not board-certified in 
dermatology. ChatGPT4 was asked to identify the top five most likely diagnoses 
and its recommended workup for each vignette. Responses were assessed 
quantitatively by calculating the percentage of correct diagnoses, with accurate 
diagnoses defined by three board-certified dermatologists, and qualitatively 
using Likert scales describing the accuracy of diagnoses and appropriateness of 
workups scored by eleven board-certified dermatologists. Overall, 52% of 
ChatGPT4's diagnoses were accurate and 62% of its recommended workup suggestions 
were deemed completely correct by board-certified dermatologists. ChatGPT4 was 
better at recommending an appropriate workup than identifying accurate diagnoses 
across vignettes. ChatGPT4 was able to accurately diagnose and workup common 
dermatologic conditions in slightly more than half of cases. ChatGPT4 was better 
at determining an appropriate workup than an accurate diagnosis.Keywords: 
artificial intelligence, ChatGPT, dermatology, diagnosis, OpenAI, workup.",yes,"This study directly evaluates ChatGPT-4's ability to provide clinical diagnoses and workup recommendations for dermatologic conditions using clinical vignettes. The performance is assessed by board-certified dermatologists, making it a clinical evaluation of an LLM in a healthcare context.",10.5070/D330464104,II,Sully,,,
Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation,"In recent years, large language models (LLMs) have emerged as a transformative force in several domains, including medical education and healthcare. This paper presents a case study on the practical application of using retrieval-augmented generation (RAG) based models for enhancing healthcare education in low- and middle-income countries. The model described in this paper, SMARThealth GPT, stems from the necessity for accessible and locally relevant medical information to aid community health workers in delivering high-quality maternal care. We describe the development process of the complete RAG pipeline, including the creation of a knowledge base of Indian pregnancy-related guidelines, knowledge embedding retrieval, parameter selection and optimization, and answer generation. This case study highlights the potential of LLMs in building frontline healthcare worker capacity and enhancing guideline-based health education; and offers insights for similar applications in resource-limited settings. It serves as a reference for machine learning scientists, educators, healthcare professionals, and policymakers aiming to harness the power of LLMs for substantial educational improvement.",yes,"The study describes the use of a retrieval-augmented LLM (SMARThealth GPT) to enhance healthcare education for frontline health workers, specifically focusing on delivering guideline-based maternal care information. While the primary focus is on education and capacity building, the LLM is used to generate answers to clinical questions and provide guideline-based information to health workers, which is a clinical support function. The study is not a review or opinion piece, and it evaluates the practical application of an LLM in a healthcare context.",10.1101/2023.12.15.23300009,II,Sully,,,
A Novel RAG Framework with Knowledge-Enhancement for Biomedical Question Answering,"The biomedical question-answering system usually provide accurate and real-time responses, which is crucial for clinical decision-making and scientific research. Although large language models achieve remarkable results in general question-answering tasks, they still face challenges in specialized fields. This paper proposes a novel framework called RAG-Chain, which aims to enhance the performance of general-domain large models on special biomedical reasoning and question-answering tasks. The RAG-Chain framework improves the knowledge retrieval and generation abilities of general models by a multi-stage processing of external knowledge and automatic construction of chain-of-thought templates combined with self-consistency validation process of choice shuffling. The experimental results show that RAG-Chain improves the accuracy of the baseline model by an average of 6.9% on the MedQA dataset without the need for pre-training or fine-tuning in biomedical fields, verifying its strong adaptability and effectiveness in different large language models. © 2024 IEEE.",yes,"This study proposes and evaluates a new framework (RAG-Chain) to enhance LLM performance on biomedical question answering, specifically using clinical datasets (MedQA). The focus is on improving LLMs' clinical reasoning and question answering, which is a clinical application. The study reports experimental results on LLMs' performance in a biomedical context.",10.1109/BIBM62325.2024.10822837,III,Sully,,,
Assessing the utility of ChatGPT as an artificial intelligence-based large language model for information to answer questions on myopia,"PURPOSE: ChatGPT is an artificial intelligence language model, which uses natural language processing to simulate human conversation. It has seen a wide range of applications including healthcare education, research and clinical practice. This study evaluated the accuracy of ChatGPT in providing accurate and quality information to answer questions on myopia. METHODS: A series of 11 questions (nine categories of general summary, cause, symptom, onset, prevention, complication, natural history, treatment and prognosis) were generated for this cross-sectional study. Each question was entered five times into fresh ChatGPT sessions (free from influence of prior questions). The responses were evaluated by a five-member team of optometry teaching and research staff. The evaluators individually rated the accuracy and quality of responses on a Likert scale, where a higher score indicated greater quality of information (1: very poor; 2: poor; 3: acceptable; 4: good; 5: very good). Median scores for each question were estimated and compared between evaluators. Agreement between the five evaluators and the reliability statistics of the questions were estimated. RESULTS: Of the 11 questions on myopia, ChatGPT provided good quality information (median scores: 4.0) for 10 questions and acceptable responses (median scores: 3.0) for one question. Out of 275 responses in total, 66 (24%) were rated very good, 134 (49%) were rated good, whereas 60 (22%) were rated acceptable, 10 (3.6%) were rated poor and 5 (1.8%) were rated very poor. Cronbach's α of 0.807 indicated good level of agreement between test items. Evaluators' ratings demonstrated 'slight agreement' (Fleiss's κ, 0.005) with a significant difference in scoring among the evaluators (Kruskal-Wallis test, p < 0.001). CONCLUSION: Overall, ChatGPT generated good quality information to answer questions on myopia. Although ChatGPT shows great potential in rapidly providing information on myopia, the presence of inaccurate responses demonstrates that further evaluation and awareness concerning its limitations are crucial to avoid potential misinterpretation.",yes,"This study evaluates ChatGPT's ability to answer clinical questions about myopia, with responses rated for accuracy and quality by optometry professionals. The focus is on the clinical utility of ChatGPT in providing patient-relevant information, which aligns with the inclusion criteria.",10.1111/opo.13207,II,Sully,,,
"Comparative Analysis of ChatGPT and Google Gemini in the Creation of Patient Education Materials for Acute Appendicitis, Cholecystitis, and Hydrocele","Patient education is essential for managing conditions like acute appendicitis, cholecystitis, and hydrocele, as it empowers patients to seek timely medical intervention, which can dramatically improve outcomes. Artificial intelligence tools are increasingly integral in healthcare, enhancing the delivery and accessibility of patient education. This study aims to compare the effectiveness of ChatGPT and Google Gemini in creating patient education guides on acute appendicitis, cholecystitis, and hydrocele. In this cross-sectional study, we generated patient education brochures for three diseases using two AI tools, ChatGPT and Google Gemini. The responses were evaluated based on several variables including word count, sentence length, readability (assessed with the Flesch-Kincaid Calculator), similarity (analyzed using Quillbot), and reliability (measured by the Modified DISCERN Score). Statistical analyses were performed using unpaired t-tests and Pearson’s correlation coefficients. The analysis revealed that there were no significant differences in readability scores, word counts, sentence lengths, or reliability scores between the materials produced by ChatGPT and Google Gemini. Additionally, correlation analyses indicated a positive correlation for ease scores and a negative correlation for reliability scores between the tools, though these findings were not statistically significant. The study concludes that ChatGPT and Google Gemini are equally effective in producing patient education materials for acute appendicitis, cholecystitis, and hydrocele. The absence of significant differences between the two AI tools suggests their potential utility as educational resources in healthcare settings. © Association of Surgeons of India 2024.",yes,"This study evaluates the use of LLMs (ChatGPT and Google Gemini) for generating patient education materials, which is a clinical application relevant to healthcare. The focus is on the quality and reliability of information provided to patients about specific medical conditions, which aligns with the inclusion criteria.",10.1007/s12262-024-04112-y,II,Sully,,,
Unregulated large language models produce medical device-like output,"Large language models (LLMs) show considerable promise for clinical decision support (CDS) but none is currently authorized by the Food and Drug Administration (FDA) as a CDS device. We evaluated whether two popular LLMs could be induced to provide device-like CDS output. We found that LLM output readily produced device-like decision support across a range of scenarios, suggesting a need for regulation if LLMs are formally deployed for clinical use. © The Author(s) 2025.",yes,"This study evaluates the ability of LLMs to provide clinical decision support, which is a direct clinical application. It assesses the output of LLMs in scenarios relevant to healthcare decision-making.",10.1038/s41746-025-01544-y,II,Sully,,,
"Emergency Medicine Assistants in the Field of Toxicology, Comparison of ChatGPT-3.5 and GEMINI Artificial Intelligence Systems; [Skubiosios medicinos pagalbos specialistų padėjėjai toksikologijos srityje, ChatGPT-3.5 ir GEMINI dirbtinio intelekto sistemų palyginimas]","Objective: Artificial intelligence models human thinking and problem-solving abilities, allowing computers to make autonomous decisions. There is a lack of studies demonstrating the clinical utility of GPT and Gemin in the field of toxicology, which means their level of competence is not well understood. This study compares the responses given by GPT-3.5 and Gemin to those provided by emergency medicine residents. Methods: This prospective study was focused on toxicology and utilized the widely recognized educational resource ‘Tintinalli Emergency Medicine: A Comprehensive Study Guide’ for the field of Emergency Medicine. A set of twenty questions, each with five options, was devised to test knowledge of toxicological data as defined in the book. These questions were then used to train ChatGPT GPT-3.5 (Generative Pre-trained Transformer 3.5) by OpenAI and Gemini by Google AI in the clinic. The resulting answers were then meticulously analyzed. Results: 28 physicians, 35.7% of whom were women, were included in our study. A comparison was made between the physician and AI scores. While a significant difference was found in the comparison (F=2.368 and p<0.001), no significant difference was found between the two groups in the post-hoc Tukey test. GPT-3.5 mean score is 9.9±0.71, Gemini mean score is 11.30±1.17 and, physicians’ mean score is 9.82±3.70 (Figure 1). Conclusions: It is clear that GPT-3.5 and Gemini respond similarly to topics in toxicology, just as resident physicians do. © 2024 Hatice Aslı Bedel,.",yes,"This study directly compares the clinical performance of two generative LLMs (ChatGPT-3.5 and Gemini) to emergency medicine residents on toxicology questions, using a recognized clinical resource. The evaluation is in a clinical context (toxicology knowledge relevant to emergency medicine), and the LLMs' responses are compared to those of clinicians.",10.15388/Amed.2024.31.2.18,III,Sully,,,
ChatGPT on guidelines: Providing contextual knowledge to GPT allows it to provide advice on appropriate colonoscopy intervals,"Background and Aim: Colonoscopy is commonly used in screening and surveillance for colorectal cancer. Multiple different guidelines provide recommendations on the interval between colonoscopies. This can be challenging for non-specialist healthcare providers to navigate. Large language models like ChatGPT are a potential tool for parsing patient histories and providing advice. However, the standard GPT model is not designed for medical use and can hallucinate. One way to overcome these challenges is to provide contextual information with medical guidelines to help the model respond accurately to queries. Our study compares the standard GPT4 against a contextualized model provided with relevant screening guidelines. We evaluated whether the models could provide correct advice for screening and surveillance intervals for colonoscopy. Methods: Relevant guidelines pertaining to colorectal cancer screening and surveillance were formulated into a knowledge base for GPT. We tested 62 example case scenarios (three times each) on standard GPT4 and on a contextualized model with the knowledge base. Results: The contextualized GPT4 model outperformed the standard GPT4 in all domains. No high-risk features were missed, and only two cases had hallucination of additional high-risk features. A correct interval to colonoscopy was provided in the majority of cases. Guidelines were appropriately cited in almost all cases. Conclusions: A contextualized GPT4 model could identify high-risk features and quote appropriate guidelines without significant hallucination. It gave a correct interval to the next colonoscopy in the majority of cases. This provides proof of concept that ChatGPT with appropriate refinement can serve as an accurate physician assistant. © 2023 Journal of Gastroenterology and Hepatology Foundation and John Wiley & Sons Australia, Ltd.",yes,"This study evaluates GPT-4 (ChatGPT) in providing clinical recommendations for colonoscopy intervals based on patient scenarios, comparing standard and contextualized models. It directly assesses the LLM's clinical decision-making in a healthcare context.",10.1111/jgh.16375,II,Sully,,,
Are artificial intelligence chatbots a reliable source of information about contact lenses?,"INTRODUCTION: Artificial Intelligence (AI) chatbots are able to explain complex 
concepts using plain language. The aim of this study was to assess the accuracy 
of three AI chatbots answering common questions related to contact lens (CL) 
wear.
METHODS: Three open access AI chatbots were compared: Perplexity, Open Assistant 
and ChatGPT 3.5. Ten general CL questions were asked to all AI chatbots on the 
same day in two different countries, with the questions asked in Spanish from 
Spain and in English from the U.K. Two independent optometrists with experience 
working in each country assessed the accuracy of the answers provided. Also, the 
AI chatbots' responses were assessed if their outputs showed any bias towards 
(or against) any eye care professional (ECP).
RESULTS: The answers obtained by the same AI chatbots were different in Spain 
and the U.K. Also, statistically significant differences were found between the 
AI chatbots for accuracy. In the U.K., ChatGPT 3.5 was the most and Open 
Assistant least accurate (p < 0.01). In Spain, Perplexity and ChatGPT were 
statistically more accurate than Open Assistant (p < 0.01). All the AI chatbots 
presented bias, except ChatGPT 3.5 in Spain.
CONCLUSIONS: AI chatbots do not always consider local CL legislation, and their 
accuracy seems to be dependent on the language used to interact with them. 
Hence, at this time, although some AI chatbots might be a good source of 
information for general CL related questions, they cannot replace an ECP.

Copyright © 2024 The Authors. Published by Elsevier Ltd.. All rights reserved.",yes,"This study evaluates the accuracy of ChatGPT 3.5 and other AI chatbots in answering clinical questions about contact lens wear, which is a healthcare-related topic. The study assesses the LLM's ability to provide patient-facing information, a clinical application relevant to patient education and decision support.",10.1016/j.clae.2024.102130,II,Sully,,,
The double-edged sword of generative AI: surpassing an expert or a deceptive “false friend”?,"BACKGROUND CONTEXT: Generative artificial intelligence (AI), ChatGPT being the most popular example, has been extensively assessed for its capability to respond to medical questions, such as queries in spine treatment approaches or technological advances. However, it often lacks scientific foundation or fabricates inauthentic references, also known as AI hallucinations. PURPOSE: To develop an understanding of the scientific basis of generative AI tools by studying the authenticity of references and reliability in comparison to the alignment of responses of evidence-based guidelines. STUDY DESIGN: Comparative study. METHODS: Thirty-three previously published North American Spine Society (NASS) guideline questions were posed as prompts to 2 freely available generative AI tools (Tools I and II). The responses were scored for correctness compared with the published NASS guideline responses using a 5-point “alignment score."" Furthermore, all cited references were evaluated for authenticity, source type, year of publication, and inclusion in the scientific guidelines. RESULTS: Both tools’ responses to guideline questions achieved an overall score of 3.5±1.1, which is considered acceptable to be equivalent to the guideline. Both tools generated 254 references to support their responses, of which 76.0% (n=193) were authentic and 24.0% (n=61) were fabricated. From these, authentic references were: peer-reviewed scientific research papers (147, 76.2%), guidelines (16, 8.3%), educational websites (9, 4.7%), books (9, 4.7%), a government website (1, 0.5%), insurance websites (6, 3.1%) and newspaper websites (5, 2.6%). Claude referenced significantly more authentic peer-reviewed scientific papers (Claude: n=111, 91.0%; Gemini: n=36, 50.7%; p<.001). The year of publication amongst all references ranged from 1988-2023, with significantly older references provided by Claude (Claude: 2008±6; Gemini: 2014±6; p<.001). Lastly, significantly more references provided by Claude were also referenced in the published NASS guidelines (Claude: n=27, 24.3%; Gemini: n=1, 2.8%; p=.04). CONCLUSIONS: Both generative AI tools provided responses that had acceptable alignment with NASS evidence-based guideline recommendations and offered references, though nearly a quarter of the references were inauthentic or nonscientific sources. This deficiency of legitimate scientific references does not meet standards for clinical implementation. Considering this limitation, caution should be exercised when applying the output of generative AI tools to clinical applications. © 2025 Elsevier Inc.",yes,"This study evaluates generative LLMs (Claude and Gemini) by comparing their responses to clinical guideline questions (NASS guidelines) and assessing the authenticity of their references. The primary focus is on the clinical accuracy and reliability of LLM-generated answers in a medical context, which fits the inclusion criteria.",10.1016/j.spinee.2025.02.010,II,Sully,,,
Embracing the future—is artificial intelligence already better? A comparative study of artificial intelligence performance in diagnostic accuracy and decision-making,"Background and purpose: The integration of artificial intelligence (AI) in healthcare has the potential to revolutionize patient care and clinical decision-making. This study aimed to explore the reliability of large language models in neurology by comparing the performance of an AI chatbot with neurologists in diagnostic accuracy and decision-making. Methods: A cross-sectional observational study was conducted. A pool of clinical cases from the American Academy of Neurology's Question of the Day application was used as the basis for the study. The AI chatbot used was ChatGPT, based on GPT-3.5. The results were then compared to neurology peers who also answered the questions—a mean of 1500 neurologists/neurology residents. Results: The study included 188 questions across 22 different categories. The AI chatbot demonstrated a mean success rate of 71.3% in providing correct answers, with varying levels of proficiency across different neurology categories. Compared to neurology peers, the AI chatbot performed at a similar level, with a mean success rate of 69.2% amongst peers. Additionally, the AI chatbot achieved a correct diagnosis in 85.0% of cases and it provided an adequate justification for its correct responses in 96.1%. Conclusions: The study highlights the potential of AI, particularly large language models, in assisting with clinical reasoning and decision-making in neurology and emphasizes the importance of AI as a complementary tool to human expertise. Future advancements and refinements are needed to enhance the AI chatbot's performance and broaden its application across various medical specialties. © 2024 The Authors. European Journal of Neurology published by John Wiley & Sons Ltd on behalf of European Academy of Neurology.",yes,"This study directly evaluates the clinical performance of a large language model (ChatGPT, GPT-3.5) in diagnostic accuracy and decision-making in neurology, comparing it to human neurologists. It uses clinical cases and assesses the LLM's ability to provide correct diagnoses and justifications, which is a clear clinical application.",10.1111/ene.16195,III,Sully,,,
"Evaluation of Rhinoplasty Information from ChatGPT, Gemini, and Claude for Readability and Accuracy","Objective: Assessment of the readability, accuracy, quality, and completeness of ChatGPT (Open AI, San Francisco, CA), Gemini (Google, Mountain View, CA), and Claude (Anthropic, San Francisco, CA) responses to common questions about rhinoplasty. Methods: Ten questions commonly encountered in the senior author’s (SPM) rhinoplasty practice were presented to ChatGPT-4, Gemini and Claude. Seven Facial Plastic and Reconstructive Surgeons with experience in rhinoplasty were asked to evaluate these responses for accuracy, quality, completeness, relevance, and use of medical jargon on a Likert scale. The responses were also evaluated using several readability indices. Results: ChatGPT achieved significantly higher evaluator scores for accuracy, and overall quality but scored significantly lower on completeness compared to Gemini and Claude. All three chatbot responses to the ten questions were rated as neutral to incomplete. All three chatbots were found to use medical jargon and scored at a college reading level for readability scores. Conclusions: Rhinoplasty surgeons should be aware that the medical information found on chatbot platforms is incomplete and still needs to be scrutinized for accuracy. However, the technology does have potential for use in healthcare education by training it on evidence-based recommendations and improving readability. Level of Evidence V: This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266. © Springer Science+Business Media, LLC, part of Springer Nature and International Society of Aesthetic Plastic Surgery 2024.",yes,"This study evaluates the accuracy, quality, and readability of responses from LLMs (ChatGPT, Gemini, Claude) to common clinical questions about rhinoplasty, as judged by expert surgeons. The focus is on the clinical information provided to patients, which is a direct clinical application relevant to patient education and decision support.",10.1007/s00266-024-04343-0,II,Sully,,,
Potential of ChatGPT and GPT-4 for Data Mining of Free-Text CT Reports on Lung Cancer,"Background: The latest large language models (LLMs) solve unseen problems via user-defined text prompts without the need for retraining, offering potentially more efficient information extraction from free-text medical records than manual annotation. Purpose: To compare the performance of the LLMs ChatGPT and GPT-4 in data mining and labeling oncologic phenotypes from free-text CT reports on lung cancer by using user-defined prompts. Materials and Methods: This retrospective study included patients who underwent lung cancer follow-up CT between September 2021 and March 2023. A subset of 25 reports was reserved for prompt engineering to instruct the LLMs in extracting lesion diameters, labeling metastatic disease, and assessing oncologic progression. This output was fed into a rule-based natural language processing pipeline to match ground truth annotations from four radiologists and derive performance metrics. The oncologic reasoning of LLMs was rated on a five-point Likert scale for factual correctness and accuracy. The occurrence of confabulations was recorded. Statistical analyses included Wilcoxon signed rank and McNemar tests. Results: On 424 CT reports from 424 patients (mean age, 65 years ± 11 [SD]; 265 male), GPT-4 outperformed ChatGPT in extracting lesion parameters (98.6% vs 84.0%, P < .001), resulting in 96% correctly mined reports (vs 67% for ChatGPT, P < .001). GPT-4 achieved higher accuracy in identification of metastatic disease (98.1% [95% CI: 97.7, 98.5] vs 90.3% [95% CI: 89.4, 91.0]) and higher performance in generating correct labels for oncologic progression (F1 score, 0.96 [95% CI: 0.94, 0.98] vs 0.91 [95% CI: 0.89, 0.94]) (both P < .001). In oncologic reasoning, GPT-4 had higher Likert scale scores for factual correctness (4.3 vs 3.9) and accuracy (4.4 vs 3.3), with a lower rate of confabulation (1.7% vs 13.7%) than ChatGPT (all P < .001). Conclusion: When using user-defined prompts, GPT-4 outperformed ChatGPT in extracting oncologic phenotypes from free-text CT reports on lung cancer and demonstrated better oncologic reasoning with fewer confabulations. © RSNA, 2023.",yes,"This study evaluates ChatGPT and GPT-4 for extracting clinical information (lesion diameters, metastatic disease, oncologic progression) from free-text CT reports in lung cancer. While the task is data mining, the extraction and labeling of oncologic phenotypes and assessment of progression are directly relevant to clinical decision-making and patient care, not just data structuring. The study also assesses the models' oncologic reasoning and factual correctness, which are clinical aspects.",10.1148/radiol.231362,I,Sully,,,
Identifying Psychiatric Manifestations in Outpatients with Depression and Anxiety: A Large Language Model-Based Approach,"Purpose: Accurate psychiatric diagnosis and assessment are crucial for effective treatment. However, while current data-driven approaches emphasize diagnostic outcomes, the process of decoding the underlying symptom expressions in patients’ language and mapping them to well-defined psychiatric terminology has received relatively little attention. This study investigates the potential of Large Language Models (LLMs) to automate the identification of diagnostic categories and symptoms from psychiatrist-patient dialogues, to provide interpretable insights and support automatic diagnosis. Methods: We analyzed audio recordings from 1160 psychiatric diagnostic interviews, primarily involving patients with depressive disorder and anxiety disorder. A clinical entities corpus was formed by leveraging clinical annotations in EMRs (e.g., chief complaints, mental status, elements in assessment scales) and widely used assessment scales. LLMs were utilized to identify clinical symptoms, rate assessment scales, and an ensemble learning pipeline was designed to classify diagnostic results and symptoms with 10-fold cross-validation. Results: The system achieved 86.9% accuracy for identifying the appearance of clinical annotations and 74.7% (77.2%) accuracy for identifying anxiety (depression) symptoms. Patients with depression and anxiety, diagnosed using ICD-10 codes, were differentiated with an accuracy of 75.5%. Analysis of LLM-generated features shows that depression cases exhibited prominent markers of anhedonia and decreased volition, whereas anxiety disorders were characterized by tension and an inability to relax. Conclusion: This study demonstrates the potential of integrating LLM technology with linguistic and acoustic features to enhance psychiatric diagnostics. The developed pipeline effectively predicts psychiatric diagnoses and provides interpretable insights, showcasing a valuable tool for clinicians in mental health assessment.",yes,"This study evaluates the use of large language models (LLMs) to identify psychiatric symptoms and support diagnosis from psychiatrist-patient dialogues, directly applying LLMs to a clinical task (diagnosis and symptom identification in mental health). The focus is on clinical application and decision support, not just data processing or technical development.",10.1101/2025.01.03.24318117,I,Sully,,,
Minimizing STOPP and Beers Criteria Risks in PIM Treatments Using PM-TOM and ChatGPT: A Case Study,"2535. Stud Health Technol Inform. 2025 Apr 8;323:149-153. doi: 10.3233/SHTI250067.

Minimizing STOPP and Beers Criteria Risks in PIM Treatments Using PM-TOM and 
ChatGPT: A Case Study.

Kulenovic A(1), Lagumdzija-Kulenovic A(1).

Author information:
(1)Absolute Information Age, Inc. Toronto, Canada.

PM-TOM (Personalized Medicine-Therapy Optimization Method) is a clinical 
decision-support tool designed to optimize polypharmacy treatments by minimizing 
their adverse drug reactions (ADRs) caused by individual drugs or drug 
interactions (DDIs, DCIs, DFIs, DGIs), along with the risks identified by the 
STOPP and Beers criteria. On the other hand, AI tools like ChatGPT 4.0, trained 
on medical literature texts, can provide broader clinical reasoning and insights 
tailored to individual patient contexts. By referring to a documented 
deprescribing case, this study demonstrates the synergistic power of PM-TOM and 
ChatGPT in optimizing potentially inappropriate medication (PIM) treatments. A 
malnourished older woman was admitted to a deprescribing facility with recurrent 
falls, hypertension, ischemic heart disease, depression, osteoarthritis, 
osteoporosis, and GERD. She was initially prescribed acetaminophen, alendronate, 
omeprazole, lisinopril, metoprolol, aspirin, citalopram, and vitamin D, which 
were assessed as inadequate. While the discharge regimen improved some 
conditions by replacing alendronate with zoledronic acid and reducing some drug 
dosages, PM-TOM revealed that key risks, stemming primarily from omeprazole, 
aspirin, and citalopram, remained unaddressed. The discharge treatment was 
optimized with PM-TOM after considering alternative drug classes suggested by 
ChatGPT and elaborated in the available medical literature. In the optimized 
treatment, omeprazole (PPI) was replaced with famotidine (H2-blocker), 
citalopram (SSRI) with agomelatine (atypical antidepressant), zoledronic acid 
(bisphosphonate) with denosumab (RANK ligand inhibitor), aspirin (NSAID) with 
ticagrelor (antiplatelet), and lisinopril with benazepril (ACE inhibitor). These 
changes significantly reduced possible ADRs and the geriatric care criteria 
risks. Finally, ChatGPT validated the proposed adjustments, confirming their 
alignment with the guidelines and highlighting the potential for longer-term 
benefits. This case study illustrates how a combined use of PM-TOM and AI tools 
can effectively support the clinical decision-making process by optimizing 
polypharmacy treatments and minimizing their PIMs, major contributors to 
morbidity in older adults and high healthcare costs.",yes,"This study evaluates the use of ChatGPT (an LLM) in a clinical decision-support context, specifically for optimizing polypharmacy treatments and minimizing potentially inappropriate medications (PIMs) in an older adult. The LLM is used to suggest alternative drug classes and validate treatment adjustments, which are directly related to clinical care and decision-making.",10.3233/SHTI250067,X,Sully,This is a case report,,
ChatGPT as a Virtual Patient: Written Empathic Expressions During Medical History Taking,"Objective: Virtual patients are already utilized in the teaching of medical history taking. Since its emergence, ChatGPT has been integrated into several areas of medical education. This study aimed to examine whether ChatGPT can be used to train empathic history taking while fostering students’ subjective autonomy. Methods: Third-year medical students took histories with ChatGPT 3.5 after entering a predefined prompt covering cardiological diseases. Afterwards, students answered a questionnaire regarding their experienced autonomy. All chats were analyzed using the Empathic Communication Coding System measuring ChatGPT’s given empathic opportunities as well as students’ responses. Results: Out of 659 interactions, 93 were identified as empathic. ChatGPT provided opportunities mostly through reporting emotional statements or challenges. Students sometimes missed reacting adequately to ChatGPT’s opportunities but more often responded by implicit recognition of patient perspective and reported a high level of experienced autonomy. Conclusions: The study yielded preliminary results that ChatGPT might be suitable as a tool mimicking a virtual patient while enabling an empathic history taking. To date, ChatGPT seems valid as a supplement to training with simulated patients. Medical faculty could consider integrating ChatGPT into teaching, such as through a flipped classroom approach, to guide students in its use as ChatGPT continues to gain attention. © The Author(s) 2025.",yes,"This study evaluates ChatGPT-3.5 as a virtual patient for training medical students in empathic history taking, a clinical education context. It assesses ChatGPT's ability to simulate patient interactions and provide empathic opportunities, which is a direct clinical application relevant to medical training.",10.1007/s40670-025-02342-7,III,Sully,,,
GPT-4 accuracy and completeness against International Consensus Statement on Allergy and Rhinology: Rhinosinusitis,"19. Int Forum Allergy Rhinol. 2023 Dec;13(12):2231-2234. doi: 10.1002/alr.23201. 
Epub 2023 Jun 16.

GPT-4 accuracy and completeness against International Consensus Statement on 
Allergy and Rhinology: Rhinosinusitis.

Yoshiyasu Y(1), Wu F(2), Dhanda AK(2), Gorelik D(2), Takashima M(2), Ahmed 
OG(2).

Author information:
(1)Department of Otolaryngology-Head and Neck Surgery, University of Texas 
Medical Branch, Galveston, Texas, USA.
(2)Department of Otolaryngology-Head and Neck Surgery, Houston Methodist 
Hospital, Houston, Texas, USA.

GPT-4 is an AI language model that can answer basic questions about rhinologic 
disease. Vetting is needed before AI models can be safely integrated into 
otolarygologic patient care.

© 2023 ARS-AAOA, LLC.",yes,"The study evaluates GPT-4's accuracy and completeness in answering questions about rhinosinusitis, referencing an international clinical consensus statement. This is a direct clinical evaluation of a frontier LLM in a healthcare context.",10.1002/alr.23201,III,Sully,,,
ChatGPT Can Often Respond Adequately to Common Patient Questions Regarding Femoroacetabular Impingement,"Objective: This study aims to analyze the ability of ChatGPT to answer frequently asked questions (FAQs) regarding FAI.We hypothesize that ChatGPT can provide accurate and thorough responses when presented with FAQs regarding FAI.Design: Ten FAQs regarding FAI were presented to ChatGPT 3.5 and initial responses were recorded and analyzed against evidence-based literature.Responses were rated as “excellent response requiring no further clarification, ” “satisfactory response requiring minimal clarification, ” “satisfactory response requiring moderate clarification, ” or “unsatisfactory response requiring substantial clarification.” Setting: Institutional.Independent Variables: Frequently asked questions regarding femoroacetabular impingement.Main Outcome Measures: Accuracy and thoroughness of ChatGPT responses to FAQs.Hypothesis was formulated before data collection.Results: Most responses from ChatGPT were rated as satisfactory and required only minimal clarification.Two responses received an excellent rating and required no further clarification, while only 1 response from ChatGPT was rated unsatisfactory and required substantial clarification.Conclusions: ChatGPT provided largely accurate and thorough responses to FAQs regarding FAI while appropriately reiterating the importance of always consulting a medical professional. Copyright © 2024 Wolters Kluwer Health, Inc.All rights reserved.",yes,"This study evaluates ChatGPT's ability to answer common patient questions about femoroacetabular impingement, directly assessing its clinical utility in patient communication. The study uses a clinical context and measures the accuracy and thoroughness of LLM-generated responses to patient FAQs.",10.1097/JSM.0000000000001327,II,Sully,,,
MedGen: An Explainable Multi-Agent Architecture for Clinical Decision Support through Multisource Knowledge Fusion,"Agents in medical decision support have been extensively researched, particularly in areas like evidence support, multimorbidity management, and patient-specific needs. However, current approaches lack a unified method to address real-time evidence updates, manage multiple diseases independently, and incorporate personalized patient needs. Existing Large Language Model (LLM) agents are limited by their reliance on static knowledge bases, hindering their ability to promptly update clinical guidelines and meet diverse patient requirements. Moreover, the interpretability of LLMs remains a significant concern, leading to skepticism in their medical application. To address these challenges, we developed MedGen, a multi-agent architecture that decomposes the clinical decision-making process into stages such as clinical goal setting, data collection, argumentation linking, and plan selection. This structured approach allows LLM agents to provide both reasoning evidence and a transparent reasoning process, enhancing the reliability and interpretability of outcomes. Finally, a case study of breast cancer and depression is combined to illustrate our architecture. © 2024 IEEE.",yes,"The study presents a multi-agent architecture (MedGen) that uses LLM agents for clinical decision support, specifically addressing clinical goal setting, data collection, and plan selection. The abstract mentions a case study involving breast cancer and depression, indicating a clinical application of LLMs. The focus is on improving interpretability and reliability in clinical decision-making, which aligns with the inclusion criteria.",10.1109/BIBM62325.2024.10822186,II,Sully,,,
Structured clinical reasoning prompt enhances LLM’s diagnostic capabilities in diagnosis please quiz cases,"Purpose: Large Language Models (LLMs) show promise in medical diagnosis, but their performance varies with prompting. Recent studies suggest that modifying prompts may enhance diagnostic capabilities. This study aimed to test whether a prompting approach that aligns with general clinical reasoning methodology—specifically, using a standardized template to first organize clinical information into predefined categories (patient information, history, symptoms, examinations, etc.) before making diagnoses, instead of one-step processing—can enhance the LLM’s medical diagnostic capabilities. Materials and methods: Three hundred twenty two quiz questions from Radiology’s Diagnosis Please cases (1998–2023) were used. We employed Claude 3.5 Sonnet, a state-of-the-art LLM, to compare three approaches: (1) Baseline: conventional zero-shot chain-of-thought prompt, (2) two-step approach: structured two-step approach: first, the LLM systematically organizes clinical information into two distinct categories (patient history and imaging findings), then separately analyzes this organized information to provide diagnoses, and (3) Summary-only approach: using only the LLM-generated summary for diagnoses. Results: The two-step approach significantly outperformed the both baseline and summary-only approaches in diagnostic accuracy, as determined by McNemar’s test. Primary diagnostic accuracy was 60.6% for the two-step approach, compared to 56.5% for baseline (p = 0.042) and 56.3% for summary-only (p = 0.035). For the top three diagnoses, accuracy was 70.5, 66.5, and 65.5% respectively (p = 0.005 for baseline, p = 0.008 for summary-only). No significant differences were observed between the baseline and summary-only approaches. Conclusion: Our results indicate that a structured clinical reasoning approach enhances LLM’s diagnostic accuracy. This method shows potential as a valuable tool for deriving diagnoses from free-text clinical information. The approach aligns well with established clinical reasoning processes, suggesting its potential applicability in real-world clinical settings. © The Author(s) 2024.",yes,"This study evaluates the diagnostic performance of a state-of-the-art LLM (Claude 3.5 Sonnet) on clinical radiology quiz cases, specifically testing how different prompting strategies affect its ability to make medical diagnoses. The focus is on clinical reasoning and diagnostic accuracy, which is a direct clinical application of LLMs.",10.1007/s11604-024-01712-2,III,Sully,,,
Evaluating large language models as patient education tools for inflammatory bowel disease: A comparative study,"BACKGROUND Inflammatory bowel disease (IBD) is a global health burden that affects millions of individuals worldwide, necessitating extensive patient education. Large language models (LLMs) hold promise for addressing patient information needs. However, LLM use to deliver accurate and comprehensible IBD-related medical information has yet to be thoroughly investigated. AIM To assess the utility of three LLMs (ChatGPT-4.0, Claude-3-Opus, and Gemini-1.5- Pro) as a reference point for patients with IBD. METHODS In this comparative study, two gastroenterology experts generated 15 IBD-related questions that reflected common patient concerns. These questions were used to evaluate the performance of the three LLMs. The answers provided by each model were independently assessed by three IBD-related medical experts using a Likert scale focusing on accuracy, comprehensibility, and correlation. Simultaneously, three patients were invited to evaluate the comprehensibility of their answers. Finally, a readability assessment was performed. RESULTS Overall, each of the LLMs achieved satisfactory levels of accuracy, comprehensibility, and completeness when answering IBD-related questions, although their performance varies. All of the investigated models demonstrated strengths in providing basic disease information such as IBD definition as well as its common symptoms and diagnostic methods. Nevertheless, when dealing with more complex medical advice, such as medication side effects, dietary adjustments, and complication risks, the quality of answers was inconsistent between the LLMs. Notably, Claude-3-Opus generated answers with better readability than the other two models. CONCLUSION LLMs have the potential as educational tools for patients with IBD; however, there are discrepancies between the models. Further optimization and the development of specialized models are necessary to ensure the accuracy and safety of the information provided.  © The Author(s) 2025.",yes,"This study evaluates the performance of three frontier LLMs (ChatGPT-4.0, Claude-3-Opus, Gemini-1.5-Pro) in answering patient questions about IBD, with a focus on clinical accuracy, comprehensibility, and readability. The LLMs are assessed in a clinical context as patient education tools, which is a direct clinical application.",10.3748/wjg.v31.i6.102090,II,Sully,,,
"Leveraging GPT-4 for identifying cancer phenotypes in electronic health records: A performance comparison between GPT-4, GPT-3.5-turbo, Flan-T5, Llama-3-8B, and spaCy's rule-based and machine learning-based methods","Objective: Accurately identifying clinical phenotypes from Electronic Health Records (EHRs) provides additional insights into patients' health, especially when such information is unavailable in structured data. This study evaluates the application of OpenAI's Generative Pre-trained Transformer (GPT)-4 model to identify clinical phenotypes from EHR text in non-small cell lung cancer (NSCLC) patients. The goal was to identify disease stages, treatments and progression utilizing GPT-4, and compare its performance against GPT-3.5-turbo, Flan-T5-xl, Flan-T5-xxl, Llama-3-8B, and 2 rule-based and machine learning-based methods, namely, scispaCy and medspaCy. Materials and Methods: Phenotypes such as initial cancer stage, initial treatment, evidence of cancer recurrence, and affected organs during recurrence were identified from 13 646 clinical notes for 63 NSCLC patients from Washington University in St. Louis, Missouri. The performance of the GPT-4 model is evaluated against GPT-3.5-turbo, Flan-T5-xxl, Flan-T5-xl, Llama-3-8B, medspaCy, and scispaCy by comparing precision, recall, and micro-F1 scores. Results: GPT-4 achieved higher F1 score, precision, and recall compared to Flan-T5-xl, Flan-T5-xxl, Llama-3-8B, medspaCy, and scispaCy's models. GPT-3.5-turbo performed similarly to that of GPT-4. GPT, Flan-T5, and Llama models were not constrained by explicit rule requirements for contextual pattern recognition. spaCy models relied on predefined patterns, leading to their suboptimal performance. Discussion and Conclusion: GPT-4 improves clinical phenotype identification due to its robust pre-training and remarkable pattern recognition capability on the embedded tokens. It demonstrates data-driven effectiveness even with limited context in the input. While rule-based models remain useful for some tasks, GPT models offer improved contextual understanding of the text, and robust clinical phenotype extraction. © 2024 The Author(s).",yes,"This study evaluates GPT-4 and other LLMs for extracting clinical phenotypes (e.g., cancer stage, treatment, recurrence) from EHRs in NSCLC patients. The task is clinically relevant as it involves identifying disease characteristics directly from clinical notes, which can impact patient care and decision-making. The comparison with other LLMs and non-LLM methods further strengthens its relevance.",10.1093/jamiaopen/ooae060,I,Sully,,,
Evaluating ChatGPT's Performance in Responding to Questions About Endoscopic Procedures for Patients,"Background and aims: We aimed to assess the accuracy, completeness, and consistency of ChatGPT's responses to frequently asked questions concerning the management and care of patients receiving endoscopic procedures and to compare its performance to Generative Pre-trained Transformer 4 (GPT-4) in providing emotional support. Methods: Frequently asked questions (N = 117) about esophagogastroduodenoscopy (EGD), colonoscopy, endoscopic ultrasound (EUS), and endoscopic retrograde cholangiopancreatography (ERCP) were collected from professional societies, institutions, and social media. ChatGPT's responses were generated and graded by board-certified gastroenterologists and advanced endoscopists. Emotional support questions were assessed by a psychiatrist. Results: ChatGPT demonstrated high accuracy in answering questions about EGD (94.8% comprehensive or correct but insufficient), colonoscopy (100% comprehensive or correct but insufficient), ERCP (91% comprehensive or correct but insufficient), and EUS (87% comprehensive or correct but insufficient). No answers were deemed entirely incorrect (0%). Reproducibility was significant across all categories. ChatGPT's emotional support performance was inferior to the newer GPT-4 model. Conclusion: ChatGPT provides accurate and consistent responses to patient questions about common endoscopic procedures and demonstrates potential as a supplementary information resource for patients and healthcare providers.",yes,"This study evaluates ChatGPT's (and GPT-4's) ability to answer patient questions about endoscopic procedures, including both informational and emotional support aspects. The responses are graded by clinical experts, and the focus is on clinical communication and support for patients. This fits the inclusion criteria as it assesses LLMs in a clinical context.",10.1101/2023.05.31.23290800,II,Sully,,,
Performance of large language artificial intelligence models on solving restorative dentistry and endodontics student assessments,"OBJECTIVES: The advent of artificial intelligence (AI) and large language model (LLM)-based AI applications (LLMAs) has tremendous implications for our society. This study analyzed the performance of LLMAs on solving restorative dentistry and endodontics (RDE) student assessment questions. MATERIALS AND METHODS: 151 questions from a RDE question pool were prepared for prompting using LLMAs from OpenAI (ChatGPT-3.5,-4.0 and -4.0o) and Google (Gemini 1.0). Multiple-choice questions were sorted into four question subcategories, entered into LLMAs and answers recorded for analysis. P-value and chi-square statistical analyses were performed using Python 3.9.16. RESULTS: The total answer accuracy of ChatGPT-4.0o was the highest, followed by ChatGPT-4.0, Gemini 1.0 and ChatGPT-3.5 (72%, 62%, 44% and 25%, respectively) with significant differences between all LLMAs except GPT-4.0 models. The performance on subcategories direct restorations and caries was the highest, followed by indirect restorations and endodontics. CONCLUSIONS: Overall, there are large performance differences among LLMAs. Only the ChatGPT-4 models achieved a success ratio that could be used with caution to support the dental academic curriculum. CLINICAL RELEVANCE: While LLMAs could support clinicians to answer dental field-related questions, this capacity depends strongly on the employed model. The most performant model ChatGPT-4.0o achieved acceptable accuracy rates in some subject sub-categories analyzed.",yes,"This study evaluates the performance of several generative LLMs (ChatGPT-3.5, ChatGPT-4.0, ChatGPT-4.0o, and Gemini 1.0) on student assessment questions in restorative dentistry and endodontics. The focus is on clinical knowledge relevant to dental education, and the results are discussed in terms of their potential to support clinicians and the dental curriculum. This fits the inclusion criteria as it assesses LLMs in a clinical/medical education context.",10.1007/s00784-024-05968-w,III,Sully,,,
Capabilities of GPT-4 in ophthalmology: an analysis of model entropy and progress towards human-level medical question answering,"Background Evidence on the performance of Generative Pre-trained Transformer 4 (GPT-4), a large language model (LLM), in the ophthalmology question-answering domain is needed. Methods We tested GPT-4 on two 260-question multiple choice question sets from the Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions question banks. We compared the accuracy of GPT-4 models with varying temperatures (creativity setting) and evaluated their responses in a subset of questions. We also compared the best-performing GPT-4 model to GPT-3.5 and to historical human performance. Results GPT-4-0.3 (GPT-4 with a temperature of 0.3) achieved the highest accuracy among GPT-4 models, with 75.8% on the BCSC set and 70.0% on the OphthoQuestions set. The combined accuracy was 72.9%, which represents an 18.3% raw improvement in accuracy compared with GPT-3.5 (p<0.001). Human graders preferred responses from models with a temperature higher than 0 (more creative). Exam section, question difficulty and cognitive level were all predictive of GPT-4-0.3 answer accuracy. GPT-4-0.3's performance was numerically superior to human performance on the BCSC (75.8% vs 73.3%) and OphthoQuestions (70.0% vs 63.0%), but the difference was not statistically significant (p=0.55 and p=0.09). Conclusion GPT-4, an LLM trained on non-ophthalmology-specific data, performs significantly better than its predecessor on simulated ophthalmology board-style exams. Remarkably, its performance tended to be superior to historical human performance, but that difference was not statistically significant in our study.  © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study evaluates GPT-4, a large language model, on ophthalmology board-style exam questions and compares its performance to GPT-3.5 and human performance. The focus is on clinical question answering, which is a direct clinical application relevant to healthcare.",10.1136/bjo-2023-324438,III,Sully,,,
TCM-GPT: Efficient pre-training of large language models for domain adaptation in Traditional Chinese Medicine,"Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retrieving from general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained model's weights and uses rank decomposition matrices to efficiently train specific dense layers for pre-training and fine-tuning, efficiently aligning the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted extensive experiments on two TCM tasks, including TCM examination and TCM diagnosis. TCM-GPT-7B archived the best performance across both datasets, outperforming other models by relative increments of 17% and 12% in accuracy, respectively. To the best of our knowledge, our study represents the pioneering validation of domain adaptation of a large language model with 7 billion parameters in TCM domain. We will release both TCM-Corpus-1B and TCM-GPT-7B model once accepted to facilitate interdisciplinary development in TCM and NLP, serving as the foundation for further study. © 2024 The Authors",yes,"This study evaluates a domain-adapted large language model (TCM-GPT-7B) on clinical tasks specific to Traditional Chinese Medicine, such as TCM examination and diagnosis. The abstract describes experimental evaluation of the LLM's performance on these clinical tasks, which fits the inclusion criteria.",10.1016/j.cmpbup.2024.100158,III,Sully,,,
Chat Generative Pre-Trained Transformer (ChatGPT) – 3.5 Responses Require Advanced Readability for the General Population and May Not Effectively Supplement Patient-Related Information Provided by the Treating Surgeon Regarding Common Questions About Rotator Cuff Repair,"Purpose: To investigate the accuracy of Chat Generative Pre-Trained Transformer (ChatGPT)’s responses to frequently asked questions prior to rotator cuff repair surgery. Methods: The 10 most common frequently asked questions related to rotator cuff repair were compiled from 4 institution websites. Questions were then input into ChatGPT-3.5 in 1 session. The provided ChatGPT-3.5 responses were analyzed by 2 orthopaedic surgeons for reliability, quality, and readability using the Journal of the American Medical Association Benchmark criteria, the DISCERN score, and the Flesch-Kincaid Grade Level. Results: The Journal of the American Medical Association Benchmark criteria score was 0, indicating the absence of reliable source material citations. The mean Flesch-Kincaid Grade Level was 13.4 (range, 11.2-15.0). The mean DISCERN score was 43.4 (range, 36-51), indicating that the quality of the responses overall was considered fair. All responses cited making final decision-making to be made with the treating physician. Conclusions: ChatGPT-3.5 provided substandard patient-related information in alignment with recommendations from the treating surgeon regarding common questions around rotator cuff repair surgery. Additionally, the responses lacked reliable source material citations, and the readability of the responses was relatively advanced with a complex language style. Clinical Relevance: The findings of this study suggest that ChatGPT-3.5 may not effectively supplement patient-related information in the context of recommendations provided by the treating surgeon prior to rotator cuff repair surgery. © 2024 Arthroscopy Association of North America",yes,"This study evaluates ChatGPT-3.5's responses to common patient questions about rotator cuff repair, assessing their reliability, quality, and readability. It directly tests the LLM's ability to provide clinical information to patients, which is a clinical application relevant to healthcare.",10.1016/j.arthro.2024.05.009,II,Sully,,,
Comparative analysis of ChatGPT and Bard in answering pathology examination questions requiring image interpretation,"Objectives: To evaluate the accuracy of ChatGPT and Bard in answering pathology examination questions requiring image interpretation. Methods: The study evaluated ChatGPT-4 and Bard's performance using 86 multiple-choice questions, with 17 (19.8%) focusing on general pathology and 69 (80.2%) on systemic pathology. Of these, 62 (72.1%) included microscopic images, and 57 (66.3%) were first-order questions focusing on diagnosing the disease. The authors presented these artificial intelligence (AI) tools with questions, both with and without clinical contexts, and assessed their answers against a reference standard set by pathologists. Results: ChatGPT-4 achieved a 100% (n = 86) accuracy rate in questions with clinical context, surpassing Bard's 87.2% (n = 75). Without context, the accuracy of both AI tools declined significantly, with ChatGPT-4 at 52.3% (n = 45) and Bard at 38.4% (n = 33). ChatGPT-4 consistently outperformed Bard across various categories, particularly in systemic pathology and first-order questions. A notable issue identified was Bard's tendency to ""hallucinate""or provide plausible but incorrect answers, especially without clinical context. Conclusions: This study demonstrated the potential of ChatGPT and Bard in pathology education, stressing the importance of clinical context for accurate AI interpretations of pathology images. It underlined the need for careful AI integration in medical education.  © 2024 The Author(s). Published by Oxford University Press on behalf of American Society for Clinical Pathology. All rights reserved.",yes,"This study evaluates the performance of ChatGPT-4 and Bard in answering pathology exam questions that require image interpretation, a clinically relevant task. The LLMs are tested on their ability to diagnose diseases from pathology images, which is a direct clinical application. The focus is on their accuracy in a medical education context, specifically on clinical reasoning and diagnosis.",10.1093/ajcp/aqae036,III,Sully,,,
Multimodal LLMs for Retinal Disease Diagnosis via OCT: Few-Shot vs Single-Shot Learning,"Background and Aim: Multimodal large language models (LLMs) have shown potential in processing both text and image data for clinical applications. This study evaluated their diagnostic performance in identifying retinal diseases from optical coherence tomography (OCT) images. Methods: We assessed the diagnostic accuracy of GPT-4o and Claude Sonnet 3.5 using two public OCT datasets (OCTID, OCTDL) containing expert-labeled images of four pathological conditions and normal retinas. Both models were tested using single-shot and few-shot prompts, with an overall of 3088 models’ API calls. Statistical analyses were performed to evaluate differences in overall and condition-specific performance. Results: GPT-4o's accuracy improved from 56.29% with single-shot prompts to 73.08% with few-shot prompts (p < 0.001). Similarly, Claude Sonnet 3.5 increased from 40.03% to 70.98% using the same approach (p < 0.001). Condition-specific analyses revealed similar trends, with absolute improvements ranging from 2% to 64%. These findings were consistent across the validation dataset. Conclusion: Few-shot prompted multimodal LLMs show promise for clinical integration, particularly in identifying normal retinas, which could help streamline referral processes in primary care. While these models fall short of the diagnostic accuracy reported in established deep learning literature, they offer simple, effective tools for assisting in routine retinal disease diagnosis. Future research should focus on further validation and integrating clinical text data with imaging.",yes,"This study evaluates the diagnostic performance of frontier multimodal LLMs (GPT-4o and Claude Sonnet 3.5) in identifying retinal diseases from OCT images, a direct clinical application. The models are assessed for their ability to make clinical diagnoses, which fits the inclusion criteria.",10.1101/2024.11.02.24316624,I,Sully,,,
"MedFrenchmark, a Small Set for Benchmarking Generative LLMs in Medical French","Generative Large Language Models (LLMs) have become ubiquitous in various fields, including healthcare and medicine. Consequently, there is growing interest in leveraging LLMs for medical applications, leading to the emergence of novel models daily. However, evaluation and benchmarking frameworks for LLMs are scarce, particularly those tailored for medical French. To address this gap, we introduce a minimal benchmark consisting of 114 open questions designed to assess the medical capabilities of LLMs in French. The proposed benchmark encompasses a wide range of medical domains, reflecting real-world clinical scenarios' complexity. A preliminary validation involved testing seven widely used LLMs with a parameter size of 7 billion. Results revealed significant variability in performance, emphasizing the importance of rigorous evaluation before deploying LLMs in medical settings. In conclusion, we present a novel and valuable resource for rapidly evaluating LLMs in medical French. By promoting greater accountability and standardization, this benchmark has the potential to enhance trustworthiness and utility in harnessing LLMs for medical applications. © 2024 The Authors.",yes,"The study introduces a benchmark for evaluating generative LLMs in medical French and reports preliminary results of testing several LLMs on clinically relevant questions. Although the main focus is on the creation of the benchmark, the abstract describes an actual evaluation of LLMs' medical capabilities, which aligns with the inclusion criteria.",10.3233/SHTI240486,III,Sully,,,
Managing class imbalance in the training of a large language model to predict patient selection for total knee arthroplasty: Results from the Artificial intelligence to Revolutionise the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project,"Introduction: This study set out to test the efficacy of different techniques used to manage to class imbalance, a type of data bias, in application of a large language model (LLM) to predict patient selection for total knee arthroplasty (TKA). Methods: This study utilised data from the Artificial Intelligence to Revolutionise the Patient Care Pathway in Hip and Knee Arthroplasty (ARCHERY) project (ISRCTN18398037). Data included the pre-operative radiology reports of patients referred to secondary care for knee-related complaints from within the North of Scotland. A clinically based LLM (GatorTron) was trained regarding prediction of selection for TKA. Three methods for managing class imbalance were assessed: a standard model, use of class weighting, and majority class undersampling. Results: A total of 7707 individual knee radiology reports were included (dated from 2015 to 2022). The mean text length was 74 words (range 26–275). Only 910/7707 (11.8%) patients underwent TKA surgery (the designated ‘minority class’). Class weighting technique performed better for minority class discrimination and calibration compared with the other two techniques (Recall 0.61/AUROC 0.73 for class weighting compared with 0.54/0.70 and 0.59/0.72 for the standard model and majority class undersampling, respectively. There was also significant data loss for majority class undersampling when compared with class-weighting. Conclusion: Use of class-weighting appears to provide the optimal method of training a an LLM to perform analytical tasks on free-text clinical information in the face of significant data bias (‘class imbalance’). Such knowledge is an important consideration in the development of high-performance clinical AI models within Trauma and Orthopaedics. © 2025 The Author(s)",yes,"The study uses a large language model (GatorTron) to predict patient selection for total knee arthroplasty based on clinical radiology reports, which is a clinical application. The focus is on evaluating methods to handle class imbalance in training the LLM for this prediction task. Since the LLM is being used for a clinical decision support task (predicting surgery selection), this fits the inclusion criteria.",10.1016/j.knee.2025.02.007,I,Sully,,,
Both Patients and Plastic Surgeons Prefer Artificial Intelligence-Generated Microsurgical Information,"BACKGROUND:  With the growing relevance of artificial intelligence (AI)-based 
patient-facing information, microsurgical-specific online information provided 
by professional organizations was compared with that of ChatGPT (Chat Generative 
Pre-Trained Transformer) and assessed for accuracy, comprehensiveness, clarity, 
and readability.
METHODS:  Six plastic and reconstructive surgeons blindly assessed responses to 
10 microsurgery-related medical questions written either by the American Society 
of Reconstructive Microsurgery (ASRM) or ChatGPT based on accuracy, 
comprehensiveness, and clarity. Surgeons were asked to choose which source 
provided the overall highest-quality microsurgical patient-facing information. 
Additionally, 30 individuals with no medical background (ages: 18-81, μ = 49.8) 
were asked to determine a preference when blindly comparing materials. 
Readability scores were calculated, and all numerical scores were analyzed using 
the following six reliability formulas: Flesch-Kincaid Grade Level, 
Flesch-Kincaid Readability Ease, Gunning Fog Index, Simple Measure of 
Gobbledygook Index, Coleman-Liau Index, Linsear Write Formula, and Automated 
Readability Index. Statistical analysis of microsurgical-specific online sources 
was conducted utilizing paired t-tests.
RESULTS:  Statistically significant differences in comprehensiveness and clarity 
were seen in favor of ChatGPT. Surgeons, 70.7% of the time, blindly choose 
ChatGPT as the source that overall provided the highest-quality microsurgical 
patient-facing information. Nonmedical individuals 55.9% of the time selected 
AI-generated microsurgical materials as well. Neither ChatGPT nor ASRM-generated 
materials were found to contain inaccuracies. Readability scores for both 
ChatGPT and ASRM materials were found to exceed recommended levels for patient 
proficiency across six readability formulas, with AI-based material scored as 
more complex.
CONCLUSION:  AI-generated patient-facing materials were preferred by surgeons in 
terms of comprehensiveness and clarity when blindly compared with online 
material provided by ASRM. Studied AI-generated material was not found to 
contain inaccuracies. Additionally, surgeons and nonmedical individuals 
consistently indicated an overall preference for AI-generated material. A 
readability analysis suggested that both materials sourced from ChatGPT and ASRM 
surpassed recommended reading levels across six readability scores.

Thieme. All rights reserved.",yes,"This study evaluates ChatGPT's ability to generate patient-facing microsurgical information and compares it to professional society materials, with assessments by both surgeons and laypeople. The focus is on the clinical utility of LLM-generated information for patient education, which is a relevant clinical application.",10.1055/a-2273-4163,II,Sully,,,
Use of large language model-based chatbots in managing the rehabilitation concerns and education needs of outpatient stroke survivors and caregivers,"Background: The utility of large language model-based (LLM) artificial intelligence (AI) chatbots in many aspects of healthcare is becoming apparent though their ability to address patient concerns remains unknown. We sought to evaluate the performance of two well-known, freely-accessible chatbots, ChatGPT and Google Bard, in responding to common questions about stroke rehabilitation posed by patients and their caregivers. Methods: We collected questions from outpatients and their caregivers through a survey, categorised them by theme, and created representative questions to be posed to both chatbots. We then evaluated the chatbots' responses based on accuracy, safety, relevance, and readability. Interrater agreement was also tracked. Results: Although both chatbots achieved similar overall scores, Google Bard performed slightly better in relevance and safety. Both provided readable responses with some general accuracy, but struggled with hallucinated responses, were often not specific, and lacked awareness of the possibility for emotional situations with the potential to turn dangerous. Additionally, interrater agreement was low, highlighting the variability in physician acceptance of their responses. Conclusions: AI chatbots show potential in patient-facing support roles, but issues remain regarding safety, accuracy, and relevance. Future chatbots should address these problems to ensure that they can reliably and independently manage the concerns and questions of stroke patients and their caregivers. 2024 Neo, Ser and Tay.",yes,"This study evaluates the performance of LLM-based chatbots (ChatGPT and Google Bard) in responding to clinical questions from stroke survivors and caregivers, focusing on accuracy, safety, relevance, and readability. The clinical context and direct patient/caregiver interaction make it eligible.",10.3389/fdgth.2024.1395501,II,Sully,,,
Evaluation of ChatGPT responses for back pain?,"2336. Eur J Orthop Surg Traumatol. 2024 Apr;34(3):1737-1739. doi: 
10.1007/s00590-024-03875-0. Epub 2024 Feb 26.

Evaluation of ChatGPT responses for back pain?

Narula P(1)(2), Deoghare R(3)(4).

Author information:
(1)Safdarjung Hospital and Vardhaman Mahavir Medical College, New Delhi, India. 
drpaarth.narula@gmail.com.
(2)NKP Salve Institute of Medical Sciences, Nagpur, India. 
drpaarth.narula@gmail.com.
(3)Safdarjung Hospital and Vardhaman Mahavir Medical College, New Delhi, India.
(4)NKP Salve Institute of Medical Sciences, Nagpur, India.

Erratum in
    Eur J Orthop Surg Traumatol. 2024 May;34(4):2223. doi: 
10.1007/s00590-024-03932-8.",yes,"The study evaluates ChatGPT's responses regarding back pain, which is a clinical topic. This fits the inclusion criteria as it assesses a generative LLM (ChatGPT) in a healthcare context, likely focusing on its ability to answer clinical questions or provide decision support related to back pain.",10.1007/s00590-024-03875-0,X,Sully,"These authors need a temporary ban on publishing, this is one of the most egregious studies in this entire list",,
Large Language Models in Otolaryngology Residency Admissions: A Random Sampling Analysis,"Objectives: To investigate potential demographic bias in artificial intelligence (AI)-based simulations of otolaryngology, residency selection committee (RSC) members tasked with selecting one applicant among candidates with varied racial, gender, and sexual orientations. Methods: This study employed random sampling of simulated RSC member decisions using a novel Application Programming Interface (API) to virtually connect to OpenAI's Generative Pre-Trained Transformers (GPT-4 and GPT-4o). Simulated RSC members with diverse demographics were tasked with ranking to match 1 applicant among 10 with varied racial, gender, and sexual orientations. All applicants had identical qualifications; only demographics of the applicants and RSC members were varied for each simulation. Each RSC simulation ran 1000 times. Chi-square tests analyzed differences across categorical variables. GPT-4o simulations additionally requested a rationale for each decision. Results: Simulated RSCs consistently showed racial, gender, and sexual orientation bias. Most applicant pairwise comparisons showed statistical significance (p < 0.05). White and Black RSCs exhibited greatest preference for applicants sharing their own demographic characteristics, favoring White and Black female applicants, respectively, over others (all pairwise p < 0.001). Asian male applicants consistently received lowest selection rates. Male RSCs favored White male and female applicants, while female RSCs preferred LGBTQIA+, White and Black female applicants (all p < 0.05). High socioeconomic status (SES) RSCs favored White female and LGBTQIA+ applicants, while low SES RSCs favored Black female and LGBTQIA+ applicants over others (all p < 0.001). Results from the newest iteration of the LLM, ChatGPT-4o, indicated evolved selection preferences favoring Black female and LGBTQIA+ applicants across all RSCs, with the rationale of prioritizing inclusivity given in >95% of such decisions. Conclusion: Utilizing publicly available LLMs to aid in otolaryngology residency selection may introduce significant racial, gender, and sexual orientation bias. Potential for significant and evolving LLM bias should be appreciated and minimized to promote a diverse and representative field of future otolaryngologists in alignment with current workforce data. Level of Evidence: NA Laryngoscope, 135:87–93, 2025. © 2024 The American Laryngological, Rhinological and Otological Society, Inc.",yes,"This study evaluates the use of generative LLMs (GPT-4 and GPT-4o) in simulating clinical residency selection committee decisions, specifically investigating demographic bias in the context of otolaryngology residency admissions. The LLMs are used to make decisions that have direct clinical and educational implications, and the study assesses their behavior in a real-world healthcare-related process. This fits the inclusion criteria as it evaluates LLMs in a healthcare/medical context, specifically in a decision-making process relevant to clinical training.",10.1002/lary.31705,X,Sully,Not a clinical task,,
Exploring the Potential of ChatGPT-4 in Predicting Refractive Surgery Categorizations: Comparative Study,"Background: Refractive surgery research aims to optimally precategorize patients by their suitability for various types of surgery. Recent advances have led to the development of artificial intelligence–powered algorithms, including machine learning approaches, to assess risks and enhance workflow. Large language models (LLMs) like ChatGPT-4 (OpenAI LP) have emerged as potential general artificial intelligence tools that can assist across various disciplines, possibly including refractive surgery decision-making. However, their actual capabilities in precategorizing refractive surgery patients based on real-world parameters remain unexplored. Objective: This exploratory study aimed to validate ChatGPT-4’s capabilities in precategorizing refractive surgery patients based on commonly used clinical parameters. The goal was to assess whether ChatGPT-4’s performance when categorizing batch inputs is comparable to those made by a refractive surgeon. A simple binary set of categories (patient suitable for laser refractive surgery or not) as well as a more detailed set were compared. Methods: Data from 100 consecutive patients from a refractive clinic were anonymized and analyzed. Parameters included age, sex, manifest refraction, visual acuity, and various corneal measurements and indices from Scheimpflug imaging. This study compared ChatGPT-4’s performance with a clinician’s categorizations using Cohen κ coefficient, a chi-square test, a confusion matrix, accuracy, precision, recall, F1-score, and receiver operating characteristic area under the curve. Results: A statistically significant noncoincidental accordance was found between ChatGPT-4 and the clinician’s categorizations with a Cohen κ coefficient of 0.399 for 6 categories (95% CI 0.256-0.537) and 0.610 for binary categorization (95% CI 0.372-0.792). The model showed temporal instability and response variability, however. The chi-square test on 6 categories indicated an association between the 2 raters’ distributions (χ25=94.7, P<.001). Here, the accuracy was 0.68, precision 0.75, recall 0.68, and F1-score 0.70. For 2 categories, the accuracy was 0.88, precision 0.88, recall 0.88, F1-score 0.88, and area under the curve 0.79. Conclusions: This study revealed that ChatGPT-4 exhibits potential as a precategorization tool in refractive surgery, showing promising agreement with clinician categorizations. However, its main limitations include, among others, dependency on solely one human rater, small sample size, the instability and variability of ChatGPT’s (OpenAI LP) output between iterations and nontransparency of the underlying models. The results encourage further exploration into the application of LLMs like ChatGPT-4 in health care, particularly in decision-making processes that require understanding vast clinical data. Future research should focus on defining the model’s accuracy with prompt and vignette standardization, detecting confounding factors, and comparing to other versions of ChatGPT-4 and other LLMs to pave the way for larger-scale validation and real-world implementation. © 2023 JMIR Publications Inc.",yes,"This study directly evaluates ChatGPT-4, a large language model, in a clinical decision-making context—specifically, its ability to categorize patients for refractive surgery based on real clinical parameters. The study compares the LLM's performance to that of a clinician, using standard clinical metrics. This fits the inclusion criteria as it assesses an LLM's clinical application.",10.2196/51798,I,Sully,,,
Postoperative Otoplasty Care With ChatGPT-4: A Study on Artificial Intelligence (AI)-Assisted Patient Concern and Education,"Background: Otoplasty is a cosmetic surgery that is performed to alter the size, shape, or position of the ear by using permanent stitches. Its main purpose is to correct protruding ears, a condition known as prominauris. After the surgery, it is crucial to provide proper care to ensure successful recovery. However, obtaining timely medical advice can be difficult, especially in remote areas or places with limited resources. To address this issue, incorporating advanced artificial intelligence (AI) tools like Chat Generative Pre-trained Transformer (ChatGPT)-4 into postsurgical care could help fill the gap in patient education and support. Aim: This study aims to assess whether ChatGPT4 can be a reliable, accurate, and effective method for answering the most common patient questions and concerns post-otoplasty. The main objective was to assess the AI chatbot’s capacity to deliver precise, concise, and pertinent information, especially in situations where health care professionals are limited in availability. Materials and Methods: In this study, over 50 patients were engaged, and ChatGPT4 was employed to present the same 5 common postoperative questions post-otoplasty surgery care. The AI chatbot’s responses were analyzed for accuracy, response time, clarity, and relevance. Results: The chatbot could potentially provide timely assistance, answer questions, and address concerns related to postsurgical care in otoplasty. The responses exhibited a perfect accuracy rate of 100%, closely corresponding to existing medical guidelines. Conclusion: This study explores the potential of AI-driven solutions to enhance patient education and support, especially in areas where access to health care professionals may be limited. However, professional medical advice is crucial in postoperative care and cannot be replaced by ChatGPT-4. By leveraging AI tools like chatbots, individuals in remote or resource-limited settings can potentially receive valuable information and guidance, contributing to successful rehabilitation and overall health care outcomes. Ethical considerations around the use of AI in health care must also be carefully addressed to ensure patient privacy, data security, and appropriate clinical oversight. Copyright © 2024 by Mutaz B. Habal, MD.",yes,"This study evaluates ChatGPT-4's ability to answer patient questions and concerns regarding postoperative otoplasty care, directly assessing its clinical utility in patient education and support. The use of ChatGPT-4 for answering real patient questions about a specific medical procedure aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1097/SCS.0000000000010678,I,Robert,,,
Retrieval Augmented Therapy Suggestion for Molecular Tumor Boards: Algorithmic Development and Validation Study,"Background: Molecular tumor boards (MTBs) require intensive manual investigation to generate optimal treatment recommendations for patients. Large language models (LLMs) can catalyze MTB recommendations, decrease human error, improve accessibility to care, and enhance the efficiency of precision oncology. Objective: In this study, we aimed to investigate the efficacy of LLM-generated treatments for MTB patients. We specifically investigate the LLMs’ ability to generate evidence-based treatment recommendations using PubMed references. Methods: We built a retrieval augmented generation pipeline using PubMed data. We prompted the resulting LLM to generate treatment recommendations with PubMed references using a test set of patients from an MTB conference at a large comprehensive cancer center at a tertiary care institution. Members of the MTB manually assessed the relevancy and correctness of the generated responses. Results: A total of 75% of the referenced articles were properly cited from PubMed, while 17% of the referenced articles were hallucinations, and the remaining were not properly cited from PubMed. Clinician-generated LLM queries achieved higher accuracy through clinician evaluation than automated queries, with clinicians labeling 25% of LLM responses as equal to their recommendations and 37.5% as alternative plausible treatments. Conclusions: This study demonstrates how retrieval augmented generation–enhanced LLMs can be a powerful tool in accelerating MTB conferences, as LLMs are sometimes capable of achieving clinician-equal treatment recommendations. However, further investigation is required to achieve stable results with zero hallucinations. LLMs signify a scalable solution to the time-intensive process of MTB investigations. However, LLM performance demonstrates that they must be used with heavy clinician supervision, and cannot yet fully automate the MTB pipeline. ©Eliza Berman, Holly Sundberg Malek, Michael Bitzer, Nisar Malek, Carsten Eickhoff.",yes,"This study evaluates the use of a large language model (LLM) to generate evidence-based treatment recommendations for molecular tumor boards, with direct clinical application and assessment by clinicians. It fits the inclusion criteria as it assesses LLM performance in a clinical decision support context.",10.2196/64364,III,Robert,,,
Assessing the Alignment of Large Language Models With Human Values for Mental Health Integration: Cross-Sectional Study Using Schwartz’s Theory of Basic Values,"Background: Large language models (LLMs) hold potential for mental health applications. However, their opaque alignment processes may embed biases that shape problematic perspectives. Evaluating the values embedded within LLMs that guide their decision-making have ethical importance. Schwartz’s theory of basic values (STBV) provides a framework for quantifying cultural value orientations and has shown utility for examining values in mental health contexts, including cultural, diagnostic, and therapist-client dynamics. Objective: This study aimed to (1) evaluate whether the STBV can measure value-like constructs within leading LLMs and (2) determine whether LLMs exhibit distinct value-like patterns from humans and each other. Methods: In total, 4 LLMs (Bard, Claude 2, Generative Pretrained Transformer [GPT]-3.5, GPT-4) were anthropomorphized and instructed to complete the Portrait Values Questionnaire—Revised (PVQ-RR) to assess value-like constructs. Their responses over 10 trials were analyzed for reliability and validity. To benchmark the LLMs’ value profiles, their results were compared to published data from a diverse sample of 53,472 individuals across 49 nations who had completed the PVQ-RR. This allowed us to assess whether the LLMs diverged from established human value patterns across cultural groups. Value profiles were also compared between models via statistical tests. Results: The PVQ-RR showed good reliability and validity for quantifying value-like infrastructure within the LLMs. However, substantial divergence emerged between the LLMs’ value profiles and population data. The models lacked consensus and exhibited distinct motivational biases, reflecting opaque alignment processes. For example, all models prioritized universalism and self-direction, while de-emphasizing achievement, power, and security relative to humans. Successful discriminant analysis differentiated the 4 LLMs’ distinct value profiles. Further examination found the biased value profiles strongly predicted the LLMs’ responses when presented with mental health dilemmas requiring choosing between opposing values. This provided further validation for the models embedding distinct motivational value-like constructs that shape their decision-making. Conclusions: This study leveraged the STBV to map the motivational value-like infrastructure underpinning leading LLMs. Although the study demonstrated the STBV can effectively characterize value-like infrastructure within LLMs, substantial divergence from human values raises ethical concerns about aligning these models with mental health applications. The biases toward certain cultural value sets pose risks if integrated without proper safeguards. For example, prioritizing universalism could promote unconditional acceptance even when clinically unwise. Furthermore, the differences between the LLMs underscore the need to standardize alignment processes to capture true cultural diversity. Thus, any responsible integration of LLMs into mental health care must account for their embedded biases and motivation mismatches to ensure equitable delivery across diverse populations. Achieving this will require transparency and refinement of alignment techniques to instill comprehensive human values. © 2024 JMIR Publications Inc.. All rights reserved.",yes,"This study evaluates the value alignment and embedded biases of several frontier LLMs (Bard, Claude 2, GPT-3.5, GPT-4) in the context of mental health integration, using a validated framework (Schwartz’s theory of basic values). It specifically examines how these value profiles might affect LLM decision-making in mental health dilemmas, which is a clinical context. The study provides insight into the ethical and practical implications of using LLMs in mental health care, which is a relevant clinical application.",10.2196/55988,X,Robert,Not sure this should even be included?,,
Performance of ChatGPT and Bard on the official part 1 FRCOphth practice questions,"Background Chat Generative Pre-trained Transformer (ChatGPT), a large language model by OpenAI, and Bard, Google's artificial intelligence (AI) chatbot, have been evaluated in various contexts. This study aims to assess these models' proficiency in the part 1 Fellowship of the Royal College of Ophthalmologists (FRCOphth) Multiple Choice Question (MCQ) examination, highlighting their potential in medical education. Methods Both models were tested on a sample question bank for the part 1 FRCOphth MCQ exam. Their performances were compared with historical human performance on the exam, focusing on the ability to comprehend, retain and apply information related to ophthalmology. We also tested it on the book € MCQs for FRCOpth part 1', and assessed its performance across subjects. Results ChatGPT demonstrated a strong performance, surpassing historical human pass marks and examination performance, while Bard underperformed. The comparison indicates the potential of certain AI models to match, and even exceed, human standards in such tasks. Conclusion The results demonstrate the potential of AI models, such as ChatGPT, in processing and applying medical knowledge at a postgraduate level. However, performance varied among different models, highlighting the importance of appropriate AI selection. The study underlines the potential for AI applications in medical education and the necessity for further investigation into their strengths and limitations. © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY. Published by BMJ.",yes,"This study evaluates the performance of ChatGPT and Bard (both LLMs) on a clinically relevant medical board exam (FRCOphth Part 1), directly assessing their ability to process and apply medical knowledge. This fits the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1136/bjo-2023-324091,III,Robert,,,
Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes,"Background: ChatGPT has emerged as the most advanced and rapidly developing large language chatbot system. With its immense potential ranging from answering a simple query to cracking highly competitive medical exams, ChatGPT continues to impress the scientists and researchers worldwide giving room for more discussions regarding its utility in various fields. One such field of attention is Psychiatry. With suboptimal diagnosis and treatment, assuring mental health and well-being is a challenge in many countries, particularly developing nations. To this regard, we conducted an evaluation to assess the performance of ChatGPT 3.5 in Psychiatry using clinical cases to provide evidence-based information regarding the implication of ChatGPT 3.5 in enhancing mental health and well-being. Methods: ChatGPT 3.5 was used in this experimental study to initiate the conversations and collect responses to clinical vignettes in Psychiatry. Using 100 clinical case vignettes, the replies were assessed by expert faculties from the Department of Psychiatry. There were 100 different psychiatric illnesses represented in the cases. We recorded and assessed the initial ChatGPT 3.5 responses. The evaluation was conducted using the objective of questions that were put forth at the conclusion of the case, and the aim of the questions was divided into 10 categories. The grading was completed by taking the mean value of the scores provided by the evaluators. Graphs and tables were used to represent the grades. Results: The evaluation report suggests that ChatGPT 3.5 fared extremely well in Psychiatry by receiving “Grade A” ratings in 61 out of 100 cases, “Grade B” ratings in 31, and “Grade C” ratings in 8. Majority of the queries were concerned with the management strategies, which were followed by diagnosis, differential diagnosis, assessment, investigation, counselling, clinical reasoning, ethical reasoning, prognosis, and request acceptance. ChatGPT 3.5 performed extremely well, especially in generating management strategies followed by diagnoses for different psychiatric conditions. There were no responses which were graded “D” indicating that there were no errors in the diagnosis or response for clinical care. Only a few discrepancies and additional details were missed in a few responses that received a “Grade C” Conclusion: It is evident from our study that ChatGPT 3.5 has appreciable knowledge and interpretation skills in Psychiatry. Thus, ChatGPT 3.5 undoubtedly has the potential to transform the field of Medicine and we emphasize its utility in Psychiatry through the finding of our study. However, for any AI model to be successful, assuring the reliability, validation of information, proper guidelines and implementation framework are necessary. © 2023 Elsevier B.V.",yes,"This study directly evaluates ChatGPT 3.5's performance on 100 clinical psychiatric case vignettes, assessing its ability to provide management strategies, diagnoses, and other clinical reasoning tasks. The evaluation is clinical in nature and uses a frontier LLM in a healthcare context.",10.1016/j.ajp.2023.103770,III,Robert,,,
Uncovering Language Disparity of ChatGPT in Healthcare: Non-English Clinical Environment for Retinal Vascular Disease Classification,"Objective: To evaluate the effectiveness and reasoning ability of ChatGPT in diagnosing retinal vascular diseases in the Chinese clinical environment. Materials and Methods: We collected 1226 fundus fluorescein angiography reports and corresponding diagnosis written in Chinese, and tested ChatGPT with four prompting strategies (direct diagnosis or diagnosis with explanation and in Chinese or English). Results: ChatGPT using English prompt for direct diagnosis achieved the best performance, with F1-score of 80.05%, which was inferior to ophthalmologists (89.35%) but close to ophthalmologist interns (82.69%). Although ChatGPT can derive reasoning process with a low error rate, mistakes such as misinformation (1.96%), and hallucination (0.59%) still exist. Discussion and Conclusions: ChatGPT can serve as a helpful medical assistant to provide diagnosis under non-English clinical environments, but there are still performance gaps, language disparity, and errors compared to professionals, which demonstrates the potential limitations and the desiration to continually explore more robust LLMs in ophthalmology practice.",yes,"This study evaluates ChatGPT's effectiveness and reasoning ability in diagnosing retinal vascular diseases using real clinical reports in a non-English (Chinese) environment. The focus is on clinical diagnosis, a direct clinical application of an LLM, and compares performance to human clinicians.",10.1101/2023.06.28.23291931,II,Robert,,,
Large language models for preventing medication direction errors in online pharmacies,"3241. Nat Med. 2024 Jun;30(6):1574-1582. doi: 10.1038/s41591-024-02933-8. Epub 2024 
Apr 25.

Large language models for preventing medication direction errors in online 
pharmacies.

Pais C(1), Liu J(2), Voigt R(2), Gupta V(2)(3), Wade E(2), Bayati M(2)(4).

Author information:
(1)Amazon, Seattle, WA, USA. crispais@amazon.com.
(2)Amazon, Seattle, WA, USA.
(3)Department of Health Metrics Sciences, University of Washington, Seattle, WA, 
USA.
(4)Operations, Information and Technology at Graduate School of Business, 
Stanford University, Stanford, CA, USA.

Errors in pharmacy medication directions, such as incorrect instructions for 
dosage or frequency, can increase patient safety risk substantially by raising 
the chances of adverse drug events. This study explores how integrating domain 
knowledge with large language models (LLMs)-capable of sophisticated text 
interpretation and generation-can reduce these errors. We introduce MEDIC 
(medication direction copilot), a system that emulates the reasoning of 
pharmacists by prioritizing precise communication of core clinical components of 
a prescription, such as dosage and frequency. It fine-tunes a first-generation 
LLM using 1,000 expert-annotated and augmented directions from Amazon Pharmacy 
to extract the core components and assembles them into complete directions using 
pharmacy logic and safety guardrails. We compared MEDIC against two LLM-based 
benchmarks: one leveraging 1.5 million medication directions and the other using 
state-of-the-art LLMs. On 1,200 expert-reviewed prescriptions, the two 
benchmarks respectively recorded 1.51 (confidence interval (CI) 1.03, 2.31) and 
4.38 (CI 3.13, 6.64) times more near-miss events-errors caught and corrected 
before reaching the patient-than MEDIC. Additionally, we tested MEDIC by 
deploying within the production system of an online pharmacy, and during this 
experimental period, it reduced near-miss events by 33% (CI 26%, 40%). This 
study shows that LLMs, with domain expertise and safeguards, improve the 
accuracy and efficiency of pharmacy operations.

© 2024. The Author(s).",yes,"This study evaluates the use of large language models (LLMs) in a clinical context—specifically, preventing medication direction errors in online pharmacies. The LLM is fine-tuned and tested for its ability to interpret and generate accurate medication instructions, directly impacting patient safety and pharmacy operations. The evaluation includes real-world deployment and comparison to other LLM-based benchmarks, making it a clear clinical application.",10.1038/s41591-024-02933-8,I,Robert,,,
"Benchmarking the performance of large language models in uveitis: a comparative analysis of ChatGPT-3.5, ChatGPT-4.0, Google Gemini, and Anthropic Claude3","Background/Objective: This study aimed to evaluate the accuracy, comprehensiveness, and readability of responses generated by various Large Language Models (LLMs) (ChatGPT-3.5, Gemini, Claude 3, and GPT-4.0) in the clinical context of uveitis, utilizing a meticulous grading methodology. Methods: Twenty-seven clinical uveitis questions were presented individually to four Large Language Models (LLMs): ChatGPT (versions GPT-3.5 and GPT-4.0), Google Gemini, and Claude. Three experienced uveitis specialists independently assessed the responses for accuracy using a three-point scale across three rounds with a 48-hour wash-out interval. The final accuracy rating for each LLM response (‘Excellent’, ‘Marginal’, or ‘Deficient’) was determined through a majority consensus approach. Comprehensiveness was evaluated using a three-point scale for responses rated ‘Excellent’ in the final accuracy assessment. Readability was determined using the Flesch-Kincaid Grade Level formula. Statistical analyses were conducted to discern significant differences among LLMs, employing a significance threshold of p < 0.05. Results: Claude 3 and ChatGPT 4 demonstrated significantly higher accuracy compared to Gemini (p < 0.001). Claude 3 also showed the highest proportion of ‘Excellent’ ratings (96.3%), followed by ChatGPT 4 (88.9%). ChatGPT 3.5, Claude 3, and ChatGPT 4 had no responses rated as ‘Deficient’, unlike Gemini (14.8%) (p = 0.014). ChatGPT 4 exhibited greater comprehensiveness compared to Gemini (p = 0.008), and Claude 3 showed higher comprehensiveness compared to Gemini (p = 0.042). Gemini showed significantly better readability compared to ChatGPT 3.5, Claude 3, and ChatGPT 4 (p < 0.001). Gemini also had fewer words, letter characters, and sentences compared to ChatGPT 3.5 and Claude 3. Conclusions: Our study highlights the outstanding performance of Claude 3 and ChatGPT 4 in providing precise and thorough information regarding uveitis, surpassing Gemini. ChatGPT 4 and Claude 3 emerge as pivotal tools in improving patient understanding and involvement in their uveitis healthcare journey. © The Author(s), under exclusive licence to The Royal College of Ophthalmologists 2024.",yes,"This study directly evaluates multiple frontier LLMs (ChatGPT-3.5, ChatGPT-4, Gemini, Claude 3) in answering clinical questions about uveitis, with assessment by clinical experts for accuracy, comprehensiveness, and readability. The focus is on clinical information provision relevant to patient care, meeting inclusion criteria.",10.1038/s41433-024-03545-9,III,Robert,,,
Which current chatbot is more competent in urological theoretical knowledge? A comparative analysis by the European board of urology in-service assessment,"Introduction: The European Board of Urology (EBU) In-Service Assessment (ISA) test evaluates urologists’ knowledge and interpretation. Artificial Intelligence (AI) chatbots are being used widely by physicians for theoretical information. This research compares five existing chatbots’ test performances and questions’ knowledge and interpretation. Materials and methods: GPT-4o, Copilot Pro, Gemini Advanced, Claude 3.5, and Sonar Huge chatbots solved 596 questions in 6 exams between 2017 and 2022. The questions were divided into two categories: questions that measure knowledge and require data interpretation. The chatbots’ exam performances were compared. Results: Overall, all chatbots except Claude 3.5 passed the examinations with a percentage of 60% overall score. Copilot Pro scored best, and Claude 3.5’s score difference was significant (71.6% vs. 56.2%, p = 0.001). When a total of 444 knowledge and 152 analysis questions were compared, Copilot Pro offered the greatest percentage of information, whereas Claude 3.5 provided the least (72.1% vs. 57.4%, p = 0.001). This was also true for analytical skills (70.4% vs. 52.6%, p = 0.019). Conclusions: Four out of five chatbots passed the exams, achieving scores exceeding 60%, while only one did not pass the EBU examination. Copilot Pro performed best in EBU ISA examinations, whereas Claude 3.5 performed worst. Chatbots scored worse on analysis than knowledge questions. Thus, although existing chatbots are successful in terms of theoretical knowledge, their competence in analyzing the questions is questionable. © The Author(s) 2025.",yes,"This study evaluates the performance of multiple generative LLM chatbots (GPT-4o, Copilot Pro, Gemini Advanced, Claude 3.5, Sonar Huge) on a clinically relevant board exam (European Board of Urology In-Service Assessment). It directly assesses their clinical/theoretical knowledge and interpretive skills, which fits the inclusion criteria.",10.1007/s00345-025-05499-3,III,Robert,,,
Ability of ChatGPT to Replace Doctors in Patient Education: Cross-Sectional Comparative Analysis of Inflammatory Bowel Disease,"Background: Although large language models (LLMs) such as ChatGPT show promise for providing specialized information, their quality requires further evaluation. This is especially true considering that these models are trained on internet text and the quality of health-related information available online varies widely. Objective: The aim of this study was to evaluate the performance of ChatGPT in the context of patient education for individuals with chronic diseases, comparing it with that of industry experts to elucidate its strengths and limitations. Methods: This evaluation was conducted in September 2023 by analyzing the responses of ChatGPT and specialist doctors to questions posed by patients with inflammatory bowel disease (IBD). We compared their performance in terms of subjective accuracy, empathy, completeness, and overall quality, as well as readability to support objective analysis. Results: In a series of 1578 binary choice assessments, ChatGPT was preferred in 48.4% (95% CI 45.9%-50.9%) of instances. There were 12 instances where ChatGPT's responses were unanimously preferred by all evaluators, compared with 17 instances for specialist doctors. In terms of overall quality, there was no significant difference between the responses of ChatGPT (3.98, 95% CI 3.93-4.02) and those of specialist doctors (3.95, 95% CI 3.90-4.00; t524=0.95, P=.34), both being considered ""good.""Although differences in accuracy (t521=0.48, P=.63) and empathy (t511=2.19, P=.03) lacked statistical significance, the completeness of textual output (t509=9.27, P<.001) was a distinct advantage of the LLM (ChatGPT). In the sections of the questionnaire where patients and doctors responded together (Q223-Q242), ChatGPT demonstrated inferior performance (t36=2.91, P=.006). Regarding readability, no statistical difference was found between the responses of specialist doctors (median: 7th grade; Q1: 4th grade; Q3: 8th grade) and those of ChatGPT (median: 7th grade; Q1: 7th grade; Q3: 8th grade) according to the Mann-Whitney U test (P=.09). The overall quality of ChatGPT's output exhibited strong correlations with other subdimensions (with empathy: r=0.842; with accuracy: r=0.839; with completeness: r=0.795), and there was also a high correlation between the subdimensions of accuracy and completeness (r=0.762). Conclusions: ChatGPT demonstrated more stable performance across various dimensions. Its output of health information content is more structurally sound, addressing the issue of variability in the information from individual specialist doctors. ChatGPT's performance highlights its potential as an auxiliary tool for health information, despite limitations such as artificial intelligence hallucinations. It is recommended that patients be involved in the creation and evaluation of health information to enhance the quality and relevance of the information. © Zelin Yan, Jingwen Liu, Yihong Fan, Shiyuan Lu, Dingting Xu, Yun Yang, Honggang Wang, Jie Mao, Hou-Chiang Tseng, Tao-Hsing Chang, Yan Chen.",yes,"This study directly evaluates ChatGPT's performance in providing patient education for individuals with inflammatory bowel disease, comparing its responses to those of specialist doctors. The evaluation includes clinical aspects such as accuracy, empathy, completeness, and overall quality in a healthcare context. This fits the inclusion criteria as it assesses a generative LLM (ChatGPT) in a clinical application relevant to patient care.",10.2196/62857,I,Robert,,,
The Accuracy of Artificial Intelligence ChatGPT in Oncology Examination Questions,"The aim of this study is to assess the accuracy of Chat Generative Pretrained Transformer (ChatGPT) in response to oncology examination questions in the setting of one-shot learning. Consecutive national radiation oncology in-service multiple-choice examinations were collected and inputted into ChatGPT 4o and ChatGPT 3.5 to determine ChatGPT's answers. ChatGPT's answers were then compared with the answer keys to determine whether ChatGPT correctly or incorrectly answered each question and to determine if improvements in responses were seen with the newer ChatGPT version. A total of 600 consecutive questions were inputted into ChatGPT. ChatGPT 4o answered 72.2% questions correctly, whereas 3.5 answered 53.8% questions correctly. There was a significant difference in performance by question category (P < .01). ChatGPT performed poorer with respect to knowledge of landmark studies and treatment recommendations and planning. ChatGPT is a promising technology, with the latest version showing marked improvement. Although it still has limitations, with further evolution, it may be considered a reliable resource for medical training and decision making in the oncology space. © 2024 American College of Radiology",yes,"This study evaluates the performance of ChatGPT (GPT-3.5 and GPT-4o) on national radiation oncology in-service examination questions, which are clinically relevant. The focus is on the LLM's ability to answer medical exam questions, a direct clinical education application.",10.1016/j.jacr.2024.07.011,III,Robert,,,
GPT-4/4V's performance on the Japanese National Medical Licensing Examination,"BACKGROUND: Recent advances in Artificial Intelligence (AI) are changing the medical world, and AI will likely replace many of the actions performed by medical professionals. The overall clinical ability of the AI has been evaluated by its ability to answer a text-based national medical examination. This study uniquely assesses the performance of Open AI's ChatGPT against all Japanese National Medical Licensing Examination (NMLE), including images, illustrations, and pictures. METHODS: We obtained the questions of the past six years of the NMLE (112th to 117th) from the Japanese Ministry of Health, Labour and Welfare website. We converted them to JavaScript Object Notation (JSON) format. We created an application programming interface (API) to output correct answers using GPT-4 for questions without images and GPT4-V(ision) or GPT4 console for questions with images. RESULTS: The percentage of image questions was 723/2400 (30.1%) over the past six years. In all years, GPT-4/4V exceeded the minimum score the examinee should score. In total, over the six years, the percentage of correct answers for basic medical knowledge questions was 665/905 (73.5%); for clinical knowledge questions, 1143/1531 (74.7%); and for image questions 497/723 (68.7%), respectively. CONCLUSIONS: Regarding medical knowledge, GPT-4/4V met the minimum criteria regardless of whether the questions included images, illustrations, and pictures. Our study sheds light on the potential utility of AI in medical education.",yes,"This study evaluates the performance of GPT-4 and GPT-4V on the Japanese National Medical Licensing Examination, including both text and image-based questions. It directly assesses the clinical knowledge and reasoning abilities of LLMs in a medical context, which aligns with the inclusion criteria.",10.1080/0142159X.2024.2342545,III,Robert,,,
Assessing the possibility of using large language models in ocular surface diseases,"AIM: To assess the possibility of using different large language models (LLMs) in ocular surface diseases by selecting five different LLMS to test their accuracy in answering specialized questions related to ocular surface diseases: ChatGPT-4, ChatGPT-3.5, Claude 2, PaLM2, and SenseNova. METHODS: A group of experienced ophthalmology professors were asked to develop a 100-question single-choice question on ocular surface diseases designed to assess the performance of LLMs and human participants in answering ophthalmology specialty exam questions. The exam includes questions on the following topics: keratitis disease (20 questions), keratoconus, keratomalaciac, corneal dystrophy, corneal degeneration, erosive corneal ulcers, and corneal lesions associated with systemic diseases (20 questions), conjunctivitis disease (20 questions), trachoma, pterygoid and conjunctival tumor diseases (20 questions), and dry eye disease (20 questions). Then the total score of each LLMs and compared their mean score, mean correlation, variance, and confidence were calculated. RESULTS: GPT-4 exhibited the highest performance in terms of LLMs. Comparing the average scores of the LLMs group with the four human groups, chief physician, attending physician, regular trainee, and graduate student, it was found that except for ChatGPT-4, the total score of the rest of the LLMs is lower than that of the graduate student group, which had the lowest score in the human group. Both ChatGPT-4 and PaLM2 were more likely to give exact and correct answers, giving very little chance of an incorrect answer. ChatGPT-4 showed higher credibility when answering questions, with a success rate of 59%, but gave the wrong answer to the question 28% of the time. CONCLUSION: GPT-4 model exhibits excellent performance in both answer relevance and confidence. PaLM2 shows a positive correlation (up to 0.8) in terms of answer accuracy during the exam. In terms of answer confidence, PaLM2 is second only to GPT4 and surpasses Claude 2, SenseNova, and GPT-3.5. Despite the fact that ocular surface disease is a highly specialized discipline, GPT-4 still exhibits superior performance, suggesting that its potential and ability to be applied in this field is enormous, perhaps with the potential to be a valuable resource for medical students and clinicians in the future. © 2025 International Journal of Ophthalmology (c/o Editorial Office). All rights reserved.",yes,"This study evaluates multiple LLMs (ChatGPT-4, ChatGPT-3.5, Claude 2, PaLM2, SenseNova) on their ability to answer specialized clinical questions about ocular surface diseases, comparing their performance to human clinicians and trainees. The focus is on clinical knowledge and decision-making relevant to medicine, not on non-clinical tasks or technical aspects of the models.",10.18240/ijo.2025.01.01,III,Robert,,,
Optimizing ChatGPT’s Interpretation and Reporting of Delirium Assessment Outcomes: Exploratory Study,"Background: Generative artificial intelligence (AI) and large language models, such as OpenAI’s ChatGPT, have shown promising potential in supporting medical education and clinical decision-making, given their vast knowledge base and natural language processing capabilities. As a general purpose AI system, ChatGPT can complete a wide range of tasks, including differential diagnosis without additional training. However, the specific application of ChatGPT in learning and applying a series of specialized, context-specific tasks mimicking the workflow of a human assessor, such as administering a standardized assessment questionnaire, followed by inputting assessment results in a standardized form, and interpretating assessment results strictly following credible, published scoring criteria, have not been thoroughly studied. Objective: This exploratory study aims to evaluate and optimize ChatGPT’s capabilities in administering and interpreting the Sour Seven Questionnaire, an informant-based delirium assessment tool. Specifically, the objectives were to train ChatGPT-3.5 and ChatGPT-4 to understand and correctly apply the Sour Seven Questionnaire to clinical vignettes using prompt engineering, assess the performance of these AI models in identifying and scoring delirium symptoms against scores from human experts, and refine and enhance the models’ interpretation and reporting accuracy through iterative prompt optimization. Methods: We used prompt engineering to train ChatGPT-3.5 and ChatGPT-4 models on the Sour Seven Questionnaire, a tool for assessing delirium through caregiver input. Prompt engineering is a methodology used to enhance the AI’s processing of inputs by meticulously structuring the prompts to improve accuracy and consistency in outputs. In this study, prompt engineering involved creating specific, structured commands that guided the AI models in understanding and applying the assessment tool’s criteria accurately to clinical vignettes. This approach also included designing prompts to explicitly instruct the AI on how to format its responses, ensuring they were consistent with clinical documentation standards. Results: Both ChatGPT models demonstrated promising proficiency in applying the Sour Seven Questionnaire to the vignettes, despite initial inconsistencies and errors. Performance notably improved through iterative prompt engineering, enhancing the models’ capacity to detect delirium symptoms and assign scores. Prompt optimizations included adjusting the scoring methodology to accept only definitive “Yes” or “No” responses, revising the evaluation prompt to mandate responses in a tabular format, and guiding the models to adhere to the 2 recommended actions specified in the Sour Seven Questionnaire. Conclusions: Our findings provide preliminary evidence supporting the potential utility of AI models such as ChatGPT in administering standardized clinical assessment tools. The results highlight the significance of context-specific training and prompt engineering in harnessing the full potential of these AI models for health care applications. Despite the encouraging results, broader generalizability and further validation in real-world settings warrant additional research. ©Yong K Choi, Shih-Yin Lin, Donna Marie Fick, Richard W Shulman, Sangil Lee, Priyanka Shrestha, Kate Santoso.",yes,"This study evaluates ChatGPT-3.5 and ChatGPT-4 in the clinical context of administering and interpreting a standardized delirium assessment tool (Sour Seven Questionnaire) using clinical vignettes. The focus is on the LLM's ability to perform a clinical task relevant to patient care, not just data processing or abstract writing. The study is not a review or opinion piece, and it directly assesses the clinical application of LLMs.",10.2196/51383,II,Vivian,"fake data, real task, not a clincial study",,
Toward expert-level medical question answering with large language models,"8.

Toward expert-level medical question answering with large language models.

Singhal K(#)(1), Tu T(#)(1), Gottweis J(#)(1), Sayres R(#)(1), Wulczyn E(1), 
Amin M(1), Hou L(1), Clark K(2), Pfohl SR(1), Cole-Lewis H(1), Neal D(1), Rashid 
QM(1), Schaekermann M(1), Wang A(1), Dash D(3), Chen JH(4)(5)(6), Shah NH(7)(8), 
Lachgar S(1), Mansfield PA(1), Prakash S(1), Green B(1), Dominowska E(2), Agüera 
Y Arcas B(1), Tomašev N(2), Liu Y(1), Wong R(1), Semturs C(1), Mahdavi SS(2), 
Barral JK(2), Webster DR(1), Corrado GS(1), Matias Y(1), Azizi S(9), 
Karthikesalingam A(10), Natarajan V(11).

Author information:
(1)Google Research, Mountain View, CA, USA.
(2)Google DeepMind, Mountain View, CA, USA.
(3)Department of Emergency Medicine, Stanford University School of Medicine, 
Stanford, CA, USA.
(4)Stanford Center for Biomedical Informatics Research, Stanford University, 
Stanford, CA, USA.
(5)Division of Hospital Medicine, Stanford University, Stanford, CA, USA.
(6)Clinical Excellence Research Center, Stanford University, Stanford, CA, USA.
(7)Department of Medicine, Stanford University School of Medicine, Stanford, CA, 
USA.
(8)Technology and Digital Solutions, Stanford Healthcare, Palo Alto, CA, USA.
(9)Google DeepMind, Mountain View, CA, USA. shekazizi@google.com.
(10)Google Research, Mountain View, CA, USA. alankarthi@google.com.
(11)Google Research, Mountain View, CA, USA. natviv@google.com.
(#)Contributed equally

Large language models (LLMs) have shown promise in medical question answering, 
with Med-PaLM being the first to exceed a 'passing' score in United States 
Medical Licensing Examination style questions. However, challenges remain in 
long-form medical question answering and handling real-world workflows. Here, we 
present Med-PaLM 2, which bridges these gaps with a combination of base LLM 
improvements, medical domain fine-tuning and new strategies for improving 
reasoning and grounding through ensemble refinement and chain of retrieval. 
Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by 
over 19%, and demonstrates dramatic performance increases across MedMCQA, 
PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations 
framework shows that physicians prefer Med-PaLM 2 answers to those from other 
physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates 
significant improvements over its predecessor across all evaluation metrics, 
particularly on new adversarial datasets designed to probe LLM limitations 
(P < 0.001). In a pilot study using real-world medical questions, specialists 
preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. 
While specialist answers were still preferred overall, both specialists and 
generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating 
its growing potential in real-world medical applications.

© 2025. The Author(s).",yes,"This study evaluates the performance of a large language model (Med-PaLM 2) on medical question answering, including USMLE-style questions and real-world clinical questions, with human (physician and specialist) evaluation of its answers. The focus is on clinical application and performance, meeting inclusion criteria.",10.1038/s41591-024-03423-7,III,Vivian,"fake data, fake task, not a clinical study",,
Performance of an Artificial Intelligence Chatbot in Ophthalmic Knowledge Assessment,"Importance: ChatGPT is an artificial intelligence (AI) chatbot that has significant societal implications. Training curricula using AI are being developed in medicine, and the performance of chatbots in ophthalmology has not been characterized. Objective: To assess the performance of ChatGPT in answering practice questions for board certification in ophthalmology. Design, Setting, and Participants: This cross-sectional study used a consecutive sample of text-based multiple-choice questions provided by the OphthoQuestions practice question bank for board certification examination preparation. Of 166 available multiple-choice questions, 125 (75%) were text-based. Exposures: ChatGPT answered questions from January 9 to 16, 2023, and on February 17, 2023. Main Outcomes and Measures: Our primary outcome was the number of board certification examination practice questions that ChatGPT answered correctly. Our secondary outcomes were the proportion of questions for which ChatGPT provided additional explanations, the mean length of questions and responses provided by ChatGPT, the performance of ChatGPT in answering questions without multiple-choice options, and changes in performance over time. Results: In January 2023, ChatGPT correctly answered 58 of 125 questions (46%). ChatGPT's performance was the best in the category general medicine (11/14; 79%) and poorest in retina and vitreous (0%). The proportion of questions for which ChatGPT provided additional explanations was similar between questions answered correctly and incorrectly (difference, 5.82%; 95% CI, -11.0% to 22.0%; χ21= 0.45; P =.51). The mean length of questions was similar between questions answered correctly and incorrectly (difference, 21.4 characters; SE, 36.8; 95% CI, -51.4 to 94.3; t = 0.58; df = 123; P =.22). The mean length of responses was similar between questions answered correctly and incorrectly (difference, -80.0 characters; SE, 65.4; 95% CI, -209.5 to 49.5; t = -1.22; df = 123; P =.22). ChatGPT selected the same multiple-choice response as the most common answer provided by ophthalmology trainees on OphthoQuestions 44% of the time. In February 2023, ChatGPT provided a correct response to 73 of 125 multiple-choice questions (58%) and 42 of 78 stand-alone questions (54%) without multiple-choice options. Conclusions and Relevance: ChatGPT answered approximately half of questions correctly in the OphthoQuestions free trial for ophthalmic board certification preparation. Medical professionals and trainees should appreciate the advances of AI in medicine while acknowledging that ChatGPT as used in this investigation did not answer sufficient multiple-choice questions correctly for it to provide substantial assistance in preparing for board certification at this time.. © 2023 American Medical Association. All rights reserved.",yes,"This study evaluates ChatGPT's performance on ophthalmology board certification practice questions, which is a clinical knowledge assessment relevant to medical education. It directly tests an LLM (ChatGPT) in a clinical context (board exam preparation), fitting the inclusion criteria.",10.1001/jamaophthalmol.2023.1144,III,Vivian,"fake data, fake task, not a clinical study",,
Development of dialogue and voice-call robot system for personalized dementia prevention and care,"Purpose The looming challenge of dementia, exacerbated by demographic shifts and an aging global population, places immense strain on society and our healthcare infrastructure. It is desirable for societies around the world to conquer this problem by developing effective care technologies for dementia patients and caregivers, as well as providing strategies for dementia prevention. In this project, we propose a robot system that is the basis for generating a method for effective communication with dementia patients and caregivers regarding dementia based on the aggregated diagnostic and living data of dementia patients. Method The system aims to integrate diagnostic and daily living data and the use of a robot. The output is a dialogue that is effective for helping the user to care about their health status. The data analysis/processing methods are constructed in two steps. In the first step, a table of effective robot dialogues for each user's diagnostic and living data is created in advance. Using the METIS Platform (Stock, 2022), we are able to integrate biometric data from wearable devices within the Internet of Medical Things (IoMT) along with supplementary patient-reported information, data-driven capabilities. In this study, as a method for establishing the needed dialogue capabilities, a scenario-based dialogue (Kumagai et al., 2022) has been adapted. In a scenario-based dialogue, the robot's utterances are designed so that a conversation can be established, regardless of the content of the user's responses. To create the lookup table, an appropriate dialogue theme depending on the user's state is determined in advance. This table allows the robot to generate simple utterances that are tailored to the user's status. In the second step, the robot conversation content is generated based on the user's data (diagnosis data and biometric data) by Chat GPT. Even if two users’ states are the same, from the perspective of getting the user to accept the robot's voice, the appropriate contents of the robot's utterances are assumed to differ depending on the individual user. For example, it may be necessary to consider how to directly point out the user's status, or whether to indirectly recommend appropriate actions. In our previous research, we used neural networks and reinforcement learning techniques to develop systems that individualize the parameterized robot's behavior by adjusting the parameters to individual users (Kumagai et al., 2018). In this project, we will use a similar approach to generate personalized robot dialogues for each user, based on both generalized knowledge and individual data. Feedback on the user's affective reactions to the robot's speech is thought to help personalize the way of the robot's speech. Results and Discussion This presentation will show the results of a simple survey to examine the robot's speech generated by the system we have built and the impressions of users’ dialogues with the robot. We also discuss challenges in connecting to robots. © (2024), (International Society for Gerontechnology). All rights reserved.",yes,"The study describes the development and evaluation of a robot system for dementia care that generates personalized dialogue using ChatGPT based on patient data. The abstract specifies that ChatGPT is used to generate conversation content tailored to users' diagnostic and biometric data, and that the system's output is evaluated via user impressions of the robot's speech. This constitutes a clinical application of a generative LLM (ChatGPT) in healthcare, specifically in dementia prevention and care.",10.4017/GT.2024.23.S.862.5.SP,II,Vivian,"real data, real task, maybe prospective (honestly can't tell if it's even a study cus no results provided in abstract). i can't access the full article",,
Efficacy and safety of artificial intelligence-based large language models for decision making support in herniology: evaluation by experts and general surgeons,"Objective. To evaluate the quality of recommendations provided by ChatGPT regarding inguinal hernia repair. Material and methods. ChatGPT was asked 5 questions about surgical management of inguinal hernias. The chat-bot was assigned the role of expert in herniology and requested to search only specialized medical databases and provide information about references and evidence. Herniology experts and surgeons (non-experts) rated the quality of recommendations generated by ChatGPT using 4-point scale (from 0 to 3 points). Statistical correlations were explored between participants' ratings and their stance regarding artificial intelligence. Results. Experts scored the quality of ChatGPT responses lower than non-experts (2 (1-2) vs. 2 (2-3), p<0.001). The chat-bot failed to provide valid references and actual evidence, as well as falsified half of references. Respondents were optimistic about the future of neural networks for clinical decision-making support. Most of them were against restricting their use in healthcare. Conclusion. We would not recommend non-specialized large language models as a single or primary source of information for clinical decision making or virtual searching assistant. © 2024 Media Sphera Publishing Group. All rights reserved.",yes,"This study evaluates ChatGPT, a large language model, in the context of providing clinical recommendations for inguinal hernia repair. The quality of its recommendations is assessed by clinical experts and general surgeons, focusing on its utility for clinical decision support. This fits the inclusion criteria as it is a clinical evaluation of an LLM in a healthcare context.",10.17116/hirurgia20240816,II,Vivian,"fake data (technically, no data - LLMs just prompted to answer questions), real task, not a clinical study",,
Evaluating the Effectiveness of advanced large language models in medical Knowledge: A Comparative study using Japanese national medical examination,"Study aims and objectives. This study aims to evaluate the accuracy of medical knowledge in the most advanced LLMs (GPT-4o, GPT-4, Gemini 1.5 Pro, and Claude 3 Opus) as of 2024. It is the first to evaluate these LLMs using a non-English medical licensing exam. The insights from this study will guide educators, policymakers, and technical experts in the effective use of AI in medical education and clinical diagnosis. Method: Authors inputted 790 questions from Japanese National Medical Examination into the chat windows of the LLMs to obtain responses. Two authors independently assessed the correctness. Authors analyzed the overall accuracy rates of the LLMs and compared their performance on image and non-image questions, questions of varying difficulty levels, general and clinical questions, and questions from different medical specialties. Additionally, authors examined the correlation between the number of publications and LLMs’ performance in different medical specialties. Results: GPT-4o achieved highest accuracy rate of 89.2% and outperformed the other LLMs in overall performance and each specific category. All four LLMs performed better on non-image questions than image questions, with a 10% accuracy gap. They also performed better on easy questions compared to normal and difficult ones. GPT-4o achieved a 95.0% accuracy rate on easy questions, marking it as an effective knowledge source for medical education. Four LLMs performed worst on “Gastroenterology and Hepatology” specialty. There was a positive correlation between the number of publications and LLM performance in different specialties. Conclusions: GPT-4o achieved an overall accuracy rate close to 90%, with 95.0% on easy questions, significantly outperforming the other LLMs. This indicates GPT-4o's potential as a knowledge source for easy questions. Image-based questions and question difficulty significantly impact LLM accuracy. “Gastroenterology and Hepatology” is the specialty with the lowest performance. The LLMs’ performance across medical specialties correlates positively with the number of related publications. © 2024 The Author(s)",yes,"This study evaluates the clinical knowledge and performance of multiple advanced LLMs (GPT-4o, GPT-4, Gemini 1.5 Pro, Claude 3 Opus) on a national medical licensing exam, which is a direct clinical evaluation relevant to medicine. The focus is on the LLMs' ability to answer medical questions, including clinical and specialty-specific content, which fits the inclusion criteria.",10.1016/j.ijmedinf.2024.105673,III,Vivian,"fake data, fake task, not a clinical stduy",,
A chatbot based question and answer system for the auxiliary diagnosis of chronic diseases based on large language model,"In recent years, artificial intelligence has made remarkable strides, improving various aspects of our daily lives. One notable application is in intelligent chatbots that use deep learning models. These systems have shown tremendous promise in the medical sector, enhancing healthcare quality, treatment efficiency, and cost-effectiveness. However, their role in aiding disease diagnosis, particularly chronic conditions, remains underexplored. Addressing this issue, this study employs large language models from the GPT series, in conjunction with deep learning techniques, to design and develop a diagnostic system targeted at chronic diseases. Specifically, performed transfer learning and fine-tuning on the GPT-2 model, enabling it to assist in accurately diagnosing 24 common chronic diseases. To provide a user-friendly interface and seamless interactive experience, we further developed a dialog-based interface, naming it Chat Ella. This system can make precise predictions for chronic diseases based on the symptoms described by users. Experimental results indicate that our model achieved an accuracy rate of 97.50% on the validation set, and an area under the curve (AUC) value reaching 99.91%. Moreover, conducted user satisfaction tests, which revealed that 68.7% of participants approved of Chat Ella, while 45.3% of participants found the system made daily medical consultations more convenient. It can rapidly and accurately assess a patient's condition based on the symptoms described and provide timely feedback, making it of significant value in the design of medical auxiliary products for household use.",yes,"This study evaluates a chatbot system based on a large language model (GPT-2) for the auxiliary diagnosis of chronic diseases, including clinical performance metrics and user satisfaction. The LLM is used in a clinical context to assess patient symptoms and provide diagnostic feedback, which fits the inclusion criteria.",10.1038/s41598-024-67429-4,I,Vivian,"real data, real task, prospective",,
Use of ChatGPT to Generate Informed Consent for Surgery in Urogynecology,"Importance Use of the publicly available Large Language Model, Chat Generative Pre-trained Transformer (ChatGPT 3.5; OpenAI, 2022), is growing in health care despite varying accuracies. Objective The aim of this study was to assess the accuracy and readability of ChatGPT's responses to questions encompassing surgical informed consent in urogynecology. Study Design Five fellowship-trained urogynecology attending physicians and 1 reconstructive female urologist evaluated ChatGPT's responses to questions about 4 surgical procedures: (1) retropubic midurethral sling, (2) total vaginal hysterectomy, (3) uterosacral ligament suspension, and (4) sacrocolpopexy. Questions involved procedure descriptions, risks/benefits/alternatives, and additional resources. Responses were rated using the DISCERN tool, a 4-point accuracy scale, and the Flesch-Kinkaid Grade Level score. Results The median DISCERN tool overall rating was 3 (interquartile range [IQR], 3-4), indicating a moderate rating (""potentially important but not serious shortcomings""). Retropubic midurethral sling received the highest overall score (median, 4; IQR, 3-4), and uterosacral ligament suspension received the lowest (median, 3; IQR, 3-3). Using the 4-point accuracy scale, 44.0% of responses received a score of 4 (""correct and adequate""), 22.6% received a score of 3 (""correct but insufficient""), 29.8% received a score of 2 (""accurate and misleading information together""), and 3.6% received a score of 1 (""wrong or irrelevant answer""). ChatGPT performance was poor for discussion of benefits and alternatives for all surgical procedures, with some responses being inaccurate. The mean Flesch-Kinkaid Grade Level score for all responses was 17.5 (SD, 2.1), corresponding to a postgraduate reading level. Conclusions Overall, ChatGPT generated accurate responses to questions about surgical informed consent. However, it produced clearly false portions of responses, highlighting the need for a careful review of responses by qualified health care professionals.  © American Urogynecologic Society. All rights reserved.",yes,"This study evaluates ChatGPT's ability to generate informed consent information for surgical procedures in urogynecology, assessing the accuracy and readability of its responses to clinically relevant questions. The evaluation is performed by clinicians and uses established rating tools. This is a direct clinical application of an LLM in a healthcare context.",10.1097/SPV.0000000000001638,II,Vivian,"fake data (technically, no data - LLMs just prompted to answer questions), real task, not a clinical study",,
Performance Assessment of Large Language Models in Medical Consultation: Comparative Study,"Background: The recent introduction of generative artificial intelligence (AI) as an interactive consultant has sparked interest in evaluating its applicability in medical discussions and consultations, particularly within the domain of depression. Objective: This study evaluates the capability of large language models (LLMs) in AI to generate responses to depression-related queries. Methods: Using the PubMedQA and QuoraQA data sets, we compared various LLMs, including BioGPT, PMC-LLaMA, GPT-3.5, and Llama2, and measured the similarity between the generated and original answers. Results: The latest general LLMs, GPT-3.5 and Llama2, exhibited superior performance, particularly in generating responses to medical inquiries from the PubMedQA data set. Conclusions: Considering the rapid advancements in LLM development in recent years, it is hypothesized that version upgrades of general LLMs offer greater potential for enhancing their ability to generate “knowledge text” in the biomedical domain compared with fine-tuning for the biomedical field. These findings are expected to contribute significantly to the evolution of AI-based medical counseling systems. © 2025 JMIR Publications Inc.. All rights reserved.",yes,"This study evaluates the performance of several LLMs (including GPT-3.5, Llama2, BioGPT, and PMC-LLaMA) in generating responses to medical consultation queries, specifically related to depression. The focus is on the ability of LLMs to answer clinical questions, which is a direct clinical application relevant to healthcare.",10.2196/64318,II,Vivian,"fake data, real task, not a clinical study",,
A Quantitative Assessment of ChatGPT as a Neurosurgical Triaging Tool,"BACKGROUND AND OBJECTIVES: ChatGPT is a natural language processing chatbot with increasing applicability to the medical workflow. Although ChatGPT has been shown to be capable of passing the American Board of Neurological Surgery board examination, there has never been an evaluation of the chatbot in triaging and diagnosing novel neurosurgical scenarios without defined answer choices. In this study, we assess ChatGPT’s capability to determine the emergent nature of neurosurgical scenarios and make diagnoses based on information one would find in a neurosurgical consult. METHODS: Thirty clinical scenarios were given to 3 attendings, 4 residents, 2 physician assistants, and 2 subinterns. Participants were asked to determine if the scenario constituted an urgent neurosurgical consultation and what the most likely diagnosis was. Attending responses provided a consensus to use as the answer key. Generative pretraining transformer (GPT) 3.5 and GPT 4 were given the same questions, and their responses were compared with the other participants. RESULTS: GPT 4 was 100% accurate in both diagnosis and triage of the scenarios. GPT 3.5 had an accuracy of 92.59%, slightly below that of a PGY1 (96.3%), an 88.24% sensitivity, 100% specificity, 100% positive predictive value, and 83.3% negative predicative value in triaging each situation. When making a diagnosis, GPT 3.5 had an accuracy of 92.59%, which was higher than the subinterns and similar to resident responders. CONCLUSION: GPT 4 is able to diagnose and triage neurosurgical scenarios at the level of a senior neurosurgical resident. There has been a clear improvement between GPT 3.5 and 4. It is likely that the recent updates in internet access and directing the functionality of ChatGPT will further improve its utility in neurosurgical triage. © Congress of Neurological Surgeons 2024. All rights reserved.",yes,"This study directly evaluates the clinical performance of ChatGPT (GPT-3.5 and GPT-4) in triaging and diagnosing neurosurgical scenarios, comparing its accuracy to that of human clinicians. The focus is on clinical decision-making in a healthcare context, which fits the inclusion criteria.",10.1227/neu.0000000000002867,II,Vivian,"can't tell if this is real data or fake data, real task, not a clinical study",,
GPT-4: a new era of artificial intelligence in medicine,"GPT-4 is the latest version of ChatGPT which is reported by OpenAI to have greater problem-solving abilities and an even broader knowledge base. We examined GPT-4’s ability to inform us about the latest literature in a given area, and to write a discharge summary for a patient following an uncomplicated surgery and its latest image analysis feature which was reported to be able to identify objects in photos. All things considered, GPT-4 has the potential to help drive medical innovation, from aiding with patient discharge notes, summarizing recent clinical trials, providing information on ethical guidelines, and much more. © 2023, The Author(s), under exclusive licence to Royal Academy of Medicine in Ireland.",yes,"The study appears to evaluate GPT-4's ability to write discharge summaries (a clinical task) and possibly to provide information about recent literature and ethical guidelines. While summarizing literature and guidelines is not a clinical application, writing discharge summaries is a clinical documentation task relevant to patient care. The mention of image analysis is less clear, but the discharge summary task justifies inclusion.",10.1007/s11845-023-03377-8,I/II,Vivian,"i don't have access to the full article. can't tell if the ""patient"" they are prompting about is real data ",,
ChatGPT efficacy for answering musculoskeletal anatomy questions: a study evaluating quality and consistency between raters and timepoints,"Purpose: There is increasing interest in the use of digital platforms such as ChatGPT for anatomy education. This study aims to evaluate the efficacy of ChatGPT in providing accurate and consistent responses to questions focusing on musculoskeletal anatomy across various time points (hours and days). Methods: A selection of 6 Anatomy-related questions were asked to ChatGPT 3.5 in 4 different timepoints. All answers were rated blindly by 3 expert raters for quality according to a 5 -point Likert Scale. Difference of 0 or 1 points in Likert scale scores between raters was considered as agreement and between different timepoints was considered as consistent indicating good reproducibility. Results: There was significant variation in the quality of the answers ranging from extremely good to very poor quality. There was also variation of consistency levels between different timepoints. Answers were rated as good quality (≥ 3 in Likert scale) in 50% of cases (3/6) and as consistent in 66.6% (4/6) of cases. In the low-quality answers, significant mistakes, conflicting data or lack of information were encountered. Conclusion: As of the time of this article, the quality and consistency of the ChatGPT v3.5 answers is variable, thus limiting its utility as independent and reliable resource of learning musculoskeletal anatomy. Validating information by reviewing the anatomical literature is highly recommended. © The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature 2024.",yes,"This study evaluates ChatGPT-3.5's ability to answer musculoskeletal anatomy questions, focusing on the quality and consistency of its responses. The context is anatomy education, which is a clinical/medical education application. The study involves expert raters assessing the clinical/educational accuracy of LLM-generated answers, which fits the inclusion criteria for evaluating LLMs in a medical context.",10.1007/s00276-024-03477-9,III,Vivian,"fake data, fake task, not a clinical study",,
Comparison of ChatGPT knowledge against 2020 consensus statement on ankyloglossia in children,"Objective: This paper evaluates ChatGPT's accuracy and consistency in providing information on ankyloglossia, a congenital oral condition. Assessing alignment with expert consensus, the study explores potential implications for patients relying on AI for medical information. Methods: Statements from the 2020 clinical consensus statement on ankyloglossia were presented to ChatGPT, and its responses were scored using a 9-point Likert scale. The study analyzed the mean and standard deviation of ChatGPT scores for each statement. Statistical analysis was conducted using Excel. Results: Among the 63 statements assessed, 67 % of ChatGPT responses closely aligned with expert consensus mean scores. However, 17 % (11/63) were statements in which the ChatGPT mean response was different from the CCS mean by 2.0 or greater, raising concerns about ChatGPT's potential influence in disseminating uncertain or debated medical information. Variations in mean scores highlighted discrepancies, with some statements showing significant deviations from expert opinions. Conclusion: While ChatGPT mirrored medical viewpoints on ankyloglossia, alignment with non-consensus statements raises caution in relying on it for medical advice. Future research should refine AI models, address inaccuracies, and explore diverse user queries for safe integration into medical decision-making. Despite potential benefits, ongoing examination of ChatGPT's power and limitations is crucial, considering its impact on health equity and information access. © 2024 Elsevier B.V.",yes,"This study evaluates ChatGPT's accuracy and consistency in providing medical information about ankyloglossia by comparing its responses to a clinical consensus statement. The focus is on ChatGPT's ability to provide clinically relevant information, which aligns with the inclusion criteria of evaluating an LLM in a healthcare context.",10.1016/j.ijporl.2024.111957,X,Vivian,"data=statements from the 2020 clinical consensus statement on anklyoglossia (technically it is ""real"" data but just a list of expert opinion about the disease), not-very-helpful task, not a clinical study",,
"Integrating Previous Suicide Attempts, Gender, and Age Into Suicide Risk Assessment Using Advanced Artificial Intelligence Models","Objective: Suicide is a critical global health concern. Research indicates that generative artificial intelligence (GenAI) and large language models, such as generative pretrained transformer-3 (GPT-3) and GPT-4, can evaluate suicide risk comparably to experts, yet the criteria these models use are unclear. This study explores how variations in prompts, specifically regarding past suicide attempts, gender, and age, influence the risk assessments provided by ChatGPT-3 and ChatGPT-4. Methods: Using a controlled scenariobased approach, 8 vignettes were created. Both ChatGPT-3.5 and ChatGPT- 4 were used to predict the likelihood of serious suicide attempts, suicide attempts, and suicidal thoughts. A univariate 3-way analysis of variance was conducted to analyze the effects of the independent variables (previous suicide attempts, gender, and age) on the dependent variables (likelihood of serious suicide attempts, suicide attempts, and suicidal thoughts). Results: Both ChatGPT-3.5 and ChatGPT-4 recognized the importance of previous suicide attempts in predicting severe suicide risks and suicidal thoughts. ChatGPT-4 also identified gender differences, associating men with a higher risk, while both models disregarded age as a risk factor. Interaction analysis revealed that ChatGPT-3.5 associated past attempts with a higher likelihood of suicidal thoughts in men, whereas ChatGPT-4 showed an increased risk for women. Conclusions: The study highlights ChatGPT-3.5 and ChatGPT-4's potential in suicide risk evaluation, emphasizing the importance of prior attempts and gender, while noting differences in their handling of interactive effects and the negligible role of age. These findings reflect the complexity of GenAI decision-making. While promising for suicide risk assessment, these models require careful application due to limitations and real-world complexities.  © 2024 Physicians Postgraduate Press, Inc.",yes,"This study directly evaluates the clinical application of LLMs (ChatGPT-3.5 and ChatGPT-4) in suicide risk assessment, using clinical vignettes to assess their decision-making. It fits the inclusion criteria as it tests LLMs in a healthcare context for a clinical task.",10.4088/JCP.24m15365,II,Vivian,"fake data, real task, not a clinical study",,
A pilot study on the efficacy of GPT-4 in providing orthopedic treatment recommendations from MRI reports,"754. Sci Rep. 2023 Nov 17;13(1):20159. doi: 10.1038/s41598-023-47500-2.

A pilot study on the efficacy of GPT-4 in providing orthopedic treatment 
recommendations from MRI reports.

Truhn D(1), Weber CD(2), Braun BJ(3), Bressem K(4), Kather JN(5)(6)(7)(8), Kuhl 
C(1), Nebelung S(9).

Author information:
(1)Department of Diagnostic and Interventional Radiology, University Hospital 
RWTH Aachen, Pauwels Street 30, 52074, Aachen, Germany.
(2)Department of Orthopaedics and Trauma Surgery, University Hospital RWTH 
Aachen, Aachen, Germany.
(3)University Hospital Tuebingen on Behalf of the Eberhard-Karls-University 
Tuebingen, BG Hospital, Schnarrenbergstr. 95, Tübingen, Germany.
(4)Department of Radiology, Charité - Universitätsmedizin Berlin, corporate 
member of Freie Universität Berlin and Humboldt-Universität zu Berlin, 
Hindenburgdamm 30, 12203, Berlin, Germany.
(5)Else Kroener Fresenius Center for Digital Health, Technical University 
Dresden, Dresden, Germany.
(6)Department of Medicine I, University Hospital Dresden, Dresden, Germany.
(7)Department of Medicine III, University Hospital RWTH Aachen, Aachen, Germany.
(8)Medical Oncology, National Center for Tumor Diseases (NCT), University 
Hospital Heidelberg, Heidelberg, Germany.
(9)Department of Diagnostic and Interventional Radiology, University Hospital 
RWTH Aachen, Pauwels Street 30, 52074, Aachen, Germany. snebelung@ukaachen.de.

Erratum in
    Sci Rep. 2024 Mar 5;14(1):5431. doi: 10.1038/s41598-024-56029-x.

Large language models (LLMs) have shown potential in various applications, 
including clinical practice. However, their accuracy and utility in providing 
treatment recommendations for orthopedic conditions remain to be investigated. 
Thus, this pilot study aims to evaluate the validity of treatment 
recommendations generated by GPT-4 for common knee and shoulder orthopedic 
conditions using anonymized clinical MRI reports. A retrospective analysis was 
conducted using 20 anonymized clinical MRI reports, with varying severity and 
complexity. Treatment recommendations were elicited from GPT-4 and evaluated by 
two board-certified specialty-trained senior orthopedic surgeons. Their 
evaluation focused on semiquantitative gradings of accuracy and clinical utility 
and potential limitations of the LLM-generated recommendations. GPT-4 provided 
treatment recommendations for 20 patients (mean age, 50 years ± 19 [standard 
deviation]; 12 men) with acute and chronic knee and shoulder conditions. The LLM 
produced largely accurate and clinically useful recommendations. However, 
limited awareness of a patient's overall situation, a tendency to incorrectly 
appreciate treatment urgency, and largely schematic and unspecific treatment 
recommendations were observed and may reduce its clinical usefulness. In 
conclusion, LLM-based treatment recommendations are largely adequate and not 
prone to 'hallucinations', yet inadequate in particular situations. Critical 
guidance by healthcare professionals is obligatory, and independent use by 
patients is discouraged, given the dependency on precise data input.

© 2023. The Author(s).",yes,"This study directly evaluates GPT-4, a generative large language model, in providing clinical treatment recommendations for orthopedic conditions based on MRI reports. The recommendations are assessed by orthopedic surgeons for accuracy and clinical utility, which is a clear clinical application of an LLM. The study is not a review, editorial, or technical report, and it does not focus on non-clinical tasks.",10.1038/s41598-023-47500-2,I,Vivian,"real data, real task, retrosepctive",,
An investigation study on the interpretation of ultrasonic medical reports using OpenAI's GPT-3.5-turbo model,"Objectives: Ultrasound medical reports are an important means of diagnosing diseases and assessing treatment effectiveness. However, their professional terms and complex sentences often make it difficult for ordinary people to understand. Therefore, this study explores the clinical value of using artificial intelligence systems based on ChatGPT to interpret ultrasound medical reports. Methods: In this study, a combination of online and offline questionnaires were used to survey both physicians and non-medical individuals. The questionnaires evaluated ChatGPT's interpretation of ultrasound reports from both professional and comprehensibility perspectives, and the results were analyzed using Excel spreadsheets. Additionally, a portion of the research content was evaluated using the Likert Scale 5-point method in the questionnaire. Results: According to survey results, 67.4% of surveyed doctors believe that using ChatGPT for interpreting ultrasound medical reports can help improve work efficiency. At the same time, 69.72% of non-medical professionals believe it is necessary to enhance their understanding of medical ultrasound reports through ChatGPT interpretation, and 62.58% support the application of ChatGPT to ultrasound medical reports. The non-medical group's understanding of ultrasound medical reports significantly improved (p < 0.01) after implementing ChatGPT, However, 67.49% of the general public are concerned about ChatGPT's imperfect functionality, which may cause misleading information. This reflects that the public's trust in new technology is not high enough, and they are also worried about possible privacy leaks and security issues with ChatGPT technology. Conclusions: The higher acceptance and support of non-medical individuals for the interpretation of medical reports by ChatGPT might be due to the system's natural language processing abilities that allow them to better understand and evaluate report contents. However, the expertise and experience of physicians are still irreplaceable. This suggests that the ChatGPT-based ultrasound medical report interpretation system has certain clinical value and application prospects, but further optimization is necessary to address its shortcomings in data quality and professionalism. This study provides a reference and inspiration for promoting the application and development of ultrasound technology and artificial intelligence systems in the medical field. © 2023 Wiley Periodicals LLC.",yes,"This study evaluates the use of GPT-3.5-turbo (ChatGPT) to interpret ultrasound medical reports for both physicians and non-medical individuals, assessing its clinical value and impact on understanding. The focus is on a clinical application—helping patients and clinicians interpret medical reports—rather than a purely technical or non-clinical task. The study involves real clinical content and assesses the LLM's utility in a healthcare context.",10.1002/jcu.23590,I,Vivian,"real data, real task, retrospective",,
Comparative analysis of diabetes diagnosis: WE-LSTM networks and WizardLM-powered DiabeTalk chatbot,"Diabetes is a chronic metabolic disorder characterized by elevated blood glucose levels due to insufficient insulin production or insulin resistance. It primarily manifests in two forms: Type 1 diabetes, an autoimmune condition typically diagnosed in younger individuals, and Type 2 diabetes, which is more prevalent and often linked to lifestyle factors such as obesity and inactivity. This study evaluates the performance of Long Short-Term Memory networks in diagnosing the two types of diabetes from Italian medical text across four progressively refined pre-processing scenarios. Each scenario incrementally builds on the previous one to enhance text cleaning and data preparation, allowing for a more refined and effective data processing pipeline. In parallel, this study introduces DiabeTalk, a chatbot developed on the WizardLM model, designed to provide specialized advice and support for diabetes diagnosis. While the WE-long short term memory models were fine-tuned with clinical data, DiabeTalk was tested without prior training on clinical diaries, allowing us to evaluate its performance in a real-world context. The results indicate that, despite the lack of domain-specific pre-training, DiabeTalk effectively employs natural language understanding and decision-making algorithms to predict diabetes type and respond to user inquiries. However, the testing revealed limitations in accuracy (77.56% versus 97.80%), with the chatbot achieving a lower performance than the WE-long short term memory model, which was applied to minimally pre-processed raw data. The findings underscore the importance of training large language models on relevant clinical datasets to enhance their response capabilities. © 2024 IEEE.",yes,"This study evaluates the clinical performance of a WizardLM-powered chatbot (DiabeTalk) in diagnosing diabetes and responding to user inquiries, comparing it to LSTM models. The LLM is used in a clinical context (diagnosis and patient support), meeting inclusion criteria.",10.1109/BIBM62325.2024.10821742,I,Vivian,"real data, real task, retrospective",,
Expert evaluation of large language models for clinical dialogue summarization,"We assessed the performance of large language models' summarizing clinical dialogues using computational metrics and human evaluations. The comparison was done between automatically generated and human-produced summaries. We conducted an exploratory evaluation of five language models: one general summarisation model, one fine-tuned for general dialogues, two fine-tuned with anonymized clinical dialogues, and one Large Language Model (ChatGPT). These models were assessed using ROUGE, UniEval metrics, and expert human evaluation was done by clinicians comparing the generated summaries against a clinician generated summary (gold standard). The fine-tuned transformer model scored the highest when evaluated with ROUGE, while ChatGPT scored the lowest overall. However, using UniEval, ChatGPT scored the highest across all the evaluated domains (coherence 0.957, consistency 0.7583, fluency 0.947, and relevance 0.947 and overall score 0.9891). Similar results were obtained when the systems were evaluated by clinicians, with ChatGPT scoring the highest in four domains (coherency 0.573, consistency 0.908, fluency 0.96 and overall clinical use 0.862). Statistical analyses showed differences between ChatGPT and human summaries vs. all other models. These exploratory results indicate that ChatGPT's performance in summarizing clinical dialogues approached the quality of human summaries. The study also found that the ROUGE metrics may not be reliable for evaluating clinical summary generation, whereas UniEval correlated well with human ratings. Large language models may provide a successful path for automating clinical dialogue summarization. Privacy concerns and the restricted nature of health records remain challenges for its integration. Further evaluations using diverse clinical dialogues and multiple initialization seeds are needed to verify the reliability and generalizability of automatically generated summaries.",yes,"This study evaluates the performance of ChatGPT (a large language model) in summarizing clinical dialogues, comparing its output to human-generated summaries and other models, with expert clinician evaluation. The task—summarizing clinical dialogues—has direct clinical relevance, as it pertains to automating clinical documentation, which is a clinical application. The study uses both computational and human expert evaluation, making it a valid clinical evaluation of an LLM.",10.1038/s41598-024-84850-x,II,Vivian,"fake data, real task, not a clincial study",,
Search for Medical Information and Treatment Options for Musculoskeletal Disorders through an Artificial Intelligence Chatbot: Focusing on Shoulder Impingement Syndrome,"Background: The ChatGPT is an artificial intelligence chatbot that processes natural language text learned through reinforcement learning based on the GPT-3.5 architecture, a large-scale language model. Natural language processing models are being used in various fields and are gradually expanding their use in the medical field. Purpose: This study aimed to investigate the medical information or treatment options that ChatGPT can provide for SIS. Method: Using ChatGPT, which is provided as a free beta test, messages related to SIS were entered, and responses to medical information and treatment options were received and analyzed. Result: ChatGPT not only provided answers to the definition, prevalence, and risk factors of SIS, but also symptoms, diseases with similar symptoms, and orthopedic tests according to the messages input. Additionally, a list of treatment options and exercises were provided. Conclusion: ChatGPT will be able to provide overall useful medical information and treatment options to patients unfamiliar with SIS. However, caution is required as it contains content that may be biased or inappropriate information for patients with SIS. Nevertheless, if natural language processing technology develops further, it is expected to be able to express more detailed medical information and treatment options.",yes,"This study evaluates ChatGPT (GPT-3.5) in providing medical information and treatment options for a specific clinical condition (shoulder impingement syndrome), analyzing its responses to clinical queries. This fits the inclusion criteria as it assesses an LLM's clinical utility in patient information and decision support.",10.1101/2022.12.16.22283512,X,Vivian,"no data, real task, not a clinical study",,
Comparing patient education tools for chronic pain medications: Artificial intelligence chatbot versus traditional patient information leaflets,"Background and Aims: Artificial intelligence (AI) chatbots like Conversational Generative Pre-trained Transformer (ChatGPT) have recently created much buzz, especially regarding patient education. Such informed patients understand and adhere to the management and get involved in shared decision making. The accuracy and understandability of the generated educational material are prime concerns. Thus, we compared ChatGPT with traditional patient information leaflets (PILs) about chronic pain medications. Methods: Patients' frequently asked questions were generated from PILs available on the official websites of the British Pain Society (BPS) and the Faculty of Pain Medicine. Eight blinded annexures were prepared for evaluation, consisting of traditional PILs from the BPS and AI-generated patient information materials structured similar to PILs by ChatGPT. The authors performed a comparative analysis to assess materials' readability, emotional tone, accuracy, actionability, and understandability. Readability was measured using Flesch Reading Ease (FRE), Gunning Fog Index (GFI), and Flesch-Kincaid Grade Level (FKGL). Sentiment analysis determined emotional tone. An expert panel evaluated accuracy and completeness. Actionability and understandability were assessed with the Patient Education Materials Assessment Tool. Results: Traditional PILs generally exhibited higher readability (P values < 0.05), with [mean (standard deviation)] FRE [62.25 (1.6) versus 48 (3.7)], GFI [11.85 (0.9) versus 13.65 (0.7)], and FKGL [8.33 (0.5) versus 10.23 (0.5)] but varied emotional tones, often negative, compared to more positive sentiments in ChatGPT-generated texts. Accuracy and completeness did not significantly differ between the two. Actionability and understandability scores were comparable. Conclusion: While AI chatbots offer efficient information delivery, ensuring accuracy and readability, patient-centeredness remains crucial. It is imperative to balance innovation with evidence-based practice. © 2024 Wolters Kluwer Medknow Publications. All rights reserved.",yes,"This study evaluates ChatGPT, a generative LLM, as a tool for patient education about chronic pain medications, comparing its output to traditional patient information leaflets. The evaluation includes clinical aspects such as accuracy, actionability, and understandability of information provided to patients. This fits the inclusion criteria as it assesses the clinical utility of an LLM in a healthcare context.",10.4103/ija.ija_204_24,II,Vivian,"fake data, real task, not a clinical study",,
A feasibility study of automating radiotherapy planning with large language model agents,"Objective. Radiotherapy planning requires significant expertise to balance tumor control and organ-at-risk (OAR) sparing. Automated planning can improve both efficiency and quality. This study introduces GPT-Plan, a novel multi-agent system powered by the GPT-4 family of large language models (LLMs), for automating the iterative radiotherapy plan optimization. Approach. GPT-Plan uses LLM-driven agents, mimicking the collaborative clinical workflow of a dosimetrist and physicist, to iteratively generate and evaluate text-based radiotherapy plans based on predefined criteria. Supporting tools assist the agents by leveraging historical plans, mitigating LLM hallucinations, and balancing exploration and exploitation. Performance was evaluated on 12 lung (IMRT) and 5 cervical (VMAT) cancer cases, benchmarked against the ECHO auto-planning method and manual plans. The impact of historical plan retrieval on efficiency was also assessed. Results. For IMRT lung cancer cases, GPT-Plan generated high-quality plans, demonstrating superior target coverage and homogeneity compared to ECHO while maintaining comparable or better OAR sparing. For VMAT cervical cancer cases, plan quality was comparable to a senior physicist and consistently superior to a junior physicist, particularly for OAR sparing. Retrieving historical plans significantly reduced the number of required optimization iterations for lung cases (p < 0.01) and yielded iteration counts comparable to those of the senior physicist for cervical cases (p = 0.313). Occasional LLM hallucinations have been mitigated by self-reflection mechanisms. One limitation was the inaccuracy of vision-based LLMs in interpreting dose images. Significance. This pioneering study demonstrates the feasibility of automating radiotherapy planning using LLM-powered agents for complex treatment decision-making tasks. While challenges remain in addressing LLM limitations, ongoing advancements hold potential for further refining and expanding GPT-Plan’s capabilities. © 2025 Institute of Physics and Engineering in Medicine. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",yes,"This study evaluates a large language model (GPT-4 family) in a clinical context—specifically, automating radiotherapy planning, which is a complex clinical decision-making task. The LLM is directly involved in generating and optimizing treatment plans, and its performance is benchmarked against both automated and manual clinical standards. This fits the inclusion criteria as it assesses the clinical application of an LLM.",10.1088/1361-6560/adbff1,I,Vivian,"real data, real task, retrospective",,
Evaluating Ai-Generated Patient Education Materials for Spinal Surgeries: Comparative Analysis of Readability and Discern Quality Across Chatgpt and Deepseek Models,"Background: Access to patient-centered health information is essential for informed decision-making. However, online medical resources vary in quality and often fail to accommodate differing degrees of health literacy. This issue is particularly evident in surgical contexts, where complex terminology obstructs patient comprehension. With the increasing reliance on AI chatbots for supplementary medical information, the reliability and readability of AI-generated content require thorough evaluation. Objective: This study aimed to evaluate four natural language processing models—ChatGPT-4o, ChatGPT-o3 mini, DeepSeek-V3, and DeepSeek-R1—in generating patient education materials for three common spinal surgeries: lumbar discectomy, spinal fusion, and decompressive laminectomy. Information quality was evaluated using the DISCERN score, and readability was assessed through Flesch-Kincaid indices. Results: DeepSeek-R1 produced the most readable responses, with Flesch-Kincaid Grade Level (FKGL) scores ranging from 7.2 to 9.0, succeeded by ChatGPT-4o. In contrast, ChatGPT-o3 exhibited the lowest readability (FKGL >10.4). The DISCERN scores for all AI models were below 60, classifying the information quality as ""fair,"" primarily due to insufficient cited references. Conclusion: DeepSeek-R1 and ChatGPT-4o generated more readable surgical information than ChatGPT-o3. Nonetheless, all models achieved merely a ""fair"" quality rating, underscoring the necessity for improvements in readability, citation practices, and personalization. Future research ought to explore interactive, multimodal approaches and incorporate patient feedback to ensure that AI-generated health information is accurate, accessible, and facilitates informed healthcare decisions.",yes,"This study evaluates the ability of LLMs (ChatGPT-4o, ChatGPT-o3 mini, DeepSeek-V3, DeepSeek-R1) to generate patient education materials for spinal surgeries, assessing both readability and information quality. The task is directly related to clinical communication and patient education, which is a clinical application. The models evaluated are generative LLMs, and the study is not a review or opinion piece.",10.2139/ssrn.5129128,X,Vivian,"no data, not-very-helpful task, not a clinical study",,
"Breaking Bones, Breaking Barriers: ChatGPT, DeepSeek, and Gemini in Hand Fracture Management","Background: Hand fracture management requires precise diagnostic accuracy and complex decision-making. Advances in artificial intelligence (AI) suggest that large language models (LLMs) may assist or even rival traditional clinical approaches. This study evaluates the effectiveness of ChatGPT-4o, DeepSeek-V3, and Gemini 1.5 in diagnosing and recommending treatment strategies for hand fractures compared to experienced surgeons. Methods: A retrospective analysis of 58 anonymized hand fracture cases was conducted. Clinical details, including fracture site, displacement, and soft-tissue involvement, were provided to the AI models, which generated management plans. Their recommendations were compared to actual surgeon decisions, assessing accuracy, precision, recall, and F1 score. Results: ChatGPT-4o demonstrated the highest accuracy (98.28%) and recall (91.74%), effectively identifying most correct interventions but occasionally proposing extraneous options (precision 58.48%). DeepSeek-V3 showed moderate accuracy (63.79%), with balanced precision (61.17%) and recall (57.89%), sometimes omitting correct treatments. Gemini 1.5 performed poorly (accuracy 18.97%), with low precision and recall, indicating substantial limitations in clinical decision support. Conclusions: AI models can enhance clinical workflows, particularly in radiographic interpretation and triage, but their limitations highlight the irreplaceable role of human expertise in complex hand trauma management. ChatGPT-4o demonstrated promising accuracy but requires refinement. Ethical concerns regarding AI-driven medical decisions, including bias and transparency, must be addressed before widespread clinical implementation. © 2025 by the authors.",yes,"This study directly evaluates the clinical performance of multiple LLMs (ChatGPT-4o, DeepSeek-V3, Gemini 1.5) in diagnosing and recommending treatment for hand fractures, comparing their outputs to those of experienced surgeons. The focus is on clinical decision-making and management, which fits the inclusion criteria.",10.3390/jcm14061983,III,Vivian,"fake data, fake task, not a clinical study",,
Evaluating AI Models: Performance Validation Using Formal Multiple-Choice Questions in Neuropsychology,"High-quality and accessible education is crucial for advancing neuropsychology. A recent study identified key barriers to board certification in clinical neuropsychology, such as time constraints and insufficient specialized knowledge. To address these challenges, this study explored the capabilities of advanced Artificial Intelligence (AI) language models, GPT-3.5 (free-version) and GPT-4.0 (under-subscription version), by evaluating their performance on 300 American Board of Professional Psychology in Clinical Neuropsychology-like questions. The results indicate that GPT-4.0 achieved a higher accuracy rate of 80.0% compared to GPT-3.5’s 65.7%. In the “Assessment” category, GPT-4.0 demonstrated a notable improvement with an accuracy rate of 73.4% compared to GPT-3.5’s 58.6% (p = 0.012). The “Assessment” category, which comprised 128 questions and exhibited the highest error rate by both AI models, was analyzed. A thematic analysis of the 26 incorrectly answered questions revealed 8 main themes and 17 specific codes, highlighting significant gaps in areas such as “Neurodegenerative Diseases” and “Neuropsychological Testing and Interpretation.” © The Author(s) 2024. Published by Oxford University Press. All rights reserved.",yes,"This study evaluates the performance of GPT-3.5 and GPT-4.0 (both LLMs) on board-style multiple-choice questions in clinical neuropsychology, which is a clinical context relevant to medicine. The focus is on the models' ability to answer clinically relevant exam questions, fitting the inclusion criteria.",10.1093/arclin/acae068,III,Vivian,"fake data, fake task, not a clinical study",,
Fact Check: Assessing the Response of ChatGPT to Alzheimer's Disease Myths,"Introduction: There are many myths regarding Alzheimer's disease (AD) that have been circulated on the internet, each exhibiting varying degrees of accuracy, inaccuracy, and misinformation. Large language models, such as ChatGPT, may be a valuable tool to help assess these myths for veracity and inaccuracy; however, they can induce misinformation as well. Objective: This study assesses ChatGPT's ability to identify and address AD myths with reliable information. Methods: We conducted a cross-sectional study of attending geriatric medicine clinicians' evaluation of ChatGPT (GPT 4.0) responses to 16 selected AD myths. We prompted ChatGPT to express its opinion on each myth and implemented a survey using REDCap to determine the degree to which clinicians agreed with the accuracy of each of ChatGPT's explanations. We also collected their explanations of any disagreements with ChatGPT's responses. We used a 5-category Likert-type scale with a score ranging from −2 to 2 to quantify clinicians' agreement in each aspect of the evaluation. Results: The clinicians (n = 10) were generally satisfied with ChatGPT's explanations. Among the 16 myths, the clinicians were generally satisfied with these explanations, with [mean (SD) score of 1.1(±0.3)]. Most clinicians selected “Agree” or “Strongly Agree” for each statement. Some statements obtained a small number of “Disagree” responses. There were no “Strongly Disagree” responses. Conclusion: Most surveyed health care professionals acknowledged the potential value of ChatGPT in mitigating AD misinformation; however, the need for more refined and detailed explanations of the disease's mechanisms and treatments was highlighted. © 2024 The Authors",yes,"This study evaluates ChatGPT's (GPT-4.0) ability to address and correct myths about Alzheimer's disease, with clinical experts assessing the accuracy of its responses. The focus is on ChatGPT's performance in providing reliable clinical information, which is a direct clinical application relevant to patient education and misinformation mitigation.",10.1016/j.jamda.2024.105178,X,Vivian,"data=myths about AD sourced from the Internet (technically ""real"" data but not patient/clinical data), real task, not a clinical study",,
Artificial Intelligence Promotes the Dunning Kruger Effect: Evaluating ChatGPT Answers to Frequently Asked Questions about Adolescent Idiopathic Scoliosis,"Introduction:Patients have long turned to the Internet for answers to common medical questions. As the ability to access information evolves beyond standard search engines, patients with adolescent idiopathic scoliosis (AIS) and their parents may use artificial intelligence chatbots such as ChatGPT as a new source of information.Methods:Ten frequently asked questions regarding AIS were posed to ChatGPT. The accuracy and adequacy of the responses were graded as excellent not requiring clarification, satisfactory requiring minimal clarification, satisfactory requiring moderate clarification, and unsatisfactory requiring substantial clarification.Results:ChatGPT gave one response that was excellent not requiring clarification, four responses that were satisfactory requiring minimal clarification, three responses that were satisfactory requiring moderate clarification, and two responses that were unsatisfactory requiring substantial clarification, with information about higher level, more complex areas of discussion such as surgical options being less accurate.Conclusion:ChatGPT provides answers to FAQs about AIS that were generally accurate, although correction was needed on specific surgical treatments. Patients may be at risk of developing a Dunning-Kruger effect by proxy from the superficial and sometimes inaccurate information provided by ChatGPT on more complex aspects of AIS. © American Academy of Orthopaedic Surgeons.",yes,"This study evaluates ChatGPT's ability to answer clinical questions frequently asked by patients about adolescent idiopathic scoliosis, assessing the accuracy and adequacy of its responses. This is a direct clinical application relevant to patient education and information provision.",10.5435/JAAOS-D-24-00297,Ii,Vivian,"fake data, real task, not a clinical study",,
Incorporating Patient Values in Large Language Model Recommendations for Surrogate and Proxy Decisions,"Background: Surrogates, proxies, and clinicians making shared treatment decisions for patients who have lost decision-making capacity often fail to honor patients' wishes, due to stress, time pressures, misunderstanding patient values, and projecting personal biases. Advance directives intend to align care with patient values but are limited by low completion rates and application to only a subset of medical decisions. Here, we investigate the potential of large language models (LLMs) to incorporate patient values in supporting critical care clinical decision-making for incapacitated patients in a proof-of-concept study. Methods: We simulated text-based scenarios for 50 decisionally incapacitated patients for whom a medical condition required imminent clinical decisions regarding specific interventions. For each patient, we also simulated five unique value profiles captured using alternative formats: numeric ranking questionnaires, text-based questionnaires, and free-text narratives. We used pre-trained generative LLMs for two tasks: 1) text extraction of the treatments under consideration and 2) prompt-based question-answering to generate a recommendation in response to the scenario information, extracted treatment, and patient value profiles. Model outputs were compared with adjudications by three domain experts who independently evaluated each scenario and decision. Results and Conclusions: Automated extractions of the treatment in question were accurate for 88% (n = 44/50) of scenarios. LLM treatment recommendations received an average Likert score by the adjudicators of 3.92 of 5.00 (five being best) across all patients for being medically plausible and reasonable treatment recommendations, and 3.58 of 5.00 for reflecting the documented values of the patient. Scores were highest when patient values were captured as short, unstructured, and free-text narratives based on simulated patient profiles. This proof-of-concept study demonstrates the potential for LLMs to function as support tools for surrogates, proxies, and clinicians aiming to honor the wishes and values of decisionally incapacitated patients.  Copyright © 2024 The Authors.",yes,"This study evaluates the use of generative LLMs to support clinical decision-making by incorporating patient values in surrogate and proxy decisions for incapacitated patients. It uses simulated clinical scenarios and assesses LLM recommendations against expert adjudication, directly addressing a clinical application of LLMs in healthcare.",10.1097/CCE.0000000000001131,II,Vivian,"fake data, real task, not a clinical study",,
Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder,"Despite the promising capacity of large language model (LLM)-powered chatbots to diagnose diseases, they have not been tested for obsessive-compulsive disorder (OCD). We assessed the diagnostic accuracy of LLMs in OCD using vignettes and found that LLMs outperformed medical and mental health professionals. This highlights the potential benefit of LLMs in assisting in the timely and accurate diagnosis of OCD, which usually entails a long delay in diagnosis and treatment. © The Author(s) 2024.",yes,This study evaluates the diagnostic accuracy of LLMs in identifying obsessive-compulsive disorder using clinical vignettes and compares their performance to that of medical and mental health professionals. This is a direct clinical application of LLMs in healthcare.,10.1038/s41746-024-01181-x,III,Vivian,"fake data, fake task, not a clinical study",,
Enhancing early detection of cognitive decline in the elderly: a comparative study utilizing large language models in clinical notes,"Background: Large language models (LLMs) have shown promising performance in various healthcare domains, but their effectiveness in identifying specific clinical conditions in real medical records is less explored. This study evaluates LLMs for detecting signs of cognitive decline in real electronic health record (EHR) clinical notes, comparing their error profiles with traditional models. The insights gained will inform strategies for performance enhancement. Methods: This study, conducted at Mass General Brigham in Boston, MA, analysed clinical notes from the four years prior to a 2019 diagnosis of mild cognitive impairment in patients aged 50 and older. We developed prompts for two LLMs, Llama 2 and GPT-4, on Health Insurance Portability and Accountability Act (HIPAA)-compliant cloud-computing platforms using multiple approaches (e.g., hard prompting, retrieval augmented generation, and error analysis-based instructions) to select the optimal LLM-based method. Baseline models included a hierarchical attention-based neural network and XGBoost. Subsequently, we constructed an ensemble of the three models using a majority vote approach. Confusion-matrix-based scores were used for model evaluation. Findings: We used a randomly annotated sample of 4949 note sections from 1969 patients (women: 1046 [53.1%]; age: mean, 76.0 [SD, 13.3] years), filtered with keywords related to cognitive functions, for model development. For testing, a random annotated sample of 1996 note sections from 1161 patients (women: 619 [53.3%]; age: mean, 76.5 [SD, 10.2] years) without keyword filtering was utilised. GPT-4 demonstrated superior accuracy and efficiency compared to Llama 2, but did not outperform traditional models. The ensemble model outperformed the individual models in terms of all evaluation metrics with statistical significance (p < 0.01), achieving a precision of 90.2% [95% CI: 81.9%–96.8%], a recall of 94.2% [95% CI: 87.9%–98.7%], and an F1-score of 92.1% [95% CI: 86.8%–96.4%]. Notably, the ensemble model showed a significant improvement in precision, increasing from a range of 70%–79% to above 90%, compared to the best-performing single model. Error analysis revealed that 63 samples were incorrectly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Interpretation: LLMs and traditional machine learning models trained using local EHR data exhibited diverse error profiles. The ensemble of these models was found to be complementary, enhancing diagnostic performance. Future research should investigate integrating LLMs with smaller, localised models and incorporating medical data and domain knowledge to enhance performance on specific tasks. Funding: This research was supported by the National Institute on Aging grants (R44AG081006, R01AG080429) and National Library of Medicine grant (R01LM014239).",yes,"This study evaluates the use of LLMs (Llama 2 and GPT-4) for detecting cognitive decline in real clinical notes, directly comparing their clinical performance to traditional models. The focus is on a clinical application (early detection of cognitive decline) using LLMs in a healthcare context, which fits the inclusion criteria.",10.1016/j.ebiom.2024.105401,I,Vivian,"real data, real task, retrosepctive",,
A Multimodal Vision-text AI Copilot for Brain Disease Diagnosis and Medical Imaging,"Integrating non-invasive brain imaging techniques, particularly computed tomography (CT) and magnetic resonance imaging (MRI), coupled with the advancement of artificial intelligence, is forging a key pathway for brain disease diagnosis, playing a vital role in safeguarding human health(1–4). A robust artificial intelligence copilot is essential for clinical emergencies, functioning as the central processing unit for brain medical imaging systems, aiming to revolutionize the imaging process, expedite the diagnosis of diseases, and support treatment(5–7). In this study, we developed an advanced multi-modal brain medical imaging foundational model named Brainfound, utilizing AI-generated content and image-text alignment technology, pre-trained on over 3 million brain CT images and over 7 million brain MRI images with their paired reports. As a clinical brain medical imaging multi-modal model, Brainfound achieved state of the art on seven downstream tasks, including brain disease diagnosis, brain lesion segmentation, MRI image enhancement, MRI cross-modality translation, automatic report generation, zero-shot brain disease classification, and free human-AI conversation. After thorough human-machine validation, Brainfound surpassed the current leading model by 51.75% in automatic report generation for brain imaging. In multiple-choice questions related to brain imaging, the accuracy of Brainfound outstripped GPT-4V by 47.68%, comparable to experienced doctors. We anticipate Brainfound, a clinical model with flexible visual and text input-output capabilities, will provide substantial support in brain medical imaging, clinical education, and human-in-the-loop medical diagnosis.",yes,"The study describes the development and evaluation of a multimodal (vision-text) AI model, Brainfound, for brain disease diagnosis and medical imaging. It includes clinical tasks such as disease diagnosis, lesion segmentation, and free human-AI conversation, and compares performance to GPT-4V on clinical questions. The model is evaluated in a clinical context and includes LLM-like capabilities (free conversation, report generation, question answering).",10.1101/2025.01.09.25320293,I,Vivian,"real data, real task, retrospective",,
ChatGPT as a prospective undergraduate and medical school student,"This article reports the results of an experiment conducted with ChatGPT to see how its performance compares to human performance on tests that require specific knowledge and skills, such as university admission tests. We chose a general undergraduate admission test and two tests for admission to biomedical programs: the Scholastic Assessment Test (SAT), the Cambridge BioMedical Admission Test (BMAT), and the Italian Medical School Admission Test (IMSAT). In particular, we looked closely at the difference in performance between ChatGPT-4 and its predecessor, ChatGPT-3.5, to assess its evolution. The performance of ChatGPT-4 showed a significant improvement over ChatGPT-3.5 and, compared to real students, was on average within the top 10% in the SAT test, while the score in the IMSAT test granted admission to the two highest ranked Italian medical schools. In addition to the performance analysis, we provide a qualitative analysis of incorrect answers and a classification of three different types of logical and computational errors made by ChatGPT-4, which reveal important weaknesses of the model. This provides insight into the skills needed to use these models effectively despite their weaknesses, and also suggests possible applications of our analysis in the field of education.",yes,"This study evaluates ChatGPT's performance on undergraduate and medical school admission tests, including the Italian Medical School Admission Test (IMSAT) and the BMAT. While these are not clinical board exams, the BMAT and IMSAT are directly relevant to medical education and admission. The study also analyzes the types of errors made by the LLM. Since the evaluation includes medical school admission tests, which are a gateway to clinical education, this fits the inclusion criteria for LLM performance in a medical context.",10.1371/journal.pone.0308157,III,Vivian,"fake data, fake task, not a clinical study",,
Automated and code-free development of a risk calculator using ChatGPT-4 for predicting diabetic retinopathy and macular edema without retinal imaging,"Background: Diabetic retinopathy (DR) and macular edema (DME) are critical causes of vision loss in patients with diabetes. In many communities, access to ophthalmologists and retinal imaging equipment is limited, making screening for diabetic retinal complications difficult in primary health care centers. We investigated whether ChatGPT-4, an advanced large-language-model chatbot, can develop risk calculators for DR and DME using health check-up tabular data without the need for retinal imaging or coding experience. Methods: Data-driven prediction models were developed using medical history and laboratory blood test data from diabetic patients in the Korea National Health and Nutrition Examination Surveys (KNHANES). The dataset was divided into training (KNHANES 2017–2020) and validation (KNHANES 2021) datasets. ChatGPT-4 was used to build prediction formulas for DR and DME and developed a web-based risk calculator tool. Logistic regression analysis was performed by ChatGPT-4 to predict DR and DME, followed by the automatic generation of Hypertext Markup Language (HTML) code for the web-based tool. The performance of the models was evaluated using areas under the curves of receiver operating characteristic curve (ROC-AUCs). Results: ChatGPT-4 successfully developed a risk calculator for DR and DME, operational on a web browser without any coding experience. The validation set showed ROC-AUCs of 0.786 and 0.835 for predicting DR and DME, respectively. The performance of the ChatGPT-4 developed models was comparable to those created using various machine-learning tools. Conclusion: By utilizing ChatGPT-4 with code-free prompts, we overcame the technical barriers associated with using coding skills for developing prediction models, making it feasible to build a risk calculator for DR and DME prediction. Our approach offers an easily accessible tool for the risk prediction of DM and DME in diabetic patients during health check-ups, without the need for retinal imaging. Based on this automatically developed risk calculator using ChatGPT-4, health care workers will be able to effectively screen patients who require retinal examinations using only medical history and laboratory data. Future research should focus on validating this approach in diverse populations and exploring the integration of more comprehensive clinical data to enhance predictive performance. © The Author(s) 2025.",yes,"The study evaluates ChatGPT-4's ability to develop a risk calculator for diabetic retinopathy and macular edema using clinical data, and assesses the clinical performance of the resulting tool. The LLM is used to generate prediction models and a web-based calculator for clinical risk assessment, which is a direct clinical application.",10.1186/s40942-025-00638-9,I,Vivian,"real data, real task, retrospective",,
Evaluating the performance of the language model ChatGPT in responding to common questions of people with epilepsy,"Objective: People with epilepsy desire to acquire accurate information about epilepsy and actively engage in its management throughout the long journey of living with seizures. ChatGPT is a large language model and we aimed to assess the accuracy and consistency of ChatGPT in responding to the common concerns of people with epilepsy and to evaluate its ability to provide emotional support. Methods: Questions were collected from the International League against Epilepsy and the China Association against Epilepsy. The responses were independently assessed by two board-certified epileptologists from the China Association against Epilepsy, and a third reviewer resolved disagreements. The reviewers assessed its ability to provide emotional support subjectively. Results: A total of 378 questions related to epilepsy and 5 questions related to emotional support were included. ChatGPT provided “correct and comprehensive” answers to 68.4% of the questions. The model provided reproducible answers for 82.3% questions. The model performed poorly in answering prognostic questions, with only 46.8% of the answers rated as comprehensive. When faced with questions requiring emotional support, the model can generate natural and understandable responses. Significance: ChatGPT provides accurate and reliable answers to patients with epilepsy and is a valuable source of information. It also provides partial emotional support, potentially assisting those experiencing emotional distress. However, ChatGPT may provide incorrect responses, leading users to inadvertently accept incorrect and potentially dangerous advice. Therefore, the direct use of ChatGPT for medical guidance is not recommended and its primary use at present is in patients education. © 2024 Elsevier Inc.",yes,"This study evaluates ChatGPT's performance in answering clinical and emotional support questions from people with epilepsy, with responses assessed by board-certified epileptologists. The focus is on the accuracy, consistency, and supportiveness of ChatGPT's answers to real patient questions, which is a direct clinical application relevant to patient education and support.",10.1016/j.yebeh.2024.109645,II,Vivian,"fake data, real task, not a clinical study",,
"Comparison of ChatGPT-3.5, ChatGPT-4, and Orthopaedic Resident Performance on Orthopaedic Assessment Examinations","Introduction:Artificial intelligence (AI) programs have the ability to answer complex queries including medical profession examination questions. The purpose of this study was to compare the performance of orthopaedic residents (ortho residents) against Chat Generative Pretrained Transformer (ChatGPT)-3.5 and GPT-4 on orthopaedic assessment examinations. A secondary objective was to perform a subgroup analysis comparing the performance of each group on questions that included image interpretation versus text-only questions.Methods:The ResStudy orthopaedic examination question bank was used as the primary source of questions. One hundred eighty questions and answer choices from nine different orthopaedic subspecialties were directly input into ChatGPT-3.5 and then GPT-4. ChatGPT did not have consistently available image interpretation, so no images were directly provided to either AI format. Answers were recorded as correct versus incorrect by the chatbot, and resident performance was recorded based on user data provided by ResStudy.Results:Overall, ChatGPT-3.5, GPT-4, and ortho residents scored 29.4%, 47.2%, and 74.2%, respectively. There was a difference among the three groups in testing success, with ortho residents scoring higher than ChatGPT-3.5 and GPT-4 (P < 0.001 and P < 0.001). GPT-4 scored higher than ChatGPT-3.5 (P = 0.002). A subgroup analysis was performed by dividing questions into question stems without images and question stems with images. ChatGPT-3.5 was more correct (37.8% vs. 22.4%, respectively, OR = 2.1, P = 0.033) and ChatGPT-4 was also more correct (61.0% vs. 35.7%, OR = 2.8, P < 0.001), when comparing text-only questions versus questions with images. Residents were 72.6% versus 75.5% correct with text-only questions versus questions with images, with no significant difference (P = 0.302).Conclusion:Orthopaedic residents were able to answer more questions accurately than ChatGPT-3.5 and GPT-4 on orthopaedic assessment examinations. GPT-4 is superior to ChatGPT-3.5 for answering orthopaedic resident assessment examination questions. Both ChatGPT-3.5 and GPT-4 performed better on text-only questions than questions with images. It is unlikely that GPT-4 or ChatGPT-3.5 would pass the American Board of Orthopaedic Surgery written examination.  © American Academy of Orthopaedic Surgeons.",yes,"This study evaluates the performance of generative LLMs (ChatGPT-3.5 and GPT-4) on orthopaedic assessment examinations and compares their results to those of orthopaedic residents. The focus is on clinical knowledge and exam performance, which is a direct clinical application relevant to medical education.",10.5435/JAAOS-D-23-00396,III,Vivian,"fake data, fake task, not a clinical study",,
Readability of Hospital Online Patient Education Materials Across Otolaryngology Specialties,"Introduction: This study evaluates the readability of online patient education materials (OPEMs) across otolaryngology subspecialties, hospital characteristics, and national otolaryngology organizations, while assessing AI alternatives. Methods: Hospitals from the US News Best ENT list were queried for OPEMs describing a chosen surgery per subspecialty; the American Academy of Otolaryngology—Head and Neck Surgery (AAO), American Laryngological Association (ALA), Ear, Nose, and Throat United Kingdom (ENTUK), and the Canadian Society of Otolaryngology—Head and Neck Surgery (CSOHNS) were similarly queried. Google was queried for the top 10 links from hospitals per procedure. Ownership (private/public), presence of respective otolaryngology fellowships, region, and median household income (zip code) were collected. Readability was assessed using seven indices and averaged: Automated Readability Index (ARI), Flesch Reading Ease Score (FRES), Flesch–Kincaid Grade Level (FKGL), Gunning Fog Readability (GFR), Simple Measure of Gobbledygook (SMOG), Coleman–Liau Readability Index (CLRI), and Linsear Write Readability Formula (LWRF). AI-generated materials from ChatGPT were compared for readability, accuracy, content, and tone. Analyses were conducted between subspecialties, against national organizations, NIH standard, and across demographic variables. Results: Across 144 hospitals, OPEMs exceeded NIH readability standards, averaging at an 8th–12th grade level across subspecialties. In rhinology, facial plastics, and sleep medicine, hospital OPEMs had higher readability scores than ENTUK's materials (11.4 vs. 9.1, 10.4 vs. 7.2, 11.5 vs. 9.2, respectively; all p < 0.05), but lower than AAO (p = 0.005). ChatGPT-generated materials averaged a 6.8-grade level, demonstrating improved readability, especially with specialized prompting, compared to all hospital and organization OPEMs. Conclusion: OPEMs from all sources exceed the NIH readability standard. ENTUK serves as a benchmark for accessible language, while ChatGPT demonstrates the feasibility of producing more readable content. Otolaryngologists might consider using ChatGPT to generate patient-friendly materials, with caution, and advocate for national-level improvements in patient education readability. © 2025 The Author(s). Laryngoscope Investigative Otolaryngology published by Wiley Periodicals LLC on behalf of The Triological Society.",yes,"This study compares the readability, accuracy, content, and tone of patient education materials generated by ChatGPT (an LLM) to those from hospitals and professional organizations. While the primary focus is on readability, the study does evaluate ChatGPT's output in a clinical context (patient education), including accuracy and content. This fits the inclusion criteria as it assesses a generative LLM in a healthcare application relevant to patients.",10.1002/lio2.70101,X,Vivian,"data=sourced from the Internet (technically it is ""real"" data but not clinical/patient data and from hospital websites)",,
The Large Language Model ChatGPT-4 Exhibits Excellent Triage Capabilities and Diagnostic Performance for Patients Presenting With Various Causes of Knee Pain,"Purpose: To provide a proof-of-concept analysis of the appropriateness and performance of ChatGPT-4 to triage, synthesize differential diagnoses, and generate treatment plans concerning common presentations of knee pain. Methods: Twenty knee complaints warranting triage and expanded scenarios were input into ChatGPT-4, with memory cleared prior to each new input to mitigate bias. For the 10 triage complaints, ChatGPT-4 was asked to generate a differential diagnosis that was graded for accuracy and suitability in comparison to a differential created by 2 orthopaedic sports medicine physicians. For the 10 clinical scenarios, ChatGPT-4 was prompted to provide treatment guidance for the patient, which was again graded. To test the higher-order capabilities of ChatGPT-4, further inquiry into these specific management recommendations was performed and graded. Results: All ChatGPT-4 diagnoses were deemed appropriate within the spectrum of potential pathologies on a differential. The top diagnosis on the differential was identical between surgeons and ChatGPT-4 for 70% of scenarios, and the top diagnosis provided by the surgeon appeared as either the first or second diagnosis in 90% of scenarios. Overall, 16 of 30 diagnoses (53.3%) in the differential were identical. When provided with 10 expanded vignettes with a single diagnosis, the accuracy of ChatGPT-4 increased to 100%, with the suitability of management graded as appropriate in 90% of cases. Specific information pertaining to conservative management, surgical approaches, and related treatments was appropriate and accurate in 100% of cases. Conclusions: ChatGPT-4 provided clinically reasonable diagnoses to triage patient complaints of knee pain due to various underlying conditions that were generally consistent with differentials provided by sports medicine physicians. Diagnostic performance was enhanced when providing additional information, allowing ChatGPT-4 to reach high predictive accuracy for recommendations concerning management and treatment options. However, ChatGPT-4 may show clinically important error rates for diagnosis depending on prompting strategy and information provided; therefore, further refinements are necessary prior to implementation into clinical workflows. Clinical Relevance: Although ChatGPT-4 is increasingly being used by patients for health information, the potential for ChatGPT-4 to serve as a clinical support tool is unclear. In this study, we found that ChatGPT-4 was frequently able to diagnose and triage knee complaints appropriately as rated by sports medicine surgeons, suggesting that it may eventually be a useful clinical support tool. © 2024 Arthroscopy Association of North America",yes,"This study directly evaluates ChatGPT-4's clinical performance in triage, diagnosis, and treatment planning for knee pain, comparing its outputs to those of orthopedic physicians. It assesses the LLM's ability to generate differentials and management recommendations in a clinical context, which fits the inclusion criteria.",10.1016/j.arthro.2024.06.021,I,Vivian,"real data, real task, retrospective",,
Analyzing the precision and readability of a healthcare focused artificial intelligence platform on common questions regarding breast augmentation,"Aim: The purpose of this study was to determine the quality and accessibility of the outputs from a healthcare-specific artificial intelligence (AI) platform for common questions during the perioperative period for a common plastic surgery procedure. Methods: Doximity GPT (Doximity, San Francisco, CA) and ChatGPT 3.5 (OpenAI, San Francisco, CA) were utilized to search 20 common perioperative patient inquiries regarding breast augmentation. The structure, content, and readability of responses were compared using t-tests and chi-square tests, with P < 0.05 used as the cutoff for significance. Results: Out of 80 total AI-generated outputs, ChatGPT responses were significantly longer (331 vs. 218 words, P < 0.001). Doximity GPT outputs were structured as a letter from a medical provider to the patient, whereas ChatGPT outputs were a bulleted list. Doximity GPT outputs were significantly more readable by four validated scales: Flesch Kincaid Reading Ease (42.6 vs. 29.9, P < 0.001) and Flesch Kincaid Grade Level (11.4 vs. 14.1 grade, P < 0.001), Coleman-Liau Index (14.9 vs. 17 grade, P < 0.001), and Automated Readability Index (11.3 vs. 14.8 grade, P < 0.001). Regarding content, there was no difference between the two platforms regarding the appropriateness of the topic (99% overall). Medical advice from all outputs was deemed reasonable. Conclusion: Doximity’s AI platform produces reasonable, accurate information in response to common patient queries. With continued reinforcement learning with human feedback (RLHF), Doximity GPT has the potential to be a useful tool to plastic surgeons and can assist with a range of tasks, such as providing basic information on procedures and writing appeal letters to insurance providers. © The Author(s) 2024.",yes,"This study evaluates the performance of two LLMs (Doximity GPT and ChatGPT 3.5) in answering common perioperative patient questions about breast augmentation, focusing on the quality, structure, and readability of their responses. The LLMs are used in a clinical context to provide information to patients, which aligns with the inclusion criteria.",10.20517/ais.2024.53,II,Vivian,"fake data, real task, not a clinical study",,
Collaborative intelligence in AI: Evaluating the performance of a council of AIs on the USMLE,"The variability in responses generated by Large Language Models (LLMs) like OpenAI’s GPT-4 poses challenges in ensuring consistent accuracy on medical knowledge assessments, such as the United States Medical Licensing Exam (USMLE). This study introduces a novel multi-agent framework—referred to as a ""Council of AIs""—to enhance LLM performance through collaborative decision-making. The Council consists of multiple GPT-4 instances that iteratively discuss and reach consensus on answers facilitated by a designated ""Facilitator AI."" This methodology was applied to 325 USMLE questions across Step 1, Step 2 Clinical Knowledge (CK), and Step 3 exams. The Council achieved consensus responses that were correct 97%, 93%, and 94% of the time for Step 1, Step 2CK, and Step 3, respectively, outperforming single-instance GPT-4 models. In cases where there wasn’t an initial unanimous response, the Council of AI deliberations achieved a consensus that was the correct answer 83% of the time. For questions that required deliberation, the Council corrected over half (53%) of responses that majority vote had gotten incorrect. At the end of deliberation, the Council often corrected majority responses that were initially incorrect: the odds of a majority voting response changing from incorrect to correct were 5 (95% CI: 1.1, 22.8) times higher than the odds of changing from correct to incorrect after discussion. We additionally characterized the semantic entropy of the response space for each question and found that deliberations impact entropy of the response space and steadily decrease it, consistently reaching an entropy of zero in all instances. This study showed that in a Council model response variability—often viewed as a limitation—could be leveraged as a strength, enabling adaptive reasoning and collaborative refinement of answers. These findings suggest new paradigms for AI implementation and reveal diversity of responses as a strength in collective decision-making even in medical question scenarios where there is a single correct response.",yes,"This study evaluates the performance of a novel multi-agent framework using multiple instances of GPT-4 (an LLM) to answer USMLE (medical licensing) exam questions, a clinically relevant task. The focus is on clinical knowledge assessment and the improvement of LLM performance in a medical context, which fits the inclusion criteria.",10.1101/2025.02.17.25322388,III,Vivian,"fake data, fake task, not a clinical study",,
An analysis of ChatGPT recommendations for the diagnosis and treatment of cervical radiculopathy,"OBJECTIVE The objective of this study was to assess the safety and accuracy of ChatGPT recommendations in comparison to the evidence-based guidelines from the North American Spine Society (NASS) for the diagnosis and treatment of cervical radiculopathy. METHODS ChatGPT was prompted with questions from the 2011 NASS clinical guidelines for cervical radiculopathy and evaluated for concordance. Selected key phrases within the NASS guidelines were identified. Completeness was measured as the number of overlapping key phrases between ChatGPT responses and NASS guidelines divided by the total number of key phrases. A senior spine surgeon evaluated the ChatGPT responses for safety and accuracy. ChatGPT responses were further evaluated on their readability, similarity, and consistency. Flesch Reading Ease scores and Flesch-Kincaid reading levels were measured to assess readability. The Jaccard Similarity Index was used to assess agreement between ChatGPT responses and NASS clinical guidelines. RESULTS A total of 100 key phrases were identified across 14 NASS clinical guidelines. The mean completeness of ChatGPT-4 was 46%. ChatGPT-3.5 yielded a completeness of 34%. ChatGPT-4 outperformed ChatGPT-3.5 by a margin of 12%. ChatGPT-4.0 outputs had a mean Flesch reading score of 15.24, which is very difficult to read, requiring a college graduate education to understand. ChatGPT-3.5 outputs had a lower mean Flesch reading score of 8.73, indicating that they are even more difficult to read and require a professional education level to do so. However, both versions of ChatGPT were more accessible than NASS guidelines, which had a mean Flesch reading score of 4.58. Furthermore, with NASS guidelines as a reference, ChatGPT-3.5 registered a mean ± SD Jaccard Similarity Index score of 0.20 ± 0.078 while ChatGPT-4 had a mean of 0.18 ± 0.068. Based on physician evaluation, outputs from ChatGPT-3.5 and ChatGPT-4.0 were safe 100% of the time. Thirteen of 14 (92.8%) ChatGPT-3.5 responses and 14 of 14 (100%) ChatGPT-4.0 responses were in agreement with current best clinical practices for cervical radiculopathy according to a senior spine surgeon. CONCLUSIONS ChatGPT models were able to provide safe and accurate but incomplete responses to NASS clinical guideline questions about cervical radiculopathy. Although the authors’ results suggest that improvements are required before ChatGPT can be reliably deployed in a clinical setting, future versions of the LLM hold promise as an updated reference for guidelines on cervical radiculopathy. Future versions must prioritize accessibility and comprehensibility for a diverse audience. ©AANS 2024, except where prohibited by US copyright law.",yes,"This study directly evaluates ChatGPT (GPT-3.5 and GPT-4) for its ability to provide clinical recommendations regarding the diagnosis and treatment of cervical radiculopathy, comparing its outputs to established clinical guidelines and assessing safety and accuracy. This is a clinical evaluation of an LLM in a healthcare context.",10.3171/2024.4.SPINE231148,II,Vivian,"fake data, real task, not a clinical study",,
"Comparative performance analysis of large language models: ChatGPT-3.5, ChatGPT-4 and Google Gemini in glucocorticoid-induced osteoporosis","Backgrounds: The use of large language models (LLMs) in medicine can help physicians improve the quality and effectiveness of health care by increasing the efficiency of medical information management, patient care, medical research, and clinical decision-making. Methods: We collected 34 frequently asked questions about glucocorticoid-induced osteoporosis (GIOP), covering topics related to the disease’s clinical manifestations, pathogenesis, diagnosis, treatment, prevention, and risk factors. We also generated 25 questions based on the 2022 American College of Rheumatology Guideline for the Prevention and Treatment of Glucocorticoid-Induced Osteoporosis (2022 ACR-GIOP Guideline). Each question was posed to the LLM (ChatGPT-3.5, ChatGPT-4, and Google Gemini), and three senior orthopedic surgeons independently rated the responses generated by the LLMs. Three senior orthopedic surgeons independently rated the answers based on responses ranging between 1 and 4 points. A total score (TS) > 9 indicated ‘good’ responses, 6 ≤ TS ≤ 9 indicated ‘moderate’ responses, and TS < 6 indicated ‘poor’ responses. Results: In response to the general questions related to GIOP and the 2022 ACR-GIOP Guidelines, Google Gemini provided more concise answers than the other LLMs. In terms of pathogenesis, ChatGPT-4 had significantly higher total scores (TSs) than ChatGPT-3.5. The TSs for answering questions related to the 2022 ACR-GIOP Guideline by ChatGPT-4 were significantly higher than those for Google Gemini. ChatGPT-3.5 and ChatGPT-4 had significantly higher self-corrected TSs than pre-corrected TSs, while Google Gemini self-corrected for responses that were not significantly different than before. Conclusions: Our study showed that Google Gemini provides more concise and intuitive responses than ChatGPT-3.5 and ChatGPT-4. ChatGPT-4 performed significantly better than ChatGPT3.5 and Google Gemini in terms of answering general questions about GIOP and the 2022 ACR-GIOP Guidelines. ChatGPT3.5 and ChatGPT-4 self-corrected better than Google Gemini. © The Author(s) 2024.",yes,"This study directly evaluates the clinical performance of multiple LLMs (ChatGPT-3.5, ChatGPT-4, Google Gemini) in answering questions about glucocorticoid-induced osteoporosis, including guideline-based and general clinical questions. The responses are rated by clinicians, and the focus is on clinical information and decision support. This fits the inclusion criteria.",10.1186/s13018-024-04996-2,II,Vivian,"fake data, real task, not a clinical study",,
A generalist medical language model for disease diagnosis assistance,"8.

A generalist medical language model for disease diagnosis assistance.

Liu X(#)(1), Liu H(#)(2), Yang G(#)(1), Jiang Z(#)(1), Cui S(#)(3), Zhang Z(2), 
Wang H(2), Tao L(4), Sun Y(5), Song Z(5), Hong T(6), Yang J(6), Gao T(1), Zhang 
J(1), Li X(1), Zhang J(7), Sang Y(7), Yang Z(8), Xue K(9), Wu S(10), Zhang P(1), 
Yang J(11), Song C(12), Wang G(13).

Author information:
(1)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China.
(2)Department of Orthopedics, Peking University Third Hospital & Beijing Key 
Laboratory of Spinal Disease & Engineering Research Center of Bone and Joint 
Precision Medicine, Beijing, China.
(3)School of Science and Engineering (SSE), Future Network of Intelligence 
Institute (FNii) and Guangdong Provincial Key Laboratory of Future Networks of 
Intelligence, Chinese University of Hong Kong, Shenzhen, China.
(4)Research Center of Clinical Epidemiology, Peking University Third Hospital, 
Beijing, China.
(5)Department of Respiratory and Critical Care Medicine, Peking University Third 
Hospital and Research Center for Chronic Airway Diseases, Peking University 
Health Science Center, Beijing, China.
(6)Department of Endocrinology and Metabolism, Peking University Third Hospital, 
Beijing, China.
(7)Department of Cardiology, The First College of Clinical Medical Science, 
China Three Gorges University and Yichang Central People's Hospital, Yichang, 
China.
(8)Peking University First Hospital and Research Center of Public Policy, Peking 
University, Beijing, China.
(9)Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, 
UK.
(10)South China Hospital, Medical School, Shenzhen University, Shenzhen, China.
(11)Department of Cardiology, The First College of Clinical Medical Science, 
China Three Gorges University and Yichang Central People's Hospital, Yichang, 
China. yangjian@ctgu.edu.cn.
(12)Department of Orthopedics, Peking University Third Hospital & Beijing Key 
Laboratory of Spinal Disease & Engineering Research Center of Bone and Joint 
Precision Medicine, Beijing, China. schl@bjmu.edu.cn.
(13)State Key Laboratory of Networking and Switching Technology, Beijing 
University of Posts and Telecommunications, Beijing, China. 
guangyu.wang24@gmail.com.
(#)Contributed equally

The delivery of accurate diagnoses is crucial in healthcare and represents the 
gateway to appropriate and timely treatment. Although recent large language 
models (LLMs) have demonstrated impressive capabilities in few-shot or zero-shot 
learning, their effectiveness in clinical diagnosis remains unproven. Here we 
present MedFound, a generalist medical language model with 176 billion 
parameters, pre-trained on a large-scale corpus derived from diverse medical 
text and real-world clinical records. We further fine-tuned MedFound to learn 
physicians' inferential diagnosis with a self-bootstrapping strategy-based 
chain-of-thought approach and introduced a unified preference alignment 
framework to align it with standard clinical practice. Extensive experiments 
demonstrate that our medical LLM outperforms other baseline LLMs and specialized 
models in in-distribution (common diseases), out-of-distribution (external 
validation) and long-tailed distribution (rare diseases) scenarios across eight 
specialties. Further ablation studies indicate the effectiveness of key 
components in our medical LLM training approach. We conducted a comprehensive 
evaluation of the clinical applicability of LLMs for diagnosis involving 
artificial intelligence (AI) versus physician comparison, AI-assistance study 
and human evaluation framework. Our proposed framework incorporates eight 
clinical evaluation metrics, covering capabilities such as medical record 
summarization, diagnostic reasoning and risk management. Our findings 
demonstrate the model's feasibility in assisting physicians with disease 
diagnosis as part of the clinical workflow.

© 2025. The Author(s), under exclusive licence to Springer Nature America, Inc.",yes,"This study presents and evaluates a large language model (MedFound) specifically designed for disease diagnosis assistance in clinical settings. The abstract describes comprehensive clinical evaluation, including comparison with physicians, AI-assistance studies, and human evaluation frameworks, all focused on clinical diagnostic tasks. This fits the inclusion criteria as it evaluates an LLM in a clinical context.",10.1038/s41591-024-03416-6,I,Vivian,"real data, real task, prospective",,
"Evaluating Large Language Models for the National Premedical Exam in India: Comparative Analysis of GPT-3.5, GPT-4, and Bard","BACKGROUND: Large language models (LLMs) have revolutionized natural language 
processing with their ability to generate human-like text through extensive 
training on large data sets. These models, including Generative Pre-trained 
Transformers (GPT)-3.5 (OpenAI), GPT-4 (OpenAI), and Bard (Google LLC), find 
applications beyond natural language processing, attracting interest from 
academia and industry. Students are actively leveraging LLMs to enhance learning 
experiences and prepare for high-stakes exams, such as the National Eligibility 
cum Entrance Test (NEET) in India.
OBJECTIVE: This comparative analysis aims to evaluate the performance of 
GPT-3.5, GPT-4, and Bard in answering NEET-2023 questions.
METHODS: In this paper, we evaluated the performance of the 3 mainstream LLMs, 
namely GPT-3.5, GPT-4, and Google Bard, in answering questions related to the 
NEET-2023 exam. The questions of the NEET were provided to these artificial 
intelligence models, and the responses were recorded and compared against the 
correct answers from the official answer key. Consensus was used to evaluate the 
performance of all 3 models.
RESULTS: It was evident that GPT-4 passed the entrance test with flying colors 
(300/700, 42.9%), showcasing exceptional performance. On the other hand, GPT-3.5 
managed to meet the qualifying criteria, but with a substantially lower score 
(145/700, 20.7%). However, Bard (115/700, 16.4%) failed to meet the qualifying 
criteria and did not pass the test. GPT-4 demonstrated consistent superiority 
over Bard and GPT-3.5 in all 3 subjects. Specifically, GPT-4 achieved accuracy 
rates of 73% (29/40) in physics, 44% (16/36) in chemistry, and 51% (50/99) in 
biology. Conversely, GPT-3.5 attained an accuracy rate of 45% (18/40) in 
physics, 33% (13/26) in chemistry, and 34% (34/99) in biology. The accuracy 
consensus metric showed that the matching responses between GPT-4 and Bard, as 
well as GPT-4 and GPT-3.5, had higher incidences of being correct, at 0.56 and 
0.57, respectively, compared to the matching responses between Bard and GPT-3.5, 
which stood at 0.42. When all 3 models were considered together, their matching 
responses reached the highest accuracy consensus of 0.59.
CONCLUSIONS: The study's findings provide valuable insights into the performance 
of GPT-3.5, GPT-4, and Bard in answering NEET-2023 questions. GPT-4 emerged as 
the most accurate model, highlighting its potential for educational 
applications. Cross-checking responses across models may result in confusion as 
the compared models (as duos or a trio) tend to agree on only a little over half 
of the correct responses. Using GPT-4 as one of the compared models will result 
in higher accuracy consensus. The results underscore the suitability of LLMs for 
high-stakes exams and their positive impact on education. Additionally, the 
study establishes a benchmark for evaluating and enhancing LLMs' performance in 
educational tasks, promoting responsible and informed use of these models in 
diverse learning environments.

©Faiza Farhat, Beenish Moalla Chaudhry, Mohammad Nadeem, Shahab Saquib Sohail, 
Dag Øivind Madsen. Originally published in JMIR Medical Education 
(https://mededu.jmir.org), 21.02.2024.",yes,"This study evaluates the performance of LLMs (GPT-3.5, GPT-4, Bard) on the NEET-2023, a premedical entrance exam. While the exam is related to medicine, the study focuses on educational performance (exam question answering) rather than clinical application or decision-making. According to the criteria, studies evaluating LLMs on medical board or licensing exams are included, as these are relevant to clinical knowledge assessment.",10.2196/51523,III,Vivian,"fake data, fake task, not a clinical study",,
Evaluating ChatGPT's Multilingual Performance in Clinical Nutrition Advice Using Synthetic Medical Text: Insights from Central Asia,"Background: Although large language models like ChatGPT-4 have demonstrated competency in English, their performance for minority groups speaking underrepresented languages, as well as their ability to adapt to specific sociocultural nuances and regional cuisines, such as those in Central Asia (for example, Kazakhstan), still requires further investigation. Objectives: To evaluate and compare the effectiveness of the ChatGPT-4 system in providing personalized, evidence-based nutritional recommendations in English, Kazakh, and Russian in Central Asia. Methods: This study was conducted from 15 May to 31 August, 2023. On the basis of 50 mock patient profiles, ChatGPT-4 generated dietary advice, and responses were evaluated for personalization, consistency, and practicality using a 5-point Likert scale. To identify significant differences between the 3 languages, the Kruskal–Wallis test was conducted. Additional pairwise comparisons for each language were carried out using the post hoc Dunn's test. Results: ChatGPT-4 showed a moderate level of performance in each category for English and Russian languages, whereas in Kazakh language, outputs were unsuitable for evaluation. The scores for English, Russian, and Kazakh were as follows: for personalization, 3.32 ± 0.46, 3.18 ± 0.38, and 1.01 ± 0.06; for consistency, 3.48 ± 0.43, 3.38 ± 0.39, and 1.09 ± 0.18; and for practicality, 3.25 ± 0.41, 3.37 ± 0.38, and 1.07 ± 0.15, respectively. The Kruskal–Wallis test indicated statistically significant differences in ChatGPT-4's performance across the 3 languages (P < 0.001). Subsequent post hoc analysis using Dunn's test showed that the performance in both English and Russian was significantly different from that in Kazakh. Conclusions: Our findings show that, despite using identical prompts across 3 distinct languages, the ChatGPT-4's capability to produce sensible outputs is limited by the lack of training data in non-English languages. Thus, a customized large language model should be developed to perform better in underrepresented languages and to take into account specific local diets and practices. © 2024 The Authors",yes,"This study evaluates ChatGPT-4's ability to provide personalized, evidence-based clinical nutrition advice in multiple languages using mock patient profiles. The focus is on the clinical application of LLMs (providing dietary recommendations) and includes a direct assessment of the model's performance in a healthcare context.",10.1016/j.tjnut.2024.12.018,II,Vivian,"fake data, real task, not a clinical study",,
Conversation between a clinical biologist and an artificial intelligence on prostate cancer biomarkers: a critical reading,"3483. Ann Biol Clin (Paris). 2023 Oct 20;81(4):395-402. doi: 10.1684/abc.2023.1829.

Conversation between a clinical biologist and an artificial intelligence on 
prostate cancer biomarkers: a critical reading.

Lamy PJ(1).

Author information:
(1)Biopathalogie et génétique des cancers, imagenome, Inovie, Montpellier, 
France, Unité de recherche clinique, Clinique BeauSoeil, Montpellier, France.

Artificial intelligence is increasingly used in the field of medicine as a 
diagnostic aid, particularly for image analysis and more generally for data 
processing. Many artificial intelligence-based tools have been specifically 
developed for clinical biology, but some more general ones can help to improve 
the dissemination of medical knowledge. To test whether and to what extent an 
automated conversation tool could answer questions on a clinical biology topic 
(i.e. prostate cancer biomarkers), we questioned ChatGPT, an artificial 
intelligence-powered model dedicated to optimize language models for dialogue. 
Then we analyzed its responses.",yes,"The study evaluates ChatGPT, a generative LLM, by having it answer questions about prostate cancer biomarkers and analyzing its responses. This is a clinical application, as it assesses the LLM's ability to provide medical information relevant to patient care.",10.1684/abc.2023.1829,II,Vivian,"fake data, real task, not a clinical study",,
Artificial intelligence-based chatbot assistance in clinical decision-making for medically complex patients in oral surgery: a comparative study,"AIM: This study aims to evaluate the potential of AI-based chatbots in assisting with clinical decision-making in the management of medically complex patients in oral surgery. MATERIALS AND METHODS: A team of oral and maxillofacial surgeons developed a pool of open-ended questions de novo. The validity of the questions was assessed using Lawshe's Content Validity Index. The questions, which focused on systemic diseases and common conditions that may raise concerns during oral surgery, were presented to ChatGPT 3.5 and Claude-instant in two separate sessions, spaced one week apart. Two experienced maxillofacial surgeons, blinded to the chatbots, assessed the responses for quality, accuracy, and completeness using a modified DISCERN tool and Likert scale. Intraclass correlation, Mann-Whitney U test, skewness, and kurtosis coefficients were employed to compare the performances of the chatbots. RESULTS: Most responses were high quality: 86% and 79.6% for ChatGPT, and 81.25% and 89% for Claude-instant in sessions 1 and 2, respectively. In terms of accuracy, ChatGPT had 92% and 93.4% of its responses rated as completely correct in sessions 1 and 2, respectively, while Claude-instant had 95.2% and 89%. For completeness, ChatGPT had 88.5% and 86.8% of its responses rated as adequate or comprehensive in sessions 1 and 2, respectively, while Claude-instant had 95.2% and 86%. CONCLUSION: Ongoing software developments and the increasing acceptance of chatbots among healthcare professionals hold promise that these tools can provide rapid solutions to the high demand for medical care, ease professionals' workload, reduce costs, and save time.",yes,"This study directly evaluates the performance of generative LLMs (ChatGPT 3.5 and Claude-instant) in assisting with clinical decision-making for medically complex patients in oral surgery. The LLMs are tested on open-ended clinical questions, and their responses are assessed for quality, accuracy, and completeness by clinicians. This fits the inclusion criteria as it is a clinical evaluation of LLMs in a healthcare context.",10.1186/s12903-025-05732-w,II,Vivian,"fake data, real task, not a clinical study",,
Large-language-model-based 10-year risk prediction of cardiovascular disease: insight from the UK biobank data,"Background Conventional cardiovascular risk prediction models provide insights into population-level risk factors and have been widely adopted in clinical practice. However, these models have limited generalizability and flexibility. Large language models (LLMs) have demonstrated remarkable proficiency for use in various industries. Methods In this study, we have investigated the feasibility of Large Language Models (LLMs) such as ChatGPT-3.5, ChatGPT-4, and Bard for predicting 10-year cardiovascular risk of a patient. We used data from the UK Biobank Cohort, a major biomedical database in the UK, and the Korean Genome and Epidemiology Study (KoGES), a large-scale prospective study in Korea, for additional validation and multi-institutional research. These databases provided a wide array of information including age, sex, medical history, lipid profile, blood pressure, and physical measurement. Based on these data, we generated language sentences for individual analysis and input these into the LLM to derive results. The performance of the LLMs was then compared with the Framingham Risk Score (FRS), a conventional risk prediction model, using this real-world data. We confirmed the model performance of both the LLMs and FRS, evaluating their accuracy, sensitivity, specificity, Positive Predictive Value (PPV), Negative Predictive Value (NPV), and F1 score. Their performance in predicting 10-year cardiovascular risk was compared through Kaplan-Meier survival analysis and Cox-hazard ratio analysis. Findings GPT-4 achieved performance comparable to the FRS in cardiovascular risk prediction in both the UK Biobank {accuracy (0·834 vs· 0·773) and F1 score (0·138 vs· 0·132)} and KoGES {accuracy (0·902 vs· 0·874)}. The Kaplan–Meier survival analysis of GPT-4 demonstrated distinct survival patterns among groups, which revealed a strong association between the GPT risk prediction output and survival outcomes. The additional analysis of limited variables using GPT-3·5 indicated that ChatGPT’s prediction performance was preserved despite the omission of a few variables in the prompt, especially without physical measurement data Interpretation This study proposed that ChatGPT can achieve performance comparable to conventional models in predicting cardiovascular risk. Furthermore, ChatGPT exhibits enhanced accessibility, flexibility, and the ability to provide user-friendly outputs. With the evolution of LLMs, such as ChatGPT, studies should focus on applying LLMs to various medical scenarios and subsequently optimizing their performance. Funding This research was supported by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HI22C0452).",yes,"This study evaluates the use of LLMs (ChatGPT-3.5, ChatGPT-4, Bard) for predicting 10-year cardiovascular risk using real patient data, comparing their performance to a conventional clinical risk model (Framingham Risk Score). The LLMs are applied directly to a clinical prediction task, and their outputs are assessed for clinical validity. This fits the inclusion criteria as it is a clinical evaluation of LLMs in a healthcare context.",10.1101/2023.05.22.23289842,I,Vivian,"real data, real task, prospective",,
ChatGPT Performs on the Chinese National Medical Licensing Examination,"ChatGPT, a language model developed by OpenAI, uses a 175 billion parameter Transformer architecture for natural language processing tasks. This study aimed to compare the knowledge and interpretation ability of ChatGPT with those of medical students in China by administering the Chinese National Medical Licensing Examination (NMLE) to both ChatGPT and medical students. We evaluated the performance of ChatGPT in three years' worth of the NMLE, which consists of four units. At the same time, the exam results were compared to those of medical students who had studied for five years at medical colleges. ChatGPT’s performance was lower than that of the medical students, and ChatGPT’s correct answer rate was related to the year in which the exam questions were released. ChatGPT’s knowledge and interpretation ability for the NMLE were not yet comparable to those of medical students in China. It is probable that these abilities will improve through deep learning. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",yes,"This study evaluates ChatGPT's performance on the Chinese National Medical Licensing Examination, directly assessing its clinical knowledge and interpretation abilities in a medical context. This fits the inclusion criteria as it tests an LLM on a clinically relevant exam.",10.1007/s10916-023-01961-0,III,Vivian,"fake data, fake task, not a clinical study",,
"Assessment of Challenging Oncologic Cases: A Comparative Analysis Between ChatGPT, Gemini, and a Multidisciplinary Tumor Board","Introduction: Since its introduction in 2022, public-access conversational AI, exemplified by ChatGPT and Gemini, has been increasingly utilized in medical decision-making, though its impact is questionable. This study aims to evaluate its efficacy in assessing complex oncologic cases compared to a multidisciplinary tumor board (MTB) comprising experts from various specialties. Methods: A 2-year retrospective analysis was conducted on 98 oncological cases at a reference medical center in Mexico City. A MTB comprising surgical oncologists, medical oncologists, radio-oncologists, pathologists, among others, reviewed and discussed each case to determine management strategies. We evaluated four key decision points, dichotomized as either affirmative or negative: the need for new imaging studies, radiation therapy, chemotherapy, and surgery. Comprehensive medical documentation accompanied each case. We then compared AI's decisions with those of the MTB using the same criteria and conducted a Cohen's Kappa test to assess agreement. Results: Agreement between ChatGPT (4o) and Gemini (1.5 Flash), and the MTB ranged from none to slight for additional imaging studies (Gemini: κ = 0.100, p = 0.087; ChatGPT 4o: κ = 0.024, p = 0.592) and chemotherapy (Gemini: κ = 0.089, p = 0.316; ChatGPT 4o: κ = 0.336, p = 0.001). Moderate agreement was observed for decisions regarding surgery (Gemini: κ = 0.194, p = 0.046; ChatGPT 4o: κ = 0.467, p = < 0.001) and radiotherapy (Gemini: κ = 0.214, p = 0.012; ChatGPT 4o: κ = 0.525, p = < 0.001). Conclusions: Both ChatGPT and Gemini showed moderate agreement with the multidisciplinary tumor board on decisions regarding surgery and radiotherapy. ChatGPT also showed moderate agreement in chemotherapy, but further assessment is needed for other interventions. ChatGPT proved to be superior to Gemini in most key points. The potential of these public access AI in oncology warrants continued exploration to refine its utility in clinical practice. © 2025 Wiley Periodicals LLC.",yes,"This study directly evaluates the clinical decision-making performance of two generative LLMs (ChatGPT-4o and Gemini 1.5 Flash) in complex oncologic cases, comparing their recommendations to those of a multidisciplinary tumor board. The focus is on key clinical management decisions (imaging, chemotherapy, surgery, radiotherapy), which is a clear clinical application. Both LLMs are frontier models, and the study is not a review, editorial, or technical report.",10.1002/jso.28121,I,Vivian,"real data, real task, retrospective",,
Examining the role of ChatGPT in the management of distal radius fractures: insights into its accuracy and consistency,"Background: The optimal management of distal radius fractures remains a challenge for orthopaedic surgeons. The emergence of Artificial Intelligence (AI) and Large Language Models (LLMs), especially ChatGPT, affords significant potential in improving healthcare and research. This study aims to assess the accuracy and consistency of ChatGPT's knowledge in managing distal radius fractures, with a focus on its capability to provide information for patients and assist in the decision-making processes of orthopaedic clinicians. Methods: We presented ChatGPT with seven questions on distal radius fracture management over two sessions, resulting in 14 responses. These questions covered a range of topics, including patient inquiries and orthopaedic clinical decision-making. We requested references for each response and involved two orthopaedic registrars and two senior orthopaedic surgeons to evaluate response accuracy and consistency. Results: All 14 responses contained a mix of both correct and incorrect information. Among the 47 cited references, 13% were accurate, 28% appeared to be fabricated, 57% were incorrect, and 2% were correct but deemed inappropriate. Consistency was observed in 71% of the responses. Conclusion: ChatGPT demonstrates significant limitations in accuracy and consistency when providing information on distal radius fractures. In its current format, it offers limited utility for patient education and clinical decision-making. © 2024 Royal Australasian College of Surgeons.",yes,"This study directly evaluates ChatGPT's performance in answering patient questions and assisting clinicians in the management of distal radius fractures, including clinical decision-making. It assesses the accuracy and consistency of ChatGPT's responses in a clinical context.",10.1111/ans.19143,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT is an Unreliable Source of Peer-Reviewed Information for Common Total Knee and Hip Arthroplasty Patient Questions,"Background: Advances in artificial intelligence (AI), machine learning, and publicly accessible language model tools such as ChatGPT-3.5 continue to shape the landscape of modern medicine and patient education. ChatGPT’s open access (OA), instant, human-sounding interface capable of carrying discussion on myriad topics makes it a potentially useful resource for patients seeking medical advice. As it pertains to orthopedic surgery, ChatGPT may become a source to answer common preoperative questions regarding total knee arthroplasty (TKA) and total hip arthroplasty (THA). Since ChatGPT can utilize the peer-reviewed literature to source its responses, this study seeks to characterize the validity of its responses to common TKA and THA questions and characterize the peer-reviewed literature that it uses to formulate its responses. Methods: Preoperative TKA and THA questions were formulated by fellowship-trained adult reconstruction surgeons based on common questions posed by patients in the clinical setting. Questions were inputted into ChatGPT with the initial request of using solely the peer-reviewed literature to generate its responses. The validity of each response was rated on a Likert scale by the fellowship-trained surgeons, and the sources utilized were characterized in terms of accuracy of comparison to existing publications, publication date, study design, level of evidence, journal of publication, journal impact factor based on the clarivate analytics factor tool, journal OA status, and whether the journal is based in the United States. Results: A total of 109 sources were cited by ChatGPT in its answers to 17 questions regarding TKA procedures and 16 THA procedures. Thirty-nine sources (36%) were deemed accurate or able to be directly traced to an existing publication. Of these, seven (18%) were identified as duplicates, yielding a total of 32 unique sources that were identified as accurate and further characterized. The most common characteristics of these sources included dates of publication between 2011 and 2015 (10), publication in The Journal of Bone and Joint Surgery (13), journal impact factors between 5.1 and 10.0 (17), internationally based journals (17), and journals that are not OA (28). The most common study designs were retrospective cohort studies and case series (seven each). The level of evidence was broadly distributed between Levels I, III, and IV (seven each). The averages for the Likert scales for medical accuracy and completeness were 4.4/6 and 1.92/3, respectively. Conclusions: Investigation into ChatGPT’s response quality and use of peer-reviewed sources when prompted with archetypal pre-TKA and pre-THA questions found ChatGPT to provide mostly reliable responses based on fellowship-trained orthopedic surgeon review of 4.4/6 for accuracy and 1.92/3 for completeness despite a 64.22% rate of citing inaccurate references. This study suggests that until ChatGPT is proven to be a reliable source of valid information and references, patients must exercise extreme caution in directing their pre-TKA and THA questions to this medium. Copyright © 2025 Jonathan D. Schwartzman et al. Advances in Orthopedics published by John Wiley & Sons Ltd.",yes,"This study evaluates ChatGPT-3.5's ability to answer common preoperative patient questions about total knee and hip arthroplasty, focusing on the validity and accuracy of its responses and cited sources. The evaluation is clinical in nature, as it assesses the LLM's performance in providing patient-facing medical information relevant to orthopedic surgery.",10.1155/aort/5534704,II,Vivian,"fake data, real task, not a clinical study",,
Evaluating the comprehension and accuracy of ChatGPT's responses to diabetes-related questions in Urdu compared to English,"Introduction: Patients with diabetes require healthcare and information that are accurate and extensive. Large language models (LLMs) like ChatGPT herald the capacity to provide such exhaustive data. To determine (a) the comprehensiveness of ChatGPT's responses in Urdu to diabetes-related questions and (b) the accuracy of ChatGPT's Urdu responses when compared to its English responses. Methods: A cross-sectional observational study was conducted. Two reviewers experienced in internal medicine and endocrinology graded 53 Urdu and English responses on diabetes knowledge, lifestyle, and prevention. A senior reviewer resolved discrepancies. Responses were assessed for comprehension and accuracy, then compared to English. Results: Among the Urdu responses generated, only two of 53 (3.8%) questions were graded as comprehensive, and five of 53 (9.4%) were graded as correct but inadequate. We found that 25 of 53 (47.2%) questions were graded as mixed with correct and incorrect/outdated data, the most significant proportion of responses being graded as such. When considering the comparison of response scale grading the comparative accuracy of Urdu and English responses, no Urdu response (0.0%) was considered to have more accuracy than English. Most of the Urdu responses were found to have an accuracy less than that of English, an overwhelming majority of 49 of 53 (92.5%) responses. Conclusion: We found that although the ability to retrieve such information about diabetes is impressive, it can merely be used as an adjunct instead of a solitary source of information. Further work must be done to optimize Urdu responses in medical contexts to approximate the boundless potential it heralds. © The Author(s) 2024.",yes,"This study evaluates ChatGPT's ability to answer diabetes-related clinical questions in both Urdu and English, focusing on the accuracy and comprehensiveness of its responses. The evaluation is directly related to the clinical application of an LLM in providing patient information and support, which fits the inclusion criteria.",10.1177/20552076241289730,II,Vivian,"fake data, real task, not a clinical study",,
Assessment of the Responses of the Artificial Intelligence-based Chatbot ChatGPT-4 to Frequently Asked Questions About Amblyopia and Childhood Myopia,"2251. J Pediatr Ophthalmol Strabismus. 2024 Mar-Apr;61(2):86-89. doi: 
10.3928/01913913-20231005-02. Epub 2023 Oct 26.

Assessment of the Responses of the Artificial Intelligence-based Chatbot 
ChatGPT-4 to Frequently Asked Questions About Amblyopia and Childhood Myopia.

Nikdel M, Ghadimi H, Tavakoli M, Suh DW.

PURPOSE: To assess the responses of the ChatGPT-4, the forerunner artificial 
intelligence-based chatbot, to frequently asked questions regarding two common 
pediatric ophthalmologic disorders, amblyopia and childhood myopia.
METHODS: Twenty-seven questions about amblyopia and 28 questions about childhood 
myopia were asked of the ChatGPT twice (totally 110 questions). The responses 
were evaluated by two pediatric ophthalmologists as acceptable, incomplete, or 
unacceptable.
RESULTS: There was remarkable agreement (96.4%) between the two pediatric 
ophthalmologists on their assessment of the responses. Acceptable responses were 
provided by the ChatGPT to 93 of 110 (84.6%) questions in total (44 of 54 
[81.5%] for amblyopia and 49 of 56 [87.5%] questions for childhood myopia). 
Seven of 54 (12.9%) responses to questions on amblyopia were graded as 
incomplete compared to 4 of 56 (7.1%) of questions on childhood myopia. The 
ChatGPT gave inappropriate responses to three questions about amblyopia (5.6%) 
and childhood myopia (5.4%). The most noticeable inappropriate responses were 
related to the definition of reverse amblyopia and the threshold of refractive 
error for prescription of spectacles to children with myopia.
CONCLUSIONS: The ChatGPT has the potential to serve as an adjunct informational 
tool for pediatric ophthalmology patients and their caregivers by demonstrating 
a relatively good performance in answering 84.6% of the most frequently asked 
questions about amblyopia and childhood myopia. [J Pediatr Ophthalmol 
Strabismus. 2024;61(2):86-89.].",yes,"This study evaluates ChatGPT-4's ability to answer frequently asked clinical questions about amblyopia and childhood myopia, with responses assessed by pediatric ophthalmologists. It directly tests the LLM's clinical informational performance for patient/caregiver questions in a healthcare context.",10.3928/01913913-20231005-02,II,Vivian,"fake data, real task, not a clinical study",,
Large language models management of complex medication regimens: a case-based evaluation,"Background: Large language models (LLMs) have shown capability in diagnosing complex medical cases and passing medical licensing exams, but to date, only limited evaluations have studied how LLMs interpret, analyze, and optimize complex medication regimens. The purpose of this evaluation was to test four LLMs ability to identify medication errors and appropriate medication interventions on complex patient cases from the intensive care unit (ICU). Methods: A series of eight patient cases were developed by critical care pharmacists including history of present illness, laboratory values, vital signs, and medication regimens. Then, four LLMs (ChatGPT (GPT-3.5), ChatGPT (GPT-4), Claude2, and Llama2-7b) were prompted to develop a medication regimen for the patient. LLM generated medication regimens were then reviewed by a panel of seven critical care pharmacists to assess for presence of medication errors and clinical relevance. For each medication regimen recommended by the LLM, clinicians were asked to assess for if they would continue a medication, identify perceived medication errors in the medications recommended, identify the presence of life-threatening medication choices, and rank overall agreement on a 5-point Likert scale. Results: The clinician panel rated to continue therapies recommended by the LLMs between 55.8-67.9% of the time. Clinicians perceived between 1.57-4.29 medication errors per recommended regimen, and life-threatening recommendations were present between 15.0-55.3% of the time. Level agreement was between 1.85-2.67 for the four LLMs. Conclusions: LLMs demonstrated potential to serve as clinical decision support for the management of complex medication regimens with further domain specific training; however, caution should be used when employing LLMs for medication management given the present capabilities.",yes,"This study directly evaluates multiple LLMs (ChatGPT, Claude2, Llama2) in a clinical context—specifically, their ability to manage complex medication regimens for ICU patients. The LLMs' recommendations are assessed by clinicians for medication errors and clinical relevance, which is a clear clinical application.",10.1101/2024.07.03.24309889,II,Vivian,"fake data, real task, not a clinical study",,
Large language model answers medical questions about standard pathology reports,"This study aims to evaluate the feasibility of large language model (LLM) in answering pathology questions based on pathology reports (PRs) of colorectal cancer (CRC). Four common questions (CQs) and corresponding answers about pathology were retrieved from public webpages. These questions were input as prompts for Chat Generative Pretrained Transformer (ChatGPT) (gpt-3.5-turbo). The quality indicators (understanding, scientificity, satisfaction) of all answers were evaluated by gastroenterologists. Standard PRs from 5 CRC patients who received radical surgeries in Shanghai Changzheng Hospital were selected. Six report questions (RQs) and corresponding answers were generated by a gastroenterologist and a pathologist. We developed an interactive PRs interpretation system which allows users to upload standard PRs as JPG images. Then the ChatGPT's responses to the RQs were generated. The quality indicators of all answers were evaluated by gastroenterologists and out-patients. As for CQs, gastroenterologists rated AI answers similarly to non-AI answers in understanding, scientificity, and satisfaction. As for RQ1-3, gastroenterologists and patients rated the AI mean scores higher than non-AI scores among the quality indicators. However, as for RQ4-6, gastroenterologists rated the AI mean scores lower than non-AI scores in understanding and satisfaction. In RQ4, gastroenterologists rated the AI scores lower than non-AI scores in scientificity (P = 0.011); patients rated the AI scores lower than non-AI scores in understanding (P = 0.004) and satisfaction (P = 0.011). In conclusion, LLM could generate credible answers to common pathology questions and conceptual questions on the PRs. It holds great potential in improving doctor-patient communication. Copyright © 2024 Wang, Zhou, Zhang, Cao, Xin, Xu and Zhou.",yes,"This study evaluates ChatGPT (gpt-3.5-turbo) in answering clinical questions about pathology reports for colorectal cancer, with responses assessed by clinicians and patients. The focus is on the LLM's ability to interpret and communicate clinical information from real patient pathology reports, which is a direct clinical application.",10.3389/fmed.2024.1402457,II,Vivian,"fake data, real task, not a clinical study",,
Artificial intelligence for image recognition in diagnosing oral and oropharyngeal cancer and leukoplakia,"Visual diagnosis is one of the key features of squamous cell carcinoma of the oral cavity (OSCC) and oropharynx (OPSCC), both subsets of head and neck squamous cell carcinoma (HNSCC) with a heterogeneous clinical appearance. Advancements in artificial intelligence led to Image recognition being introduced recently into large language models (LLMs) such as ChatGPT 4.0. This exploratory study, for the first time, evaluated the application of image recognition by ChatGPT to diagnose squamous cell carcinoma and leukoplakia based on clinical images, with images without any lesion as a control group. A total of 45 clinical images were analyzed, comprising 15 cases each of SCC, leukoplakia, and non-lesion images. ChatGPT 4.0 was tasked with providing the most likely diagnosis based on these images in scenario one. In scenario two the image and the clinical history were provided, whereas in scenario three only the clinical history was given. The results and the accuracy of the LLM were rated by two independent reviewers and the overall performance was evaluated using the modified Artificial Intelligence Performance Index (AIPI. In this study, ChatGPT 4.0 demonstrated the ability to correctly identify leukoplakia cases using image recognition alone, while the ability to diagnose SCC was insufficient, but improved by including the clinical history in the prompt. Providing only the clinical history resulted in a misclassification of most leukoplakia and some SCC cases. Oral cavity lesions were more likely to be diagnosed correctly. In this exploratory study of 45 images of oral lesions, ChatGPT 4.0 demonstrated a convincing performance for detecting SCC only when the clinical history was added, whereas Leukoplakia was detected solely by image recognition. ChatGPT is therefore currently insufficient for reliable OPSCC and OSCC diagnosis, but further technological advancements may pave the way for the use in the clinical setting.",yes,"This study evaluates ChatGPT 4.0's ability to diagnose oral and oropharyngeal cancer and leukoplakia using image recognition and clinical history, directly assessing its clinical diagnostic performance. The use of a frontier LLM (ChatGPT 4.0) in a clinical diagnostic context meets the inclusion criteria.",10.1038/s41598-025-85920-4,I,Vivian,"real data, real task, retrospective",,
Comparative Performance of Claude and GPT Models in Basic Radiological Imaging Tasks,"Background: Publicly available artificial intelligence (AI) Visual Language Models (VLMs) are constantly improving. The advent of vision capabilities on these models could enhance workflows in radiology. Evaluating their performance in radiological image interpretation is vital to their potential integration into practice. Aim: This study aims to evaluate the proficiency and consistency of the publicly available VLMs, Claude and GPT, across multiple iterations in basic image interpretation tasks. Method: Subsets from publicly available datasets, ROCOv2 and MURAv1.1, were used to evaluate 6 VLMs. A system prompt and image were inputted into each model thrice. The outputs were compared to the dataset captions to evaluate each model's accuracy in recognising the modality, anatomy, and detecting fractures on radiographs. The consistency of the output across iterations was also analysed. Results: Evaluation of the ROCOv2 dataset showed high accuracy in modality recognition, with some models achieving 100%. Anatomical recognition ranged between 61% and 85% accuracy across all models tested. On the MURAv1.1 dataset, Claude-3.5-Sonnet had the highest anatomical recognition with 57% accuracy, while GPT-4o had the best fracture detection with 62% accuracy. Claude-3.5-Sonnet was the most consistent model, with 83% and 92% consistency in anatomy and fracture detection, respectively. Conclusion: Given Claude and GPT's current accuracy and reliability, integration of these models into clinical settings is not yet feasible. This study highlights the need for ongoing development and establishment of standardised testing techniques to ensure these models achieve reliable performance.",yes,"This study evaluates the clinical performance of generative LLMs (Claude and GPT models) with vision capabilities on radiological image interpretation tasks, including modality recognition, anatomical recognition, and fracture detection. The focus is on clinical image interpretation, which is a direct clinical application. The models assessed (Claude, GPT-4o) are frontier LLMs with vision, and the study is not a review or opinion piece.",10.1101/2024.11.16.24317414,II,Vivian,"fake data, real task, not a clinical study",,
Can Patients Rely on ChatGPT to Answer Hand Pathology–Related Medical Questions?,"Background: In recent years, ChatGPT has become a popular source of information online. Physicians need to be aware of the resources their patients are using to self-inform of their conditions. This study investigates physician-graded accuracy and completeness of ChatGPT regarding various questions patients are likely to ask the artificial intelligence (AI) system concerning common upper limb orthopedic conditions. Methods: ChatGPT 3.5 was interrogated concerning 5 common orthopedic hand conditions: carpal tunnel syndrome, Dupuytren contracture, De Quervain tenosynovitis, trigger finger, and carpal metacarpal arthritis. Questions evaluated conditions’ symptoms, pathology, management, surgical indications, recovery time, insurance coverage, and workers’ compensation possibility. Each topic had 12 to 15 questions and was established as its own ChatGPT conversation. All questions regarding the same diagnosis were presented to the AI, and its answers were recorded. Each question was then graded for both accuracy (Likert scale of 1-6) and completeness (Likert scale of 1-3) by 10 fellowship trained hand surgeons. Descriptive statistics were performed. Results: Overall, the mean accuracy score for ChatGPT’s answers to common orthopedic hand diagnoses was 4.83 out of 6 ± 0.95. The mean completeness of answers was 2 out of 3 ± 0.59. Conclusions: Easily accessible online AI such as ChatGPT is becoming more advanced and thus more reliable in its ability to answer common medical questions. Physicians can anticipate such online resources being mostly correct, however incomplete. Patients should beware of relying on such resources in isolation. © The Author(s) 2024.",yes,"This study evaluates ChatGPT-3.5's ability to answer patient questions about common hand pathology conditions, with physician grading of accuracy and completeness. It directly assesses the clinical utility of an LLM in providing medical information to patients.",10.1177/15589447241247246,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT for Addressing Patient-centered Frequently Asked Questions in Glaucoma Clinical Practice,"Purpose: Large language models such as ChatGPT-3.5 are often used by the public to answer questions related to daily life, including health advice. This study evaluated the responses of ChatGPT-3.5 in answering patient-centered frequently asked questions (FAQs) relevant in glaucoma clinical practice. Design: Prospective cross-sectional survey. Participants: Expert graders. Methods: Twelve experts across a range of clinical, education, and research practices in optometry and ophthalmology. Over 200 patient-centric FAQs from authoritative professional society, hospital and advocacy websites were distilled and filtered into 40 questions across 4 themes: definition and risk factors, diagnosis and testing, lifestyle and other accompanying conditions, and treatment and follow-up. The questions were individually input into ChatGPT-3.5 to generate responses. The responses were graded by the 12 experts individually. Main Outcome Measures: A 5-point Likert scale (1 = strongly disagree; 5 = strongly agree) was used to grade ChatGPT-3.5 responses across 4 domains: coherency, factuality, comprehensiveness, and safety. Results: Across all themes and domains, median scores were all 4 (“agree”). Comprehensiveness had the lowest scores across domains (mean 3.7 ± 0.9), followed by factuality (mean 3.9 ± 0.9) and coherency and safety (mean 4.1 ± 0.8 for both). Examination of the individual 40 questions showed that 8 (20%), 17 (42.5%), 24 (60%), and 8 (20%) of the questions had average scores below 4 (i.e., below “agree”) for the coherency, factuality, comprehensiveness, and safety domains, respectively. Free-text comments by the experts highlighted omissions of facts and comprehensiveness (e.g., secondary glaucoma) and remarked on the vagueness of some responses (i.e., that the response did not account for individual patient circumstances). Conclusions: ChatGPT-3.5 responses to FAQs in glaucoma were generally agreeable in terms of coherency, factuality, comprehensiveness, and safety. However, areas of weakness were identified, precluding recommendations for routine use to provide patients with tailored counseling in glaucoma, especially with respect to development of glaucoma and its management. Financial Disclosure(s): Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article. © 2024 American Academy of Ophthalmology",yes,"This study evaluates ChatGPT-3.5's ability to answer patient-centered clinical questions about glaucoma, with expert grading of its responses on clinical domains such as coherency, factuality, comprehensiveness, and safety. The focus is on clinical information provision to patients, which is a direct clinical application of an LLM.",10.1016/j.ogla.2024.10.005,I,Vivian,"real data, real task, prospective",,
"Comparison of the Audiological Knowledge of Three Chatbots: ChatGPT, Bing Chat, and Bard","Introduction: The purpose of this study was to evaluate three chatbots - OpenAI ChatGPT, Microsoft Bing Chat (currently Copilot), and Google Bard (currently Gemini) - in terms of their responses to a defined set of audiological questions. Methods: Each chatbot was presented with the same 10 questions. The authors rated the responses on a Likert scale ranging from 1 to 5. Additional features, such as the number of inaccuracies or errors and the provision of references, were also examined. Results: Most responses given by all three chatbots were rated as satisfactory or better. However, all chatbots generated at least a few errors or inaccuracies. ChatGPT achieved the highest overall score, while Bard was the worst. Bard was also the only chatbot unable to provide a response to one of the questions. ChatGPT was the only chatbot that did not provide information about its sources. Conclusions: Chatbots are an intriguing tool that can be used to access basic information in a specialized area like audiology. Nevertheless, one needs to be careful, as correct information is not infrequently mixed in with errors that are hard to pick up unless the user is well versed in the field. © 2024 S. Karger AG, Basel.",yes,"This study evaluates the clinical knowledge and accuracy of responses from three generative LLM chatbots (ChatGPT, Bing Chat/Copilot, and Bard/Gemini) to audiology-related questions. The focus is on their ability to answer domain-specific clinical questions, which aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1159/000538983,III,Vivian,"fake data, fake task, not clinical study",,
ChatGPT is not ready yet for use in providing mental health assessment and interventions,"Background: Psychiatry is a specialized field of medicine that focuses on the diagnosis, treatment, and prevention of mental health disorders. With advancements in technology and the rise of artificial intelligence (AI), there has been a growing interest in exploring the potential of AI language models systems, such as Chat Generative Pre-training Transformer (ChatGPT), to assist in the field of psychiatry. Objective: Our study aimed to evaluates the effectiveness, reliability and safeness of ChatGPT in assisting patients with mental health problems, and to assess its potential as a collaborative tool for mental health professionals through a simulated interaction with three distinct imaginary patients. Methods: Three imaginary patient scenarios (cases A, B, and C) were created, representing different mental health problems. All three patients present with, and seek to eliminate, the same chief complaint (i.e., difficulty falling asleep and waking up frequently during the night in the last 2°weeks). ChatGPT was engaged as a virtual psychiatric assistant to provide responses and treatment recommendations. Results: In case A, the recommendations were relatively appropriate (albeit non-specific), and could potentially be beneficial for both users and clinicians. However, as complexity of clinical cases increased (cases B and C), the information and recommendations generated by ChatGPT became inappropriate, even dangerous; and the limitations of the program became more glaring. The main strengths of ChatGPT lie in its ability to provide quick responses to user queries and to simulate empathy. One notable limitation is ChatGPT inability to interact with users to collect further information relevant to the diagnosis and management of a patient’s clinical condition. Another serious limitation is ChatGPT inability to use critical thinking and clinical judgment to drive patient’s management. Conclusion: As for July 2023, ChatGPT failed to give the simple medical advice given certain clinical scenarios. This supports that the quality of ChatGPT-generated content is still far from being a guide for users and professionals to provide accurate mental health information. It remains, therefore, premature to conclude on the usefulness and safety of ChatGPT in mental health practice. Copyright © 2024 Dergaa, Fekih-Romdhane, Hallit, Loch, Glenn, Fessi, Ben Aissa, Souissi, Guelmami, Swed, El Omri, Bragazzi and Ben Saad.",yes,"This study evaluates ChatGPT's effectiveness, reliability, and safety in providing mental health assessment and interventions through simulated patient interactions. It directly assesses the clinical application of a generative LLM (ChatGPT) in a healthcare context (psychiatry/mental health).",10.3389/fpsyt.2023.1277756,II,Vivian,"fake data, real task, not a clinical study",,
Evaluating ChatGPT as an educational resource for patients with multiple myeloma: A preliminary investigation,"11.

Evaluating ChatGPT as an educational resource for patients with multiple 
myeloma: A preliminary investigation.

Saba L(1), Fu CL(1), Khouri J(2), Faiman B(3), Anwer F(2), Chaulagain CP(1).

Author information:
(1)Department of Hematology and Medical Oncology, Cleveland Clinic Florida, 
Weston, Florida, USA.
(2)Department of Hematology and Medical Oncology, Cleveland Clinic Main Campus, 
Cleveland, Ohio, USA.
(3)Department of Hematologic Oncology and Blood Disorders, Cleveland Clinic Main 
Campus, Cleveland, Ohio, USA.

The findings of this study highlight a 95% accuracy rate in ChatGPT responses, 
as assessed by five myeloma specialists, underscoring its potential as a 
reliable educational tool.

© 2024 Wiley Periodicals LLC.",yes,"This study evaluates ChatGPT's accuracy in providing educational information to patients with multiple myeloma, as assessed by clinical experts. It directly assesses the clinical utility of an LLM in patient education, which is a relevant clinical application.",10.1002/ajh.27318,II,Vivian,"fake data, real task, not a clinical study",,
Utilization of Chatgpt as a Reliable Aide for Differential Diagnosis of Histopathology in Head and Neck Surgery,"Objectives: The rise of artificial intelligence offers promising advancements in diagnostic workflows in healthcare. In oral and maxillofacial surgery, timely and accurate histopathological diagnosis is crucial for effective treatment planning. This study examines Chat Generative Pretrained Transformer (ChatGPT, OpenAI Inc., California) as an aid to providers in generating differential diagnoses for four common maxillofacial pathologies: ameloblastoma, squamous cell carcinoma, mucoepidermoid carcinoma, and pleomorphic adenoma. Study Design: A retrospective study was conducted with 200 de-identified histopathological cases, evenly divided across the four diagnostic categories. Each case included clinical summaries and histopathological images, which were input into ChatGPT to generate four differential diagnoses. The study evaluated the inclusion and ranking of the correct diagnosis in the differential list using a chi-square goodness-of-fit test. Results: ChatGPT included the correct diagnosis in all cases (100%), ranking it first in 49.5%, second in 32.5%, third in 14.5%, and fourth in 3.5%. Statistical analysis confirmed a significant preference for higher ranking of correct diagnoses (p< 0.001). Conclusion: ChatGPT shows strong reliability in generating accurate differential diagnoses for maxillofacial histopathology, ranking the correct diagnosis in the top two positions in 82% of cases. These results highlight AI’s potential to augment diagnostic workflows and enhance efficiency.",yes,"This study evaluates ChatGPT's performance in generating differential diagnoses for histopathological cases in a clinical context, specifically in head and neck surgery. It uses real clinical data and assesses the model's diagnostic accuracy, which is a direct clinical application.",10.2139/ssrn.5057092,I,Vivian,"real data, real task, retrospective",,
Towards AI-Augmented Clinical Decision-Making: An Examination of ChatGPT's Utility in Acute Ulcerative Colitis Presentations,"This study explores the potential of OpenAI's ChatGPT as a decision support tool for acute ulcerative colitis presentations in the setting of an emergency department. We assessed ChatGPT's performance in determining disease severity using TrueLove and Witts criteria and the necessity of hospitalization for patients with ulcerative colitis, comparing results with those of expert gastroenterologists. Of 20 cases, ChatGPT's assessments were found to be 80% consistent with gastroenterologist evaluations and indicated a high degree of reliability. This suggests that ChatGPT could provide as a clinical decision support tool in assessing acute ulcerative colitis, serving as an adjunct to clinical judgment. © 2023 by The National Association of Neonatal Nurses.",yes,"This study evaluates ChatGPT's performance as a clinical decision support tool in acute ulcerative colitis presentations, directly comparing its assessments to those of expert gastroenterologists. The focus is on clinical decision-making (disease severity, need for hospitalization), which fits the inclusion criteria.",10.14309/ajg.0000000000002483,I,Vivian,"real data, real world task, prospective",,
Multimodal ChatGPT-4V for Electrocardiogram Interpretation: Promise and Limitations,"This study evaluated the capabilities of the newly released ChatGPT-4V, a large language model with visual recognition abilities, in interpreting electrocardiogram waveforms and answering related multiple-choice questions for assisting with cardiovascular care. ©Lingxuan Zhu, Weiming Mou, Keren Wu, Yancheng Lai, Anqi Lin, Tao Yang, Jian Zhang, Peng Luo.",yes,"This study evaluates ChatGPT-4V, a large language model with visual capabilities, in the clinical task of interpreting ECGs and answering related clinical questions. This is a direct clinical application of an LLM in healthcare.",10.2196/54607,III,Sully,,,
"Repeatability, reproducibility, and diagnostic accuracy of a commercial large language model (ChatGPT) to perform emergency department triage using the Canadian triage and acuity scale","Purpose: The release of the ChatGPT prototype to the public in November 2022 drastically reduced the barrier to using artificial intelligence by allowing easy access to a large language model with only a simple web interface. One situation where ChatGPT could be useful is in triaging patients arriving to the emergency department. This study aimed to address the research problem: “can emergency physicians use ChatGPT to accurately triage patients using the Canadian Triage and Acuity Scale (CTAS)?”. Methods: Six unique prompts were developed independently by five emergency physicians. An automated script was used to query ChatGPT with each of the 6 prompts combined with 61 validated and previously published patient vignettes. Thirty repetitions of each combination were performed for a total of 10,980 simulated triages. Results: In 99.6% of 10,980 queries, a CTAS score was returned. However, there was considerable variations in results. Repeatability (use of the same prompt repeatedly) was responsible for 21.0% of overall variation. Reproducibility (use of different prompts) was responsible for 4.0% of overall variation. Overall accuracy of ChatGPT to triage simulated patients was 47.5% with a 13.7% under-triage rate and a 38.7% over-triage rate. More extensively detailed text given as a prompt was associated with greater reproducibility, but minimal increase in accuracy. Conclusions: This study suggests that the current ChatGPT large language model is not sufficient for emergency physicians to triage simulated patients using the Canadian Triage and Acuity Scale due to poor repeatability and accuracy. Medical practitioners should be aware that while ChatGPT can be a valuable tool, it may lack consistency and may frequently provide false information. © 2024, The Author(s), under exclusive licence to Canadian Association of Emergency Physicians (CAEP)/ Association Canadienne de Médecine d'Urgence (ACMU).",yes,"This study evaluates the diagnostic accuracy, repeatability, and reproducibility of ChatGPT (a large language model) in performing emergency department triage using the Canadian Triage and Acuity Scale (CTAS). It uses clinical vignettes and assesses ChatGPT's performance in a direct clinical decision-making context.",10.1007/s43678-023-00616-w,,,,,
Designing a computer-assisted diagnosis system for cardiomegaly detection and radiology report generation,"Chest X-ray (CXR) is a conventional diagnostic tool for cardiothoracic assessment, boasting a high degree of cost-effectiveness and versatility. However, with an increasing number of scans to be evaluated by radiologists, they can suffer from fatigue which might impede diagnostic accuracy and slow down report generation. We describe a prototype computer-assisted diagnosis (CAD) pipeline employing computer vision (CV) and Natural Language Processing (NLP) trained on the publicly available MIMIC-CXR dataset. We perform image quality assessment, view labelling, segmentation-based cardiomegaly severity classification, and use the output of the severity classification for large language model-based report generation. Four certified radiologists assessed the output accuracy of the CAD pipeline. Across the dataset composed of 377,100 CXR images and 227,827 free-text radiology reports, our system identified 0.18% of cases with mixed-sex mentions, 0.02% of poor quality images (F1=0.81), and 0.28% of wrongly labelled views (accuracy 99.4%), furthermore it assigned views for 4.18% of images which have unlabelled views. For binary cardiomegaly classification, we achieve state-of-the-art performance of 95.2% accuracy. The inter-radiologist agreement on evaluating the report’s semantics and correctness for radiologist-MIMIC is 0.62 (strict agreement) and 0.85 (relaxed agreement) similar to the radiologist-CAD agreement of 0.55 (strict) and 0.93 (relaxed). Our work found and corrected several incorrect or missing metadata annotations for the MIMIC-CXR dataset, and the performance of our CAD system suggests performance on par with human radiologists. Future improvements revolve around improved text generation and the development of CV tools for other diseases.",yes,"The study describes a computer-assisted diagnosis system that uses computer vision for image analysis and a large language model (LLM) for radiology report generation. While the primary clinical task (cardiomegaly detection) is performed by computer vision, the LLM is used for generating clinical radiology reports, which are then evaluated by radiologists for semantic and clinical correctness. Since the LLM is applied in a clinical context (report generation for patient care), this fits the inclusion criteria.",10.1101/2024.09.02.24311997,,,,,
Comparison of Large Language Models’ Performance on Neurosurgical Board Examination Questions,"Background: Multiple-choice board examinations are a primary objective measure of competency in medicine. Large language models (LLMs) have demonstrated rapid improvements in performance on medical board examinations in the past two years. We evaluated five leading LLMs on neurosurgical board exam questions. Methods: We evaluated five LLMs (OpenAI o1, OpenEvidence, Claude 3.5 Sonnet, Gemini 2.0, and xAI Grok2) on 500 multiple-choice questions from the Self-Assessment in Neurological Surgery (SANS) American Board of Neurological Surgery (ABNS) Primary Board Examination Review. Performance was analyzed across 12 subspecialty categories and compared to established passing thresholds. Results: All models exceeded the threshold for passing, with OpenAI o1 achieving the highest accuracy (87.6%), followed by OpenEvidence (84.2%), Claude 3.5 Sonnet (83.2%), Gemini 2.0 (81.0%) and xAI Grok2 (79.0%). Performance was strongest in Other General (97.4%) and Peripheral Nerve (97.1%) categories, while Neuroradiology showed the lowest accuracy (57.4%) across all models. Conclusions: State of the art LLMs continue to improve, and all models demonstrated strong performance on neurosurgical board examination questions. Medical image analysis continues to be a limitation of current LLMs. The current level of LLM performance challenges the relevance of written board examinations in trainee evaluation and suggests that LLMs are ready for implementation in clinical medicine and medical education.",yes,"This study evaluates the performance of multiple large language models (LLMs) on neurosurgical board examination questions, which is a clinical evaluation relevant to medical education. The study directly assesses LLMs' ability to answer clinically relevant exam questions, fitting the inclusion criteria.",10.1101/2025.02.20.25322623,III,Vivian,"fake data, fake task, not a clinical study",,
"Assessing ChatGPT’s Potential in HIV Prevention Communication: A Comprehensive Evaluation of Accuracy, Completeness, and Inclusivity","With the advancement of artificial intelligence(AI), platforms like ChatGPT have gained traction in different fields, including Medicine. This study aims to evaluate the potential of ChatGPT in addressing questions related to HIV prevention and to assess its accuracy, completeness, and inclusivity. A team consisting of 15 physicians, six members from HIV communities, and three experts in gender and queer studies designed an assessment of ChatGPT. Queries were categorized into five thematic groups: general HIV information, behaviors increasing HIV acquisition risk, HIV and pregnancy, HIV testing, and the prophylaxis use. A team of medical doctors was in charge of developing questions to be submitted to ChatGPT. The other members critically assessed the generated responses regarding level of expertise, accuracy, completeness, and inclusivity. The median accuracy score was 5.5 out of 6, with 88.4% of responses achieving a score ≥ 5. Completeness had a median of 3 out of 3, while the median for inclusivity was 2 out of 3. Some thematic groups, like behaviors associated with HIV transmission and prophylaxis, exhibited higher accuracy, indicating variable performance across different topics. Issues of inclusivity were identified, notably the use of outdated terms and a lack of representation for some communities. ChatGPT demonstrates significant potential in providing accurate information on HIV-related topics. However, while responses were often scientifically accurate, they sometimes lacked the socio-political context and inclusivity essential for effective health communication. This underlines the importance of aligning AI-driven platforms with contemporary health communication strategies and ensuring the balance of accuracy and inclusivity. © The Author(s) 2024.",yes,"This study evaluates ChatGPT's performance in answering clinical questions about HIV prevention, focusing on accuracy, completeness, and inclusivity. The assessment involves real clinical content and patient-facing communication, which is a direct clinical application of an LLM in healthcare.",10.1007/s10461-024-04391-2,,,,,
"Comparison of the problem-solving performance of ChatGPT-3.5, ChatGPT-4, Bing Chat, and Bard for the Korean emergency medicine board examination question bank","Large language models (LLMs) have been deployed in diverse fields, and the potential for their application in medicine has been explored through numerous studies. This study aimed to evaluate and compare the performance of ChatGPT-3.5, ChatGPT-4, Bing Chat, and Bard for the Emergency Medicine Board Examination question bank in the Korean language. Of the 2353 questions in the question bank, 150 questions were randomly selected, and 27 containing figures were excluded. Questions that required abilities such as analysis, creative thinking, evaluation, and synthesis were classified as higher-order questions, and those that required only recall, memory, and factual information in response were classified as lower-order questions. The answers and explanations obtained by inputting the 123 questions into the LLMs were analyzed and compared. ChatGPT-4 (75.6%) and Bing Chat (70.7%) showed higher correct response rates than ChatGPT-3.5 (56.9%) and Bard (51.2%). ChatGPT-4 showed the highest correct response rate for the higher-order questions at 76.5%, and Bard and Bing Chat showed the highest rate for the lower-order questions at 71.4%. The appropriateness of the explanation for the answer was significantly higher for ChatGPT-4 and Bing Chat than for ChatGPT-3.5 and Bard (75.6%, 68.3%, 52.8%, and 50.4%, respectively). ChatGPT-4 and Bing Chat outperformed ChatGPT-3.5 and Bard in answering a random selection of Emergency Medicine Board Examination questions in the Korean language. © 2024 Lippincott Williams and Wilkins. All rights reserved.",yes,"This study evaluates the clinical performance of multiple LLMs (ChatGPT-3.5, ChatGPT-4, Bing Chat, Bard) on a Korean emergency medicine board examination question bank. The focus is on their ability to answer clinically relevant exam questions, which is a direct clinical application and fits the inclusion criteria.",10.1097/MD.0000000000037325,III,Vivian,"fake data, fake task, not a clincial study",,
Performance of ChatGPT in Pediatric Audiology as Rated by Students and Experts,"Background: Despite the growing popularity of artificial intelligence (AI)-based systems such as ChatGPT, there is still little evidence of their effectiveness in audiology, particularly in pediatric audiology. The present study aimed to verify the performance of ChatGPT in this field, as assessed by both students and professionals, and to compare its Polish and English versions. Methods: ChatGPT was presented with 20 questions, which were posed twice, first in Polish and then in English. A group of 20 students and 16 professionals in the field of audiology and otolaryngology rated the answers on a Likert scale of 1 to 5 in terms of correctness, relevance, completeness, and linguistic accuracy. Both groups were also asked to assess the usefulness of ChatGPT as a source of information for patients, in educational settings for students, and in professional work. Results: Both students and professionals generally rated ChatGPT’s responses to be satisfactory. For most of the questions, ChatGPT’s responses were rated somewhat higher by the students than the professionals, although statistically significant differences were only evident for completeness and linguistic accuracy. Those who rated ChatGPT’s responses more highly also rated its usefulness more highly. Conclusions: ChatGPT can possibly be used for quick information retrieval, especially by non-experts, but it lacks the depth and reliability required by professionals. The different ratings given by students and professionals, and its language dependency, indicate it works best as a supplementary tool, not as a replacement for verifiable sources, particularly in a healthcare setting. © 2025 by the authors.",yes,"This study evaluates ChatGPT's performance in answering pediatric audiology questions, as rated by both students and professionals, and assesses its usefulness in clinical and educational contexts. The focus is on ChatGPT's ability to provide clinically relevant information, making it a direct evaluation of an LLM in a healthcare setting.",10.3390/jcm14030875,II,Vivian,"fake data, real task, not a clinical study",,
Autonomous medical evaluation for guideline adherence of large language models,"Autonomous Medical Evaluation for Guideline Adherence (AMEGA) is a comprehensive benchmark designed to evaluate large language models’ adherence to medical guidelines across 20 diagnostic scenarios spanning 13 specialties. It includes an evaluation framework and methodology to assess models’ capabilities in medical reasoning, differential diagnosis, treatment planning, and guideline adherence, using open-ended questions that mirror real-world clinical interactions. It includes 135 questions and 1337 weighted scoring elements designed to assess comprehensive medical knowledge. In tests of 17 LLMs, GPT-4 scored highest with 41.9/50, followed closely by Llama-3 70B and WizardLM-2-8x22B. For comparison, a recent medical graduate scored 25.8/50. The benchmark introduces novel content to avoid the issue of LLMs memorizing existing medical data. AMEGA’s publicly available code supports further research in AI-assisted clinical decision-making, aiming to enhance patient care by aiding clinicians in diagnosis and treatment under time constraints. © The Author(s) 2024.",yes,"This study evaluates multiple large language models, including GPT-4 and Llama-3, on their adherence to medical guidelines across a range of clinical scenarios. The evaluation focuses on clinical reasoning, diagnosis, and treatment planning, which are direct clinical applications. The study is not a review or opinion piece, and it does not focus on technical architecture or non-clinical tasks.",10.1038/s41746-024-01356-6,II,Vivian,"fake data, real task, not a clinical study",,
Evaluating GPT as an Adjunct for Radiologic Decision Making: GPT-4 Versus GPT-3.5 in a Breast Imaging Pilot,"Objective: Despite rising popularity and performance, studies evaluating the use of large language models for clinical decision support are lacking. Here, we evaluate ChatGPT (Generative Pre-trained Transformer)-3.5 and GPT-4’s (OpenAI, San Francisco, California) capacity for clinical decision support in radiology via the identification of appropriate imaging services for two important clinical presentations: breast cancer screening and breast pain. Methods: We compared ChatGPT's responses to the ACR Appropriateness Criteria for breast pain and breast cancer screening. Our prompt formats included an open-ended (OE) and a select all that apply (SATA) format. Scoring criteria evaluated whether proposed imaging modalities were in accordance with ACR guidelines. Three replicate entries were conducted for each prompt, and the average of these was used to determine final scores. Results: Both ChatGPT-3.5 and ChatGPT-4 achieved an average OE score of 1.830 (out of 2) for breast cancer screening prompts. ChatGPT-3.5 achieved a SATA average percentage correct of 88.9%, compared with ChatGPT-4’s average percentage correct of 98.4% for breast cancer screening prompts. For breast pain, ChatGPT-3.5 achieved an average OE score of 1.125 (out of 2) and a SATA average percentage correct of 58.3%, as compared with an average OE score of 1.666 (out of 2) and a SATA average percentage correct of 77.7%. Discussion: Our results demonstrate the eventual feasibility of using large language models like ChatGPT for radiologic decision making, with the potential to improve clinical workflow and responsible use of radiology services. More use cases and greater accuracy are necessary to evaluate and implement such tools. © 2023 American College of Radiology",yes,"This study directly evaluates the performance of GPT-3.5 and GPT-4 (ChatGPT) in providing clinical decision support for radiologic imaging, specifically breast cancer screening and breast pain, by comparing their recommendations to established clinical guidelines. This is a clear clinical application of LLMs in healthcare decision-making.",10.1016/j.jacr.2023.05.003,II,Vivian,"fake data, real task, not a clinical study",,
A context-based chatbot surpasses trained radiologists and generic ChatGPT in following the ACR appropriateness guidelines,"Background Radiological imaging guidelines are crucial for accurate diagnosis and optimal patient care as they result in standardized procedures and thus reduce inappropriate imaging studies. In the present study, we investigated the potential to support clinical decision-making using an interactive chatbot designed to provide personalized imaging recommendations based on indexed and vectorized American College of Radiology (ACR) appropriateness criteria documents. Methods We utilized 209 ACR appropriateness criteria documents as specialized knowledge base and employed LlamaIndex and the ChatGPT 3.5-Turbo to create an appropriateness criteria contexted chatbot (accGPT). Fifty clinical case files were used to compare the accGPT`s performance against radiologists at varying experience levels and to generic ChatGPT 3.5 and 4.0. Results All chatbots reached at least human performance level. For the 50 case files, the accGPT provided a median of 83% (95% CI 82-84) `usually appropriate` recommendations, while radiologists provided a median of 66% (95% CI 62-70). GPT 3.5-Turbo 70% (95% CI 67-73) and GPT 4 79% (95% CI 76-81) correct answers. Consistency was highest for the accGPT with almost perfect Fleiss` Kappa of 0.82. Further, the chatbots provided substantial time and cost savings, with an average decision time of 5 minutes and a cost of 0.19 Euro for all cases, compared to 50 minutes and 29.99 Euro for radiologists (both p < 0.01). Conclusion ChatGPT-based algorithms have the potential to substantially improve the decision-making for clinical imaging studies in accordance with ACR guidelines. Specifically, a context-based algorithm performed superior to its generic counterpart, demonstrating the value of tailoring AI solutions to specific healthcare applications.",yes,"This study evaluates a context-based chatbot built on ChatGPT 3.5-Turbo and compares its clinical decision-making performance (imaging appropriateness recommendations) to radiologists and generic ChatGPT models. The task is directly clinical, involving patient case files and adherence to ACR guidelines. The study uses LLMs in a clinical decision support context.",10.1101/2023.04.10.23288354,II,Vivian,"fake data, real task, not a clinical study",,
Pure Wisdom or Potemkin Villages? A Comparison of ChatGPT 3.5 and ChatGPT 4 on USMLE Step 3 Style Questions: Quantitative Analysis,"BACKGROUND: The United States Medical Licensing Examination (USMLE) has been 
critical in medical education since 1992, testing various aspects of a medical 
student's knowledge and skills through different steps, based on their training 
level. Artificial intelligence (AI) tools, including chatbots like ChatGPT, are 
emerging technologies with potential applications in medicine. However, 
comprehensive studies analyzing ChatGPT's performance on USMLE Step 3 in 
large-scale scenarios and comparing different versions of ChatGPT are limited.
OBJECTIVE: This paper aimed to analyze ChatGPT's performance on USMLE Step 3 
practice test questions to better elucidate the strengths and weaknesses of AI 
use in medical education and deduce evidence-based strategies to counteract AI 
cheating.
METHODS: A total of 2069 USMLE Step 3 practice questions were extracted from the 
AMBOSS study platform. After including 229 image-based questions, a total of 
1840 text-based questions were further categorized and entered into ChatGPT 3.5, 
while a subset of 229 questions were entered into ChatGPT 4. Responses were 
recorded, and the accuracy of ChatGPT answers as well as its performance in 
different test question categories and for different difficulty levels were 
compared between both versions.
RESULTS: Overall, ChatGPT 4 demonstrated a statistically significant superior 
performance compared to ChatGPT 3.5, achieving an accuracy of 84.7% (194/229) 
and 56.9% (1047/1840), respectively. A noteworthy correlation was observed 
between the length of test questions and the performance of ChatGPT 3.5 
(ρ=-0.069; P=.003), which was absent in ChatGPT 4 (P=.87). Additionally, the 
difficulty of test questions, as categorized by AMBOSS hammer ratings, showed a 
statistically significant correlation with performance for both ChatGPT 
versions, with ρ=-0.289 for ChatGPT 3.5 and ρ=-0.344 for ChatGPT 4. ChatGPT 4 
surpassed ChatGPT 3.5 in all levels of test question difficulty, except for the 
2 highest difficulty tiers (4 and 5 hammers), where statistical significance was 
not reached.
CONCLUSIONS: In this study, ChatGPT 4 demonstrated remarkable proficiency in 
taking the USMLE Step 3, with an accuracy rate of 84.7% (194/229), outshining 
ChatGPT 3.5 with an accuracy rate of 56.9% (1047/1840). Although ChatGPT 4 
performed exceptionally, it encountered difficulties in questions requiring the 
application of theoretical concepts, particularly in cardiology and neurology. 
These insights are pivotal for the development of examination strategies that 
are resilient to AI and underline the promising role of AI in the realm of 
medical education and diagnostics.

©Leonard Knoedler, Michael Alfertshofer, Samuel Knoedler, Cosima C Hoch, Paul F 
Funk, Sebastian Cotofana, Bhagvat Maheta, Konstantin Frank, Vanessa Brébant, 
Lukas Prantl, Philipp Lamby. Originally published in JMIR Medical Education 
(https://mededu.jmir.org), 05.01.2024.",yes,"This study evaluates the performance of two versions of ChatGPT (3.5 and 4) on USMLE Step 3 practice questions, which are clinically relevant board exam questions. The focus is on the LLMs' ability to answer medical questions, a direct clinical application in medical education and assessment.",10.2196/51148,III,Vivian,"fake data, fake task, not a clinical study",,
AI dietician: Unveiling the accuracy of ChatGPT's nutritional estimations,"We investigate the accuracy and reliability of ChatGPT, an artificial intelligence model developed by OpenAI, in providing nutritional information for dietary planning and weight management. The results have a reasonable level of accuracy, with energy values having the highest level of conformity: 97% of the artificial intelligence values fall within a 40% difference from United States Department of Agriculture data. Additionally, ChatGPT displayed consistency in its provision of nutritional data, as indicated by relatively low coefficient of variation values for each nutrient. The artificial intelligence model also proved efficient in generating a daily meal plan within a specified caloric limit, with all the meals falling within a 30% bound of the United States Department of Agriculture's caloric values. These findings suggest that ChatGPT can provide reasonably accurate and consistent nutritional information. Further research is recommended to assess the model's performance across a broader range of foods and meals.<END ABSTRACT> © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's ability to provide nutritional information and generate meal plans for dietary planning and weight management, which are clinical tasks relevant to healthcare. The focus is on the accuracy and reliability of ChatGPT in a health-related context, making it eligible for inclusion.",10.1016/j.nut.2023.112325,X,Vivian,,,
Computerized diagnostic decision support systems – a comparative performance study of Isabel Pro vs. ChatGPT4,"Objectives: Validate the diagnostic accuracy of the Artificial Intelligence Large Language Model ChatGPT4 by comparing diagnosis lists produced by ChatGPT4 to Isabel Pro. Methods: This study used 201 cases, comparing ChatGPT4 to Isabel Pro. Systems inputs were identical. Mean Reciprocal Rank (MRR) compares the correct diagnosis’s rank between systems. Isabel Pro ranks by the frequency with which the symptoms appear in the reference dataset. The mechanism ChatGPT4 uses to rank the diagnoses is unknown. A Wilcoxon Signed Rank Sum test failed to reject the null hypothesis. Results: Both systems produced comprehensive differential diagnosis lists. Isabel Pro’s list appears immediately upon submission, while ChatGPT4 takes several minutes. Isabel Pro produced 175 (87.1 %) correct diagnoses and ChatGPT4 165 (82.1 %). The MRR for ChatGPT4 was 0.428 (rank 2.31), and Isabel Pro was 0.389 (rank 2.57), an average rank of three for each. ChatGPT4 outperformed on Recall at Rank 1, 5, and 10, with Isabel Pro outperforming at 20, 30, and 40. The Wilcoxon Signed Rank Sum Test confirmed that the sample size was inadequate to conclude that the systems are equivalent. ChatGPT4 fabricated citations and DOIs, producing 145 correct references (87.9 %) but only 52 correct DOIs (31.5 %). Conclusions: This study validates the promise of Clinical Diagnostic Decision Support Systems, including the Large Language Model form of artificial intelligence (AI). Until the issue of hallucination of references and, perhaps diagnoses, is resolved in favor of absolute accuracy, clinicians will make cautious use of Large Language Model systems in diagnosis, if at all. © 2024 the author(s), published by De Gruyter.",yes,"This study directly evaluates the clinical diagnostic performance of ChatGPT-4, a large language model, by comparing its diagnostic accuracy to that of Isabel Pro using real clinical cases. The focus is on clinical decision support and diagnosis, which fits the inclusion criteria.",10.1515/dx-2024-0033,I,Vivian,"real data, real task, retrospective (analyzed an aggregate of real clinical cases from multiple studies)",,
Not the Models You Are Looking For: Traditional ML Outperforms LLMs in Clinical Prediction Tasks,"Objectives To determine the extent to which current Large Language Models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. Materials and Methods We evaluated GPT-3.5, GPT-4, and ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center and MIMIC IV. We measured predictive performance with AUROC and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. Results Traditional ML (AUROC: 0.847, 0.894 (VUMC, MIMIC)) substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration (Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 versus 0.384 versus 0.251, 0.042 versus 0.06 versus 0.219). Traditional ML is more robust than GPT-3.5 and GPT-4 to generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. Conclusion These findings suggest that LLMs are much less effective and robust than locally-trained ML for clinical prediction tasks, but they are getting better over time.",yes,"This study directly evaluates the performance of generative LLMs (GPT-3.5 and GPT-4) on clinical prediction tasks using EHR data, comparing them to traditional ML models. The focus is on clinical prediction, which is a clinical application, and the LLMs are tested in a healthcare context. Despite also including traditional ML models, the core of the study is the clinical evaluation of LLMs.",10.1101/2024.12.03.24318400,I,Vivian,"real data, real task, prospective",,
ChatGPT and most frequent urological diseases: analysing the quality of information and potential risks for patients,"Purpose: Artificial intelligence (AI) is a set of systems or combinations of algorithms, which mimic human intelligence. ChatGPT is software with artificial intelligence which was recently developed by OpenAI. One of its potential uses could be to consult the information about pathologies and treatments. Our objective was to assess the quality of the information provided by AI like ChatGPT and establish if it is a secure source of information for patients. Methods: Questions about bladder cancer, prostate cancer, renal cancer, benign prostatic hypertrophy (BPH), and urinary stones were queried through ChatGPT 4.0. Two urologists analysed the responses provided by ChatGPT using DISCERN questionary and a brief instrument for evaluating the quality of informed consent documents. Results: The overall information provided in all pathologies was well-balanced. In each pathology was explained its anatomical location, affected population and a description of the symptoms. It concluded with the established risk factors and possible treatment. All treatment answers had a moderate quality score with DISCERN (3 of 5 points). The answers about surgical options contain the recovery time, type of anaesthesia, and potential complications. After analysing all the responses related to each disease, all pathologies except BPH achieved a DISCERN score of 4. Conclusions: ChatGPT information should be used with caution since the chatbot does not disclose the sources of information and may contain bias even with simple questions related to the basics of urologic diseases. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",yes,"This study evaluates ChatGPT's ability to provide clinical information to patients about common urological diseases, assessing the quality and potential risks of its responses. The evaluation is based on clinical content and patient information, which is a direct clinical application relevant to patient care.",10.1007/s00345-023-04563-0,,,,,
Horse Diagnosis and Triage Accuracy of GPT-4o,"Animal owners may increasingly rely on large language models for gathering animal health information alongside internet sources in the future. This study therefore aims to provide initial results on the accuracy of ChatGPT-4o in triage and tentative diagnostics, using horses as a case study. Ten test vignettes were used to prompt situation assessments from the tool, which were then compared to original assessments made by a veterinary specialist for horses. The most probable diagnosis suggested by ChatGPT-4o was found to be quite accurate in most cases, with the urgency to contact a veterinarian sometimes assessed as higher than necessary. When provided with all relevant information, the tool does not seem to compromise horse health by recommending excessively long waiting times, although there is still potential for improving the relief of veterinarians' workload.  © 2024 The Authors.",yes,"This study evaluates the clinical performance of GPT-4o (a generative LLM) in the context of veterinary medicine, specifically for diagnosis and triage in horses. It uses clinical vignettes and compares LLM output to expert assessment, which fits the inclusion criteria for clinical evaluation of LLMs, even though the context is veterinary rather than human medicine.",10.3233/SHTI241092,X,Vivian,"fake data, real task, not a clinical study (also not a human clinical study)",,
Comparative Analysis of Large Language Models in Emergency Plastic Surgery Decision-Making: The Role of Physical Exam Data,"In the U.S., diagnostic errors are common across various healthcare settings due to factors like complex procedures and multiple healthcare providers, often exacerbated by inadequate initial evaluations. This study explores the role of Large Language Models (LLMs), specifically OpenAI’s ChatGPT-4 and Google Gemini, in improving emergency decision-making in plastic and reconstructive surgery by evaluating their effectiveness both with and without physical examination data. Thirty medical vignettes covering emergency conditions such as fractures and nerve injuries were used to assess the diagnostic and management responses of the models. These responses were evaluated by medical professionals against established clinical guidelines, using statistical analyses including the Wilcoxon rank-sum test. Results showed that ChatGPT-4 consistently outperformed Gemini in both diagnosis and management, irrespective of the presence of physical examination data, though no significant differences were noted within each model’s performance across different data scenarios. Conclusively, while ChatGPT-4 demonstrates superior accuracy and management capabilities, the addition of physical examination data, though enhancing response detail, did not significantly surpass traditional medical resources. This underscores the utility of AI in supporting clinical decision-making, particularly in scenarios with limited data, suggesting its role as a complement to, rather than a replacement for, comprehensive clinical evaluation and expertise. © 2024 by the authors.",yes,"This study directly evaluates the clinical performance of frontier LLMs (ChatGPT-4 and Gemini) in emergency plastic surgery decision-making, using clinical vignettes and comparing their diagnostic and management recommendations to established guidelines. The focus is on clinical application, not technical development or non-clinical tasks.",10.3390/jpm14060612,II,Vivian,"fake data, real task, not a clinical study",,
Integrating domain-specific resources: Advancing AI for foot and ankle surgery,"Large language models like ChatGPT offer significant potential for applications in medicine, including patient education and clinical support. This study evaluates the performance of ChatGPT-4, ChatGPT-4 enhanced with retrieval-augmented generation (RAG), and Gemini AI in responding to clinical vignette questions regarding Hallux Rigidus, a condition requiring specialized knowledge in foot and ankle surgery. The ChatGPT-4 + RAG model, enhanced with the 2024 ACFAS clinical consensus statements, demonstrated the highest agreement with surveyor majority responses (83.26 %) compared to ChatGPT-4 (59.54 %) and Gemini AI (53.02 %). All models provided clinically appropriate responses to most questions, with the ChatGPT-4 + RAG model excelling in accuracy, despite the rationale for answers being deemed most difficult to read. These findings highlight the limitations of generic AI models, which may propagate misinformation if used by patients seeking health information. By incorporating domain-specific resources, the RAG-augmented model showed enhanced reliability and contextual accuracy, suggesting their potential as tools for both clinical decision-making and patient education. This study emphasizes the importance of integrating verified medical resources to advance AI in healthcare, addressing critical gaps in existing capabilities while minimizing risks of misinformation.",yes,"This study evaluates the clinical performance of LLMs (ChatGPT-4, ChatGPT-4+RAG, Gemini) in answering clinical vignette questions about a specific medical condition (Hallux Rigidus), with a focus on clinical decision-making and patient education. The evaluation is directly relevant to the clinical application of LLMs in healthcare.",10.1016/j.fastrc.2024.100445,II,Vivian,"fake data, real task, not a clinical study",,
Large-Scale assessment of ChatGPT's performance in benign and malignant bone tumors imaging report diagnosis and its potential for clinical applications,"Objective: This study was designed to delve into the complexities involved in diagnosing of benign and malignant bone tumors and to assess the potential of AI technologies like ChatGPT in improving diagnostic accuracy and efficiency. The study also explores the few-shot learning as a method to optimize ChatGPT's performance in specialized medical domains such as benign and malignant bone tumors diagnosis. Methods: A total of 1366 benign and malignant bone tumors-related imaging reports were collected and diagnosed by 25 experienced physicians. The gold standard of diagnosis was established by combining clinical, imaging and pathological principles.These reports were then input into the ChatGPT model which underwent a few-shot learning method to generate diagnostic results. The diagnostic results of the physicians and the AI model were compared to evaluate the performance of ChatGPT. An experiment was conducted to assess the influence of different radiologist's reporting styles on the model's diagnostic performance. Furthermore, in-depth analysis of misdiagnosed cases was carried out, categorizing diagnostic errors and exploring possible causes. Results: The diagnostic results generated by ChatGPT showed an accuracy of 0.73, sensitivity of 0.95, and specificity of 0.58. After few-shot learning, ChatGPT demonstrated significant improvement, achieving an accuracy of 0.87, sensitivity of 0.99, and specificity of 0.73, bringing it much closer to the level of physician diagnostics. In an experiment analyzing the influence of the radiologist's reporting style, the model demonstrated higher sensitivity when interpreting reports written by high-level radiologists. In 56 benign cases, ChatGPT misdiagnosed them as malignant. Among these, 35 benign lesions- fibrous dysplasia and osteofibrous dysplasia- were incorrectly identified as metastatic tumors or osteosarcomas; 8 cases of myositis ossificans were wrongly diagnosed as extraosseous osteosarcoma. 7 cases of giant cell tumor of bone at the end of long bone were misdiagnosed as osteosarcoma by intermediate doctors. Chondroblastoma was misdiagnosed as malignant tumor in 6 cases −2 osteosarcoma and 4 chondrosarcoma-In this study, 23 osteosarcoma cases were misdiagnosed by ChatGPT as osteomyelitis; Chondrosarcoma was misdiagnosed as fibrous dysplasia or aneurysmal bone cyst in 8 cases. Four cases of spinal chordoma were misdiagnosed as spinal tuberculosis. Conclusion: Our findings highlight the potential of ChatGPT in the diagnosis of benign and malignant bone tumors, offering advantages like enhanced efficiency and a reduction in missed diagnoses. However, the necessity of collaborative interactions between physicians and ChatGPT in practical settings was underscored. With an examination into AI's capacity in benign and malignant bone tumors diagnosis, this study lays the groundwork for future AI advancements in medicine. Additionally, the benefits of few-shot learning in fine-tuning ChatGPT applications in specialized fields were also demonstrated. © 2024 The Author(s)",yes,"This study directly evaluates ChatGPT's performance in diagnosing benign and malignant bone tumors from imaging reports, comparing its results to those of physicians and analyzing diagnostic errors. The focus is on a clinical application (diagnosis) using a generative LLM (ChatGPT), meeting the inclusion criteria.",10.1016/j.jbo.2024.100525,I,Vivian,"real data, real task, retrospective",,
"Assessing prognosis in depression: Comparing perspectives of AI models, mental health professionals and the general public","Background Artificial intelligence (AI) has rapidly permeated various sectors, including healthcare, highlighting its potential to facilitate mental health assessments. This study explores the underexplored domain of AI's role in evaluating prognosis and long-term outcomes in depressive disorders, offering insights into how AI large language models (LLMs) compare with human perspectives. Methods Using case vignettes, we conducted a comparative analysis involving different LLMs (ChatGPT-3.5, ChatGPT-4, Claude and Bard), mental health professionals (general practitioners, psychiatrists, clinical psychologists and mental health nurses), and the general public that reported previously. We evaluate the LLMs ability to generate prognosis, anticipated outcomes with and without professional intervention, and envisioned long-term positive and negative consequences for individuals with depression. Results In most of the examined cases, the four LLMs consistently identified depression as the primary diagnosis and recommended a combined treatment of psychotherapy and antidepressant medication. ChatGPT-3.5 exhibited a significantly pessimistic prognosis distinct from other LLMs, professionals and the public. ChatGPT-4, Claude and Bard aligned closely with mental health professionals and the general public perspectives, all of whom anticipated no improvement or worsening without professional help. Regarding long-term outcomes, ChatGPT 3.5, Claude and Bard consistently projected significantly fewer negative long-term consequences of treatment than ChatGPT-4. Conclusions This study underscores the potential of AI to complement the expertise of mental health professionals and promote a collaborative paradigm in mental healthcare. The observation that three of the four LLMs closely mirrored the anticipations of mental health experts in scenarios involving treatment underscores the technology's prospective value in offering professional clinical forecasts. The pessimistic outlook presented by ChatGPT 3.5 is concerning, as it could potentially diminish patients' drive to initiate or continue depression therapy. In summary, although LLMs show potential in enhancing healthcare services, their utilisation requires thorough verification and a seamless integration with human judgement and skills. © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study directly evaluates multiple LLMs (ChatGPT-3.5, ChatGPT-4, Claude, Bard) in a clinical context—specifically, their ability to assess prognosis and long-term outcomes in depression, compared to mental health professionals and the public. The LLMs are used for clinical reasoning and decision support, which fits the inclusion criteria.",10.1136/fmch-2023-002583,II,Vivian,"fake data, real task, not a clinical study",,
Lung Cancer Detection: Leveraging Transfer Learning & AI Chatbot for Doctor Prescription,"This research focuses on developing end to end pipeline for the Lung Cancer Detection using Large Language Models (LLMs) & Generative AI. In the field of healthcare there is an advancement of tools and technology used for detection and diagnostics of disease. This project involves mainly two aspects of technology: MLOps and LLMOps. First, it creates a classification model that classifies the disease with the help of user's CT scan image to detect lung cancer using transfer learning model and then build an End-to-End CI/CDI CT pipeline for disease detection by handling new incoming data and version controlling. Afterward, this pipeline is integrated with a chatbot that incorporates a question-and-answer system by utilizing LLM models like GPT and Llama to help patients in receiving appropriate therapy like medicine recommendation and doctor consultancy. A dataset that used comprising of 81,584 images from TCIA is taken which consists of different types of lung cancer images. A pretrained model LLM was utilized to generate accurate doctor prescriptions and the model was fine-tuned using comprehensive medical literature. Four cutting-edge convolutional neural network (CNNs) architecture are used as transfer learning models which are DenseNET121, combination of Inception with Mobile NET, ResNET50, and VGG16 etc. The model, upon implementing, achieved an accuracy of 99.58%, with error rate of Type II being nearly zero by using the combination of Inception and Mobilenet. After identifying the disease, the patient can directly interact with the AI chatbot that is integrated with the pipeline. Both patients and healthcare professionals get the advantage for disease detection and getting diagnostics report related to disease respectively.  © 2024 IEEE.",yes,"The study describes an end-to-end pipeline for lung cancer detection using CNN-based image classifiers (not LLMs) for diagnosis, and then integrates an LLM-based chatbot (GPT, Llama) for providing doctor prescriptions and answering patient questions. While the primary diagnostic task is performed by non-LLM models, the LLM is used for clinical recommendations and patient interaction after diagnosis. Since the LLM is used to generate doctor prescriptions and provide clinical advice, this constitutes a clinical application of LLMs in healthcare.",10.1109/ICTACS62700.2024.10840502,I,Vivian,"real data, real task, retrospective",,
Performance of Publicly Available Large Language Models on Internal Medicine Board-style Questions,"Ongoing research attempts to benchmark large language models (LLM) against physicians’ fund of knowledge by assessing LLM performance on medical examinations. No prior study has assessed LLM performance on internal medicine (IM) board examination questions. Limited data exists on how knowledge supplied to the models, derived from medical texts improves LLM performance. The performance of GPT-3.5, GPT-4.0, LaMDA and Llama 2, with and without additional model input augmentation, was assessed on 240 randomly selected IM board-style questions. Questions were sourced from the Medical Knowledge Self-Assessment Program released by the American College of Physicians with each question serving as part of the LLM prompt. When available, LLMs were accessed both through their application programming interface (API) and their corresponding chatbot. Mode inputs were augmented with Harrison’s Principles of Internal Medicine using the method of Retrieval Augmented Generation. LLM-generated explanations to 25 correctly answered questions were presented in a blinded fashion alongside the MKSAP explanation to an IM board-certified physician tasked with selecting the human generated response. GPT-4.0, accessed either through Bing Chat or its API, scored 77.5–80.7% outperforming GPT-3.5, human respondents, LaMDA and Llama 2 in that order. GPT-4.0 outperformed human MKSAP users on every tested IM subject with its highest and lowest percentile scores in Infectious Disease (80th) and Rheumatology (99.7th), respectively. There is a 3.2–5.3% decrease in performance of both GPT-3.5 and GPT-4.0 when accessing the LLM through its API instead of its online chatbot. There is 4.5–7.5% increase in performance of both GPT-3.5 and GPT-4.0 accessed through their APIs after additional input augmentation. The blinded reviewer correctly identified the human generated MKSAP response in 72% of the 25-question sample set. GPT-4.0 performed best on IM board-style questions outperforming human respondents. Augmenting with domain-specific information improved performance rendering Retrieval Augmented Generation a possible technique for improving accuracy in medical examination LLM responses. © 2024 Public Library of Science. All rights reserved.",yes,"This study evaluates the performance of several large language models (GPT-3.5, GPT-4.0, LaMDA, Llama 2) on internal medicine board-style questions, which is a direct clinical evaluation relevant to medical education and decision support. The study also explores the impact of input augmentation with medical texts. This fits the inclusion criteria as it assesses LLMs in a clinical/medical context.",10.1371/journal.pdig.0000604,III,Vivian,"fake data, fake task, not a clinical study",,
Artificial intelligence chatbots in transfusion medicine: A cross-sectional study,"Background and Objectives: The recent rise of artificial intelligence (AI) chatbots has attracted many users worldwide. However, expert evaluation is essential before relying on them for transfusion medicine (TM)-related information. This study aims to evaluate the performance of AI chatbots for accuracy, correctness, completeness and safety. Materials and Methods: Six AI chatbots (ChatGPT 4, ChatGPT 4-o, Gemini Advanced, Copilot, Anthropic Claude 3.5 Sonnet, Meta AI) were tested using TM-related prompts at two time points, 30 days apart. Their responses were assessed by four TM experts. Evaluators' scores underwent inter-rater reliability testing. Responses from Day 30 were compared with those from Day 1 to evaluate consistency and potential evolution over time. Results: All six chatbots exhibited some level of inconsistency and varying degrees of evolution in their responses over 30 days. None provided entirely correct, complete or safe answers to all questions. Among the chatbots tested, ChatGPT 4-o and Anthropic Claude 3.5 Sonnet demonstrated the highest accuracy and consistency, while Microsoft Copilot and Google Gemini Advanced showed the greatest evolution in their responses. As a limitation, the 30-day period may be too short for a precise assessment of chatbot evolution. Conclusion: At the time of the conduct of this study, none of the AI chatbots provided fully reliable, complete or safe responses to all TM-related prompts. However, ChatGPT 4-o and Anthropic Claude 3.5 Sonnet show the highest promise for future integration into TM practices. Given their variability and ongoing development, AI chatbots should not yet be relied upon as authoritative sources in TM without expert validation. © 2025 International Society of Blood Transfusion.",yes,"This study evaluates the clinical performance of several generative LLM chatbots (including ChatGPT 4, ChatGPT 4-o, Gemini Advanced, Claude 3.5 Sonnet, etc.) in the context of transfusion medicine by assessing their accuracy, correctness, completeness, and safety in response to clinical prompts. The evaluation is performed by domain experts, and the focus is on clinical information provision, making it directly relevant to clinical applications.",10.1111/vox.70009,II,Vivian,"fake data? (TM-related prompts but can't access the article so not sure if based on any patient/clinical data), real task, not a clinical study",,
Adequacy of prostate cancer prevention and screening recommendations provided by an artificial intelligence-powered large language model,"Purpose: We aimed to assess the appropriateness of ChatGPT in providing answers related to prostate cancer (PCa) screening, comparing GPT-3.5 and GPT-4. Methods: A committee of five reviewers designed 30 questions related to PCa screening, categorized into three difficulty levels. The questions were formulated identically for both GPTs three times, varying the prompts. Each reviewer assigned a score for accuracy, clarity, and conciseness. The readability was assessed by the Flesch Kincaid Grade (FKG) and Flesch Reading Ease (FRE). The mean scores were extracted and compared using the Wilcoxon test. We compared the readability across the three different prompts by ANOVA. Results: In GPT-3.5 the mean score (SD) for accuracy, clarity, and conciseness was 1.5 (0.59), 1.7 (0.45), 1.7 (0.49), respectively for easy questions; 1.3 (0.67), 1.6 (0.69), 1.3 (0.65) for medium; 1.3 (0.62), 1.6 (0.56), 1.4 (0.56) for hard. In GPT-4 was 2.0 (0), 2.0 (0), 2.0 (0.14), respectively for easy questions; 1.7 (0.66), 1.8 (0.61), 1.7 (0.64) for medium; 2.0 (0.24), 1.8 (0.37), 1.9 (0.27) for hard. GPT-4 performed better for all three qualities and difficulty levels than GPT-3.5. The FKG mean for GPT-3.5 and GPT-4 answers were 12.8 (1.75) and 10.8 (1.72), respectively; the FRE for GPT-3.5 and GPT-4 was 37.3 (9.65) and 47.6 (9.88), respectively. The 2nd prompt has achieved better results in terms of clarity (all p < 0.05). Conclusions: GPT-4 displayed superior accuracy, clarity, conciseness, and readability than GPT-3.5. Though prompts influenced the quality response in both GPTs, their impact was significant only for clarity. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.",yes,"This study evaluates the clinical appropriateness of ChatGPT (GPT-3.5 and GPT-4) in providing recommendations for prostate cancer screening, a direct clinical application. The study assesses the accuracy, clarity, and conciseness of LLM-generated answers to clinical questions, which fits the inclusion criteria.",10.1007/s11255-024-04009-5,II,Vivian,"fake data, real task, not a clinical study",,
Evaluation of the Usability of ChatGPT-4 and Google Gemini in Patient Education About Rhinosinusitis,"Introduction: Artificial intelligence (AI) based chat robots are increasingly used by users for patient education about common diseases in the health field, as in every field. This study aims to evaluate and compare patient education materials on rhinosinusitis created by two frequently used chat robots, ChatGPT-4 and Google Gemini. Method: One hundred nine questions taken from patient information websites were divided into 4 different categories: general knowledge, diagnosis, treatment, surgery and complications, then asked to chat robots. The answers given were evaluated by two different expert otolaryngologists, and on questions where the scores were different, a third, more experienced otolaryngologist finalised the evaluation. Questions were scored from 1 to 4: (1) comprehensive/correct, (2) incomplete/partially correct, (3) accurate and inaccurate data, potentially misleading and (4) completely inaccurate/irrelevant. Results: In evaluating the answers given by ChatGPT-4, all answers in the Diagnosis category were evaluated as comprehensive/correct. In the evaluation of the answers given by Google Gemini, the answers evaluated as completely inaccurate/irrelevant in the treatment category were found to be statistically significantly higher, and the answers evaluated as incomplete/partially correct in the surgery and complications category were found to be statistically significantly higher. In the comparison between the two chat robots, in the treatment category, ChatGPT-4 had a higher correct evaluation rate than Google Gemini and was found to be statistically significant. Conclusion: The answers given by ChatGPT-4 and Google Gemini chat robots regarding rhinosinusitis were evaluated as sufficient and informative. © 2025 John Wiley & Sons Ltd.",yes,"This study evaluates the clinical usability of two LLMs (ChatGPT-4 and Google Gemini) in providing patient education about rhinosinusitis, using real patient questions and expert clinical assessment. The focus is on the accuracy and quality of clinical information provided to patients, which is a direct clinical application.",10.1111/coa.14273,II,Vivian,"fake data (taken from patient information websites - not directly from patients), real task, not a clinical study",,
Expert-Guided Large Language Models for Clinical Decision Support in Precision Oncology,"PURPOSERapidly expanding medical literature challenges oncologists seeking targeted cancer therapies. General-purpose large language models (LLMs) lack domain-specific knowledge, limiting their clinical utility. This study introduces the LLM system Medical Evidence Retrieval and Data Integration for Tailored Healthcare (MEREDITH), designed to support treatment recommendations in precision oncology. Built on Google's Gemini Pro LLM, MEREDITH uses retrieval-augmented generation and chain of thought.METHODSWe evaluated MEREDITH on 10 publicly available fictional oncology cases with iterative feedback from a molecular tumor board (MTB) at a major German cancer center. Initially limited to PubMed-indexed literature (draft system), MEREDITH was enhanced to incorporate clinical studies on drug response within the specific tumor type, trial databases, drug approval status, and oncologic guidelines. The MTB provided a benchmark with manually curated treatment recommendations and assessed the clinical relevance of LLM-generated options (qualitative assessment). We measured semantic cosine similarity between LLM suggestions and clinician responses (quantitative assessment).RESULTSMEREDITH identified a broader range of treatment options (median 4) compared with MTB experts (median 2). These options included therapies on the basis of preclinical data and combination treatments, expanding the treatment possibilities for consideration by the MTB. This broader approach was achieved by incorporating a curated medical data set that contextualized molecular targetability. Mirroring the approach MTB experts use to evaluate MTB cases improved the LLM's ability to generate relevant suggestions. This is supported by high concordance between LLM suggestions and expert recommendations (94.7% for the enhanced system) and a significant increase in semantic similarity from the draft to the enhanced system (from 0.71 to 0.76, P =.01).CONCLUSIONExpert feedback and domain-specific data augment LLM performance. Future research should investigate responsible LLM integration into real-world clinical workflows. © 2024 by American Society of Clinical Oncology.",yes,"This study evaluates a large language model (LLM) system (MEREDITH, based on Gemini Pro) for clinical decision support in precision oncology, specifically for generating treatment recommendations. The evaluation involves clinical experts and compares LLM-generated options to expert recommendations, directly assessing clinical utility.",10.1200/PO-24-00478,II,Vivian,"fake data, real task, not a clinical study",,
Evaluation of ChatGPT pathology knowledge using board-style questions,"Objectives: ChatGPT is an artificial intelligence chatbot developed by OpenAI. Its extensive knowledge and unique interactive capabilities enable its use in various innovative ways in the medical field, such as writing clinical notes and simplifying radiology reports. Through this study, we aimed to analyze the pathology knowledge of ChatGPT to advocate its role in transforming pathology education. Methods: The American Society for Clinical Pathology Resident Question Bank 2022 was used to test ChatGPT, version 4. Practice tests were created in each subcategory and answered based on the input that ChatGPT provided. Questions that required interpretation of images were excluded. We analyzed ChatGPT performance and compared it with average peer performance. Results: The overall performance of ChatGPT was 56.98%, lower than that of the average peer performance of 62.81%. ChatGPT performed better on clinical pathology (60.42%) than on anatomic pathology (54.94%). Furthermore, its performance was better on easy questions (68.47%) than on intermediate (52.88%) and difficult questions (37.21%). Conclusions: ChatGPT has the potential to be a valuable resource in pathology education if trained on a larger, specialized medical data set. Those relying on it (in its current form) solely for the purpose of pathology training should be cautious. © the author(s) 2023. published by oxford University press on behalf of american society for clinical pathology. all rights reserved.",yes,"This study evaluates ChatGPT-4's performance on board-style pathology questions, directly assessing its clinical knowledge relevant to medical education. The use of a standardized question bank and comparison to peer performance aligns with the inclusion criteria for clinical evaluation of LLMs in a healthcare context.",10.1093/ajcp/aqad158,III,Vivian,"fake data, fake task, not a clinical study",,
Evaluating Performance of ChatGPT on MKSAP Cardiology Board Review Questions,"Chat Generative Pretrained Transformer (ChatGPT) is a natural language processing tool created by OpenAI. Much of the discussion regarding artificial intelligence (AI) in medicine is the ability of the language to enhance medical practice, improve efficiency and decrease errors. The objective of this study was to analyze the ability of ChatGPT to answer board-style cardiovascular medicine questions by using the Medical Knowledge Self-Assessment Program (MKSAP).The study evaluated the performance of ChatGPT (versions 3.5 and 4), alongside internal medicine residents and internal medicine and cardiology attendings, in answering 98 multiple-choice questions (MCQs) from the Cardiovascular Medicine Chapter of MKSAP. ChatGPT-4 demonstrated an accuracy of 74.5 %, comparable to internal medicine (IM) intern (63.3 %), senior resident (63.3 %), internal medicine attending physician (62.2 %), and ChatGPT-3.5 (64.3 %) but significantly lower than cardiology attending physician (85.7 %). Subcategory analysis revealed no statistical difference between ChatGPT and physicians, except in valvular heart disease where cardiology attending outperformed ChatGPT (p = 0.031) for version 3.5, and for heart failure (p = 0.046) where ChatGPT-4 outperformed senior resident. While ChatGPT shows promise in certain subcategories, in order to establish AI as a reliable educational tool for medical professionals, performance of ChatGPT will likely need to surpass the accuracy of instructors, ideally achieving the near-perfect score on posed questions. © 2024 Elsevier B.V.",yes,"This study evaluates the performance of ChatGPT (versions 3.5 and 4) on board-style cardiovascular medicine questions, comparing its accuracy to that of medical residents and attendings. The focus is on clinical knowledge assessment, which is a direct clinical application relevant to medical education and decision support.",10.1016/j.ijcard.2024.132576,III,Vivian,"fake data, fake task, not a clinical study",,
Assessing clinical acuity in the Emergency Department using the GPT-3.5 Artificial Intelligence Model,"This paper evaluates the performance of the Chat Generative Pre-trained Transformer (ChatGPT; GPT-3.5) in accurately identifying higher acuity patients in a real-world clinical context. Using a dataset of 10,000 pairs of patient Emergency Department (ED) visits with varying acuity levels, we demonstrate that GPT-3.5 can successfully determine the patient with higher acuity based on clinical history sections extracted from ED physician notes. The model achieves an accuracy of 84% and an F1 score of 0.83, with improved performance for more disparate acuity scores. Among the 500 pair subsample that was also manually classified by a resident physician, GPT-3.5 achieved similar performance (Accuracy = 0.84; F1 score = 0.85) compared to the physician (Accuracy = 0.86, F1 score = 0.87). Our results suggest that, in real-world settings, GPT-3.5 can perform comparably to physicians on the clinical reasoning task of ED acuity determination.",yes,"This study evaluates the clinical performance of GPT-3.5 (ChatGPT) in determining patient acuity in the Emergency Department, a direct clinical application. The model's performance is compared to that of a resident physician, making it a clear example of LLM evaluation in a healthcare context.",10.1101/2023.08.09.23293795,I,Vivian,"real data, real task, retrospective",,
neuroGPT-X: Towards an Accountable Expert Opinion Tool for Vestibular Schwannoma,"Background The global launch of ChatGPT on November 30, 2022 has sparked widespread public interest in large language models (LLMs), and interest in the medical community is growing. Indeed, recent preprints on medRxiv have examined ChatGPT and GPT-3 in the context of standardized exams, such as the United States Medical Licensing Examination. These studies demonstrate modest performance relative to national averages. In this work, we enhance OpenAI's GPT-3 model through zero-shot learning, anticipating that it outperforms experienced neurosurgeons in written question-answer tasks for common clinical and surgical questions on vestibular schwannoma. We aimed to address LLM accountability by including in-text citations and references to the responses provided by GPT-3. Methods The analysis involved (i) creating a dataset through web scraping, (ii) developing a chat-based platform called neuroGPT-X, (iii) enlisting expert neurosurgeons across international centers to create and answer questions and evaluate responses, and (iv) analyzing the evaluation results on the management of vestibular schwannoma. The survey had a blinded and unblinded phase. In the blinded phase, a neurosurgeon with 30+ years of experience curated 15 questions regarding common clinical and surgical contexts of vestibular schwannoma. Then, four neurosurgeons, ChatGPT (January 30, 2023 model, aka naive GPT), and a context-enriched GPT model independently provided their responses. Three experienced neurosurgeons blindly evaluated the responses for accuracy, coherence, relevance, thoroughness, speed, and overall rating. Then, all seven neurosurgeons were unblinded to all responses and provided their thoughts on the potential of expert LLMs in the clinical setting. Findings Both the naive and content-enriched GPT models provided faster responses to the standardized question set (p<0.01) than expert neurosurgeon respondents. Moreover, responses from both models were consistently non-inferior in accuracy, coherence, relevance, thoroughness, and overall performance, and were often rated higher than expert responses. Importantly, context enrichment of GPT with relevant scientific literature did not significantly affect speed (p>0.999) or performance across the aforementioned domains (p>0.999). Of interest, all expert surgeons expressed concerns about the reliability of GPT in accurately addressing the nuances and controversies surrounding the management of vestibular schwannoma. Further, we developed neuroGPT-X, a chat-based platform designed to provide point-of-care clinical support and mitigate limitations of human memory. neuroGPT-X incorporates features such as in-text citations and references to enable accurate, relevant, and reliable information in real-time. Interpretation A context-enriched GPT model provided non-inferior responses compared to experienced neurosurgeons in generating written responses to a complex neurosurgical problem for which evidence-based consensus for management is lacking. We show that context enrichment of LLMs is well-suited to transform clinical practice by providing subspecialty-level answers to clinical questions in an accountable manner.",yes,"This study evaluates a GPT-3-based LLM (including a context-enriched version) in answering clinical and surgical questions about vestibular schwannoma, comparing its performance to expert neurosurgeons. The LLM is assessed for accuracy, coherence, relevance, and other clinically relevant metrics, and is intended for point-of-care clinical support. This is a direct clinical evaluation of an LLM in a medical context.",10.1101/2023.02.25.23286117,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT-3.5 passes Poland’s medical final examination—Is it possible for ChatGPT to become a doctor in Poland?,"Objectives: ChatGPT is an advanced chatbot based on Large Language Model that has the ability to answer questions. Undoubtedly, ChatGPT is capable of transforming communication, education, and customer support; however, can it play the role of a doctor? In Poland, prior to obtaining a medical diploma, candidates must successfully pass the Medical Final Examination. Methods: The purpose of this research was to determine how well ChatGPT performed on the Polish Medical Final Examination, which passing is required to become a doctor in Poland (an exam is considered passed if at least 56% of the tasks are answered correctly). A total of 2138 categorized Medical Final Examination questions (from 11 examination sessions held between 2013–2015 and 2021–2023) were presented to ChatGPT-3.5 from 19 to 26 May 2023. For further analysis, the questions were divided into quintiles based on difficulty and duration, as well as question types (simple A-type or complex K-type). The answers provided by ChatGPT were compared to the official answer key, reviewed for any changes resulting from the advancement of medical knowledge. Results: ChatGPT correctly answered 53.4%–64.9% of questions. In 8 out of 11 exam sessions, ChatGPT achieved the scores required to successfully pass the examination (60%). The correlation between the efficacy of artificial intelligence and the level of complexity, difficulty, and length of a question was found to be negative. AI outperformed humans in one category: psychiatry (77.18% vs. 70.25%, p = 0.081). Conclusions: The performance of artificial intelligence is deemed satisfactory; however, it is observed to be markedly inferior to that of human graduates in the majority of instances. Despite its potential utility in many medical areas, ChatGPT is constrained by its inherent limitations that prevent it from entirely supplanting human expertise and knowledge. © The Author(s) 2024.",yes,"This study evaluates the performance of ChatGPT-3.5 on a national medical board examination, directly assessing its clinical knowledge and potential as a medical practitioner. This fits the inclusion criteria as it is a clinical evaluation of an LLM in a healthcare context.",10.1177/20503121241257777,III,Vivian,"fake data, fake task, not a clinical study",,
Revolutionizing Nurse Practitioner Training: Integrating Virtual Reality and Large Language Models for Enhanced Clinical Education,"This project introduces an innovative virtual reality (VR) training program for student Nurse Practitioners, incorporating advanced 3D modeling, animation, and Large Language Models (LLMs). Designed to simulate realistic patient interactions, the program aims to improve communication, history taking, and clinical decision-making skills in a controlled, authentic setting. This abstract outlines the methods, results, and potential impact of this cutting-edge educational tool on nursing education.  © 2024 The Authors.",yes,"The study describes the integration of LLMs into a VR training program for nurse practitioners, specifically to simulate patient interactions and enhance clinical skills such as communication, history taking, and decision-making. Since the LLM is used in a clinical education context to simulate realistic patient interactions and support clinical decision-making, this fits the inclusion criteria.",10.3233/SHTI240272,X,Vivian,this is a pilot study with no results... ,,
Is ChatGPT accurate and reliable in answering questions regarding head and neck cancer?,"Background and objective: Chat Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence (AI)-based language processing model using deep learning to create human-like text dialogue. It has been a popular source of information covering vast number of topics including medicine. Patient education in head and neck cancer (HNC) is crucial to enhance the understanding of patients about their medical condition, diagnosis, and treatment options. Therefore, this study aims to examine the accuracy and reliability of ChatGPT in answering questions regarding HNC. Methods: 154 head and neck cancer-related questions were compiled from sources including professional societies, institutions, patient support groups, and social media. These questions were categorized into topics like basic knowledge, diagnosis, treatment, recovery, operative risks, complications, follow-up, and cancer prevention. ChatGPT was queried with each question, and two experienced head and neck surgeons assessed each response independently for accuracy and reproducibility. Responses were rated on a scale: (1) comprehensive/correct, (2) incomplete/partially correct, (3) a mix of accurate and inaccurate/misleading, and (4) completely inaccurate/irrelevant. Discrepancies in grading were resolved by a third reviewer. Reproducibility was evaluated by repeating questions and analyzing grading consistency. Results: ChatGPT yielded “comprehensive/correct” responses to 133/154 (86.4%) of the questions whereas, rates of “incomplete/partially correct” and “mixed with accurate and inaccurate data/misleading” responses were 11% and 2.6%, respectively. There were no “completely inaccurate/irrelevant” responses. According to category, the model provided “comprehensive/correct” answers to 80.6% of questions regarding “basic knowledge”, 92.6% related to “diagnosis”, 88.9% related to “treatment”, 80% related to “recovery – operative risks – complications – follow-up”, 100% related to “cancer prevention” and 92.9% related to “other”. There was not any significant difference between the categories regarding the grades of ChatGPT responses (p=0.88). The rate of reproducibility was 94.1% (145 of 154 questions). Conclusion: ChatGPT generated substantially accurate and reproducible information to diverse medical queries related to HNC. Despite its limitations, it can be a useful source of information for both patients and medical professionals. With further developments in the model, ChatGPT can also play a crucial role in clinical decision support to provide the clinicians with up-to-date information. Copyright © 2023 Kuşcu, Pamuk, Sütay Süslü and Hosal.",yes,"This study directly evaluates ChatGPT's accuracy and reliability in answering clinical questions about head and neck cancer, with responses assessed by clinicians. The focus is on patient education and clinical information, fitting the inclusion criteria for clinical evaluation of an LLM in a healthcare context.",10.3389/fonc.2023.1256459,II,Vivian,"fake data, real task, not a clinical study",,
Exploring the Diagnostic Potential of LLMs in Schizophrenia Detection through EEG Analysis,"Schizophrenia is a psychiatric disorder that presents significant diagnostic challenges due to its complex neurophysiological characteristics. This paper investigates the potential of Large Language Models (LLMs), such as OpenAI's GPT-4 and GPT-o1, in detecting schizophrenia through electroencephalography (EEG) analysis. Using the LMSU public ScZ EEG dataset, we conducted a series of experiments involving different types of input data, including raw EEG signals, frequency band summaries, and graphical representations of brain activity. Our findings demonstrate that LLMs can accurately classify schizophrenic and healthy individuals while offering interpretable, clinically relevant insights aligned with established EEG markers. By integrating these models into the diagnostic workflow, we explore the concept of Symbiotic AI, where LLMs act as cognitive collaborators, enhancing clinicians' ability to analyze complex data efficiently and transparently. This approach not only improves diagnostic accuracy but also facilitates real-time decision-making, paving the way for earlier and more precise detection of schizophrenia in clinical settings. © 2024 IEEE.",yes,"The study evaluates the use of LLMs (specifically GPT-4 and GPT-o1) for the clinical task of diagnosing schizophrenia from EEG data, and discusses their integration into clinical workflows for real-time decision-making. This is a direct clinical application of LLMs in healthcare, meeting the inclusion criteria.",10.1109/BIBM62325.2024.10821830,I,Vivian,"real data, real task, retrospective",,
From Bytes to Best Practices: Tracing ChatGPT-3.5’s Evolution and Alignment With the National Comprehensive Cancer Network® Guidelines in Pancreatic Adenocarcinoma Management,"Introduction: Artificial intelligence continues to play an increasingly important role in modern health care. ChatGPT-3.5 (OpenAI, San Francisco, CA) has gained attention for its potential impact in this domain. Objective: To explore the role of ChatGPT-3.5 in guiding clinical decision-making specifically in the context of pancreatic adenocarcinoma and to assess its growth over a period of time. Participants: We reviewed the National Comprehensive Cancer Network® (NCCN) Clinical Practice Guidelines for the Management of Pancreatic Adenocarcinoma and formulated a complex clinical question for each decision-making page. ChatGPT-3.5 was queried in a reproducible fashion. We scored answers on the following Likert scale: 5) Correct; 4) Correct, with missing information requiring clarification; 3) Correct, but unable to complete answer; 2) Partially incorrect; 1) Absolutely incorrect. We repeated this protocol at 3-months. Score frequencies were compared, and subgroup analysis was conducted on Correctness (defined as scores 1-2 vs 3-5) and Accuracy (scores 1-3 vs 4-5). Results: In total, 50-pages of the NCCN Guidelines® were analyzed, generating 50 complex clinical questions. On subgroup analysis, the percentage of Acceptable answers improved from 60% to 76%. The score improvement was statistically significant (Mann-Whitney U-test; Mean Rank = 44.52 vs 56.48, P =.027). Conclusion: ChatGPT-3.5 represents an interesting but limited tool for assistance in clinical decision-making. We demonstrate that the platform evolved, and its responses to our standardized questions improved over a relatively short period (3-months). Future research is needed to determine the validity of this tool for this clinical application. © The Author(s) 2024.",yes,"This study evaluates ChatGPT-3.5's ability to answer complex clinical questions based on NCCN guidelines for pancreatic adenocarcinoma, directly assessing its performance in clinical decision-making. This fits the inclusion criteria as it tests an LLM in a clinical context.",10.1177/00031348241248801,II,Vivian,"fake data, real task, not a clinical study",,
Generative artificial intelligence and non-pharmacological bias: an experimental study on cancer patient sexual health communications,"2578. BMJ Health Care Inform. 2024 Apr 4;31(1):e100924. doi: 
10.1136/bmjhci-2023-100924.

Generative artificial intelligence and non-pharmacological bias: an experimental 
study on cancer patient sexual health communications.

Hanai A(1)(2), Ishikawa T(3)(2)(4)(5), Kawauchi S(3), Iida Y(3), Kawakami 
E(3)(2).

Author information:
(1)Medical Data Mathematical Reasoning Team, Advanced Data Science Project, 
RIKEN Information R&D and Strategy Headquarters, RIKEN, Yokohama, Japan 
hanaaki0803@gmail.com.
(2)Department of Artificial Intelligence Medicine, Graduate School of Medicine, 
Chiba University, Chiba, Japan.
(3)Medical Data Mathematical Reasoning Team, Advanced Data Science Project, 
RIKEN Information R&D and Strategy Headquarters, RIKEN, Yokohama, Japan.
(4)Department of Extended Intelligence for Medicine, The Ishii-Ishibashi 
Laboratory, Keio University School of Medicine, Tokyo, Japan.
(5)Collective Intelligence Research Laboratory, Graduate School of Arts and 
Sciences, The University of Tokyo, Tokyo, Japan.

Objectives The objective of this study was to explore the feature of generative 
artificial intelligence (AI) in asking sexual health among cancer survivors, 
which are often challenging for patients to discuss.Methods We employed the 
Generative Pre-trained Transformer-3.5 (GPT) as the generative AI platform and 
used DocsBot for citation retrieval (June 2023). A structured prompt was devised 
to generate 100 questions from the AI, based on epidemiological survey data 
regarding sexual difficulties among cancer survivors. These questions were 
submitted to Bot1 (standard GPT) and Bot2 (sourced from two clinical 
guidelines).Results No censorship of sexual expressions or medical terms 
occurred. Despite the lack of reflection on guideline recommendations, 
'consultation' was significantly more prevalent in both bots' responses compared 
with pharmacological interventions, with ORs of 47.3 (p<0.001) in Bot1 and 97.2 
(p<0.001) in Bot2.Discussion Generative AI can serve to provide health 
information on sensitive topics such as sexual health, despite the potential for 
policy-restricted content. Responses were biased towards non-pharmacological 
interventions, which is probably due to a GPT model designed with the 's 
prohibition policy on replying to medical topics. This shift warrants attention 
as it could potentially trigger patients' expectations for non-pharmacological 
interventions.

© Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No 
commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study evaluates GPT-3.5's responses to sexual health questions for cancer survivors, specifically analyzing the nature of its clinical advice (non-pharmacological vs. pharmacological). The LLM is used in a clinical context to provide health information to patients, meeting the inclusion criteria.",10.1136/bmjhci-2023-100924,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT-4: An assessment of an upgraded artificial intelligence chatbot in the United States Medical Licensing Examination,"PURPOSE: ChatGPT-4 is an upgraded version of an artificial intelligence chatbot. The performance of ChatGPT-4 on the United States Medical Licensing Examination (USMLE) has not been independently characterized. We aimed to assess the performance of ChatGPT-4 at responding to USMLE Step 1, Step 2CK, and Step 3 practice questions. METHOD: Practice multiple-choice questions for the USMLE Step 1, Step 2CK, and Step 3 were compiled. Of 376 available questions, 319 (85%) were analyzed by ChatGPT-4 on March 21st, 2023. Our primary outcome was the performance of ChatGPT-4 for the practice USMLE Step 1, Step 2CK, and Step 3 examinations, measured as the proportion of multiple-choice questions answered correctly. Our secondary outcomes were the mean length of questions and responses provided by ChatGPT-4. RESULTS: ChatGPT-4 responded to 319 text-based multiple-choice questions from USMLE practice test material. ChatGPT-4 answered 82 of 93 (88%) questions correctly on USMLE Step 1, 91 of 106 (86%) on Step 2CK, and 108 of 120 (90%) on Step 3. ChatGPT-4 provided explanations for all questions. ChatGPT-4 spent 30.8 ± 11.8 s on average responding to practice questions for USMLE Step 1, 23.0 ± 9.4 s per question for Step 2CK, and 23.1 ± 8.3 s per question for Step 3. The mean length of practice USMLE multiple-choice questions that were answered correctly and incorrectly by ChatGPT-4 was similar (difference = 17.48 characters, SE = 59.75, 95%CI = [-100.09,135.04], t = 0.29, p = 0.77). The mean length of ChatGPT-4's correct responses to practice questions was significantly shorter than the mean length of incorrect responses (difference = 79.58 characters, SE = 35.42, 95%CI = [9.89,149.28], t = 2.25, p = 0.03). CONCLUSIONS: ChatGPT-4 answered a remarkably high proportion of practice questions correctly for USMLE examinations. ChatGPT-4 performed substantially better at USMLE practice questions than previous models of the same AI chatbot.",yes,"This study evaluates the clinical performance of ChatGPT-4 by testing it on USMLE Step 1, Step 2CK, and Step 3 practice questions, which are directly relevant to medical knowledge and clinical reasoning. The study measures the accuracy of ChatGPT-4 in answering these clinically oriented exam questions, fitting the inclusion criteria.",10.1080/0142159X.2023.2249588,III,Vivian,"fake data, fake task, not a clinical study",,
Applying GPT-4 to the Plastic Surgery Inservice Training Examination,"Background: The recent introduction of Generative Pre-trained Transformer (GPT)-4 has demonstrated the potential to be a superior version of ChatGPT-3.5. According to many, GPT-4 is seen as a more reliable and creative version of GPT-3.5. Objective: In conjugation with our prior manuscript, we wanted to determine if GPT-4 could be exploited as an instrument for plastic surgery graduate medical education by evaluating its performance on the Plastic Surgery Inservice Training Examination (PSITE). Methods: Sample assessment questions from the 2022 PSITE were obtained from the American Council of Academic Plastic Surgeons website and manually inputted into GPT-4. Responses by GPT-4 were qualified using the properties of natural coherence. Incorrect answers were stratified into the consequent categories: informational, logical, or explicit fallacy. Results: From a total of 242 questions, GPT-4 provided correct answers for 187, resulting in a 77.3% accuracy rate. Logical reasoning was utilized in 95.0% of questions, internal information in 98.3%, and external information in 97.5%. Upon separating the questions based on incorrect and correct responses, a statistically significant difference was identified in GPT-4's application of logical reasoning. Conclusion: GPT-4 has shown to be more accurate and reliable for plastic surgery resident education when compared to GPT-3.5. Users should look to utilize the tool to enhance their educational curriculum. Those who adopt the use of such models may be better equipped to deliver high-quality care to their patients. © 2023 British Association of Plastic, Reconstructive and Aesthetic Surgeons",yes,"This study evaluates GPT-4's performance on the Plastic Surgery Inservice Training Examination, a clinically relevant board exam for medical education. It directly assesses the clinical knowledge and reasoning capabilities of a generative LLM (GPT-4) in a healthcare context, which fits the inclusion criteria.",10.1016/j.bjps.2023.09.027,III,Vivian,"fake data, fake task, not a clinical study",,
Comprehensiveness of Large Language Models in Patient Queries on Gingival and Endodontic Health,"AIM: Given the increasing interest in using large language models (LLMs) for self-diagnosis, this study aimed to evaluate the comprehensiveness of two prominent LLMs, ChatGPT-3.5 and ChatGPT-4, in addressing common queries related to gingival and endodontic health across different language contexts and query types. METHODS: We assembled a set of 33 common real-life questions related to gingival and endodontic healthcare, including 17 common-sense questions and 16 expert questions. Each question was presented to the LLMs in both English and Chinese. Three specialists were invited to evaluate the comprehensiveness of the responses on a five-point Likert scale, where a higher score indicated greater quality responses. RESULTS: LLMs performed significantly better in English, with an average score of 4.53, compared to 3.95 in Chinese (Mann-Whitney U test, P < .05). Responses to common sense questions received higher scores than those to expert questions, with averages of 4.46 and 4.02 (Mann-Whitney U test, P < .05). Among the LLMs, ChatGPT-4 consistently outperformed ChatGPT-3.5, achieving average scores of 4.45 and 4.03 (Mann-Whitney U test, P < .05). CONCLUSIONS: ChatGPT-4 provides more comprehensive responses than ChatGPT-3.5 for queries related to gingival and endodontic health. Both LLMs perform better in English and on common sense questions. However, the performance discrepancies across different language contexts and the presence of inaccurate responses suggest that further evaluation and understanding of their limitations are crucial to avoid potential misunderstandings. CLINICAL RELEVANCE: This study revealed the performance differences of ChatGPT-3.5 and ChatGPT-4 in handling gingival and endodontic health issues across different language contexts, providing insights into the comprehensiveness and limitations of LLMs in addressing common oral healthcare queries.",yes,"This study evaluates ChatGPT-3.5 and ChatGPT-4 in answering patient queries about gingival and endodontic health, using real-life clinical questions and specialist evaluation. The focus is on the clinical comprehensiveness and accuracy of LLM responses to patient health questions, which is a direct clinical application.",10.1016/j.identj.2024.06.022,II,Vivian,"fake data, real task, not a clinical study",,
"Trends in Accuracy and Appropriateness of Alopecia Areata Information Obtained from a Popular Online Large Language Model, ChatGPT","Background: Patients with alopecia areata (AA) may access a wide range of sources for information about AA, including the recently developed ChatGPT. Assessing the quality of health information provided by these sources is crucial, as patients are utilizing them in increasing numbers. Objectives: The aim of the study was to evaluate appropriateness and accuracy of responses to common patient questions about AA generated by ChatGPT. Methods: Responses generated by ChatGPT 3.5 and ChatGPT 4.0 to 25 questions addressing common patient concerns were assessed by multiple attending dermatologists in an academic center for appropriateness and accuracy. Appropriateness of responses by both models for use in two hypothetical contexts as follows: (1) for patient-facing general information websites, and (2) for electronic health record (EHR) message drafts. Results: The accuracy across all responses was 4.41 out of 5. Accuracy scores of responses ChatGPT 3.5 responses had a mean of 4.29, whereas those generated by ChatGPT 4.0 had mean accuracy score of 4.53. Assessments ranged from 100% of responses rated as appropriate for the general question category to 79% questions about management for an EHR message draft. Raters largely preferred responses generated by ChatGPT 4.0 versus ChatGPT 3.5. Reviewer agreement was found to be moderate across all questions, with a 53.7% agreement and Fleiss' κ co-efficient of 0.522 (p value <0.001). Conclusions: The large language model ChatGPT outputted mostly appropriate information for common patient concerns. While not all responses were accurate, the trend toward improvement with newer iterations suggests potential future utility for patients and dermatologists.  © 2023 S. Karger AG. All rights reserved.",yes,"This study evaluates the accuracy and appropriateness of ChatGPT (GPT-3.5 and GPT-4.0) responses to common patient questions about alopecia areata, assessed by dermatologists. The focus is on clinical information provided to patients, which is a direct clinical application of LLMs.",10.1159/000534005,II,Vivian,"fake data, real task, not a clinical study",,
"Comparative accuracy of ChatGPT-4, Microsoft Copilot and Google Gemini in the Italian entrance test for healthcare sciences degrees: a cross-sectional study","BACKGROUND: Artificial intelligence (AI) chatbots are emerging educational tools for students in healthcare science. However, assessing their accuracy is essential prior to adoption in educational settings. This study aimed to assess the accuracy of predicting the correct answers from three AI chatbots (ChatGPT-4, Microsoft Copilot and Google Gemini) in the Italian entrance standardized examination test of healthcare science degrees (CINECA test). Secondarily, we assessed the narrative coherence of the AI chatbots' responses (i.e., text output) based on three qualitative metrics: the logical rationale behind the chosen answer, the presence of information internal to the question, and presence of information external to the question. METHODS: An observational cross-sectional design was performed in September of 2023. Accuracy of the three chatbots was evaluated for the CINECA test, where questions were formatted using a multiple-choice structure with a single best answer. The outcome is binary (correct or incorrect). Chi-squared test and a post hoc analysis with Bonferroni correction assessed differences among chatbots performance in accuracy. A p-value of < 0.05 was considered statistically significant. A sensitivity analysis was performed, excluding answers that were not applicable (e.g., images). Narrative coherence was analyzed by absolute and relative frequencies of correct answers and errors. RESULTS: Overall, of the 820 CINECA multiple-choice questions inputted into all chatbots, 20 questions were not imported in ChatGPT-4 (n = 808) and Google Gemini (n = 808) due to technical limitations. We found statistically significant differences in the ChatGPT-4 vs Google Gemini and Microsoft Copilot vs Google Gemini comparisons (p-value < 0.001). The narrative coherence of AI chatbots revealed ""Logical reasoning"" as the prevalent correct answer (n = 622, 81.5%) and ""Logical error"" as the prevalent incorrect answer (n = 40, 88.9%). CONCLUSIONS: Our main findings reveal that: (A) AI chatbots performed well; (B) ChatGPT-4 and Microsoft Copilot performed better than Google Gemini; and (C) their narrative coherence is primarily logical. Although AI chatbots showed promising accuracy in predicting the correct answer in the Italian entrance university standardized examination test, we encourage candidates to cautiously incorporate this new technology to supplement their learning rather than a primary resource. TRIAL REGISTRATION: Not required.",yes,"This study evaluates the performance of LLMs (ChatGPT-4, Microsoft Copilot, and Google Gemini) on a standardized entrance exam for healthcare sciences. While the exam is related to healthcare, the focus is on entrance-level knowledge rather than clinical decision-making or application in a clinical context. The primary task is educational (exam performance), not clinical care, diagnosis, or patient interaction. However, the inclusion criteria allow for studies evaluating LLMs on board exams or similar, and entrance exams for healthcare degrees are closely related to this. Therefore, this study fits the inclusion criteria.",10.1186/s12909-024-05630-9,III,Vivian,"fake data, fake task, not a clinical study",,
Performance of an artificial intelligence-based chatbot when acting as EMS dispatcher in a cardiac arrest scenario,"786. Intern Emerg Med. 2023 Nov;18(8):2449-2452. doi: 10.1007/s11739-023-03399-1. 
Epub 2023 Aug 21.

Performance of an artificial intelligence-based chatbot when acting as EMS 
dispatcher in a cardiac arrest scenario.

Birkun A(1).

Author information:
(1)Department of General Surgery, Anaesthesiology, Resuscitation and Emergency 
Medicine, Medical Academy Named After S. I. Georgievsky of V. I. Vernadsky 
Crimean Federal University, Simferopol, Russian Federation. 
birkunalexei@gmail.com.",yes,"The study evaluates the performance of an AI-based chatbot acting as an EMS dispatcher in a cardiac arrest scenario, which is a clinical application relevant to healthcare. Although the abstract does not specify the exact model, the use of 'chatbot' in this context and the publication date suggest it is likely a generative LLM. If further details confirm the use of a generative LLM, it should be included.",10.1007/s11739-023-03399-1,X,Vivian,"model is Bing, not an LLM",,
Challenging the Chatbot: An Assessment of ChatGPT's Diagnoses and Recommendations for DBP Case Studies,"Objective:Chat Generative Pretrained Transformer-3.5 (ChatGPT) is a publicly available and free artificial intelligence chatbot that logs billions of visits per day; parents may rely on such tools for developmental and behavioral medical consultations. The objective of this study was to determine how ChatGPT evaluates developmental and behavioral pediatrics (DBP) case studies and makes recommendations and diagnoses.Methods:ChatGPT was asked to list treatment recommendations and a diagnosis for each of 97 DBP case studies. A panel of 3 DBP physicians evaluated ChatGPT's diagnostic accuracy and scored treatment recommendations on accuracy (5-point Likert scale) and completeness (3-point Likert scale). Physicians also assessed whether ChatGPT's treatment plan correctly addressed cultural and ethical issues for relevant cases. Scores were analyzed using Python, and descriptive statistics were computed.Results:The DBP panel agreed with ChatGPT's diagnosis for 66.2% of the case reports. The mean accuracy score of ChatGPT's treatment plan was deemed by physicians to be 4.6 (between entirely correct and more correct than incorrect), and the mean completeness was 2.6 (between complete and adequate). Physicians agreed that ChatGPT addressed relevant cultural issues in 10 out of the 11 appropriate cases and the ethical issues in the single ethical case.Conclusion:While ChatGPT can generate a comprehensive and adequate list of recommendations, the diagnosis accuracy rate is still low. Physicians must advise caution to patients when using such online sources. © 2024 Lippincott Williams and Wilkins. All rights reserved.",yes,"This study evaluates ChatGPT-3.5's ability to provide diagnoses and treatment recommendations for developmental and behavioral pediatrics case studies, with clinical experts assessing its accuracy and completeness. This is a direct clinical evaluation of an LLM in a healthcare context.",10.1097/DBP.0000000000001255,II,Vivian,"data=case reports from real patients (per predetermined definitions this is fake data), real task, not a clinical study",,
Comparative assessment of three AI platforms in answering USMLE Step 1 anatomy questions or identifying anatomical structures on radiographs,"The application of artificial intelligence (AI) in education has gained great attention recently. Integration of AI tools in anatomy teaching is currently engaging researchers and academics worldwide. Several AI chatbots have been generated, the most popular being ChatGPT (OpenAI: San Francisco, California, USA). Since its first public release in November 2022, several research papers have pointed to its potential role in anatomy education. However, it is not yet known whether it will prove superior to other available AI tools in this role. This article sheds some light on the current status of research concerning AI applications in anatomy education and compares the performances of three well-known chatbots (ChatGPT, Gemini, and Claude) in answering anatomy questions. A total of 23 questions were used as prompts for each chatbot. These questions comprised 10 knowledge-based, 10 analysis-based USMLE Step 1-type, and three radiographs. ChatGPT was the most accurate of the three, scoring 100% accuracy. However, in terms of comprehensiveness, Claude was the best; it gave very organized anatomical responses. Gemini performed less well than the other two, with a scored accuracy of 60% and less scientific explanations. On the basis of these findings, this study recommends the incorporation of Claude and ChatGPT in anatomy education, but not Gemini, at least in its current state. © 2024 American Association of Clinical Anatomists and British Association of Clinical Anatomists.",yes,"This study evaluates the performance of three generative LLMs (ChatGPT, Gemini, Claude) in answering USMLE Step 1 anatomy questions and identifying anatomical structures on radiographs. The focus is on their ability to answer clinically relevant exam questions, which is a direct clinical application in medical education.",10.1002/ca.24243,III,Vivian,"fake data, fake task, not a cilnical study",,
A medical multimodal large language model for future pandemics,"Deep neural networks have been integrated into the whole clinical decision procedure which can improve the efficiency of diagnosis and alleviate the heavy workload of physicians. Since most neural networks are supervised, their performance heavily depends on the volume and quality of available labels. However, few such labels exist for rare diseases (e.g., new pandemics). Here we report a medical multimodal large language model (Med-MLLM) for radiograph representation learning, which can learn broad medical knowledge (e.g., image understanding, text semantics, and clinical phenotypes) from unlabelled data. As a result, when encountering a rare disease, our Med-MLLM can be rapidly deployed and easily adapted to them with limited labels. Furthermore, our model supports medical data across visual modality (e.g., chest X-ray and CT) and textual modality (e.g., medical report and free-text clinical note); therefore, it can be used for clinical tasks that involve both visual and textual data. We demonstrate the effectiveness of our Med-MLLM by showing how it would perform using the COVID-19 pandemic “in replay”. In the retrospective setting, we test the model on the early COVID-19 datasets; and in the prospective setting, we test the model on the new variant COVID-19-Omicron. The experiments are conducted on 1) three kinds of input data; 2) three kinds of downstream tasks, including disease reporting, diagnosis, and prognosis; 3) five COVID-19 datasets; and 4) three different languages, including English, Chinese, and Spanish. All experiments show that our model can make accurate and robust COVID-19 decision-support with little labelled data. © 2023, The Author(s).",yes,"This study introduces and evaluates a medical multimodal large language model (Med-MLLM) for clinical decision support, specifically in the context of COVID-19 diagnosis, reporting, and prognosis. The model is tested on real clinical tasks using both textual and visual data, and its performance is assessed in clinical scenarios. This fits the inclusion criteria as it evaluates an LLM in a clinical context.",10.1038/s41746-023-00952-2,I,Vivian,"real data, real task, retrospective",,
GPT-4 as a Clinical Decision Support Tool in Ischemic Stroke Management: Evaluation Study,"Background: Cerebrovascular diseases are the second most common cause of death worldwide and one of the major causes of disability burden. Advancements in artificial intelligence have the potential to revolutionize health care delivery, particularly in critical decision-making scenarios such as ischemic stroke management. Objective: This study aims to evaluate the effectiveness of GPT-4 in providing clinical support for emergency department neurologists by comparing its recommendations with expert opinions and real-world outcomes in acute ischemic stroke management. Methods: A cohort of 100 patients with acute stroke symptoms was retrospectively reviewed. Data used for decision-making included patients’ history, clinical evaluation, imaging study results, and other relevant details. Each case was independently presented to GPT-4, which provided scaled recommendations (1-7) regarding the appropriateness of treatment, the use of tissue plasminogen activator, and the need for endovascular thrombectomy. Additionally, GPT-4 estimated the 90-day mortality probability for each patient and elucidated its reasoning for each recommendation. The recommendations were then compared with a stroke specialist’s opinion and actual treatment decisions. Results: In our cohort of 100 patients, treatment recommendations by GPT-4 showed strong agreement with expert opinion (area under the curve [AUC] 0.85, 95% CI 0.77-0.93) and real-world treatment decisions (AUC 0.80, 95% CI 0.69-0.91). GPT-4 showed near-perfect agreement with real-world decisions in recommending endovascular thrombectomy (AUC 0.94, 95% CI 0.89-0.98) and strong agreement for tissue plasminogen activator treatment (AUC 0.77, 95% CI 0.68-0.86). Notably, in some cases, GPT-4 recommended more aggressive treatment than human experts, with 11 instances where GPT-4 suggested tissue plasminogen activator use against expert opinion. For mortality prediction, GPT-4 accurately identified 10 (77%) out of 13 deaths within its top 25 high-risk predictions (AUC 0.89, 95% CI 0.8077-0.9739; hazard ratio 6.98, 95% CI 2.88-16.9; P<.001), outperforming supervised machine learning models such as PRACTICE (AUC 0.70; log-rank P=.02) and PREMISE (AUC 0.77; P=.07). Conclusions: This study demonstrates the potential of GPT-4 as a viable clinical decision-support tool in the management of acute stroke. Its ability to provide explainable recommendations without requiring structured data input aligns well with the routine workflows of treating physicians. However, the tendency toward more aggressive treatment recommendations highlights the importance of human oversight in clinical decision-making. Future studies should focus on prospective validations and exploring the safe integration of such artificial intelligence tools into clinical practice. © Amit Haim Shmilovitch, Mark Katson, Michal Cohen-Shelly, Shlomi Peretz, Dvir Aran, Shahar Shelly.",yes,"This study directly evaluates GPT-4, a generative large language model, as a clinical decision support tool in the management of acute ischemic stroke. It compares GPT-4's recommendations with expert opinions and real-world outcomes, and assesses its performance in clinical decision-making and mortality prediction. The study is focused on a clinical application and uses a frontier LLM (GPT-4).",10.2196/60391,I,Vivian,"real data, real task, retrospective",,
How Does ChatGPT Perform on the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment,"Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT’s selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSSStep1 dataset. We found logical justification for ChatGPT’s answer selection was present in 100% of outputs. Internal information to the question was present in >90% of all questions. The presence of information external to the question was respectively 54.5% and 27% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60% threshold on the NBME-Free-Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT’s ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.",yes,"This study evaluates ChatGPT's performance on USMLE Step 1 and Step 2 exam questions, which are clinically relevant medical licensing exams. The study assesses the model's accuracy and interpretability in a medical education context, directly testing its clinical knowledge and reasoning. This fits the inclusion criteria as it is a clinical evaluation of an LLM in medicine.",10.1101/2022.12.23.22283901,III,Vivian,"fake data, fake task, not a clinical stdy",,
ChatGPT Can Offer Satisfactory Responses to Common Patient Questions Regarding Elbow Ulnar Collateral Ligament Reconstruction,"Purpose: To determine whether ChatGPT effectively responds to 10 commonly asked questions concerning ulnar collateral ligament (UCL) reconstruction. Methods: A comprehensive list of 90 UCL reconstruction questions was initially created, with a final set of 10 “most commonly asked” questions ultimately selected. Questions were presented to ChatGPT and its response was documented. Responses were evaluated independently by 3 authors using an evidence-based methodology, resulting in a grading system categorized as follows: (1) excellent response not requiring clarification; (2) satisfactory requiring minimal clarification; (3) satisfactory requiring moderate clarification; and (4) unsatisfactory requiring substantial clarification. Results: Six of 10 ten responses were rated as “excellent” or “satisfactory.” Of those 6 responses, 2 were determined to be “excellent response not requiring clarification,” 3 were “satisfactory requiring minimal clarification,” and 1 was “satisfactory requiring moderate clarification.” Four questions encompassing inquiries about “What are the potential risks of UCL reconstruction surgery?” “Which type of graft should be used for my UCL reconstruction?” and “Should I have UCL reconstruction or repair?” were rated as “unsatisfactory requiring substantial clarification.” Conclusions: ChatGPT exhibited the potential to improve a patient's basic understanding of UCL reconstruction and provided responses that were deemed satisfactory to excellent for 60% of the most commonly asked questions. For the other 40% of questions, ChatGPT gave unsatisfactory responses, primarily due to a lack of relevant details or the need for further explanation. Clinical Relevance: ChatGPT can assist in patient education regarding UCL reconstruction; however, its ability to appropriately answer more complex questions remains to be an area of skepticism and future improvement. © 2024 The Authors",yes,"This study evaluates ChatGPT's ability to answer common patient questions about a specific surgical procedure (UCL reconstruction), with responses graded for clinical adequacy. It directly assesses the clinical utility of an LLM in patient education, which fits the inclusion criteria.",10.1016/j.asmr.2024.100893,II,Vivian,"fake data, real task, not a clinical study",,
"Realization of Humanoid Doctor and Real-Time Diagnostics of Disease Using Internet of Things, Edge Impulse Platform, and ChatGPT","3256. Ann Biomed Eng. 2024 Apr;52(4):738-740. doi: 10.1007/s10439-023-03316-9. Epub 
2023 Jul 15.

Realization of Humanoid Doctor and Real-Time Diagnostics of Disease Using 
Internet of Things, Edge Impulse Platform, and ChatGPT.

Venkataswamy R(1), Janamala V(2), Cherukuri RC(3).

Author information:
(1)Department of Electrical and Electronics Engineering, Christ (Deemed to be 
University), Kanminike, Bangalore, Karnataka, 560074, India. 
venkataswamy.r@christuniversity.in.
(2)Department of Electrical and Electronics Engineering, Christ (Deemed to be 
University), Kanminike, Bangalore, Karnataka, 560074, India.
(3)Department of Computer Science and Engineering, Christ (Deemed to be 
University), Kanminike, Bangalore, Karnataka, 560074, India.

Humanoid doctor is an AI-based robot that featured remote bi-directional 
communication and is embedded with disruptive technologies. Accurate and 
real-time responses are the main characteristics of a humanoid doctor which 
diagnoses disease in a patient. The patient details are obtained by Internet of 
Things devices, edge devices, and text formats. The inputs from the patient are 
processed by the humanoid doctor, and it provides its opinion to the patient. 
The historical patient data are trained using cloud artificial intelligence 
platform and the model is tested against the patient sample data acquired using 
medical IoT and edge devices. Disease is identified at three different stages 
and analyzed. The humanoid doctor is expected to identify the diseases well in 
comparison with human healthcare professionals. The humanoid doctor is 
under-trusted because of the lack of a multi-featured accurate model, 
accessibility, availability, and standardization. In this letter, patient input, 
artificial intelligence, and response zones are encapsulated and the humanoid 
doctor is realized.

© 2023. The Author(s) under exclusive licence to Biomedical Engineering Society.",yes,"The study describes a 'humanoid doctor' system that uses ChatGPT as part of its diagnostic process, with patient data collected via IoT and edge devices. The abstract suggests that ChatGPT is used to provide diagnostic opinions to patients, which constitutes a clinical application of an LLM. Although the abstract is somewhat general and lacks detailed results, it appears to evaluate the use of ChatGPT in real-time disease diagnosis, which fits the inclusion criteria.",10.1007/s10439-023-03316-9,???,Vivian,can't access the full article,,
"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans","We introduce CounseLLMe as a multilingual, multi-model dataset of 400 simulated mental health counselling dialogues between two state-of-the-art Large Language Models (LLMs). These conversations - of 20 quips each - were generated either in English (using OpenAI's GPT 3.5 and Claude-3’s Haiku) or Italian (with Claude-3’s Haiku and LLaMAntino) and with prompts tuned with the help of a professional in psychotherapy. We investigate the resulting conversations through comparison against human mental health conversations on the same topic of depression. To compare linguistic features, knowledge structure and emotional content between LLMs and humans, we employed textual forma mentis networks, i.e. cognitive networks where nodes represent concepts and links indicate syntactic or semantic relationships between concepts in the dialogues’ quips. We find that the emotional structure of LLM-LLM English conversations matches the one of humans in terms of patient-therapist trust exchanges, i.e. 1 in 5 LLM-LLM quips contain trust along 10 conversational turns versus the 24% rate found in humans. ChatGPT and Haiku's simulated English patients can also reproduce human feelings of conflict and pessimism. However, human patients display non-negligible levels of anger/frustration that is missing in LLMs. Italian LLMs’ conversations are worse in reproducing human patterns. All LLM-LLM conversations reproduced human syntactic patterns of increased absolutist pronoun usage in patients and second-person, trust-inducing, pronoun usage in therapists. Our results indicate that LLMs can realistically reproduce several aspects of human patient-therapist conversations and we thusly release CounseLLMe as a public dataset for novel data-informed opportunities in mental health and machine psychology. © 2025 The Authors",yes,"This study evaluates the ability of LLMs (ChatGPT, Haiku, LLaMAntino) to simulate mental health counseling dialogues and compares their conversational features to those of humans. While the dialogues are simulated (LLM-LLM), the focus is on the LLMs' ability to reproduce clinically relevant conversational and emotional patterns in mental health contexts. This constitutes a clinical evaluation of LLMs in a healthcare-relevant scenario (mental health counseling), even though the dialogues are not with real patients. The study does not merely focus on technical aspects or non-clinical tasks, and it is not a review or opinion piece.",10.1016/j.etdah.2025.100170,II,Vivian,"fake data, real task, not a clinical study",,
"Comparative study of Claude 3.5-Sonnet and human physicians in generating discharge summaries for patients with renal insufficiency: assessment of efficiency, accuracy, and quality","Background: The rapid development of artificial intelligence (AI) has shown great potential in medical document generation. This study aims to evaluate the performance of Claude 3.5-Sonnet, an advanced AI model, in generating discharge summaries for patients with renal insufficiency, compared to human physicians. Methods: A prospective, comparative study was conducted involving 100 patients (50 with acute kidney injury and 50 with chronic kidney disease) from the nephrology department of Ningbo Hangzhou Bay Hospital between January and June 2024. Discharge summaries were independently generated by Claude 3.5-Sonnet and human physicians. The main evaluation indicators included accuracy, generation time, and overall quality. Results: Claude 3.5-Sonnet demonstrated comparable accuracy to human physicians in generating discharge summaries for both AKI (90 vs. 92 points, p > 0.05) and CKD patients (88 vs. 90 points, p > 0.05). The AI model significantly outperformed human physicians in terms of efficiency, requiring only about 30 s to generate a summary compared to over 15 min for physicians (p < 0.001). The overall quality scores showed no significant difference between AI-generated and physician-written summaries for both AKI (26 vs. 27 points, p > 0.05) and CKD patients (25 vs. 26 points, p > 0.05). Conclusion: Claude 3.5-Sonnet demonstrates high efficiency and reliability in generating discharge summaries for patients with renal insufficiency, with accuracy and quality comparable to those of human physicians. These findings suggest that AI has significant potential to improve the efficiency of medical documentation, though further research is needed to optimize its integration into clinical practice and address ethical and privacy concerns. 2024 Jin, Guo, Lin, Wu, Hu and Li.",yes,"This study evaluates the performance of the Claude 3.5-Sonnet LLM in generating discharge summaries for patients with renal insufficiency, directly comparing its clinical documentation output to that of human physicians. The task—generating discharge summaries—is a clinical application, as these documents are essential for patient care transitions and contain clinical reasoning and recommendations. The study assesses accuracy, efficiency, and quality, all relevant to clinical practice.",10.3389/fdgth.2024.1456911,I,Vivian,"real data, real task, prospective",,
Artificial Intelligence Versus Expert Plastic Surgeon: Comparative Study Shows ChatGPT “Wins” Rhinoplasty Consultations: Should We Be Worried?,"Introduction: Large language models, such as ChatGPT, hold tremendous promise to bridge gaps in patient education and enhance the decision-making resources available online for patients seeking nasal surgery. Objective: To compare the performance of ChatGPT in answering preoperative and postoperative patient questions related to septorhinoplasty. Methods: Two sets of responses were collected for the questions: one from an expert rhinoplasty surgeon with over two decades of experience, and the other from ChatGPT-3.5. Seven expert rhinoplasty surgeons, blinded to the source of responses, independently assessed the responses using a 5-point Likert scale in four performance areas: empathy, accuracy, completeness, and overall quality. Results: ChatGPT outperformed physician responses in three of the four performance areas, earning significantly higher ratings in accuracy, completeness, and overall quality (p < 0.001). In addition, ChatGPT was overwhelmingly preferred over physician responses (p < 0.001), with evaluators favoring ChatGPT in 80.95% of instances. Conclusions: ChatGPT has demonstrated its remarkable ability to deliver accurate, complete, and high-quality responses to preoperative and postoperative patient questions. Although certain improvements are warranted, this artificial intelligence tool has shown its potential to effectively counsel and educate prospective septorhinoplasty patients at a level comparable with or exceeding that of an expert surgeon. Copyright 2023, American Academy of Facial Plastic and Reconstructive Surgery, Inc.",yes,"This study directly evaluates ChatGPT-3.5's ability to answer clinical patient questions about septorhinoplasty, comparing its responses to those of an expert surgeon. The evaluation focuses on clinical communication and education, which is a relevant clinical application of LLMs in healthcare.",10.1089/fpsam.2023.0224,II,Vivian,"fake data (can't access full article but seems to be generic task-specific questions), real task, not a clinical study",,
Can ChatGPT pass the MRCP (UK) written examinations? Analysis of performance and errors using a clinical decision-reasoning framework,"Objective Large language models (LLMs) such as ChatGPT are being developed for use in research, medical education and clinical decision systems. However, as their usage increases, LLMs face ongoing regulatory concerns. This study aims to analyse ChatGPT’s performance on a postgraduate examination to identify areas of strength and weakness, which may provide further insight into their role in healthcare. Design We evaluated the performance of ChatGPT 4 (24 May 2023 version) on official MRCP (Membership of the Royal College of Physicians) parts 1 and 2 written examination practice questions. Statistical analysis was performed using Python. Spearman rank correlation assessed the relationship between the probability of correctly answering a question and two variables: question difficulty and question length. Incorrectly answered questions were analysed further using a clinical reasoning framework to assess the errors made. Setting Online using ChatGPT web interface. Primary and secondary outcome measures Primary outcome was the score (percentage questions correct) in the MRCP postgraduate written examinations. Secondary outcomes were qualitative categorisation of errors using a clinical decision-making framework. Results ChatGPT achieved accuracy rates of 86.3% (part 1) and 70.3% (part 2). Weak but significant correlations were found between ChatGPT’s accuracy and both just-passing rates in part 2 (r=0.34, p=0.0001) and question length in part 1 (r=−0.19, p=0.008). Eight types of error were identified, with the most frequent being factual errors, context errors and omission errors. Conclusion ChatGPT performance greatly exceeded the passing mark for both exams. Multiple choice examinations provide a benchmark for LLM performance which is comparable to human demonstrations of knowledge, while also highlighting the errors LLMs make. Understanding the reasons behind ChatGPT’s errors allows us to develop strategies to prevent them in medical devices that incorporate LLM technology. © Author(s) (or their employer(s)) 2024.",yes,"This study evaluates ChatGPT-4's performance on the MRCP (UK) written examinations, which are clinically relevant postgraduate medical exams. The study analyzes accuracy, error types, and clinical reasoning, directly assessing the LLM's clinical knowledge and decision-making capabilities. This fits the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1136/bmjopen-2023-080558,III,Vivian,"fake data, fake task, not a clinical study",,
AI in the ED: Assessing the efficacy of GPT models vs. physicians in medical score calculation,"BACKGROUND AND AIMS: Artificial Intelligence (AI) models like GPT-3.5 and GPT-4 
have shown promise across various domains but remain underexplored in 
healthcare. Emergency Departments (ED) rely on established scoring systems, such 
as NIHSS and HEART score, to guide clinical decision-making. This study aims to 
evaluate the proficiency of GPT-3.5 and GPT-4 against experienced ED physicians 
in calculating five commonly used medical scores.
METHODS: This retrospective study analyzed data from 150 patients who visited 
the ED over one week. Both AI models and two human physicians were tasked with 
calculating scores for NIH Stroke Scale, Canadian Syncope Risk Score, Alvarado 
Score for Acute Appendicitis, Canadian CT Head Rule, and HEART Score. Cohen's 
Kappa statistic and AUC values were used to assess inter-rater agreement and 
predictive performance, respectively.
RESULTS: The highest level of agreement was observed between the human 
physicians (Kappa = 0.681), while GPT-4 also showed moderate to substantial 
agreement with them (Kappa values of 0.473 and 0.576). GPT-3.5 had the lowest 
agreement with human scorers. These results highlight the superior predictive 
performance of human expertise over the currently available automated systems 
for this specific medical outcome. Human physicians achieved a higher ROC-AUC on 
3 of the 5 scores, but none of the differences were statistically significant.
CONCLUSIONS: While AI models demonstrated some level of concordance with human 
expertise, they fell short in emulating the complex clinical judgments that 
physicians make. The study suggests that current AI models may serve as 
supplementary tools but are not ready to replace human expertise in high-stakes 
settings like the ED. Further research is needed to explore the capabilities and 
limitations of AI in emergency medicine.

Copyright © 2024 Elsevier Inc. All rights reserved.",yes,"This study directly evaluates the clinical performance of GPT-3.5 and GPT-4 in calculating established medical scores in the emergency department, comparing them to human physicians. The task is clinically relevant and involves frontier LLMs in a healthcare context.",10.1016/j.ajem.2024.02.016,I,Vivian,"real data, real task, retrospective",,
Evaluating Artificial Intelligence Chatbots in Oral and Maxillofacial Surgery Board Exams: Performance and Potential,"Background: While artificial intelligence has significantly impacted medicine, the application of large language models (LLMs) in oral and maxillofacial surgery (OMS) remains underexplored. Purpose: This study aimed to measure and compare the accuracy of 4 leading LLMs on OMS board examination questions and to identify specific areas for improvement. Study Design, Setting, and Sample: An in-silico cross-sectional study was conducted to evaluate 4 artificial intelligence chatbots on 714 OMS board examination questions. Predictor Variable: The predictor variable was the LLM used — LLM 1 (Generative Pretrained Transformer 4o [GPT-4o], OpenAI, San Francisco, CA), LLM 2 (Generative Pretrained Transformer 3.5 [GPT-3.5], OpenAI, San Francisco, CA), LLM 3 (Gemini, Google, Mountain View, CA), and LLM 4 (Copilot, Microsoft, Redmond, WA). Main Outcome Variables: The primary outcome variable was accuracy, defined as the percentage of correct answers provided by each LLM. Secondary outcomes included the LLMs' ability to correct errors on subsequent attempts and their performance across 11 specific OMS subject domains: medicine and anesthesia, dentoalveolar and implant surgery, maxillofacial trauma, maxillofacial infections, maxillofacial pathology, salivary glands, oncology, maxillofacial reconstruction, temporomandibular joint anatomy and pathology, craniofacial and clefts, and orthognathic surgery. Covariates: No additional covariates were considered. Analyses: Statistical analysis included one-way ANOVA and post hoc Tukey honest significant difference (HSD) to compare performance across chatbots. χ2 tests were used to assess response consistency and error correction, with statistical significance set at P < .05. Results: LLM 1 achieved the highest accuracy with an average score of 83.69%, statistically significantly outperforming LLM 3 (66.85%, P = .002), LLM 2 (64.83%, P = .001), and LLM 4 (62.18%, P < .001). Across the 11 OMS subject domains, LLM 1 consistently had the highest accuracy rates. LLM 1 also corrected 98.2% of errors, while LLM 2 corrected 93.44%, both statistically significantly higher than LLM 4 (29.26%) and LLM 3 (70.71%) (P < .001). Conclusion and Relevance: LLM 1 (GPT-4o) significantly outperformed other models in both accuracy and error correction, indicating its strong potential as a tool for enhancing OMS education. However, the variability in performance across different domains highlights the need for ongoing refinement and continued evaluation to integrate these LLMs more effectively into the OMS field. © 2025 American Association of Oral and Maxillofacial Surgeons",yes,"This study evaluates the performance of multiple frontier LLMs (GPT-4o, GPT-3.5, Gemini, Copilot) on oral and maxillofacial surgery board examination questions, which is a clinical evaluation relevant to medical education. The focus is on the accuracy and error correction of LLMs in a clinical knowledge context, meeting the inclusion criteria.",10.1016/j.joms.2024.11.007,III,Vivian,"fake data, fake task, not a clinical study",,
Model development for bespoke large language models for digital triage assistance in mental health care,"Contemporary large language models (LLMs) may have utility for processing unstructured, narrative free-text clinical data contained in electronic health records (EHRs) — a particularly important use-case for mental health where a majority of routinely-collected patient data lacks structured, machine-readable content. A significant problem for the United Kingdom's National Health Service (NHS) are the long waiting lists for specialist mental healthcare. According to NHS data (NHS Digital, 2024), in each month of 2023, there were between 370,000 and 470,000 individual new referrals into secondary mental healthcare services. Referrals must be triaged by clinicians, using clinical information contained in the patient's EHR to arrive at a decision about the most appropriate mental healthcare team to assess and potentially treat these patients. The ability to efficiently recommend a relevant team by ingesting potentially voluminous clinical notes could help services both reduce referral waiting times and with the right technology, improve the evidence available to justify triage decisions. We present and evaluate three different approaches for LLM-based, end-to-end ingestion of variable-length clinical EHR data to assist clinicians when triaging referrals. Our model is able to deliver triage recommendations consistent with existing clinical practices and its architecture was implemented on a single GPU, making it practical for implementation in resource-limited NHS environments where private implementations of LLM technology will be necessary to ensure confidential clinical data are appropriately controlled and governed. Code available at: https://github.com/NtaylorOX/BespokeLLM_Triage. © 2024 The Authors",yes,"This study evaluates bespoke LLMs for assisting clinicians in triaging mental health referrals using unstructured EHR data, a direct clinical application. The focus is on the LLM's ability to make triage recommendations consistent with clinical practice, which fits the inclusion criteria.",10.1016/j.artmed.2024.102988,I,Vivian,"real data, real task, retrospective",,
Comparison of Patient Education Materials Generated by Chat Generative Pre-Trained Transformer Versus Experts: An Innovative Way to Increase Readability of Patient Education Materials,"Introduction Improving patient education materials may improve patient outcomes. This study aims to explore the possibility of generating patient education materials with the assistance of a large language model, Chat Generative Pre-Trained Transformer (ChatGPT). In addition, we compare the accuracy and readability of ChatGPT-generated materials versus expert-generated materials. Methods Patient education materials in implant-based breast reconstruction were generated by experts and ChatGPT independently. Readability and accuracy of the materials are the main outcomes. Readability of the materials was compared using Flesch-Kincaid score. Accuracy of the materials generated by ChatGPT was evaluated by 2 independent reviewers. Content errors are categorized into information errors, statistical errors, and multiple errors (errors more than 2 types). Results The content generated by experts had higher readability. The Flesch-Kincaid score is at the 7.5 grade for expert-generated materials, whereas the content generated by ChatGPT is at the 10.5 grade (despite ChatGPT being asked to generate content at the seventh grade level). The accuracy of ChatGPT-generated content is 50%, with most errors being information errors. ChatGPT often provides information about breast reduction or breast augmentation, despite being asked specifically about breast reconstruction. Despite its limitation, ChatGPT significantly reduced the time required to generate patient education materials. Although it takes experts 1 month to generate patient education materials, ChatGPT generates materials within 30 minutes. Conclusions ChatGPT can be a powerful starting tool to generate patient education materials. However, its readability and accuracy still require improvements. © 2023 Wolters Kluwer Health, Inc. All rights reserved.",yes,"This study evaluates ChatGPT's ability to generate patient education materials and compares its output to expert-generated materials in terms of readability and accuracy. The focus is on the clinical application of LLMs for patient education, which is a relevant healthcare context. The study assesses the accuracy of medical information provided by ChatGPT, making it a clinical evaluation.",10.1097/SAP.0000000000003634,II,Vivian,"fake data (??), real task, not a clinical study ",,
Validation of the Quality Analysis of Medical Artificial Intelligence (QAMAI) tool: a new tool to assess the quality of health information provided by AI platforms,"BACKGROUND: The widespread diffusion of Artificial Intelligence (AI) platforms 
is revolutionizing how health-related information is disseminated, thereby 
highlighting the need for tools to evaluate the quality of such information. 
This study aimed to propose and validate the Quality Assessment of Medical 
Artificial Intelligence (QAMAI), a tool specifically designed to assess the 
quality of health information provided by AI platforms.
METHODS: The QAMAI tool has been developed by a panel of experts following 
guidelines for the development of new questionnaires. A total of 30 responses 
from ChatGPT4, addressing patient queries, theoretical questions, and clinical 
head and neck surgery scenarios were assessed by 27 reviewers from 25 academic 
centers worldwide. Construct validity, internal consistency, inter-rater and 
test-retest reliability were assessed to validate the tool.
RESULTS: The validation was conducted on the basis of 792 assessments for the 30 
responses given by ChatGPT4. The results of the exploratory factor analysis 
revealed a unidimensional structure of the QAMAI with a single factor comprising 
all the items that explained 51.1% of the variance with factor loadings ranging 
from 0.449 to 0.856. Overall internal consistency was high (Cronbach's 
alpha = 0.837). The Interclass Correlation Coefficient was 0.983 (95% CI 
0.973-0.991; F (29,542) = 68.3; p < 0.001), indicating excellent reliability. 
Test-retest reliability analysis revealed a moderate-to-strong correlation with 
a Pearson's coefficient of 0.876 (95% CI 0.859-0.891; p < 0.001).
CONCLUSIONS: The QAMAI tool demonstrated significant reliability and validity in 
assessing the quality of health information provided by AI platforms. Such a 
tool might become particularly important/useful for physicians as patients 
increasingly seek medical information on AI platforms.

© 2024. The Author(s).",yes,"Although the primary focus of the study is the development and validation of a tool (QAMAI) to assess the quality of health information from AI platforms, the validation process involved evaluating ChatGPT-4's responses to patient queries, theoretical questions, and clinical scenarios in head and neck surgery. This means the study includes a clinical evaluation of an LLM (ChatGPT-4) in a healthcare context, which meets the inclusion criteria.",10.1007/s00405-024-08710-0,II,Vivian,"fake data, real task, not a clinical study",,
Assessing the clinical reasoning of ChatGPT for mechanical thrombectomy in patients with stroke,"Background Artificial intelligence (AI) has become a promising tool in medicine. ChatGPT, a large language model AI Chatbot, shows promise in supporting clinical practice. We assess the potential of ChatGPT as a clinical reasoning tool for mechanical thrombectomy in patients with stroke. Methods An internal validation of the abilities of ChatGPT was first performed using artificially created patient scenarios before assessment of real patient scenarios from the medical center's stroke database. All patients with large vessel occlusions who underwent mechanical thrombectomy at Tulane Medical Center between January 1, 2022 and December 31, 2022 were included in the study. The performance of ChatGPT in evaluating which patients should undergo mechanical thrombectomy was compared with the decisions made by board-certified stroke neurologists and neurointerventionalists. The interpretation skills, clinical reasoning, and accuracy of ChatGPT were analyzed. Results 102 patients with large vessel occlusions underwent mechanical thrombectomy. ChatGPT agreed with the physician's decision whether or not to pursue thrombectomy in 54.3% of the cases. ChatGPT had mistakes in 8.8% of the cases, consisting of mathematics, logic, and misinterpretation errors. In the internal validation phase, ChatGPT was able to provide nuanced clinical reasoning and was able to perform multi-step thinking, although with an increased rate of making mistakes. Conclusion ChatGPT shows promise in clinical reasoning, including the ability to factor a patient's underlying comorbidities when considering mechanical thrombectomy. However, ChatGPT is prone to errors as well and should not be relied on as a sole decision-making tool in its present form, but it has potential to assist clinicians with more efficient work flow.  © Author(s) (or their employer(s)) 2024. No commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study directly evaluates ChatGPT's clinical reasoning and decision-making in the context of mechanical thrombectomy for stroke patients, comparing its performance to that of board-certified clinicians. It uses real patient scenarios and assesses clinical accuracy, making it a clear example of clinical evaluation of an LLM in medicine.",10.1136/jnis-2023-021163,I,Vivian,"real data, real task, retrospective",,
Transcending Language Barriers: Can ChatGPT Be the Key to Enhancing Multilingual Accessibility in Health Care?,"Objective: To explore the capabilities of Chat Generative Pre-trained Transformer (ChatGPT) for the purpose of simplifying and translating radiology reports into Spanish, Hindi, and Russian languages, with comparisons to its performance in simplifying to the English language. Methods: Fifty deidentified abdomen-pelvis CT reports were fed to ChatGPT (4.0), instructing it to simplify and translate the report. The processed reports were rated on factual correctness (category 1), potential harmful errors (category 2), completeness (category 3), and explanation of medical terms (category 4). The translated versions were also rated on the quality of translation (category 5). The scores in each category were compared between the translated versions and each translated version was compared with the English version in the first four categories. The original reports and the simplified English reports were rated on the Flesch Reading Ease Score and the Flesch Kincaid Grade Level. Results: The Spanish translation outperformed the Hindi and Russian version significantly in categories 1 and 3 (P < .05). All translated versions performed significantly worse compared with the English version in category 4 (P < .001). Notably, the Hindi translated version performed significantly worse in all four categories (P < .05). The Russian translated version was also significantly worse in category 3 (P < .05). In the first three categories, the Spanish translation, and in the first two categories, the Russian translation demonstrated no statistically significant difference from the English version. No statistically significant difference was observed in the Flesch Reading Ease Score and Flesch Kincaid Grade Level of the simplified English reports. Typographical errors in the original reports negatively affected the translation. Conclusion: ChatGPT demonstrates potential ability in translating reports and communicating pertinent clinical information with limited errors. More training and tailoring are required for languages that are not as commonly used in medical literature. Large language models can be used for translating and simplifying radiology reports, potentially improving access to health care and helping reduce health care costs. © 2024 American College of Radiology",yes,"This study evaluates ChatGPT-4.0's ability to simplify and translate radiology reports into multiple languages, assessing factual correctness, completeness, and explanation of medical terms. The focus is on the clinical communication of information to patients or providers in different languages, which is a direct clinical application relevant to healthcare accessibility. The use of LLMs for translating and simplifying clinical reports fits the inclusion criteria.",10.1016/j.jacr.2024.05.009,I,Vivian,"real data, real task, retrospective",,
Generative artificial intelligence writing open notes: A mixed methods assessment of the functionality of GPT 3.5 and GPT 4.0,"Background: Worldwide, patients are increasingly being offered access to their full online clinical records including the narrative reports written by clinicians (so-called “open notes”). Against these developments, there is growing interest in the use of generative artificial intelligence (AI) such as OpenAI's ChatGPT to co-assist clinicians with patient-facing documentation. Objective: This study aimed to explore the effectiveness of OpenAI's ChatGPT 3.5 and GPT 4.0 in generating three patient-facing clinical notes from fictional general practice narrative reports. Methods: On 1 October 2023 and 1 November 2023, we used ChatGPT 3.5 and 4.0 to generate notes for three validated fictional general practice notes, using a prompt in the style of a British primary care note for three commonly presented conditions: (1) type 2 diabetes, (2) major depressive disorder, and (3) a differential diagnosis for suspected bowel cancer. Outputs were analyzed for reading ease, sentiment analysis, empathy, and medical fidelity. Results: ChatGPT 3.5 and 4.0 wrote longer notes than the original, and embedded more second person pronouns, with ChatGPT 3.5 scoring higher on both. ChatGPT expanded abbreviations, but readability metrics showed that the notes required a higher reading proficiency, with ChatGPT 3.5 demanding the most advanced level. Across all notes, ChatGPT offered higher signatures of empathy across cognitive, compassion/sympathy, and prosocial cues. Medical fidelity ratings varied across all three cases with ChatGPT 4.0 rated superior. Conclusions: While ChatGPT improved sentiment and empathy metrics in the transformed notes, compared to the original they also required higher reading proficiency and omitted details impacting medical fidelity. © The Author(s) 2024.",yes,"This study evaluates ChatGPT 3.5 and GPT-4.0 in generating patient-facing clinical notes from fictional general practice reports. The focus is on the quality, readability, empathy, and medical fidelity of the generated notes, which are intended for patient communication. This is a clinical application relevant to healthcare, as it assesses LLMs' ability to assist in clinical documentation for patients.",10.1177/20552076241291384,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT Achieves Only Fair Agreement with ACFAS Expert Panelist Clinical Consensus Statements,"Introduction: As artificial intelligence (AI) becomes increasingly integrated into medicine and surgery, its applications are expanding rapidly—from aiding clinical documentation to providing patient information. However, its role in medical decision-making remains uncertain. This study evaluates an AI language model’s alignment with clinical consensus statements in foot and ankle surgery. Methods: Clinical consensus statements from the American College of Foot and Ankle Surgeons (ACFAS; 2015-2022) were collected and rated by ChatGPT-o1 as being inappropriate, neither appropriate nor inappropriate, and appropriate. Ten repetitions of the statements were entered into ChatGPT-o1 in a random order, and the model was prompted to assign a corresponding rating. The AI-generated scores were compared to the expert panel’s ratings, and intra-rater analysis was performed. Results: The analysis of 9 clinical consensus documents and 129 statements revealed an overall Cohen’s kappa of 0.29 (95% CI: 0.12, 0.46), indicating fair alignment between expert panelists and ChatGPT. Overall, ankle arthritis and heel pain showed the highest concordance at 100%, while flatfoot exhibited the lowest agreement at 25%, reflecting variability between ChatGPT and expert panelists. Among the ChatGPT ratings, Cohen’s kappa values ranged from 0.41 to 0.92, highlighting variability in internal reliability across topics. Conclusion: ChatGPT achieved overall fair agreement and demonstrated variable consistency when repetitively rating ACFAS expert panel clinical practice guidelines representing a variety of topics. These data reflect the need for further study of the causes, impacts, and solutions for this disparity between intelligence and human intelligence. Level of Evidence: Level IV: Retrospective cohort study © 2025 The Author(s).",yes,"This study evaluates ChatGPT's alignment with clinical consensus statements in foot and ankle surgery, specifically assessing its ability to rate the appropriateness of clinical statements compared to expert panelists. This is a direct clinical evaluation of an LLM's decision-making capabilities in a medical context.",10.1177/19386400251319567,II,Vivian,"fake data, real task, not a clinical study",,
Use of Artificial Intelligence Chatbots for Cancer Treatment Information,"Plain Language Summary: This survey study examines the performance of a large 
language model chatbot in providing cancer treatment recommendations that are 
concordant with National Comprehensive Cancer Network guidelines.",yes,"The study evaluates the performance of a large language model chatbot in providing cancer treatment recommendations, specifically assessing concordance with clinical guidelines. This is a direct clinical application of an LLM in healthcare.",10.1001/jamaoncol.2023.2954,II,Vivian,"fake data, real task, not a clinical study",,
Comparative analysis of diagnostic accuracy in endodontic assessments: dental students vs. artificial intelligence,"Objectives: This study evaluates the comparative diagnostic accuracy of dental students and artificial intelligence (AI), specifically a modified ChatGPT 4, in endodontic assessments related to pulpal and apical conditions. The findings are intended to offer insights into the potential role of AI in augmenting dental education. Methods: Involving 109 dental students divided into junior (54) and senior (55) groups, the study compared their diagnostic accuracy against ChatGPT’s across seven clinical scenarios. Juniors had the American Association of Endodontists (AEE) terminology assistance, while seniors relied on prior knowledge. Accuracy was measured against a gold standard by experienced endodontists, using statistical analysis including Kruskal-Wallis and Dwass-Steel-Critchlow-Fligner tests. Results: ChatGPT achieved significantly higher accuracy (99.0 %) compared to seniors (79.7 %) and juniors (77.0 %). Median accuracy was 100.0 % for ChatGPT, 85.7 % for seniors, and 82.1 % for juniors. Statistical tests indicated significant differences between ChatGPT and both student groups (p<0.001), with no notable difference between the student cohorts. Conclusions: The study reveals AI’s capability to outperform dental students in diagnostic accuracy regarding endodontic assessments. This underscores AIs potential as a reference tool that students could utilize to enhance their understanding and diagnostic skills. Nevertheless, the potential for overreliance on AI, which may affect the development of critical analytical and decision-making abilities, necessitates a balanced integration of AI with human expertise and clinical judgement in dental education. Future research is essential to navigate the ethical and legal frameworks for incorporating AI tools such as ChatGPT into dental education and clinical practices effectively. © 2024 the author(s), published by De Gruyter.",yes,"This study directly evaluates the diagnostic accuracy of a modified ChatGPT-4 in endodontic (dental) assessments compared to dental students, using clinical scenarios. The focus is on clinical decision-making and diagnosis, which fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1515/dx-2024-0034,II,Vivian,"fake data, real task, not a clinical study",,
AI-Driven Patient Education in Chronic Kidney Disease: Evaluating Chatbot Responses against Clinical Guidelines,"Chronic kidney disease (CKD) patients can benefit from personalized education on lifestyle and nutrition management strategies to enhance healthcare outcomes. The potential use of chatbots, introduced in 2022, as a tool for educating CKD patients has been explored. A set of 15 questions on lifestyle modification and nutrition, derived from a thorough review of three specific KDIGO guidelines, were developed and posed in various formats, including original, paraphrased with different adverbs, incomplete sentences, and misspellings. Four versions of AI were used to answer these questions: ChatGPT 3.5 (March and September 2023 versions), ChatGPT 4, and Bard AI. Additionally, 20 questions on lifestyle modification and nutrition were derived from the NKF KDOQI guidelines for nutrition in CKD (2020 Update) and answered by four versions of chatbots. Nephrologists reviewed all answers for accuracy. ChatGPT 3.5 produced largely accurate responses across the different question complexities, with occasional misleading statements from the March version. The September 2023 version frequently cited its last update as September 2021 and did not provide specific references, while the November 2023 version did not provide any misleading information. ChatGPT 4 presented answers similar to 3.5 but with improved reference citations, though not always directly relevant. Bard AI, while largely accurate with pictorial representation at times, occasionally produced misleading statements and had inconsistent reference quality, although an improvement was noted over time. Bing AI from November 2023 had short answers without detailed elaboration and sometimes just answered “YES”. Chatbots demonstrate potential as personalized educational tools for CKD that utilize layman’s terms, deliver timely and rapid responses in multiple languages, and offer a conversational pattern advantageous for patient engagement. Despite improvements observed from March to November 2023, some answers remained potentially misleading. ChatGPT 4 offers some advantages over 3.5, although the differences are limited. Collaboration between healthcare professionals and AI developers is essential to improve healthcare delivery and ensure the safe incorporation of chatbots into patient care. © 2024 by the authors.",yes,"This study evaluates the performance of generative LLMs (ChatGPT 3.5, ChatGPT 4, Bard, Bing AI) in answering patient education questions about chronic kidney disease, comparing their responses to clinical guidelines and having nephrologists assess accuracy. The focus is on clinical patient education, a relevant healthcare application.",10.3390/diseases12080185,II,Vivian,"fake data, real task, not a clinical study",,
ChatGPT-A double-edged sword for healthcare education? Implications for assessments of dental students,"INTRODUCTION: Open-source generative artificial intelligence (AI) applications are fast-transforming access to information and allow students to prepare assignments and offer quite accurate responses to a wide range of exam questions which are routinely used in assessments of students across the board including undergraduate dental students. This study aims to evaluate the performance of Chat Generative Pre-trained Transformer (ChatGPT), a generative AI-based application, on a wide range of assessments used in contemporary healthcare education and discusses the implications for undergraduate dental education. MATERIALS AND METHODS: This was an exploratory study investigating the accuracy of ChatGPT to attempt a range of recognised assessments in healthcare education curricula. A total of 50 independent items encompassing 50 different learning outcomes (n = 10 per item) were developed by the research team. These included 10 separate items based on each of the five commonly used question formats including multiple-choice questions (MCQs); short-answer questions (SAQs); short essay questions (SEQs); single true/false questions; and fill in the blanks items. Chat GPT was used to attempt each of these 50 questions. In addition, ChatGPT was used to generate reflective reports based on multisource feedback; research methodology; and critical appraisal of the literature. RESULTS: ChatGPT application provided accurate responses to majority of knowledge-based assessments based on MCQs, SAQs, SEQs, true/false and fill in the blanks items. However, it was only able to answer text-based questions and did not allow processing of questions based on images. Responses generated to written assignments were also satisfactory apart from those for critical appraisal of literature. Word count was the key limitation observed in outputs generated by the free version of ChatGPT. CONCLUSION: Notwithstanding their current limitations, generative AI-based applications have the potential to revolutionise virtual learning. Instead of treating it as a threat, healthcare educators need to adapt teaching and assessments in medical and dental education to the benefits of the learners while mitigating against dishonest use of AI-based technology.",yes,"This study evaluates ChatGPT's performance on various assessment formats used in healthcare (specifically dental) education, including MCQs, SAQs, essays, and other written assignments. The focus is on ChatGPT's ability to answer exam questions and generate written reports, which are part of medical/dental education. This fits the inclusion criteria as it assesses the clinical/educational performance of an LLM in a healthcare context.",10.1111/eje.12937,III,Vivian,"fake data, fake task, not a clinical study",,
Can generative artificial intelligence pass the orthopaedic board examination?,"Background: Resident training programs in the US use the Orthopaedic In-Training Examination (OITE) developed by the American Academy of Orthopaedic Surgeons (AAOS) to assess the current knowledge of their residents and to identify the residents at risk of failing the Amerian Board of Orthopaedic Surgery (ABOS) examination. Optimal strategies for OITE preparation are constantly being explored. There may be a role for Large Language Models (LLMs) in orthopaedic resident education. ChatGPT, an LLM launched in late 2022 has demonstrated the ability to produce accurate, detailed answers, potentially enabling it to aid in medical education and clinical decision-making. The purpose of this study is to evaluate the performance of ChatGPT on Orthopaedic In-Training Examinations using Self-Assessment Exams from the AAOS database and approved literature as a proxy for the Orthopaedic Board Examination. Methods: 301 SAE questions from the AAOS database and associated AAOS literature were input into ChatGPT's interface in a question and multiple-choice format and the answers were then analyzed to determine which answer choice was selected. A new chat was used for every question. All answers were recorded, categorized, and compared to the answer given by the OITE and SAE exams, noting whether the answer was right or wrong. Results: Of the 301 questions asked, ChatGPT was able to correctly answer 183 (60.8%) of them. The subjects with the highest percentage of correct questions were basic science (81%), oncology (72.7%, shoulder and elbow (71.9%), and sports (71.4%). The questions were further subdivided into 3 groups: those about management, diagnosis, or knowledge recall. There were 86 management questions and 47 were correct (54.7%), 45 diagnosis questions with 32 correct (71.7%), and 168 knowledge recall questions with 102 correct (60.7%). Conclusions: ChatGPT has the potential to provide orthopedic educators and trainees with accurate clinical conclusions for the majority of board-style questions, although its reasoning should be carefully analyzed for accuracy and clinical validity. As such, its usefulness in a clinical educational context is currently limited but rapidly evolving. Clinical relevance: ChatGPT can access a multitude of medical data and may help provide accurate answers to clinical questions. © 2023 Professor P K Surendran Memorial Education Foundation",yes,"This study evaluates ChatGPT, a generative LLM, on its ability to answer orthopaedic board-style exam questions, including management and diagnosis, which are clinically relevant tasks. The focus is on clinical education and decision-making, not just knowledge recall or non-clinical tasks.",10.1016/j.jor.2023.10.026,III,Vivian,"fake data, fake task, not a clinical study",,
Classification of Chronic Dizziness Using Large Language Models,"Efficiently classifying chronic dizziness disorders, including persistent postural-perceptual dizziness (PPPD), anxiety, and depressive disorders, is crucial, particularly in primary healthcare settings. This study introduces DizzyInsight, an innovative etiological classification model, designed to enhance the accuracy and reliability of large language model (LLM) and machine learning approaches for etiological classification of chronic dizziness. Eight physicians specializing in chronic dizziness diagnosis, affiliated with the Clinical Center for Vertigo and Balance Disturbance at Beijing Tiantan Hospital, Capital Medical University, furnished comprehensive definitions and evaluations of chronic dizziness characteristics. The study included 260 patients, consisting of 105 males and 155 females, with a mean age of 59.52 ± 13 years. These patients were recruited from the same center between July 2021 and October 2023. For comparative analysis, we utilized the general models bidirectional encoder representations from transformers (BERT) and LLM to assess different outcomes. Seven major categories and 33 subcategory evidence have been defined for etiological classification of chronic dizziness. With DizzyInsight, we constructed the feature dataset regarding chronic dizziness. The DizzyInsight based on the identified evidence of LLM method yielded a positive predictive value of 0.69, a sensitivity of 0.86 for persistent postural-perceptual dizziness (PPPD), a positive predictive value of 0.81, and a sensitivity of 0.66 for anxiety and depressive disorders. These findings highlight the potential of DizzyInsight leveraging LLM to improve the efficacy and interpretability of machine learning models in etiological classification of chronic dizziness disorders. Further research and model development are necessary to improve the accuracy of evidence identification and assess the applicability of DizzyInsight in primary care settings, as well as to evaluate its external validity. © The Author(s) 2024.",yes,"This study evaluates a large language model (LLM) for the clinical task of etiological classification of chronic dizziness disorders, comparing its performance to BERT and using real patient data. The LLM is applied directly to a clinical diagnostic problem, which fits the inclusion criteria.",10.1007/s41666-024-00178-1,I,Vivian,"real data, real task, retrospective",,
Decoding the Clavien-Dindo Classification: Artificial Intelligence (AI) as a Novel Tool to Grade Postoperative Complications,"Objective: To assess ChatGPT's capability of grading postoperative complications using the Clavien-Dindo classification (CDC) via artificial intelligence (AI) with natural language processing. Background: The CDC standardizes the grading of postoperative complications. However, consistent and precise application in dynamic clinical settings is challenging. AI offers a potential solution for efficient automated grading. Methods: ChatGPT's accuracy in defining the CDC, generating clinical examples, grading complications from existing scenarios, and interpreting complications from fictional as well as real world clinical summaries were tested. Results: ChatGPT 4 precisely mirrored the CDC, outperforming version 3.5. In generating clinical examples, ChatGPT 4 showcased 99% agreement with minor errors in urinary catheterization. For single complications, it achieved 97% accuracy. ChatGPT was able to accurately extract, grade, and analyze complications from free-text fictional discharge summaries. It demonstrated near-perfect performance when confronted with real-world discharge summaries: comparison between the human and ChatGPT4 grading showed a κ value of 0.92 (95% CI: 0.82-1) (P<0.001). Conclusions: ChatGPT 4 demonstrates promising proficiency and accuracy in applying the CDC. In the future, AI has the potential to become the mainstay tool to accurately capture, extract, and analyze CDC data from clinical data sets. © 2024 Wolters Kluwer Health, Inc. All rights reserved.",yes,"This study evaluates ChatGPT's ability to grade postoperative complications using the Clavien-Dindo classification, including performance on real-world clinical summaries. The LLM is applied to a clinically relevant task (grading complications), which is a direct clinical application.",10.1097/SLA.0000000000006399,,,,,
Assess the precision of ChatGPT's responses regarding systemic lupus erythematosus (SLE) inquiries,"2850. Skin Res Technol. 2023 Oct;29(10):e13500. doi: 10.1111/srt.13500.

Assess the precision of ChatGPT's responses regarding systemic lupus 
erythematosus (SLE) inquiries.

Huang C(1), Hong D(2), Chen L(3), Chen X(1).

Author information:
(1)Department of General Practice, The Second Affiliated Hospital of Fujian 
Medical University, Quanzhou City, Fujian, China.
(2)Department of Ultrasonography, The Second Affiliated Hospital of Fujian 
Medical University, Quanzhou City, Fujian, China.
(3)Department of Endocrinology, The Second Affiliated Hospital of Fujian Medical 
University, Quanzhou City, Fujian, China.",yes,"This study evaluates the precision of ChatGPT's responses to questions about systemic lupus erythematosus (SLE), which is a clinical application relevant to healthcare. It directly assesses the LLM's ability to answer disease-specific inquiries, fitting the inclusion criteria.",10.1111/srt.13500,,,,,
Assessing question characteristic influences on ChatGPT's performance and response-explanation consistency: Insights from Taiwan's Nursing Licensing Exam,"Background: Investigates the integration of an artificial intelligence tool, specifically ChatGPT, in nursing education, addressing its effectiveness in exam preparation and self-assessment. Objective: This study aims to evaluate the performance of ChatGPT, one of the most promising artificial intelligence-driven linguistic understanding tools in answering question banks for nursing licensing examination preparation. It further analyzes question characteristics that might impact the accuracy of ChatGPT-generated answers and examines its reliability through human expert reviews. Design: Cross-sectional survey comparing ChatGPT-generated answers and their explanations. Setting: 400 questions from Taiwan's 2022 Nursing Licensing Exam. Methods: The study analyzed 400 questions from five distinct subjects of Taiwan's 2022 Nursing Licensing Exam using the ChatGPT model which provided answers and in-depth explanations for each question. The impact of various question characteristics, such as type and cognitive level, on the accuracy of the ChatGPT-generated responses was assessed using logistic regression analysis. Additionally, human experts evaluated the explanations for each question, comparing them with the ChatGPT-generated answers to determine consistency. Results: ChatGPT exhibited overall accuracy at 80.75 % for Taiwan's National Nursing Exam, which passes the exam. The accuracy of ChatGPT-generated answers diverged significantly across test subjects, demonstrating a hierarchy ranging from General Medicine at 88.75 %, Medical–Surgical Nursing at 80.0 %, Psychology and Community Nursing at 70.0 %, Obstetrics and Gynecology Nursing at 67.5 %, down to Basic Nursing at 63.0 %. ChatGPT had a higher probability of eliciting incorrect responses for questions with certain characteristics, notably those with clinical vignettes [odds ratio 2.19, 95 % confidence interval 1.24–3.87, P = 0.007] and complex multiple-choice questions [odds ratio 2.37, 95 % confidence interval 1.00–5.60, P = 0.049]. Furthermore, 14.25 % of ChatGPT-generated answers were inconsistent with their explanations, leading to a reduction in the overall accuracy to 74 %. Conclusions: This study reveals the ChatGPT's capabilities and limitations in nursing exam preparation, underscoring its potential as an auxiliary educational tool. It highlights the model's varied performance across different question types and notable inconsistencies between its answers and explanations. The study contributes significantly to the understanding of artificial intelligence in learning environments, guiding the future development of more effective and reliable artificial intelligence-based educational technologies. Tweetable abstract: New study reveals ChatGPT's potential and challenges in nursing education: Achieves 80.75 % accuracy in exam prep but faces hurdles with complex questions and logical consistency. #AIinNursing #AIinEducation #NursingExams #ChatGPT © 2024 Elsevier Ltd",yes,"This study evaluates ChatGPT's performance on a national nursing licensing exam, including its ability to answer clinical questions and provide explanations, with results reviewed by human experts. The focus is on clinical exam performance, which is a direct clinical application relevant to healthcare education and assessment.",10.1016/j.ijnurstu.2024.104717,,,,,
A multimodal generative AI copilot for human pathology,"Computational pathology(1,2) has witnessed considerable progress in the development of both task-specific predictive models and task-agnostic self-supervised vision encoders(3,4). However, despite the explosive growth of generative artificial intelligence (AI), there have been few studies on building general-purpose multimodal AI assistants and copilots(5) tailored to pathology. Here we present PathChat, a vision-language generalist AI assistant for human pathology. We built PathChat by adapting a foundational vision encoder for pathology, combining it with a pretrained large language model and fine-tuning the whole system on over 456,000 diverse visual-language instructions consisting of 999,202 question and answer turns. We compare PathChat with several multimodal vision-language AI assistants and GPT-4V, which powers the commercially available multimodal general-purpose AI assistant ChatGPT-4 (ref. (6)). PathChat achieved state-of-the-art performance on multiple-choice diagnostic questions from cases with diverse tissue origins and disease models. Furthermore, using open-ended questions and human expert evaluation, we found that overall PathChat produced more accurate and pathologist-preferable responses to diverse queries related to pathology. As an interactive vision-language AI copilot that can flexibly handle both visual and natural language inputs, PathChat may potentially find impactful applications in pathology education, research and human-in-the-loop clinical decision-making.",yes,"This study presents and evaluates PathChat, a multimodal vision-language AI assistant for pathology, which combines a vision encoder with a pretrained large language model. The evaluation includes clinical diagnostic questions and human expert assessment, directly addressing clinical decision-making and diagnostic support in pathology. The study also compares PathChat to GPT-4V, a frontier LLM. This fits the inclusion criteria as it evaluates a generative LLM in a clinical context.",10.1038/s41586-024-07618-3,,,,,
Evaluating ChatGPT's efficacy in assessing the safety of non-prescription medications and supplements in patients with kidney disease,"Background: This study investigated the efficacy of ChatGPT-3.5 and ChatGPT-4 in assessing drug safety for patients with kidney diseases, comparing their performance to Micromedex, a well-established drug information source. Despite the perception of non-prescription medications and supplements as safe, risks exist, especially for those with kidney issues. The study's goal was to evaluate ChatGPT's versions for their potential in clinical decision-making regarding kidney disease patients. Method: The research involved analyzing 124 common non-prescription medications and supplements using ChatGPT-3.5 and ChatGPT-4 with queries about their safety for people with kidney disease. The AI responses were categorized as “generally safe,” “potentially harmful,” or “unknown toxicity.” Simultaneously, these medications and supplements were assessed in Micromedex using similar categories, allowing for a comparison of the concordance between the two resources. Results: Micromedex identified 85 (68.5%) medications as generally safe, 35 (28.2%) as potentially harmful, and 4 (3.2%) of unknown toxicity. ChatGPT-3.5 identified 89 (71.8%) as generally safe, 11 (8.9%) as potentially harmful, and 24 (19.3%) of unknown toxicity. GPT-4 identified 82 (66.1%) as generally safe, 29 (23.4%) as potentially harmful, and 13 (10.5%) of unknown toxicity. The overall agreement between Micromedex and ChatGPT-3.5 was 64.5% and ChatGPT-4 demonstrated a higher agreement at 81.4%. Notably, ChatGPT-3.5's suboptimal performance was primarily influenced by a lower concordance rate among supplements, standing at 60.3%. This discrepancy could be attributed to the limited data on supplements within ChatGPT-3.5, with supplements constituting 80% of medications identified as unknown. Conclusion: ChatGPT's capabilities in evaluating the safety of non-prescription drugs and supplements for kidney disease patients are modest compared to established drug information resources. Neither ChatGPT-3.5 nor ChatGPT-4 can be currently recommended as reliable drug information sources for this demographic. The results highlight the need for further improvements in the model's accuracy and reliability in the medical domain. © The Author(s) 2024.",yes,"This study directly evaluates the clinical utility of ChatGPT-3.5 and ChatGPT-4 in assessing the safety of non-prescription medications and supplements for patients with kidney disease, comparing its performance to a standard clinical resource (Micromedex). The focus is on clinical decision-making and patient safety, which fits the inclusion criteria.",10.1177/20552076241248082,,,,,
A pen mark is all you need - Incidental prompt injection attacks on Vision Language Models in real-life histopathology,"Vision-language models (VLMs) can analyze multimodal medical data. However, a significant weakness of VLMs, as we have recently described, is their susceptibility to prompt injection attacks. Here, the model receives conflicting instructions, leading to potentially harmful outputs. In this study, we hypothesized that handwritten labels and watermarks on pathological images could act as inadvertent prompt injections, influencing decision-making in histopathology. We conducted a quantitative study with a total of N = 3888 observations on the state-of-the-art VLMs Claude 3 Opus, Claude 3.5 Sonnet and GPT-4o. We designed various real-world inspired scenarios in which we show that VLMs rely entirely on (false) labels and watermarks if presented with those next to the tissue. All models reached almost perfect accuracies (90 - 100 %) for ground-truth leaking labels and abysmal accuracies (0 - 10 %) for misleading watermarks, despite baseline accuracies between 30-65 % for various multiclass problems. Overall, all VLMs accepted human-provided labels as infallible, even when those inputs contained obvious errors. Furthermore, these effects could not be mitigated by prompt engineering. It is therefore imperative to consider the presence of labels or other influencing features during future evaluation of VLMs in medicine and other fields.",yes,"This study evaluates the clinical reliability and safety of frontier vision-language models (Claude 3 Opus, Claude 3.5 Sonnet, GPT-4o) in the context of histopathology, specifically examining their susceptibility to prompt injection attacks via handwritten labels and watermarks on medical images. The models' clinical decision-making is directly tested in a real-world medical scenario, making this a relevant clinical evaluation of LLMs/VLMs.",10.1101/2024.12.11.24318840,,,,,
Doctor Versus Artificial Intelligence: Patient and Physician Evaluation of Large Language Model Responses to Rheumatology Patient Questions in a Cross-Sectional Study,"Objective: The objective of the current study was to assess the quality of large language model (LLM) chatbot versus physician-generated responses to patient-generated rheumatology questions. Methods: We conducted a single-center cross-sectional survey of rheumatology patients (n = 17) in Edmonton, Alberta, Canada. Patients evaluated LLM chatbot versus physician-generated responses for comprehensiveness and readability, with four rheumatologists also evaluating accuracy by using a Likert scale from 1 to 10 (1 being poor, 10 being excellent). Results: Patients rated no significant difference between artificial intelligence (AI) and physician-generated responses in comprehensiveness (mean 7.12 ± SD 0.99 vs 7.52 ± 1.16; P = 0.1962) or readability (7.90 ± 0.90 vs 7.80 ± 0.75; P = 0.5905). Rheumatologists rated AI responses significantly poorer than physician responses on comprehensiveness (AI 5.52 ± 2.13 vs physician 8.76 ± 1.07; P < 0.0001), readability (AI 7.85 ± 0.92 vs physician 8.75 ± 0.57; P = 0.0003), and accuracy (AI 6.48 ± 2.07 vs physician 9.08 ± 0.64; P < 0.0001). The proportion of preference to AI- versus physician-generated responses by patients and physicians was 0.45 ± 0.18 and 0.15 ± 0.08, respectively (P = 0.0106). After learning that one answer for each question was AI generated, patients were able to correctly identify AI-generated answers at a lower proportion compared to physicians (0.49 ± 0.26 vs 0.97 ± 0.04; P = 0.0183). The average word count of AI answers was 69.10 ± 25.35 words, as compared to 98.83 ± 34.58 words for physician-generated responses (P = 0.0008). Conclusion: Rheumatology patients rated AI-generated responses to patient questions similarly to physician-generated responses in terms of comprehensiveness, readability, and overall preference. However, rheumatologists rated AI responses significantly poorer than physician-generated responses, suggesting that LLM chatbot responses are inferior to physician responses, a difference that patients may not be aware of. (Figure presented.). © 2023 The Authors. Arthritis & Rheumatology published by Wiley Periodicals LLC on behalf of American College of Rheumatology.",yes,"This study directly evaluates a large language model (LLM) chatbot's responses to patient-generated rheumatology questions, comparing them to physician-generated responses. The evaluation includes clinical aspects such as comprehensiveness, readability, and accuracy, as assessed by both patients and rheumatologists. This fits the inclusion criteria as it assesses the clinical utility and quality of LLM responses in a healthcare context.",10.1002/art.42737,,,,,
Effectiveness of a large language model for clinical information retrieval regarding shoulder arthroplasty,"Purpose: To determine the scope and accuracy of medical information provided by ChatGPT-4 in response to clinical queries concerning total shoulder arthroplasty (TSA), and to compare these results to those of the Google search engine. Methods: A patient-replicated query for ‘total shoulder replacement’ was performed using both Google Web Search (the most frequently used search engine worldwide) and ChatGPT-4. The top 10 frequently asked questions (FAQs), answers, and associated sources were extracted. This search was performed again independently to identify the top 10 FAQs necessitating numerical responses such that the concordance of answers could be compared between Google and ChatGPT-4. The clinical relevance and accuracy of the provided information were graded by two blinded orthopaedic shoulder surgeons. Results: Concerning FAQs with numeric responses, 8 out of 10 (80%) had identical answers or substantial overlap between ChatGPT-4 and Google. Accuracy of information was not significantly different (p = 0.32). Google sources included 40% medical practices, 30% academic, 20% single-surgeon practice, and 10% social media, while ChatGPT-4 used 100% academic sources, representing a statistically significant difference (p = 0.001). Only 3 out of 10 (30%) FAQs with open-ended answers were identical between ChatGPT-4 and Google. The clinical relevance of FAQs was not significantly different (p = 0.18). Google sources for open-ended questions included academic (60%), social media (20%), medical practice (10%) and single-surgeon practice (10%), while 100% of sources for ChatGPT-4 were academic, representing a statistically significant difference (p = 0.0025). Conclusion: ChatGPT-4 provided trustworthy academic sources for medical information retrieval concerning TSA, while sources used by Google were heterogeneous. Accuracy and clinical relevance of information were not significantly different between ChatGPT-4 and Google. Level of Evidence: Level IV cross-sectional. © 2024 The Author(s). Journal of Experimental Orthopaedics published by John Wiley & Sons Ltd on behalf of European Society of Sports Traumatology, Knee Surgery and Arthroscopy.",yes,"This study evaluates ChatGPT-4's ability to provide accurate and clinically relevant information in response to patient-style queries about total shoulder arthroplasty, comparing its performance to Google. The evaluation is clinical in nature, focusing on the accuracy and relevance of medical information provided to patients, which fits the inclusion criteria.",10.1002/jeo2.70114,,,,,
Enhancing Patient-Physician Communication: Simulating African American Vernacular English in Medical Diagnostics with Large Language Models,"Effective communication is crucial in reducing health disparities. However, linguistic differences, such as African American Vernacular English (AAVE), can lead to communication gaps between patients and physicians, negatively affecting care and outcomes. This study examines whether large language models (LLMs), specifically GPT-4 and Llama 3.3, can replicate AAVE in simulated clinical dialogues to improve cultural sensitivity. We tested four prompt types—BaseP, DemoP, LingP, and CompP—using United States Medical Licensing Examination (USMLE) case simulations. Statistical analyses on the models’ outputs showed a significant difference among prompt types for both GPT-4 (F(2,70) = 6.218, p = 0.003) and Llama 3.3 (F(2,70) = 12.124, p < 0.001), indicating that including demographic information and/or explicit AAVE cues influences each model’s output. Combining demographic and linguistic cues (CompP) yielded the highest mean AAVE feature counts (e.g., 9.83 for GPT-4 vs. 16.06 for Llama 3.3), although neither model fully captured the diversity of AAVE. Moreover, simply mentioning African American demographics triggers extra informal forms, suggesting built-in stereotypes or biases in both models. Overall, these findings highlight the promise of LLMs for culturally sensitive healthcare communication, while underscoring the need for continued refinement to address stereotypes and more accurately represent diverse linguistic styles. © The Author(s) 2025.",yes,"This study evaluates the use of LLMs (GPT-4 and Llama 3.3) in simulating African American Vernacular English during clinical dialogues, specifically in the context of USMLE case simulations. The focus is on the models' ability to enhance culturally sensitive communication in medical diagnostics, which is a clinical application relevant to patient-physician interaction. The study does not focus on technical architecture or non-clinical tasks, and it is not a review or opinion piece.",10.1007/s41666-025-00194-9,,,,,
Boosting GPT Models for Genomics Analysis: Generating Trusted Genetic Variant Annotations and Interpretations through RAG and fine-tuning,"Large language models (LLMs) have acquired a remarkable level of knowledge through their initial training. However, they lack expertise in particular domains such as genomics. Variant annotation data, an important component of genomics, is crucial for interpreting and prioritizing disease-related variants among millions of variants identified by genetic sequencing. In our project, we aimed to improve LLM performance in genomics by adding variant annotation data to LLMs by retrieval-augmented generation (RAG) and fine-tuning techniques. Using RAG, we successfully integrated 190 million highly accurate variant annotations, curated from 5 major annotation datasets and tools, into GPT-4o. This integration empowers users to query specific variants and receive accurate variant annotations and interpretations supported by advanced reasoning and language understanding capabilities of LLMs. Additionally, fine-tuning GPT-4 on variant annotation data also improved model performance in some annotation fields, although the accuracy across more fields remains suboptimal. Our model significantly improved the accessibility and efficiency of the variant interpretation process by leveraging LLM capabilities. Our project also revealed that RAG outperforms fine-tuning in factual knowledge injection in terms of data volume, accuracy, and cost-effectiveness. As a pioneering study for adding genomics knowledge to LLMs, our work paves the way for developing more comprehensive and informative genomics AI systems to support clinical diagnosis and research projects, and it demonstrates the potential of LLMs in specialized domains.",yes,"This study evaluates the use of GPT-4o and GPT-4 LLMs, enhanced with genomics data, to provide variant annotations and interpretations. The LLM is used to answer queries about specific genetic variants, which is a clinically relevant task (variant interpretation is a key step in clinical genomics and diagnosis). The focus is on the LLM's ability to support clinical diagnosis and research by providing accurate variant information, which fits the inclusion criteria.",10.1101/2024.11.12.623275,,,,,
Application of ChatGPT as a support tool in the diagnosis and management of acute bacterial tonsillitis,"Introduction: Artificial intelligence (AI) is transforming medicine through techniques like machine learning and deep learning. AI aids diagnosis, enhances patient care, and streamlines healthcare systems. Despite potential benefits, challenges of bias and trust must be managed. Tonsillitis is a common otolaryngological condition with economic implications. The study assesses ChatGPT’s utility in the diagnosis management of bacterial tonsillitis, highlighting its potential for patient-professional interaction. Methods: 2 methods evaluated ChatGPT 3.5 diagnostic ability for tonsillitis: patient-written cases and specialist-created cases. The scenarios involved real patients and fictional cases, assessed by 15 otolaryngologists and 5 pediatricians. Variables included diagnosis accuracy, recommendations quality, and message count. Results: A total of 35 conversations were conducted. ChatGPT achieved accurate diagnoses in 100% of cases, with an average of 3.7 ± 1.1 chat entries for diagnosis. No significant difference existed between professional and patient scenarios (p = 0.977). Recommendations were categorized: appropriate (48.57%), incomplete (45.71%), inappropriate (5.71%), with no significant intergroup difference (p = 0.196). ChatGPT consistently advised consulting a doctor and exhibited expertise in guiding medical consultations. Conclusion: ChatGPT demonstrates promise in providing medical insights and general advice. Its diagnostic accuracy for tonsillitis is notable, but it relies on static data and lacks individual history assessment. ChatGPT shows potential for diagnostics in simpler cases like tonsillitis, but accuracy for complex conditions needs refinement. Further research is needed for validation and broader application.",yes,"This study directly evaluates ChatGPT-3.5's performance in diagnosing and managing acute bacterial tonsillitis using both patient and specialist-created cases, with assessment by medical professionals. The focus is on clinical application, including diagnostic accuracy and quality of recommendations, which fits the inclusion criteria.",10.1007/s12553-024-00858-3,,,,,
ChatGPT: aid to medical ethics decision making?; [ChatGPT: Hilfe bei der medizinethischen Entscheidungsfindung?],"Background: Physicians have to make countless decisions every day. The medical, ethical and legal aspects are often intertwined and subject to change over time. Involving an ethics committee or arranging an ethical consultation are examples of potential aids to decision making. Whether and how artificial intelligence (AI) and the large language model (LLM) of the company OpenAI (San Francisco, CA, USA), known under the name ChatGPT, can also help and support ethical decision making is increasingly becoming a matter of controversial debate. Material and methods: Based on a case example, in which a female physician is confronted with ethical and legal issues and presents these to ChatGPT to come up with answers, the first indications of the strengths and weaknesses are ascertained. Conclusion: Due to the rapid technical development and access to ever increasing quantities of data, the utilization should be closely observed and evaluated. © The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, ein Teil von Springer Nature 2024.",yes,"This study uses a case example where a physician presents ethical and legal clinical issues to ChatGPT and evaluates its responses, providing initial insights into its strengths and weaknesses in medical ethics decision making. This constitutes a clinical application of an LLM in healthcare decision support.",10.1007/s00101-024-01385-6,,,,,
The Quality of CLP-Related Information for Patients Provided by ChatGPT,"Objective: To assess the quality, reliability, readability, and similarity of the data that a recently created NLP-based artificial intelligence model ChatGPT 4 provides to users in Cleft Lip and Palate (CLP)-related information. Design: In the evaluation of the responses provided by the OpenAI ChatGPT to the CLP-related 50 questions, several tools were utilized, including the Ensuring Quality Information for Patients (EQIP) tool, Reliability Scoring System (Adapted from DISCERN), Flesh Reading Ease Formula (FRES) and Flesch-Kinkaid Reading Grade Level (FKRGL) formulas, Global Quality Scale (GQS), and Similarity Index with plagiarism-detection tool. Jamovi (The Jamovi Project, 2022, version 2.3; Sydney, Australia) software was used for all statistical analyses. Results: Based on the reliability and GQS values, ChatGPT demonstrated high reliability and good quality attributable to CLP. Furthermore, according to the FRES results, ChatGPT's readability is difficult, and the similarity index values of this software exhibit an acceptable level of similarity ratio. There is no significant difference in EQIP, Reliability Score System, FRES, FKGRL, GQS, and Similarity Index values among the two categories. Conclusion: OpenAI ChatGPT provides a highly reliable, high-quality, but challenging to read, and acceptable similarity rate in providing information related to CLP. Ensuring that information obtained through these models is verified and assessed by a qualified medical expert is crucial. © 2023, American Cleft Palate Craniofacial Association.",yes,"This study evaluates ChatGPT's ability to provide clinical information to patients about cleft lip and palate (CLP), assessing the quality, reliability, and readability of its responses to patient questions. This is a direct clinical application of an LLM in healthcare, specifically in patient education and information provision.",10.1177/10556656231222387,,,,,
Use of a large language model (LLM) for ambulance dispatch and triage,"Introduction: Large language models (LLMs) have grown in popularity in recent months and have demonstrated advanced clinical reasoning ability. Given the need to prioritize the sickest patients requesting emergency medical services (EMS), we attempted to identify if an LLM could accurately triage ambulance requests using real-world data from a major metropolitan area. Methods: An LLM (ChatGPT 4o Mini, Open AI, San Francisco, CA, USA) with no prior task-specific training was given real ambulance requests from a major metropolitan city in the United States. Requests were batched into groups of four, and the LLM was prompted to identify which of the four patients should be prioritized. The same groupings of four requests were then shown to a panel of experienced critical care paramedics who voted on which patient should be prioritized. Results: Across 98 groupings of four ambulance requests (392 total requests), the LLM agreed with the paramedic panel in most cases (76.5 %, n = 75). In groupings where the paramedic panel was unanimous in their decision (n = 48), the LLM agreed with the unanimous panel in 93.8 % of groupings (n = 45). Conclusions: Our preliminary analysis indicates LLMs may have the potential to become a useful tool for triage and resource allocation in emergency care settings, especially in cases where there is consensus among subject matter experts. Further research is needed to better understand and clarify how they may best be of service. © 2024 Elsevier Inc.",yes,"This study evaluates the use of a generative LLM (ChatGPT 4o Mini) for clinical triage in ambulance dispatch, comparing its prioritization decisions to those of experienced paramedics using real-world data. This is a direct clinical application of an LLM in healthcare decision-making.",10.1016/j.ajem.2024.12.032,,,,,
All You Need Is Context: Clinician Evaluations of various iterations of a Large Language Model-Based First Aid Decision Support Tool in Ghana.,"As advancements in research and development expand the capabilities of Large Language Models (LLMs), there is a growing focus on their applications within the healthcare sector, driven by the large volume of data generated in healthcare. There are a few medicine-oriented evaluation datasets and benchmarks for assessing the performance of various LLMs in clinical scenarios; however, there is a paucity of information on the real-world usefulness of LLMs in contextspecific scenarios in resource-constrained settings. In this work, 5 iterations of a decision support tool for medical emergencies using 5 distinct generalized LLMs were constructed, alongside a combination of Prompt Engineering and Retrieval Augmented Generation techniques. Quantitative and qualitative evaluations of the LLM responses were provided by 12 physicians (general practitioners) with an average of 2 years of practice experience managing medical emergencies in resource-constrained settings in Ghana.",yes,"This study evaluates multiple LLM-based decision support tools for first aid in real-world clinical scenarios, with direct clinician assessment. It focuses on clinical application and LLM performance in healthcare, meeting inclusion criteria.",10.1101/2024.04.03.24305276,,,,,
Evaluation of GPT-4 concordance with north American spine society guidelines for lumbar fusion surgery,"Background: Concordance with evidence-based medicine (EBM) guidelines is associated with improved clinical outcomes in spine surgery. The North American Spine Society (NASS) has published coverage guidelines on indications for lumbar fusion surgery, with a recent survey demonstrating a 60% concordance rate across its members. GPT-4 is a popular deep learning model that receives knowledge training across public databases including those containing EBM guidelines. There is prior research exploring the potential utility of artificial intelligence (AI) software in adherence with spine surgery practices and guidelines, inviting opportunity to further investigate application in the setting of lumbar fusion surgery with current AI models. Methods: Seventeen well-validated clinical vignettes with specific indications for or against lumbar fusion based on NASS criteria were obtained from a prior published research study. Each case was transcribed into a standardized prompt and entered into GPT-4 to obtain a decision whether fusion is indicated. Interquery reliability was assessed with serial identical queries utilizing the Fleiss’ Kappa statistic. Majority response among serial queries was considered as the final GPT-4 decision. Queries were all entered in separate strings. The investigator entering the prompts was blinded to the NASS-concordant decisions for the cases prior to complete data collection. Decisions by GPT-4 and NASS guidelines were compared with Chi-square analysis. Results: GPT-4 responses for 15/17 (88.2%) of the clinical vignettes were in concordance with NASS EBM lumbar fusion guidelines. There was a significant association in clinical decision-making when determining indication for spine fusion surgery between GPT-4 and NASS guidelines (χ² = 9.75; p<.01). There was substantial agreement among the sets of responses generated by GPT-4 for each clinical case (K = 0.71; p<.001). Conclusions: There is significant concordance between GPT-4 responses and NASS EBM indications for lumbar fusion surgery. AI and deep learning models may prove to be an effective adjunct tool for clinical decision-making within modern spine surgery practices. © 2024 The Authors",yes,"This study evaluates GPT-4's ability to make clinical decisions regarding indications for lumbar fusion surgery, comparing its responses to established clinical guidelines (NASS). The use of clinical vignettes and assessment of concordance with evidence-based guidelines constitutes a direct clinical application of a frontier LLM in medicine.",10.1016/j.xnsj.2024.100580,,,,,
Evaluating Large Language Models for Automated CPT Code Prediction in Endovascular Neurosurgery,"Large language models (LLMs) have been utilized to automate tasks like writing discharge summaries and operative reports in neurosurgery. The present study evaluates their ability to identify current procedural terminology (CPT) codes from operative reports. Three LLMs (ChatGPT 4.0, AtlasGPT and Gemini) were evaluated in their ability to provide CPT codes for diagnostic or interventional procedures in endovascular neurosurgery at a single institution. Responses were classified as correct, partially correct or incorrect, and the percentage of correctly identified CPT codes were calculated. The Chi-Square test and Kruskal Wallis test were used to compare responses across LLMs. A total of 30 operative notes were used in the present study. AtlasGPT identified CPT codes for 98.3% procedures with partially correct responses, while ChatGPT and Gemini provided partially correct responses for 86.7% and 30% procedures, respectively (P < 0.001). AtlasGPT identified CPT codes correctly in an average of 35.3% of procedures, followed by ChatGPT (35.1%) and Gemini (8.9%) (P < 0.001). A pairwise comparison among three LLMs revealed that AtlasGPT and ChatGPT outperformed Gemini. Untrained LLMs have the ability to identify partially correct CPT codes in endovascular neurosurgery. Training these models could further enhance their ability to identify CPT codes and minimize healthcare expenditure. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",yes,"This study evaluates the ability of multiple LLMs (ChatGPT 4.0, AtlasGPT, Gemini) to predict CPT codes from operative reports in endovascular neurosurgery. While CPT coding is an administrative task, the process directly relates to clinical documentation and has downstream effects on clinical workflow and billing. The LLMs are being evaluated in a real-world clinical context (operative reports), and the task is closely tied to clinical care delivery. The study is not a review, editorial, or technical architecture paper, and it uses frontier LLMs.",10.1007/s10916-025-02149-4,,,,,
Accuracy of ChatGPT for the diagnosis of clinical entities in the field of internal medicine,"2766. Gac Med Mex. 2023;159(5):439-442. doi: 10.24875/GMM.M23000824.

Accuracy of ChatGPT for the diagnosis of clinical entities in the field of 
internal medicine.

[Article in English]

Andrade-Castellanos CA(1)(2), Paz MTT(2), Farfán-Flores PE(3).

Author information:
(1)Health Sciences Education Master's Degree Program, University Center for 
Health Sciences, Universidad de Guadalajara.
(2)Internal Medicine Department, Hospital Civil de Guadalajara ""Dr. Juan I. 
Menchaca"".
(3)Postgraduate Education Coordinator, University Center for Health Sciences, 
Universidad de Guadalajara. Jalisco, Mexico.",yes,"The study evaluates the accuracy of ChatGPT for diagnosing clinical entities in internal medicine, which is a direct clinical application of a generative LLM. This fits the inclusion criteria.",10.24875/GMM.M23000824,,,,,
GPT-4 as a Board-Certified Surgeon: A Pilot Study,"Purpose: Large language models (LLMs), such as GPT-4 (OpenAI; San Francisco, CA), are promising tools for surgical education. However, skepticism surrounding their accuracy and reliability remains a significant barrier to their widespread adoption. Although GPT-4 has demonstrated a remarkable ability to pass multiple-choice tests, its general surgery knowledge and clinical judgment in complex oral-based examinations are less clear. This study aims to evaluate GPT-4’s general surgery knowledge on mock written and oral board-style examinations to drive improvements that will enable the tool to revolutionize surgical education and practice. Methods: We tested GPT-4’s ability to answer 250 random multiple-choice questions (MCQs) from the Surgical Council on Resident Education (SCORE) question bank and navigate 4 oral board scenarios derived from the Entrustable Professional Activities (EPA) topic list. Two former oral board examiners assessed the responses independently for accuracy. Results: On MCQs, GPT-4 answered 197 out of 250 (78.8%) correctly, corresponding to a 92% probability of passing the American Board of Surgery Qualifying Examination (ABS QE). On oral board scenarios, GPT-4 committed critical failures in 3 out of 4 (75%) clinical cases. Common reasons for failure were incorrect timing of intervention and incorrect suggested operation. Conclusions: While GPT-4’s high performance on MCQs mirrored prior studies, the model struggled to generate accurate long-form content in our mock oral board examination. Future efforts should use specialized datasets and advanced reinforcement learning to improve LLM performance in complex, high-stakes clinical decision-making. © The Author(s) under exclusive licence to International Association of Medical Science Educators 2025.",yes,"This study evaluates GPT-4's performance on both written and oral board-style examinations in general surgery, directly assessing its clinical knowledge and judgment. The focus is on clinical decision-making and education, which fits the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1007/s40670-025-02352-5,,,,,
Artificial intelligence in neurovascular decision-making: a comparative analysis of ChatGPT-4 and multidisciplinary expert recommendations for unruptured intracranial aneurysms,"In the multidisciplinary treatment of cerebrovascular diseases, specialists from different disciplines strive to develop patient-specific treatment recommendations. ChatGPT is a natural language processing chatbot with increasing applicability in medical practice. This study evaluates ChatGPT’s ability to provide treatment recommendations for patients with unruptured intracranial aneurysms (UIA). Anonymized patient data and radiological reports of 20 patients with UIAs were provided to GPT-4 in a standardized format and used to generate a treatment recommendation for different clinical scenarios. GPT-4 responses were evaluated by a multidisciplinary panel of specialists by means of the Likert scale and subsequently benchmarked against the Unruptured Intracranial Aneurysm Treatment Score (UIATS) as well as the actual treatment decision made by the multidisciplinary institutional neurovascular board (INVB). Agreement between expert raters was measured using linear weighted Fleiss-Kappa coefficient. GPT-4 analyzed individual pathological features of the radiological reports and formulated a corresponding assessment for each aspect. None of the recommendations generated reflected evidence of factual hallucination, although in 25% of the case studies no specific recommendation could be derived from the GPT-4 responses. The expert panel rated the overall quality of the GPT-4 recommendations with a median of 3.4 out of 5 points. The GPT-4 recommendations were congruent with those of the INBI in 65% of cases. Interrater reliability among experts showed moderate to low agreement in the assessment of AI-assisted decision making. GPT-4 appears to be able to process clinical information about UIAs and generate treatment recommendations. However, the level of ambiguity and the utilization of scientific evidence in the recommendations are not yet patient/case specific enough to substitute the decision-making of a multidisciplinary neurovascular board. A prospective evaluation of GPT-4 competence as a companion in decision-making panels is deemed necessary. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.",yes,"This study directly evaluates GPT-4 (a generative LLM) in providing clinical treatment recommendations for unruptured intracranial aneurysms, comparing its output to expert panels and established clinical decision tools. The focus is on clinical decision-making, which fits the inclusion criteria.",10.1007/s10143-025-03341-3,,,,,
Importance of Patient History in Artificial Intelligence-Assisted Medical Diagnosis: Comparison Study,"BACKGROUND: Medical history contributes approximately 80% to a diagnosis, 
although physical examinations and laboratory investigations increase a 
physician's confidence in the medical diagnosis. The concept of artificial 
intelligence (AI) was first proposed more than 70 years ago. Recently, its role 
in various fields of medicine has grown remarkably. However, no studies have 
evaluated the importance of patient history in AI-assisted medical diagnosis.
OBJECTIVE: This study explored the contribution of patient history to 
AI-assisted medical diagnoses and assessed the accuracy of ChatGPT in reaching a 
clinical diagnosis based on the medical history provided.
METHODS: Using clinical vignettes of 30 cases identified in The BMJ, we 
evaluated the accuracy of diagnoses generated by ChatGPT. We compared the 
diagnoses made by ChatGPT based solely on medical history with the correct 
diagnoses. We also compared the diagnoses made by ChatGPT after incorporating 
additional physical examination findings and laboratory data alongside history 
with the correct diagnoses.
RESULTS: ChatGPT accurately diagnosed 76.6% (23/30) of the cases with only the 
medical history, consistent with previous research targeting physicians. We also 
found that this rate was 93.3% (28/30) when additional information was included.
CONCLUSIONS: Although adding additional information improves diagnostic 
accuracy, patient history remains a significant factor in AI-assisted medical 
diagnosis. Thus, when using AI in medical diagnosis, it is crucial to include 
pertinent and correct patient histories for an accurate diagnosis. Our findings 
emphasize the continued significance of patient history in clinical diagnoses in 
this age and highlight the need for its integration into AI-assisted medical 
diagnosis systems.

© Fumitoshi Fukuzawa, Yasutaka Yanagita, Daiki Yokokawa, Shun Uchida, Shiho 
Yamashita, Yu Li, Kiyoshi Shikino, Tomoko Tsukamoto, Kazutaka Noda, Takanori 
Uehara, Masatomi Ikusaka. Originally published in JMIR Medical Education 
(https://mededu.jmir.org).",yes,"This study evaluates the diagnostic accuracy of ChatGPT (an LLM) using clinical vignettes, comparing its performance with different levels of clinical information (history alone vs. history plus exam/labs). The focus is on clinical diagnosis, a direct clinical application of an LLM.",10.2196/52674,,,,,
Evaluation of ChatGPT-4 for the detection of surgical site infections from electronic health records after colorectal surgery: A pilot diagnostic accuracy study,"Background: Surveillance of surgical site infection (SSI) relies on manual methods that are time-consuming and prone to subjectivity. This study evaluates the diagnostic accuracy of ChatGPT for detecting SSI from electronic health records after colorectal surgery via comparison with the results of a nationwide surveillance programme. Methods: This pilot, retrospective, multicentre analysis included 122 patients who underwent colorectal surgery. Patient records were reviewed by both manual surveillance and ChatGPT, which was tasked with identifying SSI and categorizing them as superficial, deep, or organ-space infections. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated. Receiver operating characteristic (ROC) curve analysis determined the model's diagnostic performance. Results: ChatGPT achieved a sensitivity of 100 %, correctly identifying all SSIs detected by manual methods. The specificity was 54 %, indicating the presence of false positives. The PPV was 67 %, and the NPV was 100 %. The area under the ROC curve was 0.77, indicating good overall accuracy for distinguishing between SSI and non-SSI cases. Minor differences in outcomes were observed between colon and rectal surgeries, as well as between the hospitals participating in the study. Conclusions: ChatGPT shows high sensitivity and good overall accuracy for detecting SSI. It appears to be a useful tool for initial screenings and for reducing manual review workload. The moderate specificity suggests a need for further refinement to reduce the rate of false positives. The integration of ChatGPT alongside electronic medical records, antibiotic consumption and imaging data results for real-time analysis may further improve the surveillance of SSI. ClinicalTrials.gov Identifier: NCT06556017. © 2024 The Author(s)",yes,"This study directly evaluates ChatGPT-4's diagnostic accuracy in detecting surgical site infections from electronic health records, comparing its performance to manual surveillance. The use of ChatGPT-4 for a clinical diagnostic task in a real-world healthcare context meets the inclusion criteria.",10.1016/j.jiph.2024.102627,,,,,
Exploring the potential of ChatGPT for clinical reasoning and decision-making: a cross-sectional study on the Italian Medical Residency Exam,"Background. This study aimed to assess the performance of ChatGPT, a large language model (LLM), on the Italian State Exam for Medical Residency (SSM) test to determine its potential as a tool for medical education and clinical decision-making support. Materials and methods. A total of 136 questions were obtained from the official SSM test. ChatGPT responses were analyzed and compared to the performance of medical doctors who took the test in 2022. Questions were classified into clinical cases (CC) and notional questions (NQ). Results. ChatGPT achieved an overall accuracy of 90.44%, with higher performance on clinical cases (92.45%) than on notional questions (89.15%). Compared to medical doctors' scores, ChatGPT performance was higher than 99.6% of the participants. Conclusions. These results suggest that ChatGPT holds promise as a valuable tool in clinical decision-making, particularly in the context of clinical reasoning. Further research is needed to explore the potential applications and implementation of large language models (LLMs) in medical education and medical practice. © 2024 Istituto Superiore di Sanita. All rights reserved.",yes,"This study evaluates ChatGPT's performance on the Italian Medical Residency Exam, specifically analyzing its ability to answer clinical case questions and comparing its results to those of human doctors. The focus is on clinical reasoning and decision-making, which aligns with the inclusion criteria for clinical evaluation of LLMs in medicine.",10.4415/ANN_23_04_05,,,,,
Unveiling GPT-4V's hidden challenges behind high accuracy on USMLE questions: Observational Study,"Background: Recent advancements in artificial intelligence, such as GPT-3.5 Turbo (OpenAI) and GPT-4, have demonstrated significant potential by achieving good scores on text-only United States Medical Licensing Examination (USMLE) exams and effectively answering questions from physicians. However, the ability of these models to interpret medical images remains underexplored. Objective: This study aimed to comprehensively evaluate the performance, interpretability, and limitations of GPT-3.5 Turbo, GPT-4, and its successor, GPT-4 Vision (GPT-4V), specifically focusing on GPT-4V’s newly introduced image-understanding feature. By assessing the models on medical licensing examination questions that require image interpretation, we sought to highlight the strengths and weaknesses of GPT-4V in handling complex multimodal clinical information, thereby exposing hidden flaws and providing insights into its readiness for integration into clinical settings. Methods: This cross-sectional study tested GPT-4V, GPT-4, and ChatGPT-3.5 Turbo on a total of 227 multiple-choice questions with images from USMLE Step 1 (n=19), Step 2 clinical knowledge (n=14), Step 3 (n=18), the Diagnostic Radiology Qualifying Core Exam (DRQCE) (n=26), and AMBOSS question banks (n=150). AMBOSS provided expert-written hints and question difficulty levels. GPT-4V’s accuracy was compared with 2 state-of-the-art large language models, GPT-3.5 Turbo and GPT-4. The quality of the explanations was evaluated by choosing human preference between an explanation by GPT-4V (without hint), an explanation by an expert, or a tie, using 3 qualitative metrics: comprehensive explanation, question information, and image interpretation. To better understand GPT-4V’s explanation ability, we modified a patient case report to resemble a typical “curbside consultation” between physicians. Results: For questions with images, GPT-4V achieved an accuracy of 84.2%, 85.7%, 88.9%, and 73.1% in Step 1, Step 2 clinical knowledge, Step 3 of USMLE, and DRQCE, respectively. It outperformed GPT-3.5 Turbo (42.1%, 50%, 50%, 19.2%) and GPT-4 (63.2%, 64.3%, 66.7%, 26.9%). When GPT-4V answered correctly, its explanations were nearly as good as those provided by domain experts from AMBOSS. However, incorrect answers often had poor explanation quality: 18.2% (10/55) contained inaccurate text, 45.5% (25/55) had inference errors, and 76.3% (42/55) demonstrated image misunderstandings. With human expert assistance, GPT-4V reduced errors by an average of 40% (22/55). GPT-4V accuracy improved with hints, maintaining stable performance across difficulty levels, while medical student performance declined as difficulty increased. In a simulated curbside consultation scenario, GPT-4V required multiple specific prompts to interpret complex case data accurately. Conclusions: GPT-4V achieved high accuracy on multiple-choice questions with images, highlighting its potential in medical assessments. However, significant shortcomings were observed in the quality of explanations when questions were answered incorrectly, particularly in the interpretation of images, which could not be efficiently resolved through expert interaction. These findings reveal hidden flaws in the image interpretation capabilities of GPT-4V, underscoring the need for more comprehensive evaluations beyond multiple-choice questions before integrating GPT-4V into clinical settings. ©Zhichao Yang, Zonghai Yao, Mahbuba Tasmin, Parth Vashisht, Won Seok Jang, Feiyun Ouyang, Beining Wang, David McManus, Dan Berlowitz, Hong Yu.",yes,"This study evaluates the clinical performance of generative LLMs (GPT-3.5 Turbo, GPT-4, and GPT-4V) on medical licensing exam questions that require both text and image interpretation, including assessment of explanation quality and readiness for clinical integration. The focus is on clinical reasoning and image-based question answering, which are relevant to healthcare applications.",10.2196/65146,,,,,
Ammalarsi di una patologia rara in tempi di intelligenza artificiale,"Summary. Introduction. The text examines the impact of artificial intelligence (AI) in the context of rare diseases, exploring how patients turn to AI resources for health information, especially in situations where doctor-patient communication is limited. The article features the case of a doctor specializing in clinical psychology and psychotherapy, diagnosed with thymoma and Good’s syndrome, who uses AI resources during his illness. Methods. The capabilities of five chatbots based on Large Language Models (LLMs), such as GPT-3.5, GPT-4, Bing Chat, Google Bard, and Anthropic Claude are explored. The AIs were queried on various aspects of the disease, from pre-diagnosis and diagnosis to therapeutic, psychological, and caregiver management issues. The responses were evaluated by five experts based on criteria such as: accuracy, relevance, coherence, clarity, practical utility, ethical considerations, empathy, and capacity to respond to questions and concerns. Results. The results indicate consistency in the evaluators’ assessments, with generally high scores across all dimensions. Particularly, systems like Bard and GPT-4 received high ratings in terms of information accuracy and the ability to respond to questions and concerns. Bing and Claude were appreciated for their empathy and tone. Overall, the AI systems’ responses were considered appropriate, respectful of ethics and privacy, and useful in the clinical context. Discussion. The article emphasizes the importance of understanding the reliability and precision of responses provided by AI systems in the clinical field. Although these systems offer high-quality responses, there is significant variability in their performance. Healthcare professionals must be aware of these differences and use such tools cautiously. AI can provide support in some aspects of care but cannot replace genuine human empathy and understanding. Integrating AI into clinical practice presents potential but also challenges, particularly the possibility of providing incorrect information. Conclusions. The AI systems demonstrate the ability to provide useful advice on clinical and psychological issues, but their use requires caution. It is crucial to distinguish the benefits of AI for patients from the challenges it presents for healthcare professionals. As AI technology continues to evolve, it is essential that its integration into the clinical field is accompanied by continuous research and evaluations, to ensure safe and effective use in the healthcare sector. © 2024 Il Pensiero Scientifico Editore s.r.l.. All rights reserved.",yes,"This study evaluates multiple generative LLMs (GPT-3.5, GPT-4, Bing Chat, Bard, Claude) in the context of providing clinical and psychological information to a patient with rare diseases. The LLMs are assessed for their ability to answer disease-related questions, and their responses are evaluated by experts for clinical accuracy, empathy, and utility. This fits the inclusion criteria as it directly assesses LLMs in a clinical context.",10.1701/4197.41839,,,,,
The performance of artificial intelligence models in generating responses to general orthodontic questions: ChatGPT vs Google Bard,"INTRODUCTION: This study aimed to evaluate and compare the performance of 2 artificial intelligence (AI) models, Chat Generative Pretrained Transformer-3.5 (ChatGPT-3.5; OpenAI, San Francisco, Calif) and Google Bidirectional Encoder Representations from Transformers (Google Bard; Bard Experiment, Google, Mountain View, Calif), in terms of response accuracy, completeness, generation time, and response length when answering general orthodontic questions. METHODS: A team of orthodontic specialists developed a set of 100 questions in 10 orthodontic domains. One author submitted the questions to both ChatGPT and Google Bard. The AI-generated responses from both models were randomly assigned into 2 forms and sent to 5 blinded and independent assessors. The quality of AI-generated responses was evaluated using a newly developed tool for accuracy of information and completeness. In addition, response generation time and length were recorded. RESULTS: The accuracy and completeness of responses were high in both AI models. The median accuracy score was 9 (interquartile range [IQR]: 8-9) for ChatGPT and 8 (IQR: 8-9) for Google Bard (Median difference: 1; P <0.001). The median completeness score was similar in both models, with 8 (IQR: 8-9) for ChatGPT and 8 (IQR: 7-9) for Google Bard. The odds of accuracy and completeness were higher by 31% and 23% in ChatGPT than in Google Bard. Google Bard's response generation time was significantly shorter than that of ChatGPT by 10.4 second/question. However, both models were similar in terms of response length generation. CONCLUSIONS: Both ChatGPT and Google Bard generated responses were rated with a high level of accuracy and completeness to the posed general orthodontic questions. However, acquiring answers was generally faster using the Google Bard model.",yes,"This study directly evaluates the clinical performance of two LLMs (ChatGPT-3.5 and Google Bard) in answering general orthodontic questions, assessing their accuracy and completeness. The focus is on their ability to provide clinically relevant information, which fits the inclusion criteria.",10.1016/j.ajodo.2024.01.012,,,,,
The performance of ChatGPT in day surgery and pre-anesthesia risk assessment: a case-control study of 150 simulated patient presentations,"Background: Day surgery has developed rapidly in China in recent years, although it still faces a shortage of anesthesiologists to handle pre-anesthesia routine before surgery. We hypothesized that ChatGPT may assist anesthesia practitioners in preoperative assessment and answer questions on the concerns of patients. The aims of this study were to examine the ability of ChatGPT to assess preoperative risk and determine its accuracy in answering questions regarding knowledge and management of day surgery anesthesia. Methods: One-hundred fifty patient profiles were generated to simulate day surgery patient presentations that involved complications of varying acuity and severity. The ChatGPT group and the expert group were both required to evaluate the profiles of 150 simulated patients to determine their ASA-PS classification and whether day surgery was recommended. ChatGPT was then asked to answer 131 questions about day surgery anesthesia that represented the most common issues encountered in clinical practice. The performance of ChatGPT was assessed and graded independently by two experienced anesthesiologists. Results: A total of 150 patient profiles were included in the study (75 males [50.0%] and 75 females [50.0%]). There was no difference between the ChatGPT group and the expert group for the ASA-PS classification and assessment of anesthesia risk in the patient profiles (P > 0.05). Regarding recommendation for day surgery in patients with certain comorbidities (ASA ≥ II), the expert group was inclined to require further examination or treatment. In addition, the proportion of conclusions made by ChatGPT was smaller than that of the experts (i.e., ChatGPT n (%) vs. expert n (%): day surgery can be performed, 67 (47.9) vs. 31 (25.4); needs further treatment and evaluation, 56 (37.3) vs. 66 (44.0); and day surgery is not recommended, 18 (12.9) vs. 29 (9.3), P < 0.05). We showed that ChatGPT had extensive knowledge related to day surgery anesthesia (94.0% correct), with most of the points (70%) considered comprehensive. The performance of ChatGPT was also better in the domains of peri-anesthesia concerns, lifestyle, and emotional support. Conclusions: ChatGPT can assist anesthesia practitioners and surgeons by alerting them to the ASA-PS classification and assessing perioperative risk in day surgery patients. ChatGPT can also be trusted to answer questions and concerns related to pre-anesthesia and therefore has the potential to provide important assistance in clinical work.",yes,"This study evaluates ChatGPT's performance in clinically relevant tasks: pre-anesthesia risk assessment, ASA-PS classification, and answering patient questions about day surgery anesthesia. The study uses simulated patient profiles and compares ChatGPT's performance to experts, directly assessing its clinical utility in a healthcare context.",10.1186/s13741-024-00469-6,,,,,
Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting,"Objective: To assess large language models on their ability to accurately infer cancer disease response from free-text radiology reports. Materials and Methods: We assembled 10 602 computed tomography reports from cancer patients seen at a single institution. All reports were classified into: no evidence of disease, partial response, stable disease, or progressive disease. We applied transformer models, a bidirectional long short-term memory model, a convolutional neural network model, and conventional machine learning methods to this task. Data augmentation using sentence permutation with consistency loss as well as prompt-based fine-tuning were used on the best-performing models. Models were validated on a hold-out test set and an external validation set based on Response Evaluation Criteria in Solid Tumors (RECIST) classifications. Results: The best-performing model was the GatorTron transformer which achieved an accuracy of 0.8916 on the test set and 0.8919 on the RECIST validation set. Data augmentation further improved the accuracy to 0.8976. Prompt-based fine-tuning did not further improve accuracy but was able to reduce the number of training reports to 500 while still achieving good performance. Discussion: These models could be used by researchers to derive progression-free survival in large datasets. It may also serve as a decision support tool by providing clinicians an automated second opinion of disease response. Conclusions: Large clinical language models demonstrate potential to infer cancer disease response from radiology reports at scale. Data augmentation techniques are useful to further improve performance. Prompt-based fine-tuning can significantly reduce the size of the training dataset. VC The Author(s) 2023.",yes,"This study evaluates large language models (LLMs) on their ability to infer cancer disease response from free-text radiology reports, a clinically relevant task. The LLMs are assessed for their potential use as decision support tools for clinicians, which aligns with the inclusion criteria. Although other non-LLM models are also tested, the primary focus is on LLMs and their clinical application.",10.1093/jamia/ocad133,,,,,
Exploring the Pitfalls of Large Language Models: Inconsistency and Inaccuracy in Answering Pathology Board Examination-Style Questions,"In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions sourced from the PathologyOutlines.com Question Bank, a well-established resource for pathology examination preparation. The assessment, encompassing 15 subspecialties in pathology, evaluated the accuracy and consistency of responses by these LLMs. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. In addition to accuracy, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a lower consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors, underscoring the need for ongoing model refinement and human oversight. In conclusion, while LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",yes,"This study evaluates the performance of LLMs (ChatGPT and Bard) on pathology board examination-style questions, which are clinically relevant and used in medical education. The focus is on the accuracy and consistency of LLMs in answering these questions, which aligns with the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1101/2023.08.03.23293401,,,,,
Performance of GPT-4 with Vision on Text- and Image-based ACR Diagnostic Radiology In-Training Examination Questions,"Background: Recent advancements, including image processing capabilities, present new potential applications of large language models such as ChatGPT (OpenAI), a generative pretrained transformer, in radiology. However, baseline performance of ChatGPT in radiology-related tasks is understudied. Purpose: To evaluate the performance of GPT-4 with vision (GPT-4V) on radiology in-training examination questions, including those with images, to gauge the model’s baseline knowledge in radiology. Materials and Methods: In this prospective study, conducted between September 2023 and March 2024, the September 2023 release of GPT-4V was assessed using 386 retired questions (189 image-based and 197 text-only questions) from the American College of Radiology Diagnostic Radiology In-Training Examinations. Nine question pairs were identified as duplicates; only the first instance of each duplicate was considered in ChatGPT’s assessment. A subanalysis assessed the impact of different zero-shot prompts on performance. Statistical analysis included χ2 tests of independence to ascertain whether the performance of GPT-4V varied between question types or subspecialty. The McNemar test was used to evaluate performance differences between the prompts, with Benjamin-Hochberg adjustment of the P values conducted to control the false discovery rate (FDR). A P value threshold of less than.05 denoted statistical significance. Results: GPT-4V correctly answered 246 (65.3%) of the 377 unique questions, with significantly higher accuracy on text-only questions (81.5%, 159 of 195) than on image-based questions (47.8%, 87 of 182) (χ2 test, P < .001). Subanalysis revealed differences between prompts on text-based questions, where chain-of-thought prompting outperformed long instruction by 6.1% (McNemar, P = .02; FDR = 0.063), basic prompting by 6.8% (P = .009, FDR = 0.044), and the original prompting style by 8.9% (P = .001, FDR = 0.014). No differences were observed between prompts on image-based questions with P values of .27 to >.99. Conclusion: While GPT-4V demonstrated a level of competence in text-based questions, it showed deficits interpreting radiologic images. © 2024 Radiological Society of North America Inc.. All rights reserved.",yes,"This study evaluates the clinical performance of GPT-4V (a generative LLM with vision capabilities) on radiology in-training exam questions, including both text and image-based items. The focus is on the model's ability to answer clinically relevant questions, which aligns with the inclusion criteria. The study is not a review, editorial, or technical architecture report, and it uses a frontier LLM in a clinical context.",10.1148/radiol.240153,,,,,
Patient Centric Summarization of Radiology Findings using Large Language Models,"Objective. Develop automated AI models for patient-sensitive summarization of radiology reports. Level of medical education or socio-economic background of a patient may dictate their level of understanding of medical jargon. Inability to understand primary findings from a radiology report may lead to unnecessary anxiety among patients or result in missed follow up. Materials and Methods. Computed tomography exams of chest were selected as a use-case for this study. Approximately 7K chest CT reports were collected from Mayo Clinic Enterprise. Summarization model was built on the T5 large language model (LLM) as its text-to-text transfer architecture is intuitively suited for abstractive text summarization, resulting in a model size of ~0.77B. Noisy groundtruth for model training was collected by prompting LLaMA 13B model. Results. We recruited both experts (board-certified radiologists) and laymen to manually evaluate summaries generated by model. Model-generated summaries rarely missed information as marked by majority opinion of radiologists. Laymen indicated 63% improvement in their understanding by reading layman summaries generated by the model. Comparative study with zero-shot performance of LLaMA indicated that LLaMA hallucinated and missed information 3 and 4 times more often, respectively, than the proposed model. Discussion. The proposed patient-sensitive summarization model can generate summaries for radiology reports understandable by patients with vastly different levels of medical knowledge. In addition, task-specific training allows for more reliable performance compared to much larger off-the-shelf models. Conclusions. The proposed model could improve adherence to follow up treatment suggested by radiology reports by increasing patients’ level of understanding of these reports.",yes,"The study evaluates a large language model (T5, with comparison to LLaMA) for generating patient-friendly summaries of radiology reports, aiming to improve patient understanding and potentially clinical outcomes. The model is assessed by both clinicians and laypeople for its effectiveness in conveying clinical information. This is a clinical application of LLMs, as it directly impacts patient communication and understanding of their medical findings.",10.1101/2024.02.01.24302145,,,,,
Performance evaluation of ChatGPT in detecting diagnostic errors and their contributing factors: an analysis of 545 case reports of diagnostic errors,"Background Manual chart review using validated assessment tools is a standardised methodology for detecting diagnostic errors. However, this requires considerable human resources and time. ChatGPT, a recently developed artificial intelligence chatbot based on a large language model, can effectively classify text based on suitable prompts. Therefore, ChatGPT can assist manual chart reviews in detecting diagnostic errors. Objective This study aimed to clarify whether ChatGPT could correctly detect diagnostic errors and possible factors contributing to them based on case presentations. Methods We analysed 545 published case reports that included diagnostic errors. We imputed the texts of case presentations and the final diagnoses with some original prompts into ChatGPT (GPT-4) to generate responses, including the judgement of diagnostic errors and contributing factors of diagnostic errors. Factors contributing to diagnostic errors were coded according to the following three taxonomies: Diagnosis Error Evaluation and Research (DEER), Reliable Diagnosis Challenges (RDC) and Generic Diagnostic Pitfalls (GDP). The responses on the contributing factors from ChatGPT were compared with those from physicians. Results ChatGPT correctly detected diagnostic errors in 519/545 cases (95%) and coded statistically larger numbers of factors contributing to diagnostic errors per case than physicians: DEER (median 5 vs 1, p<0.001), RDC (median 4 vs 2, p<0.001) and GDP (median 4 vs 1, p<0.001). The most important contributing factors of diagnostic errors coded by ChatGPT were 'failure/delay in considering the diagnosis' (315, 57.8%) in DEER, 'atypical presentation' (365, 67.0%) in RDC, and 'atypical presentation' (264, 48.4%) in GDP. Conclusion ChatGPT accurately detects diagnostic errors from case presentations. ChatGPT may be more sensitive than manual reviewing in detecting factors contributing to diagnostic errors, especially for 'atypical presentation'.  © 2024 BMJ Publishing Group. All rights reserved.",yes,"This study evaluates ChatGPT (GPT-4) in a clinical context by testing its ability to detect diagnostic errors and contributing factors from real case reports, comparing its performance to that of physicians. The task is directly related to clinical decision support and error detection, which are relevant clinical applications.",10.1136/bmjoq-2023-002654,,,,,
Accuracy of ChatGPT-Generated Information on Head and Neck and Oromaxillofacial Surgery: A Multicenter Collaborative Analysis,"2601. Otolaryngol Head Neck Surg. 2024 Jun;170(6):1492-1503. doi: 10.1002/ohn.489. 
Epub 2023 Aug 18.

Accuracy of ChatGPT-Generated Information on Head and Neck and Oromaxillofacial 
Surgery: A Multicenter Collaborative Analysis.

Vaira LA(1)(2), Lechien JR(3)(4), Abbate V(5), Allevi F(6), Audino G(5), 
Beltramini GA(7)(8), Bergonzani M(9), Bolzoni A(7), Committeri U(5), Crimi 
S(10), Gabriele G(11), Lonardi F(12), Maglitto F(13), Petrocelli M(14), Pucci 
R(15), Saponaro G(16), Tel A(17), Vellone V(18), Chiesa-Estomba CM(19), 
Boscolo-Rizzo P(20), Salzano G(5), De Riu G(1).

Author information:
(1)Maxillofacial Surgery Operative Unit, Department of Medicine, Surgery and 
Pharmacy, University of Sassari, Sassari, Italy.
(2)Biomedical Sciences Department, PhD School of Biomedical Science, University 
of Sassari, Sassari, Italy.
(3)Department of Anatomy and Experimental Oncology, Mons School of Medicine, 
UMONS, Research Institute for Health Sciences and Technology, University of Mons 
(UMons), Mons, Belgium.
(4)Department of Otolaryngology-Head Neck Surgery, Elsan Polyclinic of Poitiers, 
Poitiers, France.
(5)Head and Neck Section, Department of Neurosciences, Reproductive and 
Odontostomatological Science, Federico II University of Naples, Naples, Italy.
(6)Maxillofacial Surgery Department, ASSt Santi Paolo e Carlo, University of 
Milan, Milan, Italy.
(7)Department of Biomedical, Surgical and Dental Sciences, University of Milan, 
Milan, Italy.
(8)Maxillofacial and Dental Unit, Fondazione IRCCS Cà Granda Ospedale Maggiore 
Policlinico, Milan, Italy.
(9)Maxillo-Facial Surgery Division, Head and Neck Department, University 
Hospital of Parma, Parma, Italy.
(10)Operative Unit of Maxillofacial Surgery, Policlinico San Marco, University 
of Catania, Catania, Italy.
(11)Department of Maxillofacial Surgery, University of Siena, Siena, Italy.
(12)Department of Maxillofacial Surgery, University of Verona, Verona, Italy.
(13)Maxillo-Facial Surgery Unit, University of Bari ""Aldo Moro"", Bari, Italy.
(14)Maxillofacial Surgery Operative Unit, Bellaria and Maggiore Hospital, 
Bologna, Italy.
(15)Maxillofacial Surgery Unit, San Camillo-Forlanini Hospital, Rome, Italy.
(16)Maxillo-Facial Surgery Unit, IRCSS ""A. Gemelli"" Foundation-Catholic, 
University of the Sacred Heart, Rome, Italy.
(17)Department of Head and Neck Surgery and Neuroscience, Clinic of 
Maxillofacial Surgery, University Hospital of Udine, Udine, Italy.
(18)Maxillofacial Surgery Unit, ""S. Maria"" Hospital, Terni, Italy.
(19)Department of Otorhinolaryngology-Head and Neck Surgery, Hospital 
Universitario Donostia, San Sebastian, Spain.
(20)Department of Medical, Surgical and Health Sciences, Section of 
Otolaryngology, University of Trieste, Trieste, Italy.

OBJECTIVE: To investigate the accuracy of Chat-Based Generative Pre-trained 
Transformer (ChatGPT) in answering questions and solving clinical scenarios of 
head and neck surgery.
STUDY DESIGN: Observational and valuative study.
SETTING: Eighteen surgeons from 14 Italian head and neck surgery units.
METHODS: A total of 144 clinical questions encompassing different 
subspecialities of head and neck surgery and 15 comprehensive clinical scenarios 
were developed. Questions and scenarios were inputted into ChatGPT4, and the 
resulting answers were evaluated by the researchers using accuracy (range 1-6), 
completeness (range 1-3), and references' quality Likert scales.
RESULTS: The overall median score of open-ended questions was 6 (interquartile 
range[IQR]: 5-6) for accuracy and 3 (IQR: 2-3) for completeness. Overall, the 
reviewers rated the answer as entirely or nearly entirely correct in 87.2% of 
cases and as comprehensive and covering all aspects of the question in 73% of 
cases. The artificial intelligence (AI) model achieved a correct response in 
84.7% of the closed-ended questions (11 wrong answers). As for the clinical 
scenarios, ChatGPT provided a fully or nearly fully correct diagnosis in 81.7% 
of cases. The proposed diagnostic or therapeutic procedure was judged to be 
complete in 56.7% of cases. The overall quality of the bibliographic references 
was poor, and sources were nonexistent in 46.4% of the cases.
CONCLUSION: The results generally demonstrate a good level of accuracy in the 
AI's answers. The AI's ability to resolve complex clinical scenarios is 
promising, but it still falls short of being considered a reliable support for 
the decision-making process of specialists in head-neck surgery.

© 2023 The Authors. Otolaryngology–Head and Neck Surgery published by Wiley 
Periodicals LLC on behalf of American Academy of Otolaryngology–Head and Neck 
Surgery Foundation.",yes,"This study directly evaluates ChatGPT-4's performance in answering clinical questions and solving clinical scenarios in head and neck and oromaxillofacial surgery. It assesses the LLM's accuracy, completeness, and reference quality in a clinical context, making it a clear example of clinical evaluation of an LLM in medicine.",10.1002/ohn.489,,,,,
"Performance evaluation of ChatGPT, GPT-4, and Bard on the official board examination of the Japan Radiology Society","Purpose: Herein, we assessed the accuracy of large language models (LLMs) in generating responses to questions in clinical radiology practice. We compared the performance of ChatGPT, GPT-4, and Google Bard using questions from the Japan Radiology Board Examination (JRBE). Materials and methods: In total, 103 questions from the JRBE 2022 were used with permission from the Japan Radiological Society. These questions were categorized by pattern, required level of thinking, and topic. McNemar’s test was used to compare the proportion of correct responses between the LLMs. Fisher’s exact test was used to assess the performance of GPT-4 for each topic category. Results: ChatGPT, GPT-4, and Google Bard correctly answered 40.8% (42 of 103), 65.0% (67 of 103), and 38.8% (40 of 103) of the questions, respectively. GPT-4 significantly outperformed ChatGPT by 24.2% (p < 0.001) and Google Bard by 26.2% (p < 0.001). In the categorical analysis by level of thinking, GPT-4 correctly answered 79.7% of the lower-order questions, which was significantly higher than ChatGPT or Google Bard (p < 0.001). The categorical analysis by question pattern revealed GPT-4’s superiority over ChatGPT (67.4% vs. 46.5%, p = 0.004) and Google Bard (39.5%, p < 0.001) in the single-answer questions. The categorical analysis by topic revealed that GPT-4 outperformed ChatGPT (40%, p = 0.013) and Google Bard (26.7%, p = 0.004). No significant differences were observed between the LLMs in the categories not mentioned above. The performance of GPT-4 was significantly better in nuclear medicine (93.3%) than in diagnostic radiology (55.8%; p < 0.001). GPT-4 also performed better on lower-order questions than on higher-order questions (79.7% vs. 45.5%, p < 0.001). Conclusion: ChatGPTplus based on GPT-4 scored 65% when answering Japanese questions from the JRBE, outperforming ChatGPT and Google Bard. This highlights the potential of using LLMs to address advanced clinical questions in the field of radiology in Japan. © 2023, The Author(s).",yes,"This study evaluates the performance of generative LLMs (ChatGPT, GPT-4, Bard) on a clinical board examination (Japan Radiology Board Examination), which is a direct clinical application relevant to healthcare. The study compares LLMs' accuracy in answering clinical radiology questions, fitting the inclusion criteria.",10.1007/s11604-023-01491-2,,,,,
Large Language Model GPT-4 Compared to Endocrinologist Responses on Initial Choice of Glucose-Lowering Medication Under Conditions of Clinical Uncertainty,"OBJECTIVE To explore how the commercially available large language model (LLM) GPT-4 compares to endocrinologists when addressing medical questions when there is uncertainty regarding the best answer. RESEARCH DESIGN AND METHODS This study compared responses from GPT-4 to responses from 31 endocrinolo-gists outcome was the choice between metformin and other treatments. RESULTS With a simple prompt, GPT-4 chose metformin in 12% (95% CI 7.9–17%) of responses, compared with 31% (95% CI 23–39%) of endocrinologist responses. After modifying the prompt to encourage metformin use, the selection of metformin by GPT-4 increased to 25% (95% CI 22–28%). GPT-4 rarely selected metformin in patients with impaired kidney function, or a history of gastrointestinal distress (2.9% of responses, 95% CI 1.4–5.5%). In contrast, endocrinologists often prescribed metformin even in patients with a history of gastrointestinal distress (21% of responses, 95% CI 12–36%). GPT-4 responses showed low variability on repeated runs except at intermediate levels of kidney function. CONCLUSIONS In clinical scenarios with no single right answer, GPT-4’s responses were reasonable, but differed from endocrinologists’ responses in clinically important ways. Value judgments are needed to determine when these differences should be addressed by adjusting the model. We recommend against reliance on LLM output until it is shown to align not just with clinical guidelines but also with patient and clinician preferen-ces, or it demonstrates improvement in clinical outcomes over standard of care. © 2024 by the American Diabetes Association.",yes,"This study directly compares GPT-4, a large language model, to endocrinologists in making clinical decisions about glucose-lowering medication under uncertainty. The evaluation is in a clinical context and assesses the LLM's decision-making compared to human clinicians.",10.2337/dc24-1067,,,,,
Large Language Model as Clinical Decision Support System Augments Medication Safety in 16 Clinical Specialties,"Medication errors significantly compromise patient safety. Conventional rules-based clinical decision support systems (CDSS) are limited by clinical irrelevancy and ""alert fatigue,"" leading to heavy reliance on pharmacists. We propose that a Large Language Model (LLM)-based CDSS can overcome these limitations and augment pharmacist performance when implemented in a co-pilot modality. In a prospective, crossover, open-label study involving 91 error scenarios based on 40 clinical vignettes across 16 medical and surgical specialties, we develop and validate five LLM models (Gemini Flash, Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-4 Turbo, and GPT-4 Omni) using a retrieval augmented generation framework. The best-performing model evaluates three modalities: LLM-based CDSS alone, pharmacist plus LLM-based CDSS (co-pilot), and pharmacist alone. The co-pilot arm demonstrates the best performance with an accuracy of 61% (precision 0.57, recall 0.61, F1 0.59). In detecting errors posing serious harm, the co-pilot mode increases accuracy by 1.5-fold over the pharmacist alone. We demonstrate that effective LLM integration into clinical workflows for complex tasks like medication chart reviews can enhance healthcare professional performance, thereby improving patient safety.",yes,"This study evaluates multiple frontier LLMs (Gemini, Claude, GPT-4 variants) as clinical decision support systems for medication safety across various specialties. It directly assesses LLMs in a clinical application (medication error detection and chart review), including comparison with human pharmacists and co-pilot modalities. This fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.2139/ssrn.5100060,,,,,
Large Language Models lack essential metacognition for reliable medical reasoning,"3480. Nat Commun. 2025 Jan 14;16(1):642. doi: 10.1038/s41467-024-55628-6.

Large Language Models lack essential metacognition for reliable medical 
reasoning.

Griot M(1)(2), Hemptinne C(3)(4), Vanderdonckt J(5), Yuksel D(3)(6).

Author information:
(1)Institute of NeuroScience, Université catholique de Louvain, Brussels, 
Belgium. maxime.griot@uclouvain.be.
(2)Louvain Research Institute in Management and Organizations, Université 
catholique de Louvain, Louvain-la-Neuve, Belgium. maxime.griot@uclouvain.be.
(3)Institute of NeuroScience, Université catholique de Louvain, Brussels, 
Belgium.
(4)Ophthalmology, Cliniques Universitaires Saint-Luc, Brussels, Belgium.
(5)Louvain Research Institute in Management and Organizations, Université 
catholique de Louvain, Louvain-la-Neuve, Belgium.
(6)Medical Information Department, Cliniques Universitaires Saint-Luc, Brussels, 
Belgium.

Large Language Models have demonstrated expert-level accuracy on medical board 
examinations, suggesting potential for clinical decision support systems. 
However, their metacognitive abilities, crucial for medical decision-making, 
remain largely unexplored. To address this gap, we developed MetaMedQA, a 
benchmark incorporating confidence scores and metacognitive tasks into 
multiple-choice medical questions. We evaluated twelve models on dimensions 
including confidence-based accuracy, missing answer recall, and unknown recall. 
Despite high accuracy on multiple-choice questions, our study revealed 
significant metacognitive deficiencies across all tested models. Models 
consistently failed to recognize their knowledge limitations and provided 
confident answers even when correct options were absent. In this work, we show 
that current models exhibit a critical disconnect between perceived and actual 
capabilities in medical reasoning, posing significant risks in clinical 
settings. Our findings emphasize the need for more robust evaluation frameworks 
that incorporate metacognitive abilities, essential for developing reliable 
Large Language Model enhanced clinical decision support systems.

© 2025. The Author(s).",yes,"This study evaluates multiple large language models (LLMs) on their ability to perform metacognitive tasks in the context of medical reasoning, specifically using a benchmark with confidence scores and metacognitive challenges on medical multiple-choice questions. The focus is on clinical decision support and the reliability of LLMs in medical reasoning, which is a direct clinical application.",10.1038/s41467-024-55628-6,,,,,
Patients prefer human psychiatrists over chatbots: a cross-sectional study,"Aim To rate the level of patients’ satisfaction with responses on questions regarding mental health provided by human psychiatrists, pharmacists, and chatbot platforms. Methods This cross-sectional study enrolled 89 patients who were pharmacologically treated for their mental disorder in one institution in Croatia and one in Bosnia and Her-zegovina during October 2023. They asked psychiatrists, pharmacists, ChatGPT, and one Croatian chatbot questions about their mental disorder and medications and rated the satisfaction with the responses. Results Almost half of the patients had used ChatGPT be-fore the study, and only 12.4% had used the Croatian plat-form. The patients were most satisfied with the information provided by psychiatrists (4.67 out of 5 about mental disorder and 4.51 about medications), followed by pharmacists (3.94 about medications), ChatGPT (3.66 about mental disorder and 3.45 about medications), and the Croatian plat-form (3.66 about mental disorder and 3.44 about medications). Almost half of the participants believed it was easier for them to put a question to a psychiatrist than to a chat-bot, and only 10% claimed it was easier to ask ChatGPT. Conclusion Patients with mental health disorders were more satisfied with responses from their psychiatrists than from chatbots, and satisfaction with chatbots’ knowledge on mental disorders and medications was still too low to justify their usage in these patients. © 2025, Medicinska Naklada Zagreb. All rights reserved.",yes,This study evaluates ChatGPT (an LLM) in a clinical context by comparing patient satisfaction with its responses to mental health questions against those from human psychiatrists and pharmacists. The focus is on the clinical utility and patient perception of LLM-generated answers regarding mental disorders and medications.,10.3325/cmj.2025.66.13,,,,,
The evaluation of the performance of ChatGPT in the management of labor analgesia,"ChatGPT4 is a leading large language model (LLM) chatbot released by OpenAI in 2023. ChatGPT4 can respond to free-text queries, answer questions and make suggestions regarding virtually any topic. ChatGPT4 has successfully answered anesthesia and even obstetric anesthesia knowledge-based questions with reasonable accuracy. However, ChatGPT4 has yet to be challenged in obstetric anesthesia clinical decision-making. Study Objective: In this study, we evaluated the performance of ChatGPT4 in the management of clinical labor analgesia scenarios compared to expert obstetric anesthesiologists. Intervention: Eight clinical questions with progressively increasing medical complexity were posed to ChatGPT4. Measurements: The ChatGPT4 responses were rated by seven expert obstetric anesthesiologists based on safety, accuracy and completeness of each response using a five-point Likert rating scale. Main Results: ChatGPT4 was deemed safe in 73% of responses to the presented obstetric anesthesia clinical scenarios (27% of responses were deemed unsafe). None of the ChatGPT4 responses were unanimously deemed to be safe by all seven expert obstetric anesthesiologists. Moreover, ChatGPT4 responses were overall partly accurate (score 4 out of 5) and somewhat incomplete (score 3.5 out of 5). Conclusions: In summary, approximately one quarter of all responses by ChatGPT4 were deemed unsafe by expert obstetric anesthesiologists. These findings may suggest the need for more fine-tuning and training of LLMs such as ChatGPT4 specifically for clinical decision making in obstetric anesthesia or other specialized medical fields. These LLMs may come to play an important future role in assisting obstetric anesthesiologists in clinical decision making and enhancing overall patient care. © 2024",yes,"This study directly evaluates ChatGPT-4's performance in clinical decision-making for labor analgesia scenarios, comparing its responses to those of expert clinicians. It assesses safety, accuracy, and completeness in a clinical context, which fits the inclusion criteria.",10.1016/j.jclinane.2024.111582,,,,,
Consistent Performance of GPT-4o in Rare Disease Diagnosis Across Nine Languages and 4967 Cases,"Background Large language models (LLMs) are increasingly used in the medical field for diverse applications including differential diagnostic support. The estimated training data used to create LLMs such as the Generative Pretrained Transformer (GPT) predominantly consist of English-language texts, but LLMs could be used across the globe to support diagnostics if language barriers could be overcome. Initial pilot studies on the utility of LLMs for differential diagnosis in languages other than English have shown promise, but a large-scale assessment on the relative performance of these models in a variety of European and non-European languages on a comprehensive corpus of challenging rare-disease cases is lacking. Methods We created 4967 clinical vignettes using structured data captured with Human Phenotype Ontology (HPO) terms with the Global Alliance for Genomics and Health (GA4GH) Phenopacket Schema. These clinical vignettes span a total of 378 distinct genetic diseases with 2618 associated phenotypic features. We used translations of the Human Phenotype Ontology together with language-specific templates to generate prompts in English, Chinese, Czech, Dutch, German, Italian, Japanese, Spanish, and Turkish. We applied GPT-4o, version gpt-4o-2024-08-06, to the task of delivering a ranked differential diagnosis using a zero-shot prompt. An ontology-based approach with the Mondo disease ontology was used to map synonyms and to map disease subtypes to clinical diagnoses in order to automate evaluation of LLM responses.",yes,"This study evaluates GPT-4o, a generative large language model, in the clinical context of rare disease diagnosis across multiple languages using thousands of clinical vignettes. The focus is on the model's diagnostic performance, which is a direct clinical application.",10.1101/2025.02.26.25322769,,,,,
ChatGPT and Gemini Are Not Consistently Concordant With the 2020 American Academy of Orthopaedic Surgeons Clinical Practice Guidelines When Evaluating Rotator Cuff Injury,"Purpose: To evaluate the accuracy of suggestions given by ChatGPT and Gemini (previously known as “Bard”), 2 widely used publicly available large language models, to evaluate the management of rotator cuff injuries. Methods: The 2020 American Academy of Orthopaedic Surgeons (AAOS) Clinical Practice Guidelines (CPGs) were the basis for determining recommended and non-recommended treatments in this study. ChatGPT and Gemini were queried on 16 treatments based on these guidelines examining rotator cuff interventions. The responses were categorized as “concordant” or “discordant” with the AAOS CPGs. The Cohen κ coefficient was calculated to assess inter-rater reliability. Results: ChatGPT and Gemini showed concordance with the AAOS CPGs for 13 of the 16 treatments queried (81%) and 12 of the 16 treatments queried (75%), respectively. ChatGPT provided discordant responses with the AAOS CPGs for 3 treatments (19%), whereas Gemini provided discordant responses for 4 treatments (25%). Assessment of inter-rater reliability showed a Cohen κ coefficient of 0.98, signifying agreement between the raters in classifying the responses of ChatGPT and Gemini to the AAOS CPGs as being concordant or discordant. Conclusions: ChatGPT and Gemini do not consistently provide responses that align with the AAOS CPGs. Clinical Relevance: This study provides evidence that cautions patients not to rely solely on artificial intelligence for recommendations about rotator cuff injuries. © 2025 Arthroscopy Association of North America",yes,"This study evaluates the clinical accuracy of ChatGPT and Gemini (both LLMs) in providing management recommendations for rotator cuff injuries, comparing their responses to established clinical guidelines. This is a direct clinical application and assessment of LLMs in healthcare.",10.1016/j.arthro.2025.01.039,,,,,
Smart Pharmaceutical Monitoring System With Personalized Medication Schedules and Self-Management Programs for Patients With Diabetes: Development and Evaluation Study,"Background: With the climbing incidence of type 2 diabetes, the health care system is under pressure to manage patients with this condition properly. Particularly, pharmacological therapy constitutes the most fundamental means of controlling blood glucose levels and preventing the progression of complications. However, its effectiveness is often hindered by factors such as treatment complexity, polypharmacy, and poor patient adherence. As new technologies, artificial intelligence and digital technologies are covering all aspects of the medical and health care field, but their application and evaluation in the domain of diabetes research remain limited. Objective: This study aims to develop and establish a stand-alone diabetes management service system designed to enhance self-management support for patients, as well as to assess its performance with experienced health care professionals. Methods: Diabetes Universal Medication Schedule (DUMS) system is grounded in official medicine instructions and evidence-based data to establish medication constraints and drug-drug interaction profiles. Individualized medication schedules and self-management programs were generated based on patient-specific conditions and needs, using an app framework to build patient-side contact pathways. The system’s ability to provide medication guidance and health management was assessed by senior health care professionals using a 5-point Likert scale across 3 groups: outputs generated by the system (DUMS group), outputs refined by pharmacists (intervention group), and outputs generated by ChatGPT-4 (GPT-4 group). Results: We constructed a cloud-based drug information management system loaded with 475 diabetes treatment–related medications; 684 medication constraints; and 12,351 drug-drug interactions and theoretical supports. The generated personalized medication plan and self-management program included recommended dosing times, disease education, dietary considerations, and lifestyle recommendations to help patients with diabetes achieve correct medication use and active disease management. Reliability analysis demonstrated that the DUMS group outperformed the GPT-4 group in medication schedule accuracy and safety, as well as comprehensiveness and richness of the self-management program (P<.001). The intervention group outperformed the DUMS and GPT-4 groups on all indicator scores. Conclusions: DUMS’s treatment monitoring service can provide reliable self-management support for patients with diabetes. ChatGPT-4, powered by artificial intelligence, can act as a collaborative assistant to health care professionals in clinical contexts, although its performance still requires further training and optimization. ©Jian Xiao, Mengyao Li, Ruwen Cai, Hangxing Huang, Huimin Yu, Ling Huang, Jingyang Li, Ting Yu, Jiani Zhang, Shuqiao Cheng.",yes,This study evaluates ChatGPT-4 (an LLM) in a clinical context by comparing its ability to generate medication guidance and self-management programs for diabetes patients against a custom system and pharmacist-refined outputs. The evaluation is performed by healthcare professionals and focuses on clinical outputs relevant to patient care.,10.2196/56737,,,,,
Assessing the accuracy of the GPT-4 model in multidisciplinary tumor board decision prediction,"Purpose: Artificial intelligence models like GPT-4 (OpenAI) have the potential to support clinical decision-making in oncology. This study aimed to assess the consistency between multidisciplinary tumor board (MTB) decisions and GPT-4 model predictions in cancer patient management. Patients and methods: A cross-sectional study was conducted involving patients aged ≥ 18 years with definite or suspicious cancer diagnoses presented at MTBs in Ankara University Hospitals, Türkiye, from February 2021 to June 2023. GPT-4 was utilized to generate treatment recommendations based on case summaries. Three independent raters evaluated the compatibility between MTB decisions and GPT-4 predictions using a 4-point Likert scale. Cases with mean compatibility scores equal to or below 2 were reviewed by two expert oncologists for appropriateness. Results: A total of 610 patients were included. The mean compatibility score across raters was 3.59 (SD = 0.81), indicating high agreement between GPT-4 predictions and MTB decisions. Cronbach’s alpha was 0.950 (95% CI 0.935–0.960), demonstrating excellent interrater reliability. Sixty-two cases (10.2%) had mean compatibility scores below the threshold of 2. The first expert oncologist deemed GPT-4's predictions inappropriate in 8 of these cases (12.9%), while the second deemed them inappropriate in 16 cases (25.8%). Cohen’s kappa showed moderate agreement (κ = 0.50, 95% CI 0.25–0.75, p < 0.001). Discrepancies were often due to rare cases lacking guideline information or misunderstandings of case presentations. Conclusion: GPT-4 exhibited high compatibility with MTB decisions in cancer patient management, suggesting its potential as a supportive tool in clinical oncology. However, limitations exist, especially in rare or complex cases. © The Author(s) 2025.",yes,"This study directly evaluates the performance of GPT-4, a large language model, in predicting multidisciplinary tumor board decisions for cancer patient management. It compares GPT-4's recommendations to real clinical decisions, making it a clinical evaluation of an LLM in a healthcare context.",10.1007/s12094-025-03905-1,,,,,
ChatGPT Responses to Frequently Asked Questions on Ménière's Disease: A Comparison to Clinical Practice Guideline Answers,"Objective: Evaluate the quality of responses from Chat Generative Pre-Trained Transformer (ChatGPT) models compared to the answers for “Frequently Asked Questions” (FAQs) from the American Academy of Otolaryngology–Head and Neck Surgery (AAO-HNS) Clinical Practice Guidelines (CPG) for Ménière's disease (MD). Study Design: Comparative analysis. Setting: The AAO-HNS CPG for MD includes FAQs that clinicians can give to patients for MD-related questions. The ability of ChatGPT to properly educate patients regarding MD is unknown. Methods: ChatGPT-3.5 and 4.0 were each prompted with 16 questions from the MD FAQs. Each response was rated in terms of (1) comprehensiveness, (2) extensiveness, (3) presence of misleading information, and (4) quality of resources. Readability was assessed using Flesch-Kincaid Grade Level (FKGL) and Flesch Reading Ease Score (FRES). Results: ChatGPT-3.5 was comprehensive in 5 responses whereas ChatGPT-4.0 was comprehensive in 9 (31.3% vs 56.3%, P =.2852). ChatGPT-3.5 and 4.0 were extensive in all responses (P = 1.0000). ChatGPT-3.5 was misleading in 5 responses whereas ChatGPT-4.0 was misleading in 3 (31.3% vs 18.75%, P =.6851). ChatGPT-3.5 had quality resources in 10 responses whereas ChatGPT-4.0 had quality resources in 16 (62.5% vs 100%, P =.0177). AAO-HNS CPG FRES (62.4 ± 16.6) demonstrated an appropriate readability score of at least 60, while both ChatGPT-3.5 (39.1 ± 7.3) and 4.0 (42.8 ± 8.5) failed to meet this standard. All platforms had FKGL means that exceeded the recommended level of 6 or lower. Conclusion: While ChatGPT-4.0 had significantly better resource reporting, both models have room for improvement in being more comprehensive, more readable, and less misleading for patients. © 2024 The Author(s). OTO Open published by Wiley Periodicals LLC on behalf of American Academy of Otolaryngology–Head and Neck Surgery Foundation.",yes,"This study evaluates the quality of ChatGPT-3.5 and 4.0 responses to patient FAQs about Ménière's disease, comparing them to clinical practice guideline answers. It assesses clinical communication, comprehensiveness, accuracy, and readability, which are all relevant to clinical application of LLMs in patient education.",10.1002/oto2.163,,,,,
The promising role of chatbots in keratorefractive surgery patient education; [Le rôle prometteur des chatbots dans l’éducation des patients en chirurgie kératoréfractive],"Purpose: To evaluate the appropriateness, understandability, actionability, and readability of responses provided by ChatGPT-3.5, Bard, and Bing Chat to frequently asked questions about keratorefractive surgery (KRS). Method: Thirty-eight frequently asked questions about KRS were directed three times to a fresh ChatGPT-3.5, Bard, and Bing Chat interfaces. Two experienced refractive surgeons categorized the chatbots’ responses according to their appropriateness and the accuracy of the responses was assessed using the Structure of the Observed Learning Outcome (SOLO) taxonomy. Flesch Reading Ease (FRE) and Coleman-Liau Index (CLI) were used to evaluate the readability of the responses of chatbots. Furthermore, the understandability scores of responses were evaluated using the Patient Education Materials Assessment Tool (PEMAT). Results: The appropriateness of the ChatGPT-3.5, Bard, and Bing Chat responses was 86.8% (33/38), 84.2% (32/38), and 81.5% (31/38), respectively (P > 0.05). According to the SOLO test, ChatGPT-3.5 (3.91 ± 0.44) achieved the highest mean accuracy and followed by Bard (3.64 ± 0.61) and Bing Chat (3.19 ± 0.55). For understandability (mean PEMAT-U score the ChatGPT-3.5: 68.5%, Bard: 78.6%, and Bing Chat: 67.1%, P < 0.05), and actionability (mean PEMAT-A score the ChatGPT-3.5: 62.6%, Bard: 72.4%, and Bing Chat: 60.9%, P < 0.05) the Bard scored better than the other chatbots. Two readability analyses showed that Bing had the highest readability, followed by the ChatGPT-3.5 and Bard, however, the understandability and readability scores were more challenging than the recommended level. Conclusion: Artificial intelligence supported chatbots have the potential to provide detailed and appropriate responses at acceptable levels in KRS. Chatbots, while promising for patient education in KRS, require further progress, especially in readability and understandability aspects. © 2024 Elsevier Masson SAS; Objectif: Évaluer la pertinence, la compréhensibilité, l'applicabilité et la lisibilité des réponses fournies par ChatGPT-3.5, Bard et Bing Chat aux questions fréquemment posées sur la chirurgie kératoréfractive (CKR). Méthode: Trente-huit questions fréquemment posées sur la CKR ont été adressées trois fois à des interfaces de ChatGPT-3.5, Bard et Bing Chat. Deux chirurgiens réfractifs expérimentés ont catégorisé les réponses des chatbots selon leur pertinence et l'exactitude des réponses a été évaluée à l'aide de la taxonomie SOLO (Structure of the Observed Learning Outcome). L'indice de facilité de lecture de Flesch (FRE) et l'indice de Coleman-Liau (CLI) ont été utilisés pour évaluer la lisibilité des réponses des chatbots. De plus, les scores de compréhensibilité des réponses ont été évalués à l'aide de l'outil d’évaluation des matériels pédagogiques pour les patients (PEMAT). Résultats: La pertinence des réponses de ChatGPT-3.5, Bard et Bing Chat était respectivement de 86,8 % (33/38), 84,2 % (32/38) et 81,5 % (31/38) (p > 0,05). D'après le test SOLO, ChatGPT-3.5 a obtenu la précision moyenne la plus élevée (3,91 ± 0,44), suivi de Bard (3,64 ± 0,61) et de Bing Chat (3,19 ± 0,55). Pour la compréhensibilité (score moyen PEMAT-U: ChatGPT-3.5: 68,5 %, Bard: 78,6 %, et Bing Chat: 67,1 %, p < 0,05) et l'applicabilité (score moyen PEMAT-A: ChatGPT-3.5: 62,6 %, Bard: 72,4 %, et Bing Chat: 60,9 %, p < 0,05), Bard a obtenu de meilleurs résultats que les autres chatbots. Deux analyses de lisibilité ont montré que Bing avait la meilleure lisibilité, suivie de ChatGPT-3.5 et Bard, cependant, les scores de compréhensibilité et de lisibilité étaient plus difficiles que le niveau recommandé. Conclusion: Les chatbots soutenus par l'intelligence artificielle ont le potentiel de fournir des réponses détaillées et appropriées à des niveaux acceptables en CKR. Les chatbots, bien qu'ils soient prometteurs pour l’éducation des patients en CKR, nécessitent des progrès supplémentaires, en particulier en matière de lisibilité et de compréhensibilité. © 2024 Elsevier Masson SAS",yes,"This study evaluates the clinical performance of generative LLMs (ChatGPT-3.5, Bard, Bing Chat) in answering patient questions about keratorefractive surgery, focusing on appropriateness, accuracy, understandability, and readability. The LLMs are assessed in a direct patient education context, which is a clinical application.",10.1016/j.jfo.2024.104381,,,,,
ChatGPT (GPT-4) versus doctors on complex cases of the Swedish family medicine specialist examination: an observational comparative study,"Background Recent breakthroughs in artificial intelligence research include the development of generative pretrained transformers (GPT). ChatGPT has been shown to perform well when answering several sets of medical multiple-choice questions. However, it has not been tested for writing free-text assessments of complex cases in primary care. Objectives To compare the performance of ChatGPT, version GPT-4, with that of real doctors. Design and setting A blinded observational comparative study conducted in the Swedish primary care setting. Responses from GPT-4 and real doctors to cases from the Swedish family medicine specialist examination were scored by blinded reviewers, and the scores were compared. Participants Anonymous responses from the Swedish family medicine specialist examination 2017-2022 were used. Outcome measures Primary: the mean difference in scores between GPT-4's responses and randomly selected responses by human doctors, as well as between GPT-4's responses and top-tier responses by human doctors. Secondary: the correlation between differences in response length and response score; the intraclass correlation coefficient between reviewers; and the percentage of maximum score achieved by each group in different subject categories. Results The mean scores were 6.0, 7.2 and 4.5 for randomly selected doctor responses, top-tier doctor responses and GPT-4 responses, respectively, on a 10-point scale. The scores for the random doctor responses were, on average, 1.6 points higher than those of GPT-4 (p<0.001, 95% CI 0.9 to 2.2) and the top-tier doctor scores were, on average, 2.7 points higher than those of GPT-4 (p<0.001, 95 % CI 2.2 to 3.3). Following the release of GPT-4o, the experiment was repeated, although this time with only a single reviewer scoring the answers. In this follow-up, random doctor responses were scored 0.7 points higher than those of GPT-4o (p=0.044). Conclusion In complex primary care cases, GPT-4 performs worse than human doctors taking the family medicine specialist examination. Future GPT-based chatbots may perform better, but comprehensive evaluations are needed before implementing chatbots for medical decision support in primary care.  © Author(s) (or their employer(s)) 2024.",yes,"This study directly compares the performance of GPT-4 (and GPT-4o) to human doctors on complex cases from a family medicine specialist examination, a clear clinical evaluation of an LLM in a healthcare context. The focus is on clinical reasoning and decision-making, not technical aspects or non-clinical tasks.",10.1136/bmjopen-2024-086148,,,,,
Artificial intelligence insights into osteoporosis: assessing ChatGPT’s information quality and readability,"Summary: Accessible, accurate information, and readability play crucial role in empowering individuals managing osteoporosis. This study showed that the responses generated by ChatGPT regarding osteoporosis had serious problems with quality and were at a level of complexity that that necessitates an educational background of approximately 17 years. Purpose: The use of artificial intelligence (AI) applications as a source of information in the field of health is increasing. Readable and accurate information plays a critical role in empowering patients to make decisions about their disease. The aim was to examine the quality and readability of responses provided by ChatGPT, an AI chatbot, to commonly asked questions regarding osteoporosis, representing a major public health problem. Methods: “Osteoporosis,” “female osteoporosis,” and “male osteoporosis” were identified by using Google trends for the 25 most frequently searched keywords on Google. A selected set of 38 keywords was sequentially inputted into the chat interface of the ChatGPT. The responses were evaluated with tools of the Ensuring Quality Information for Patients (EQIP), the Flesch-Kincaid Grade Level (FKGL), and the Flesch-Kincaid Reading Ease (FKRE). Results: The EQIP score of the texts ranged from a minimum of 36.36 to a maximum of 61.76 with a mean value of 48.71 as having “serious problems with quality.” The FKRE scores spanned from 13.71 to 56.06 with a mean value of 28.71 and the FKGL varied between 8.48 and 17.63, with a mean value of 13.25. There were no statistically significant correlations between the EQIP score and the FKGL or FKRE scores. Conclusions: Although ChatGPT is easily accessible for patients to obtain information about osteoporosis, its current quality and readability fall short of meeting comprehensive healthcare standards. © International Osteoporosis Foundation and Bone Health and Osteoporosis Foundation 2024.",yes,"This study evaluates ChatGPT's ability to answer patient questions about osteoporosis, focusing on the quality and readability of its responses. It directly assesses the clinical utility of an LLM in providing health information to patients, which is a relevant clinical application.",10.1007/s11657-024-01376-5,,,,,
Diagnostic and Management Applications of ChatGPT in Structured Otolaryngology Clinical Scenarios,"Objective: To evaluate the clinical applications and limitations of chat generative pretrained transformer (ChatGPT) in otolaryngology. Study Design: Cross-sectional survey. Setting: Tertiary academic center. Methods: ChatGPT 4.0 was queried for diagnoses and management plans for 20 physician-written clinical vignettes in otolaryngology. Attending physicians were then asked to rate the difficulty of the clinical vignettes and agreement with the differential diagnoses and management plans of ChatGPT responses on a 5-point Likert scale. Summary statistics were calculated. Univariate ordinal regression was then performed between vignette difficulty and quality of the diagnoses and management plans. Results: Eleven attending physicians completed the survey (61% response rate). Overall, vignettes were rated as very easy to neutral difficulty (range of median score: 1.00-4.00; overall median 2.00). There was a high agreement with the differential diagnosis provided by ChatGPT (range of median score: 3.00-5.00; overall median: 5.00). There was also high agreement with treatment plans (range of median score: 3.00-5.00; overall median: 5.00). There was no association between vignette difficulty and agreement with differential diagnosis or treatment. Lower diagnosis scores had greater odds of having lower treatment scores. Conclusion: Generative artificial intelligence models like ChatGPT are being rapidly adopted in medicine. Performance with curated, easy-to-moderate difficulty otolaryngology scenarios indicate high agreement with physicians for diagnosis and management. However, a decreased quality in diagnosis is associated with decreased quality in management. Further research is necessary on ChatGPT's ability to handle unstructured clinical information. © 2023 The Authors. OTO Open published by Wiley Periodicals LLC on behalf of American Academy of Otolaryngology–Head and Neck Surgery Foundation.",yes,"This study evaluates ChatGPT-4.0's performance in providing diagnoses and management plans for clinical vignettes in otolaryngology, with physician assessment of its clinical outputs. It directly assesses the LLM's clinical utility in diagnosis and management, fitting the inclusion criteria.",10.1002/oto2.67,,,,,
Exploring artificial intelligence-powered virtual assistants to understand their potential to support older adults’ search needs,"Objective: We investigated the accuracy and amount of information provided by artificial intelligence (AI)-powered virtual assistants in response to queries relevant to aging adults in the domains of Medicare, long-term care insurance, and resource access. Background: Older adults are faced with complex decisions and must gather and integrate information from diverse sources to help support these decisions (e.g., across various websites and online resources). Information-seeking, integration, and decision-making are cognitively demanding and can be impacted by age-related cognitive changes. Virtual assistants powered by AI have the potential to provide older adults with easy access to information and answers to their queries. However, it is unclear how accurate this information and these answers might be. Method: Alexa, Google Assistant, Bard, and ChatGPT-4 were queried. Coders assessed the accuracy of these responses, and the amount of supplemental information provided as a measure of response complexity. Results: Overall, Large Language Model (LLM)-based virtual assistants (Bard, ChatGPT-4) responded more accurately than non-LLM assistants (e.g., 6 % inaccurate responses for Bard vs. 60 % for Alexa) and provided substantially more supplemental information (79 % of responses with high supplemental information for Bard and 37 % for Chat-GPT, vs. 20 % or less for others). We note, however, that responses can vary over time. Conclusion: Based on their ability to provide largely accurate responses, LLMs may be helpful tools for older adults seeking information related to health, insurance, and available resources. However, the potential for error, high response complexity, and response variability should be considered. Application: LLM-based virtual assistants may be a helpful tool for older adults seeking information to support health and financial decisions. © 2025",yes,"This study evaluates the performance of LLM-powered virtual assistants (Bard, ChatGPT-4) in answering queries relevant to older adults, specifically in the context of health, insurance, and resource access. The focus is on the accuracy and complexity of responses to health-related questions, which is a clinical application relevant to healthcare decision support.",10.1016/j.hfh.2025.100092,,,,,
Leveraging Large Language Models to Analyze Continuous Glucose Monitoring Data: A Case Study,"Continuous glucose monitors (CGM) provide patients and clinicians with valuable insights about glycemic control that aid in diabetes management. The advent of large language models (LLMs), such as GPT-4, has enabled real-time text generation and summarization of medical data. Further, recent advancements have enabled the integration of data analysis features in chatbots, such that raw data can be uploaded and analyzed when prompted. Studying both the accuracy and suitability of LLM-derived data analysis performed on medical time series data, such as CGM data, is an important area of research. The objective of this study was to assess the strengths and limitations of using an LLM to analyze raw CGM data and produce summaries of 14 days of data for patients with type 1 diabetes. This study used simulated CGM data from 10 different cases. We first evaluated the ability of GPT-4 to compute quantitative metrics specific to diabetes found in an Ambulatory Glucose Profile (AGP). Then, using two independent clinician graders, we evaluated the accuracy, completeness, safety, and suitability of qualitative descriptions produced by GPT-4 across five different CGM analysis tasks. We demonstrated that GPT-4 performs well across measures of accuracy, completeness, and safety when producing summaries of CGM data across all tasks. These results highlight the capabilities of using an LLM to produce accurate and safe narrative summaries of medical time series data. We highlight several limitations of the work, including concerns related to how GPT-4 may misprioritize highlighting instances of hypoglycemia and hyperglycemia. Our work serves as a preliminary study on how generative language models can be integrated into diabetes care through CGM analysis, and more broadly, the potential to leverage LLMs for streamlined medical time series analysis.",yes,"This study evaluates GPT-4, a generative LLM, in the clinical context of analyzing and summarizing continuous glucose monitoring (CGM) data for diabetes management. The LLM's outputs are assessed for accuracy, completeness, safety, and suitability by clinicians, and the application is directly related to clinical care (summarizing patient data for diabetes management). This fits the inclusion criteria as it is a clinical evaluation of an LLM in healthcare.",10.1101/2024.04.06.24305022,,,,,
Accuracy and Consistency of Gemini Responses Regarding the Management of Traumatized Permanent Teeth,"BACKGROUND: The aim of this cross-sectional observational analytical study was to assess the accuracy and consistency of responses provided by Google Gemini (GG), a free-access high-performance multimodal large language model, to questions related to the European Society of Endodontology position statement on the management of traumatized permanent teeth (MTPT). MATERIALS AND METHODS: Three academic endodontists developed a set of 99 yes/no questions covering all areas of the MTPT. Nine general dentists and 22 endodontic specialists evaluated these questions for clarity and comprehension through an iterative process. Two academic dental trauma experts categorized the knowledge required to answer each question into three levels. The three academic endodontists submitted the 99 questions to the GG, resulting in 297 responses, which were then assessed for accuracy and consistency. Accuracy was evaluated using the Wald binomial method, while the consistency of GG responses was assessed using the kappa-Fleiss coefficient with a confidence interval of 95%. A 5% significance level chi-squared test was used to evaluate the influence of question level of knowledge on accuracy and consistency. RESULTS: The responses generated by Gemini showed an overall moderate accuracy of 80.81%, with no significant differences found between the responses of the academic endodontists. Overall, high consistency (95.96%) was demonstrated, with no significant differences between GG responses across the three accounts. The analysis also revealed no correlation between question level of knowledge and accuracy or consistency, with no significant differences. CONCLUSIONS: The results of this study could significantly impact the potential use of Gemini as a free-access source of information for clinicians in the MTPT.",yes,"This study evaluates the clinical accuracy and consistency of responses provided by the Gemini large language model to questions about the management of traumatized permanent teeth, a clinical scenario. The focus is on the LLM's ability to provide clinically relevant information, making it a direct clinical evaluation of an LLM in healthcare.",10.1111/edt.13004,,,,,
A comprehensive evaluation of ChatGPT consultation quality for augmentation mammoplasty: A comparative analysis between plastic surgeons and laypersons,"Objectives: ChatGPT has gained significant popularity as a source of healthcare information among the general population. Evaluating the quality of chatbot responses is crucial, requiring comprehensive and qualitative analysis. This study aims to assess the answers provided by ChatGPT during hypothetical breast augmentation consultations across various categories and depths. The evaluation involves the utilization of validated tools and a comparison of scores between plastic surgeons and laypersons. Methods: A panel consisting of five plastic surgeons and five laypersons evaluated ChatGPT's responses to 25 questions spanning consultation, procedure, recovery, and sentiment categories. The DISCERN and PEMAT tools were employed to assess the responses, while emotional context was examined through ten specific questions. Additionally, readability was measured using the Flesch Reading Ease score. Qualitative analysis was performed to identify the overall strengths and weaknesses. Results: Plastic surgeons generally scored lower than laypersons across most domains. Scores for each evaluation domain varied by category, with the consultation category demonstrating lower scores in terms of DISCERN reliability, information quality, and DISCERN score. Plastic surgeons assigned significantly lower overall quality ratings to the procedure category compared to other question categories. They also gave lower emotion scores in the procedure category compared to laypersons. The depth of the questions did not impact the scoring. Conclusions: Existing health information evaluation tools may not be entirely suitable for comprehensively evaluating the quality of individual responses generated by ChatGPT. Consequently, the development and implementation of appropriate evaluation tools to assess the appropriateness and quality of AI consultations are necessary. © 2023 The Authors",yes,"This study evaluates ChatGPT's responses to hypothetical breast augmentation consultation questions, comparing the quality of its answers as judged by both plastic surgeons and laypersons. The focus is on the clinical quality and appropriateness of ChatGPT's responses in a healthcare context, specifically patient consultation. This fits the inclusion criteria as it assesses a generative LLM (ChatGPT) in a clinical application.",10.1016/j.ijmedinf.2023.105219,,,,,
ChatGPT: when the artificial intelligence meets standardized patients in clinical training,,yes,"The title suggests the study involves ChatGPT (an LLM) interacting with standardized patients in the context of clinical training, which implies a clinical application or evaluation in a medical education setting. This fits the inclusion criteria for clinical evaluation of LLMs.",10.1186/s12967-023-04314-0,,,,,
DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients,"In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA -7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%, respectively. Additionally, our findings indicate that DRG-LLaMA ’s performance correlates with increased model parameters and input context lengths. © 2024, The Author(s).",yes,"This study evaluates a large language model (LLaMA) fine-tuned to predict diagnosis-related groups (DRGs) from clinical notes, which is a clinically relevant task involving patient diagnosis and hospital billing. The model is directly applied to a clinical prediction problem, comparing its performance to other models. This fits the inclusion criteria as it assesses an LLM in a clinical context.",10.1038/s41746-023-00989-3,,,,,
Beyond the Scalpel: Assessing ChatGPT's potential as an auxiliary intelligent virtual assistant in oral surgery,"AI has revolutionized the way we interact with technology. Noteworthy advances in AI algorithms and large language models (LLM) have led to the development of natural generative language (NGL) systems such as ChatGPT. Although these LLM can simulate human conversations and generate content in real time, they face challenges related to the topicality and accuracy of the information they generate. This study aimed to assess whether ChatGPT-4 could provide accurate and reliable answers to general dentists in the field of oral surgery, and thus explore its potential as an intelligent virtual assistant in clinical decision making in oral surgery. Thirty questions related to oral surgery were posed to ChatGPT4, each question repeated 30 times. Subsequently, a total of 900 responses were obtained. Two surgeons graded the answers according to the guidelines of the Spanish Society of Oral Surgery, using a three-point Likert scale (correct, partially correct/incomplete, and incorrect). Disagreements were arbitrated by an experienced oral surgeon, who provided the final grade Accuracy was found to be 71.7%, and consistency of the experts' grading across iterations, ranged from moderate to almost perfect. ChatGPT-4, with its potential capabilities, will inevitably be integrated into dental disciplines, including oral surgery. In the future, it could be considered as an auxiliary intelligent virtual assistant, though it would never replace oral surgery experts. Proper training and verified information by experts will remain vital to the implementation of the technology. More comprehensive research is needed to ensure the safe and successful application of AI in oral surgery.",yes,"This study evaluates ChatGPT-4's ability to answer clinical questions in oral surgery, assessing its accuracy and reliability as a potential virtual assistant for dentists. The evaluation is based on clinical guidelines and involves expert grading, making it a direct clinical application of an LLM.",10.1016/j.csbj.2023.11.058,,,,,
"Evaluation of the reliability, usefulness, quality and readability of ChatGPT's responses on Scoliosis","2562. Eur J Orthop Surg Traumatol. 2025 Mar 18;35(1):123. doi: 
10.1007/s00590-025-04198-4.

Evaluation of the reliability, usefulness, quality and readability of ChatGPT's 
responses on Scoliosis.

Çıracıoğlu AM(1), Dal Erdoğan S(2).

Author information:
(1)Eskisehir City Hospital, Eskisehir, Türkiye. draysemerveaydogan@gmail.com.
(2)Sincan Training and Research Hospital, Ankara, Türkiye.

OBJECTIVE: This study evaluates the reliability, usefulness, quality, and 
readability of ChatGPT's responses to frequently asked questions 
about scoliosis.
METHODS: Sixteen frequently asked questions, identified through an analysis of 
Google Trends data and clinical feedback, were presented to ChatGPT for 
evaluation. Two independent experts assessed the responses using a 7-point 
Likert scale for reliability and usefulness. Additionally, the overall quality 
was also rated using the Global Quality Scale (GQS). To assess readability, 
various established metrics were employed, including the Flesch Reading Ease 
score (FRE), the Simple Measure of Gobbledygook (SMOG) Index, the Coleman-Liau 
Index (CLI), the Gunning Fog Index (GFI), the Flesch-Kinkaid Grade Level (FKGL), 
the FORCAST Grade Level, and the Automated Readability Index (ARI).
RESULTS: The mean reliability scores were 4.68 ± 0.73 (Median: 5, IQR 4-5), 
while the mean usefulness scores were 4.84 ± 0.84 (Median: 5, IQR 
4-5). Additionally the mean GQS scores were 4.28 ± 0.58 (Median: 4, IQR 4-5). 
Inter-rater reliability analysis using the Intraclass correlation coefficient 
showed excellent agreement: 0.942 for reliability, 0.935 for usefulness, and 
0.868 for GQS. While general informational questions received high scores, 
responses to treatment-specific and personalized inquiries required greater 
depth and comprehensiveness. Readability analysis indicated that ChatGPT's 
responses required at least a high school senior to college-level reading 
ability.
CONCLUSION: ChatGPT provides reliable, useful, and moderate quality information 
on scoliosis but has limitations in addressing treatment-specific and 
personalized inquiries. Caution is essential when using Artificial Intelligence 
(AI) in patient education and medical decision-making.

© 2025. The Author(s), under exclusive licence to Springer-Verlag France SAS, 
part of Springer Nature.",yes,"This study evaluates ChatGPT's responses to frequently asked clinical questions about scoliosis, assessing their reliability, usefulness, quality, and readability. The focus is on the clinical information provided to patients, which is a direct clinical application of an LLM in healthcare.",10.1007/s00590-025-04198-4,,,,,
Assessing Large Language Models for Oncology Data Inference from Radiology Reports,"Purpose: We examined the effectiveness of proprietary and open Large Language Models (LLMs) in detecting disease presence, location, and treatment response in pancreatic cancer from radiology reports. Methods: We analyzed 203 deidentified radiology reports, manually annotated for disease status, location, and indeterminate nodules needing follow-up. Utilizing GPT-4, GPT-3.5-turbo, and open models like Gemma-7B and Llama3-8B, we employed strategies such as ablation and prompt engineering to boost accuracy. Discrepancies between human and model interpretations were reviewed by a secondary oncologist. Results: Among 164 pancreatic adenocarcinoma patients, GPT-4 showed the highest accuracy in inferring disease status, achieving a 75.5% correctness (F1-micro). Open models Mistral-7B and Llama3-8B performed comparably, with accuracies of 68.6% and 61.4%, respectively. Mistral-7B excelled in deriving correct inferences from ""Objective Findings"" directly. Most tested models demonstrated proficiency in identifying disease containing anatomical locations from a list of choices, with GPT-4 and Llama3-8B showing near parity in precision and recall for disease site identification. However, open models struggled with differentiating benign from malignant post-surgical changes, impacting their precision in identifying findings indeterminate for cancer. A secondary review occasionally favored GPT-3.5's interpretations, indicating the variability in human judgment. Conclusion: LLMs, especially GPT-4, are proficient in deriving oncological insights from radiology reports. Their performance is enhanced by effective summarization strategies, demonstrating their potential in clinical support and healthcare analytics. This study also underscores the possibility of zero-shot open model utility in environments where proprietary models are restricted. Finally, by providing a set of annotated radiology reports, this paper presents a valuable dataset for further LLM research in oncology.",yes,"This study evaluates multiple LLMs (GPT-4, GPT-3.5-turbo, Llama3-8B, Mistral-7B, Gemma-7B) for their ability to infer clinically relevant information (disease presence, location, treatment response) from radiology reports in pancreatic cancer patients. The task is directly clinical, involving disease status and management-relevant findings. The study is not a review or editorial, and the LLMs are used for clinical inference, not just data structuring.",10.1101/2024.05.23.24307579,,,,,
ChatGPT-4 Consistency in Interpreting Laryngeal Clinical Images of Common Lesions and Disorders,"Objective: To investigate the consistency of Chatbot Generative Pretrained Transformer (ChatGPT)-4 in the analysis of clinical pictures of common laryngological conditions. Study Design: Prospective uncontrolled study. Setting: Multicenter study. Methods: Patient history and clinical videolaryngostroboscopic images were presented to ChatGPT-4 for differential diagnoses, management, and treatment(s). ChatGPT-4 responses were assessed by 3 blinded laryngologists with the artificial intelligence performance instrument (AIPI). The complexity of cases and the consistency between practitioners and ChatGPT-4 for interpreting clinical images were evaluated with a 5-point Likert Scale. The intraclass correlation coefficient (ICC) was used to measure the strength of interrater agreement. Results: Forty patients with a mean complexity score of 2.60 ± 1.15. were included. The mean consistency score for ChatGPT-4 image interpretation was 2.46 ± 1.42. ChatGPT-4 perfectly analyzed the clinical images in 6 cases (15%; 5/5), while the consistency between GPT-4 and judges was high in 5 cases (12.5%; 4/5). Judges reported an ICC of 0.965 for the consistency score (P =.001). ChatGPT-4 erroneously documented vocal fold irregularity (mass or lesion), glottic insufficiency, and vocal cord paralysis in 21 (52.5%), 2 (0.05%), and 5 (12.5%) cases, respectively. ChatGPT-4 and practitioners indicated 153 and 63 additional examinations, respectively (P =.001). The ChatGPT-4 primary diagnosis was correct in 20.0% to 25.0% of cases. The clinical image consistency score was significantly associated with the AIPI score (rs = 0.830; P =.001). Conclusion: The ChatGPT-4 is more efficient in primary diagnosis, rather than in the image analysis, selecting the most adequate additional examinations and treatments. © 2024 American Academy of Otolaryngology–Head and Neck Surgery Foundation.",yes,"This study evaluates ChatGPT-4's consistency and accuracy in interpreting clinical images and making differential diagnoses, management, and treatment recommendations for laryngological conditions. The LLM is directly assessed in a clinical context, including image interpretation and clinical decision-making.",10.1002/ohn.897,,,,,
Assessing the accuracy and consistency of ChatGPT in clinical pharmacy management: A preliminary analysis with clinical pharmacy experts worldwide,"Background: ChatGPT conversation system has ushered in a revolutionary new era of information retrieval and stands as one of the fastest-growing platforms. Clinical pharmacy, as a dynamic discipline, necessitates an advanced comprehension of drugs and diseases. The process of decision-making in clinical pharmacy demands accuracy and consistency in medical information, as it directly affects patient safety. Objective: The objective was to evaluate ChatGPT's accuracy and consistency in managing pharmacotherapy cases across multiple time points. Additionally, input was gathered from global clinical pharmacy experts, and the agreement between ChatGPT's responses and those of clinical pharmacy experts worldwide was assessed. Methods: A set of 20 cases of pharmacotherapy was entered into ChatGPT at three different time points. Reliability analysis was performed using inter-rater reliability to measure the accuracy of the output generated by ChatGPT at each time point. Test-retest reliability was performed to measure the consistency of the output generated by ChatGPT across the three time points. Pharmacy expert performance was evaluated, and the overall results were compared. Results: ChatGPT achieved a hit rate of 70.83% at week 1, 79.2% at week 3, and 75% at week 5. The percent agreement between weeks 1 and 3 was 79.2%, whereas it was 87.5% between weeks 3 and 5, and 83.3% between weeks 1 and 5. In contrast, accuracy rates among clinical pharmacy experts showed considerable variation according to their geographic location. The highest agreement between clinical pharmacist responses and ChatGPT responses was observed at the last time point examined. Conclusions: Overall, the analysis suggested that ChatGPT is capable of generating clinically relevant pharmaceutical information, albeit with some variation in accuracy and consistency. It should be noted that clinical pharmacy experts worldwide may provide varying degrees of accuracy depending on their expertise. This study highlights the potential of AI chatbots in clinical pharmacy. © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's accuracy and consistency in managing pharmacotherapy cases, directly assessing its clinical performance in pharmacy management. The comparison with clinical pharmacy experts and the focus on clinical decision-making make it a clear example of LLM evaluation in a healthcare context.",10.1016/j.sapharm.2023.08.012,,,,,
Reinforcement Explainability of ChatGPT Prompts by Embedding Breast Cancer Self-Screening Rules into AI Responses,"Addressing the global challenge of breast cancer, this research explores the fusion of generative AI and the intricacies of breast cancer risk assessment. This study seeks to bridge the technology gap between intelligent machines and clinicians by demonstrating ChatGPT's proficiency in reasoning. The methodology employs a supervised prompt-engineering approach to enforce detailed explanations for ChatGPT's recom-mendations. Synthetic use cases, generated algorithmi-cally, serve as the testing ground for the encoded rules, evaluating the model's processing prowess. Findings highlight ChatGPT's promising capacity in processing rules comparable to Expert System Shells, with a focus on natural language reasoning. The research introduces the concept of reinforcement explainability, showcasing its potential in elucidating outcomes and facilitating user-friendly interfaces for breast cancer risk assessment.  © 2024 IEEE.",yes,"This study evaluates ChatGPT's ability to provide explanations and recommendations for breast cancer self-screening, using synthetic clinical use cases. The focus is on ChatGPT's reasoning and explainability in a clinical context, specifically breast cancer risk assessment. Although synthetic cases are used, the application is clinical and the LLM is being evaluated for a healthcare task.",10.1109/MedAI62885.2024.00059,,,,,
Evaluating Risk Progression in Mental Health Chatbots Using Escalating Prompts,"The safety of large language models (LLMs) as mental health chatbots is not fully established. This study evaluated the risk escalation responses of publicly available ChatGPT conversational agents when presented with prompts of increasing depression severity and suicidality. The average referral point to a human was at the midpoint of escalating prompts. However, most agents only definitively recommended professional help at the highest level of risk. Few agents included crisis resources like suicide hotlines. The results suggest current LLMs may fail to escalate mental health risk scenarios appropriately. More rigorous testing and oversight are needed before deployment in mental healthcare settings.",yes,"This study evaluates ChatGPT's responses to escalating mental health risk scenarios, specifically focusing on its clinical safety and appropriateness in recommending professional help or crisis resources. It directly assesses the LLM's clinical behavior in a mental health context.",10.1101/2023.09.10.23295321,,,,,
Generative artificial intelligence as a source of breast cancer information for patients: Proceed with caution,"Background: This study evaluated the accuracy, clinical concordance, and readability of the chatbot interface generative pretrained transformer (ChatGPT) 3.5 as a source of breast cancer information for patients. Methods: Twenty questions that patients are likely to ask ChatGPT were identified by breast cancer advocates. These were posed to ChatGPT 3.5 in July 2023 and were repeated three times. Responses were graded in two domains: accuracy (4-point Likert scale, 4 = worst) and clinical concordance (information is clinically similar to physician response; 5-point Likert scale, 5 = not similar at all). The concordance of responses with repetition was estimated using intraclass correlation coefficient (ICC) of word counts. Response readability was calculated using the Flesch Kincaid readability scale. References were requested and verified. Results: The overall average accuracy was 1.88 (range 1.0–3.0; 95% confidence interval [CI], 1.42–1.94), and clinical concordance was 2.79 (range 1.0–5.0; 95% CI, 1.94–3.64). The average word count was 310 words per response (range, 146–441 words per response) with high concordance (ICC, 0.75; 95% CI, 0.59–0.91; p <.001). The average readability was poor at 37.9 (range, 18.0–60.5) with high concordance (ICC, 0.73; 95% CI, 0.57–0.90; p <.001). There was a weak correlation between ease of readability and better clinical concordance (−0.15; p =.025). Accuracy did not correlate with readability (0.05; p =.079). The average number of references was 1.97 (range, 1–4; total, 119). ChatGPT cited peer-reviewed articles only once and often referenced nonexistent websites (41%). Conclusions: Because ChatGPT 3.5 responses were incorrect 24% of the time and did not provide real references 41% of the time, patients should be cautioned about using ChatGPT for medical information. © 2024 American Cancer Society.",yes,"This study evaluates ChatGPT 3.5's performance in answering patient questions about breast cancer, focusing on accuracy, clinical concordance, and readability. It directly assesses the LLM's clinical utility as a source of patient information, which is a clinical application.",10.1002/cncr.35521,,,,,
ChatGPT encounters multiple opportunities and challenges in neurosurgery,"BACKGROUND: ChatGPT, powered by the GPT model and Transformer architecture, has demonstrated remarkable performance in the domains of medicine and healthcare, providing customized and informative responses. In our study, we investigated the potential of ChatGPT in the field of neurosurgery, focusing on its applications at the patient, neurosurgery student/resident, and neurosurgeon levels. METHOD: The authors conducted inquiries with ChatGPT from the viewpoints of patients, neurosurgery students/residents, and neurosurgeons, covering a range of topics, such as disease diagnosis, treatment options, prognosis, rehabilitation, and patient care. The authors also explored concepts related to neurosurgery, including fundamental principles and clinical aspects, as well as tools and techniques to enhance the skills of neurosurgery students/residents. Additionally, the authors examined disease-specific medical interventions and the decision-making processes involved in clinical practice. RESULTS: The authors received individual responses from ChatGPT, but they tended to be shallow and repetitive, lacking depth and personalization. Furthermore, ChatGPT may struggle to discern a patient's emotional state, hindering the establishment of rapport and the delivery of appropriate care. The language used in the medical field is influenced by technical and cultural factors, and biases in the training data can result in skewed or inaccurate responses. Additionally, ChatGPT's limitations include the inability to conduct physical examinations or interpret diagnostic images, potentially overlooking complex details and individual nuances in each patient's case. Moreover, its absence in the surgical setting limits its practical utility. CONCLUSION: Although ChatGPT is a powerful language model, it cannot substitute for the expertise and experience of trained medical professionals. It lacks the capability to perform physical examinations, make diagnoses, administer treatments, establish trust, provide emotional support, and assist in the recovery process. Moreover, the implementation of Artificial Intelligence in healthcare necessitates careful consideration of legal and ethical concerns. While recognizing the potential of ChatGPT, additional training with comprehensive data is necessary to fully maximize its capabilities. Copyright © 2023 The Author(s). Published by Wolters Kluwer Health, Inc.",yes,"This study appears to be an evaluation of ChatGPT's responses to clinical and educational queries in neurosurgery from the perspectives of patients, trainees, and neurosurgeons. It assesses ChatGPT's performance in clinical scenarios such as diagnosis, treatment options, prognosis, and decision-making, which aligns with the inclusion criteria. Although the abstract discusses limitations and challenges, it is not a review or opinion piece but rather presents results from direct interaction with the LLM in a clinical context.",10.1097/JS9.0000000000000571,,,,,
The credibility of dietary advice formulated by ChatGPT: Robo-diets for people with food allergies,"The introduction of ChatGPT has sparked enormous public interest in large language (deep-learning) models, which have been sophisticated enough to perform well on a variety of tasks. One way people are using these models is to construct diets. The prompts often include food restrictions that are an obligatory part of everyday life for millions of people worldwide. The aim of this study was to investigate the safety and accuracy of 56 diets, constructed for hypothetical individuals who are allergic to food allergens. Four levels, corresponding to the “baseline” ability of ChatGPT without prompting for specifics, as well as its ability to prepare appropriate diets when an individual has an adverse food reaction to two allergens or solicits a low-calorie diet, were defined. Findings from our study demonstrated that ChatGPT, although generally accurate, has the potential to produce harmful diets. More common errors involve inaccuracies in portions or calories of food, meals, or diets. We discuss here how the accuracy of large language models could be increased and the trade-offs involved. We propose that prompting for elimination diets can serve as one way to assess differences between such models. © 2023 The Author(s)",yes,"This study evaluates ChatGPT's ability to generate safe and accurate dietary advice for individuals with food allergies, a clinical application relevant to patient care. It assesses the model's performance in constructing diets with specific medical restrictions, which is a direct clinical use case.",10.1016/j.nut.2023.112076,,,,,
Use of Artificial Intelligence Chatbots in Interpretation of Pathology Reports,"Importance: Anatomic pathology reports are an essential part of health care, containing vital diagnostic and prognostic information. Currently, most patients have access to their test results online. However, the reports are complex and are generally incomprehensible to laypeople. Artificial intelligence chatbots could potentially simplify pathology reports. Objective: To evaluate the ability of large language model chatbots to accurately explain pathology reports to patients. Design, Setting, and Participants: This cross-sectional study used 1134 pathology reports from January 1, 2018, to May 31, 2023, from a multispecialty hospital in Brooklyn, New York. A new chat was started for each report, and both chatbots (Bard [Google Inc], hereinafter chatbot 1; GPT-4 [OpenAI], hereinafter chatbot 2) were asked in sequential prompts to explain the reports in simple terms and identify key information. Chatbot responses were generated between June 1 and August 31, 2023. The mean readability scores of the original and simplified reports were compared. Two reviewers independently screened and flagged reports with potential errors. Three pathologists reviewed the flagged reports and categorized them as medically correct, partially medically correct, or medically incorrect; they also recorded any instances of hallucinations. Main Outcomes and Measures: Outcomes included improved mean readability scores and a medically accurate interpretation. Results: For the 1134 reports included, the Flesch-Kincaid grade level decreased from a mean of 13.19 (95% CI, 12.98-13.41) to 8.17 (95% CI, 8.08-8.25; t = 45.29; P <.001) by chatbot 1 and 7.45 (95% CI, 7.35-7.54; t = 49.69; P <.001) by chatbot 2. The Flesch Reading Ease score was increased from a mean of 10.32 (95% CI, 8.69-11.96) to 61.32 (95% CI, 60.80-61.84; t = -63.19; P <.001) by chatbot 1 and 70.80 (95% CI, 70.32-71.28; t = -74.61; P <.001) by chatbot 2. Chatbot 1 interpreted 993 reports (87.57%) correctly, 102 (8.99%) partially correctly, and 39 (3.44%) incorrectly; chatbot 2 interpreted 1105 reports (97.44%) correctly, 24 (2.12%) partially correctly, and 5 (0.44%) incorrectly. Chatbot 1 had 32 instances of hallucinations (2.82%), while chatbot 2 had 3 (0.26%). Conclusions and Relevance: The findings of this cross-sectional study suggest that artificial intelligence chatbots were able to simplify pathology reports. However, some inaccuracies and hallucinations occurred. Simplified reports should be reviewed by clinicians before distribution to patients.. © 2024 American Medical Association. All rights reserved.",yes,"This study evaluates the ability of LLM-based chatbots (Bard and GPT-4) to explain pathology reports to patients, assessing both readability and medical accuracy. The LLMs are used in a clinical context to interpret and communicate medical information to patients, which is a direct clinical application.",10.1001/jamanetworkopen.2024.12767,,,,,
"A Comparative Study of Large Language Models, Human Experts, and Expert-Edited Large Language Models to Neuro-Ophthalmology Questions","Background:While large language models (LLMs) are increasingly used in medicine, their effectiveness compared with human experts remains unclear. This study evaluates the quality and empathy of Expert + AI, human experts, and LLM responses in neuro-ophthalmology.Methods:This randomized, masked, multicenter cross-sectional study was conducted from June to July 2023. We randomly assigned 21 neuro-ophthalmology questions to 13 experts. Each expert provided an answer and then edited a ChatGPT-4-generated response, timing both tasks. In addition, 5 LLMs (ChatGPT-3.5, ChatGPT-4, Claude 2, Bing, Bard) generated responses. Anonymized and randomized responses from Expert + AI, human experts, and LLMs were evaluated by the remaining 12 experts. The main outcome was the mean score for quality and empathy, rated on a 1-5 scale.Results:Significant differences existed between response types for both quality and empathy (P < 0.0001, P < 0.0001). For quality, Expert + AI (4.16 ± 0.81) performed the best, followed by GPT-4 (4.04 ± 0.92), GPT-3.5 (3.99 ± 0.87), Claude (3.6 ± 1.09), Expert (3.56 ± 1.01), Bard (3.5 ± 1.15), and Bing (3.04 ± 1.12). For empathy, Expert + AI (3.63 ± 0.87) had the highest score, followed by GPT-4 (3.6 ± 0.88), Bard (3.54 ± 0.89), GPT-3.5 (3.5 ± 0.83), Bing (3.27 ± 1.03), Expert (3.26 ± 1.08), and Claude (3.11 ± 0.78). For quality (P < 0.0001) and empathy (P = 0.002), Expert + AI performed better than Expert. Time taken for expert-created and expert-edited LLM responses was similar (P = 0.75).Conclusions:Expert-edited LLM responses had the highest expert-determined ratings of quality and empathy warranting further exploration of their potential benefits in clinical settings. © 2024 by North American Neuro-Ophthalmology Society.",yes,"This study directly evaluates the performance of several LLMs (ChatGPT-3.5, ChatGPT-4, Claude 2, Bing, Bard) in answering neuro-ophthalmology questions, comparing them to human experts and expert-edited LLM responses. The focus is on clinical question answering, a relevant clinical application. The study is not a review or opinion piece, and it uses frontier LLMs in a clinical context.",10.1097/WNO.0000000000002145,,,,,
Evaluating prompt engineering on GPT-3.5's performance in USMLE-style medical calculations and clinical scenarios generated by GPT-4,"This study was designed to assess how different prompt engineering techniques, specifically direct prompts, Chain of Thought (CoT), and a modified CoT approach, influence the ability of GPT-3.5 to answer clinical and calculation-based medical questions, particularly those styled like the USMLE Step 1 exams. To achieve this, we analyzed the responses of GPT-3.5 to two distinct sets of questions: a batch of 1000 questions generated by GPT-4, and another set comprising 95 real USMLE Step 1 questions. These questions spanned a range of medical calculations and clinical scenarios across various fields and difficulty levels. Our analysis revealed that there were no significant differences in the accuracy of GPT-3.5's responses when using direct prompts, CoT, or modified CoT methods. For instance, in the USMLE sample, the success rates were 61.7% for direct prompts, 62.8% for CoT, and 57.4% for modified CoT, with a p-value of 0.734. Similar trends were observed in the responses to GPT-4 generated questions, both clinical and calculation-based, with p-values above 0.05 indicating no significant difference between the prompt types. The conclusion drawn from this study is that the use of CoT prompt engineering does not significantly alter GPT-3.5's effectiveness in handling medical calculations or clinical scenario questions styled like those in USMLE exams. This finding is crucial as it suggests that performance of ChatGPT remains consistent regardless of whether a CoT technique is used instead of direct prompts. This consistency could be instrumental in simplifying the integration of AI tools like ChatGPT into medical education, enabling healthcare professionals to utilize these tools with ease, without the necessity for complex prompt engineering.",yes,"This study evaluates GPT-3.5 (ChatGPT) on its ability to answer clinical and calculation-based medical questions, specifically USMLE-style questions, and compares different prompt engineering techniques. The focus is on the LLM's clinical performance in a medical education context, which fits the inclusion criteria.",10.1038/s41598-024-66933-x,,,,,
Performance of generative pre-trained transformer-4 on the certification test for mental health management: A factorial design,"OBJECTIVE: This study aimed to investigate the performance of generative pre-trained transformer-4 (GPT-4) on the Certification Test for Mental Health Management and whether tuned prompts could improve its performance. METHODS: This study used a 3 × 2 factorial design to examine the performance according to test difficulty (courses) and prompt conditions. We prepared 200 multiple-choice questions (600 questions overall) for each course using the Certification Test for Mental Health Management (levels I-III) and essay questions from the level I test for the previous four examinations. Two conditions were used: a simple prompt condition using the questions as prompts and tuned prompt condition using techniques to obtain better answers. GPT-4 (gpt-4-0613) was adopted and implemented using the OpenAI API. RESULTS: The simple prompt condition scores were 74.5, 71.5, and 64.0 for levels III, II, and I, respectively. The tuned and simple prompt condition scores had no significant differences (Odds ratio = 1.03, 95% Confidence interval; 0.65-1.62, p = 0.908). Incorrect answers were observed in the simple prompt condition because of the inability to make choices, whereas no incorrect answers were observed in the tuned prompt condition. The average score for the essay questions under the simple prompt condition was 22.5 out of 50 points (45.0%). CONCLUSION: GPT-4 had a sufficient knowledge network for occupational mental health, surpassing the criteria for levels II and III tests. For the level I test, which required the ability to describe more advanced knowledge accurately, GPT-4 did not meet the criteria. External information may be needed when using GPT-4 at this level. Although the tuned prompts did not significantly improve the performance, they were promising in avoiding unintended outputs and organizing output formats. UMIN trial registration: UMIN-CTR ID = UMIN000053582.",yes,"This study evaluates GPT-4's performance on a certification test for mental health management, including both multiple-choice and essay questions. The test is clinically relevant, and the study assesses the LLM's ability to answer questions that are part of a professional certification process in healthcare. This fits the inclusion criteria as it is a clinical evaluation of an LLM in a medical context.",10.1539/sangyoeisei.2024-017-B,,,,,
Evaluating the application of ChatGPT in China's residency training education: An exploratory study,"OBJECTIVE: The purpose of this study was to assess the utility of information generated by ChatGPT for residency education in China. METHODS: We designed a three-step survey to evaluate the performance of ChatGPT in China's residency training education including residency final examination questions, patient cases, and resident satisfaction scores. First, 204 questions from the residency final exam were input into ChatGPT's interface to obtain the percentage of correct answers. Next, ChatGPT was asked to generate 20 clinical cases, which were subsequently evaluated by three instructors using a pre-designed Likert scale with 5 points. The quality of the cases was assessed based on criteria including clarity, relevance, logicality, credibility, and comprehensiveness. Finally, interaction sessions between 31 third-year residents and ChatGPT were conducted. Residents' perceptions of ChatGPT's feedback were assessed using a Likert scale, focusing on aspects such as ease of use, accuracy and completeness of responses, and its effectiveness in enhancing understanding of medical knowledge. RESULTS: Our results showed ChatGPT-3.5 correctly answered 45.1% of exam questions. In the virtual patient cases, ChatGPT received mean ratings of 4.57 ± 0.50, 4.68 ± 0.47, 4.77 ± 0.46, 4.60 ± 0.53, and 3.95 ± 0.59 points for clarity, relevance, logicality, credibility, and comprehensiveness from clinical instructors, respectively. Among training residents, ChatGPT scored 4.48 ± 0.70, 4.00 ± 0.82 and 4.61 ± 0.50 points for ease of use, accuracy and completeness, and usefulness, respectively. CONCLUSION: Our findings demonstrate ChatGPT's immense potential for personalized Chinese medical education.",yes,"This study evaluates ChatGPT's performance on residency exam questions, its ability to generate clinical cases, and its interaction with residents in a medical education context. It includes clinical exam performance and simulated patient cases, which are relevant to clinical education and decision-making. The focus is on the clinical application of LLMs in medical training, which fits the inclusion criteria.",10.1080/0142159X.2024.2377808,,,,,
Using Large Language Models to Detect Depression From User-Generated Diary Text Data as a Novel Approach in Digital Mental Health Screening: Instrument Validation Study,"Background: Depressive disorders have substantial global implications, leading to various social consequences, including decreased occupational productivity and a high disability burden. Early detection and intervention for clinically significant depression have gained attention; however, the existing depression screening tools, such as the Center for Epidemiologic Studies Depression Scale, have limitations in objectivity and accuracy. Therefore, researchers are identifying objective indicators of depression, including image analysis, blood biomarkers, and ecological momentary assessments (EMAs). Among EMAs, user-generated text data, particularly from diary writing, have emerged as a clinically significant and analyzable source for detecting or diagnosing depression, leveraging advancements in large language models such as ChatGPT. Objective: We aimed to detect depression based on user-generated diary text through an emotional diary writing app using a large language model (LLM). We aimed to validate the value of the semistructured diary text data as an EMA data source. Methods: Participants were assessed for depression using the Patient Health Questionnaire and suicide risk was evaluated using the Beck Scale for Suicide Ideation before starting and after completing the 2-week diary writing period. The text data from the daily diaries were also used in the analysis. The performance of leading LLMs, such as ChatGPT with GPT-3.5 and GPT-4, was assessed with and without GPT-3.5 fine-tuning on the training data set. The model performance comparison involved the use of chain-of-thought and zero-shot prompting to analyze the text structure and content. Results: We used 428 diaries from 91 participants; GPT-3.5 fine-tuning demonstrated superior performance in depression detection, achieving an accuracy of 0.902 and a specificity of 0.955. However, the balanced accuracy was the highest (0.844) for GPT-3.5 without fine-tuning and prompt techniques; it displayed a recall of 0.929. Conclusions: Both GPT-3.5 and GPT-4.0 demonstrated relatively reasonable performance in recognizing the risk of depression based on diaries. Our findings highlight the potential clinical usefulness of user-generated text data for detecting depression. In addition to measurable indicators, such as step count and physical activity, future research should increasingly emphasize qualitative digital expression. ©Daun Shin, Hyoseung Kim, Seunghwan Lee, Younhee Cho, Whanbo Jung.",yes,"This study evaluates the performance of large language models (ChatGPT with GPT-3.5 and GPT-4) in detecting depression from user-generated diary text, a clinically relevant application in digital mental health screening. The LLMs are directly assessed for their ability to identify depression, which is a clinical task.",10.2196/54617,,,,,
"Evaluating Computer Vision, Large Language, and Genome-Wide Association Models in a Limited Sized Patient Cohort for Pre-Operative Risk Stratification in Adult Spinal Deformity Surgery","Background: Adult spinal deformities (ASD) are varied spinal abnormalities, often necessitating surgical intervention when associated with pain, worsening deformity, or worsening function. Predicting post-operative complications and revision surgery is critical for surgical planning and patient counseling. Due to the relatively small number of cases of ASD surgery, machine learning applications have been limited to traditional models (e.g., logistic regression or standard neural networks) and coarse clinical variables. We present the novel application of advanced models (CNN, LLM, GWAS) using complex data types (radiographs, clinical notes, genomics) for ASD outcome prediction. Methods: We developed a CNN trained on 209 ASD patients (1549 radiographs) from the Stanford Research Repository, a CNN pre-trained on VinDr-SpineXR (10,468 spine radiographs), and an LLM using free-text clinical notes from the same 209 patients, trained via Gatortron. Additionally, we conducted a GWAS using the UK Biobank, contrasting 540 surgical ASD patients with 7355 non-surgical ASD patients. Results: The LLM notably outperformed the CNN in predicting pulmonary complications (F1: 0.545 vs. 0.2881), neurological complications (F1: 0.250 vs. 0.224), and sepsis (F1: 0.382 vs. 0.132). The pre-trained CNN showed improved sepsis prediction (AUC: 0.638 vs. 0.534) but reduced performance for neurological complication prediction (AUC: 0.545 vs. 0.619). The LLM demonstrated high specificity (0.946) and positive predictive value (0.467) for neurological complications. The GWAS identified 21 significant (p < 10−5) SNPs associated with ASD surgery risk (OR: mean: 3.17, SD: 1.92, median: 2.78), with the highest odds ratio (8.06) for the LDB2 gene, which is implicated in ectoderm differentiation. Conclusions: This study exemplifies the innovative application of cutting-edge models to forecast outcomes in ASD, underscoring the utility of complex data in outcome prediction for neurosurgical conditions. It demonstrates the promise of genetic models when identifying surgical risks and supports the integration of complex machine learning tools for informed surgical decision-making in ASD. © 2024 by the authors.",yes,"This study evaluates a large language model (LLM) trained on clinical notes to predict post-operative complications in adult spinal deformity surgery, directly comparing its clinical performance to other models. The LLM is used for a clinical prediction task, which fits the inclusion criteria.",10.3390/jcm13030656,,,,,
Heart-to-heart with ChatGPT: the impact of patients consulting AI for cardiovascular health advice,"Objectives The advent of conversational artificial intelligence (AI) systems employing large language models such as ChatGPT has sparked public, professional and academic debates on the capabilities of such technologies. This mixed-methods study sets out to review and systematically explore the capabilities of ChatGPT to adequately provide health advice to patients when prompted regarding four topics from the field of cardiovascular diseases. Methods As of 30 May 2023, 528 items on PubMed contained the term ChatGPT in their title and/or abstract, with 258 being classified as journal articles and included in our thematic state-of-the-art review. For the experimental part, we systematically developed and assessed 123 prompts across the four topics based on three classes of users and two languages. Medical and communications experts scored ChatGPT’s responses according to the 4Cs of language model evaluation proposed in this article: correct, concise, comprehensive and comprehensible. Results The articles reviewed were fairly evenly distributed across discussing how ChatGPT could be used for medical publishing, in clinical practice and for education of medical personnel and/or patients. Quantitatively and qualitatively assessing the capability of ChatGPT on the 123 prompts demonstrated that, while the responses generally received above-average scores, they occupy a spectrum from the concise and correct via the absurd to what only can be described as hazardously incorrect and incomplete. Prompts formulated at higher levels of health literacy generally yielded higher-quality answers. Counterintuitively, responses in a lower-resource language were often of higher quality. Conclusions The results emphasise the relationship between prompt and response quality and hint at potentially concerning futures in personalised medicine. The widespread use of large language models for health advice might amplify existing health inequalities and will increase the pressure on healthcare systems by providing easy access to many seemingly likely differential diagnoses and recommendations for seeing a doctor for even harmless ailments. © Author(s) (or their employer(s)) 2023.",yes,"This study systematically evaluates ChatGPT's ability to provide cardiovascular health advice to patients, using expert scoring of its responses to clinical prompts. The focus is on the clinical application of an LLM (ChatGPT) in patient-facing health advice, which fits the inclusion criteria.",10.1136/openhrt-2023-002455,,,,,
Use of natural language processing techniques to predict patient selection for total hip and knee arthroplasty from radiology reports,"Aims To examine whether natural language processing (NLP) using a clinically based large language model (LLM) could be used to predict patient selection for total hip or total knee arthroplasty (THA/TKA) from routinely available free-text radiology reports. Methods Data pre-processing and analyses were conducted according to the Artificial intelligence to Revolutionize the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project protocol.This included use of de-identified Scottish regional clinical data of patients referred for consideration ofTHA/TKA, held in a secure data environment designed for artificial intelligence (AI) inference. Only preoperative radiology reports were included. NLP algorithms were based on the freely available GatorTron model, a LLM trained on over 82 billion words of de-identified clinical text.Two inference tasks were performed: assessment after model-fine tuning (50 Epochs and three cycles of k-fold cross validation), and external validation. Results ForTHA, there were 5,558 patient radiology reports included, of which 4,137 were used for model training and testing, and 1,421 for external validation. Following training, model performance demonstrated average (mean across three folds) accuracy, F1 score, and area under the receiver operating curve (AUROC) values of 0.850 (95% confidence interval (CI) 0.833 to 0.867), 0.813 (95% CI 0.785 to 0.841), and 0.847 (95% CI 0.822 to 0.872), respectively. ForTKA, 7,457 patient radiology reports were included, with 3,478 used for model training and testing, and 3,152 for external validation. Performance metrics included accuracy, F1 score, and AUROC values of 0.757 (95% CI 0.702 to 0.811), 0.543 (95% CI 0.479 to 0.607), and 0.717 (95% CI 0.657 to 0.778) respectively.There was a notable deterioration in performance on external validation in both cohorts. Conclusion The use of routinely available preoperative radiology reports provides promising potential to help screen suitable candidates forTHA, but not forTKA.The external validation results demonstrate the importance of further model testing and training when confronted with new clinical cohorts. © 2024 The British Editorial Society of Bone & Joint Surgery.",yes,"This study evaluates a clinically trained large language model (GatorTron) for predicting patient selection for hip and knee arthroplasty based on radiology reports. The LLM is used for a clinical decision support task (screening candidates for surgery), which fits the inclusion criteria.",10.1302/0301-620X.106B7.BJJ-2024-0136,,,,,
Characterizing phonemic fluency by transfer learning with deep language models,"Though phonemic fluency tasks are traditionally indexed by the number of correct responses, the underlying disorder may shape the specific choice of words - both correct and erroneous. We report the first comprehensive qualitative analysis of incorrect and correct words generated on the phonemic ('S') fluency test, in a large sample of patients (n = 239) with focal, unilateral frontal or posterior lesions and healthy controls (n = 136). We conducted detailed qualitative analyses of the single words generated in the phonemic fluency task using categorical descriptions for different types of errors, low-frequency words and clustering/switching. We further analysed patients' and healthy controls' entire sequences of words by employing stochastic block modelling of Generative Pretrained Transformer 3-based deep language representations. We conducted predictive modelling to investigate whether deep language representations of word sequences improved the accuracy of detecting the presence of frontal lesions using the phonemic fluency test. Our qualitative analyses of the single words generated revealed several novel findings. For the different types of errors analysed, we found a non-lateralized frontal effect for profanities, left frontal effects for proper nouns and permutations and a left posterior effect for perseverations. For correct words, we found a left frontal effect for low-frequency words. Our novel large language model-based approach found five distinct communities whose varied word selection patterns reflected characteristic demographic and clinical features. Predictive modelling showed that a model based on Generative Pretrained Transformer 3-derived word sequence representations predicted the presence of frontal lesions with greater fidelity than models of native features. Our study reveals a characteristic pattern of phonemic fluency responses produced by patients with frontal lesions. These findings demonstrate the significant inferential and diagnostic value of characterizing qualitative features of phonemic fluency performance with large language models and stochastic block modelling. © 2023 The Author(s).",yes,"This study uses GPT-3-based deep language representations to analyze and model patient responses in a clinical phonemic fluency task, and evaluates whether these LLM-derived features improve detection of frontal lesions. The LLM is used for clinical inference and diagnostic support, which fits the inclusion criteria.",10.1093/braincomms/fcad318,,,,,
Educating patients on osteoporosis and bone health: Can “ChatGPT” provide high-quality content?,"Purpose: The rise of artificial intelligence (AI) models like ChatGPT offers potential for varied applications, including patient education in healthcare. With gaps in osteoporosis and bone health knowledge and adherence to prevention and treatment, this study aims to evaluate the accuracy of ChatGPT in delivering evidence-based information related to osteoporosis. Methods: Twenty of the most common frequently asked questions (FAQs) related to osteoporosis were subcategorized into diagnosis, diagnostic method, risk factors, and treatment and prevention. These FAQs were sourced online and inputted into ChatGPT-3.5. Three orthopedic surgeons and one advanced practice provider who routinely treat patients with fragility fractures independently reviewed the ChatGPT-generated answers, grading them on a scale from 0 (harmful) to 4 (excellent). Mean response accuracy scores were calculated. To compare the variance of the means across the four categories, a one-way analysis of variance (ANOVA) was used. Results: ChatGPT displayed an overall mean accuracy score of 91%. Its responses were graded as “accurate requiring minimal clarification” or “excellent,” with a mean response score ranging from 3.25 to 4. No answers were deemed inaccurate or harmful. No significant difference was observed in the means of responses across the defined categories. Conclusion: ChatGPT-3.5 provided high-quality educational content. It showcased a high degree of accuracy in addressing osteoporosis-related questions, aligning closely with expert opinions and current literature, with structured and inclusive answers. However, while AI models can enhance patient information accessibility, they should be used as an adjunct rather than a substitute for human expertise and clinical judgment. © The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature 2024.",yes,"This study evaluates ChatGPT-3.5's ability to provide accurate, evidence-based answers to patient FAQs about osteoporosis, with clinical experts grading the responses. The focus is on the LLM's performance in a clinical communication context (patient education), which is a direct clinical application.",10.1007/s00590-024-03990-y,,,,,
Prompt Engineering an Informational Chatbot for Education on Mental Health Using a Multiagent Approach for Enhanced Compliance With Prompt Instructions: Algorithm Development and Validation,"Background: People with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. Education platforms powered by large language models (LLMs) have the potential to improve the accessibility of mental health information. However, the black-box nature of LLMs raises ethical and safety concerns regarding the controllability of chatbots. In particular, prompt-engineered chatbots may drift from their intended role as the conversation progresses and become more prone to hallucinations. Objective: This study aimed to develop and evaluate a critical analysis filter (CAF) system that ensures that an LLM-powered prompt-engineered chatbot reliably complies with its predefined instructions and scope while delivering validated mental health information. Methods: For a proof of concept, we prompt engineered an educational chatbot for schizophrenia powered by GPT-4 that could dynamically access information from a schizophrenia manual written for people with schizophrenia and their caregivers. In the CAF, a team of prompt-engineered LLM agents was used to critically analyze and refine the chatbot’s responses and deliver real-time feedback to the chatbot. To assess the ability of the CAF to re-establish the chatbot’s adherence to its instructions, we generated 3 conversations (by conversing with the chatbot with the CAF disabled) wherein the chatbot started to drift from its instructions toward various unintended roles. We used these checkpoint conversations to initialize automated conversations between the chatbot and adversarial chatbots designed to entice it toward unintended roles. Conversations were repeatedly sampled with the CAF enabled and disabled. In total, 3 human raters independently rated each chatbot response according to criteria developed to measure the chatbot’s integrity, specifically, its transparency (such as admitting when a statement lacked explicit support from its scripted sources) and its tendency to faithfully convey the scripted information in the schizophrenia manual. Results: In total, 36 responses (3 different checkpoint conversations, 3 conversations per checkpoint, and 4 adversarial queries per conversation) were rated for compliance with the CAF enabled and disabled. Activating the CAF resulted in a compliance score that was considered acceptable (≥2) in 81% (7/36) of the responses, compared to only 8.3% (3/36) when the CAF was deactivated. Conclusions: Although more rigorous testing in realistic scenarios is needed, our results suggest that self-reflection mechanisms could enable LLMs to be used effectively and safely in educational mental health platforms. This approach harnesses the flexibility of LLMs while reliably constraining their scope to appropriate and accurate interactions. © 2025, JMIR Publications Inc.. All rights reserved.",yes,"This study evaluates a GPT-4-powered chatbot designed to provide validated mental health information to people with schizophrenia and their caregivers. The evaluation focuses on the chatbot's compliance with clinical/educational instructions and its ability to deliver accurate, script-based information, which is a clinical application relevant to patient education. The study uses human raters to assess the chatbot's performance in a healthcare context.",10.2196/69820,,,,,
Evaluation of the accuracy and readability of ChatGPT-4 and Google Gemini in providing information on retinal detachment: a multicenter expert comparative study,"Background: Large language models (LLMs) such as ChatGPT-4 and Google Gemini show potential for patient health education, but concerns about their accuracy require careful evaluation. This study evaluates the readability and accuracy of ChatGPT-4 and Google Gemini in answering questions about retinal detachment. Methods: Comparative study analyzing responses from ChatGPT-4 and Google Gemini to 13 retinal detachment questions, categorized by difficulty levels (D1, D2, D3). Masked responses were reviewed by ten vitreoretinal specialists and rated on correctness, errors, thematic accuracy, coherence, and overall quality grading. Analysis included Flesch Readability Ease Score, word and sentence counts. Results: Both Artificial Intelligence tools required college-level understanding for all difficulty levels. Google Gemini was easier to understand (p = 0.03), while ChatGPT-4 provided more correct answers for the more difficult questions (p = 0.0005) with fewer serious errors. ChatGPT-4 scored highest on most challenging questions, showing superior thematic accuracy (p = 0.003). ChatGPT-4 outperformed Google Gemini in 8 of 13 questions, with higher overall quality grades in the easiest (p = 0.03) and hardest levels (p = 0.0002), showing a lower grade as question difficulty increased. Conclusions: ChatGPT-4 and Google Gemini effectively address queries about retinal detachment, offering mostly accurate answers with few critical errors, though patients require higher education for comprehension. The implementation of AI tools may contribute to improving medical care by providing accurate and relevant healthcare information quickly. © The Author(s) 2024.",yes,"This study directly evaluates the clinical accuracy and readability of two frontier LLMs (ChatGPT-4 and Google Gemini) in providing patient-facing information about retinal detachment. The LLMs are assessed on their ability to answer clinical questions, with expert review of correctness and quality. This fits the inclusion criteria as it tests LLMs in a healthcare context, specifically their performance in patient education and information provision.",10.1186/s40942-024-00579-9,,,,,
Performance of ChatGPT in French language Parcours d'Accès Spécifique Santé test and in OBGYN,"Objectives: To evaluate the performance of ChatGPT in a French medical school entrance examination. Methods: A cross-sectional study using a consecutive sample of text-based multiple-choice practice questions for the Parcours d'Accès Spécifique Santé. ChatGPT answered questions in French. We compared performance of ChatGPT in obstetrics and gynecology (OBGYN) and in the whole test. Results: Overall, 885 questions were evaluated. The mean test score was 34.0% (306; maximal score of 900). The performance of ChatGPT was 33.0% (292 correct answers, 885 questions). The performance of ChatGPT was lower in biostatistics (13.3% ± 19.7%) than in anatomy (34.2% ± 17.9%; P = 0.037) and also lower than in histology and embryology (40.0% ± 18.5%; P = 0.004). The OBGYN part had 290 questions. There was no difference in the test scores and the performance of ChatGPT in OBGYN versus the whole entrance test (P = 0.76 vs P = 0.10, respectively). Conclusions: ChatGPT answered one-third of questions correctly in the French test preparation. The performance in OBGYN was similar. © 2023 The Authors. International Journal of Gynecology & Obstetrics published by John Wiley & Sons Ltd on behalf of International Federation of Gynecology and Obstetrics.",yes,"This study evaluates the performance of ChatGPT on a French medical school entrance exam, including a specific focus on obstetrics and gynecology (OBGYN) questions. The use of ChatGPT to answer clinically relevant exam questions aligns with the inclusion criteria, as it assesses the LLM's clinical knowledge and reasoning in a healthcare context.",10.1002/ijgo.15083,,,,,
Evaluation of the Impact of ChatGPT on the Selection of Surgical Technique in Bariatric Surgery,"Purpose: With the growing interest in artificial intelligence (AI) applications in medicine, this study explores ChatGPT’s potential to influence surgical technique selection in metabolic and bariatric surgery (MBS), contrasting AI recommendations with established clinical guidelines and expert consensus. Materials and Methods: Conducting a single-center retrospective analysis, the study involved 161 patients who underwent MBS between January 2022 and December 2023. ChatGPT4 was used to analyze patient data, including demographics, pathological history, and BMI, to recommend the most suitable surgical technique. These AI recommendations were then compared with the hospital’s algorithm-based decisions. Results: ChatGPT recommended Roux-en-Y gastric bypass in over half of the cases. However, a significant difference was observed between AI suggestions and actual surgical techniques applied, with only a 34.16% match rate. Further analysis revealed any significant correlation between ChatGPT recommendations and the established surgical algorithm. Conclusion: Despite ChatGPT’s ability to process and analyze large datasets, its recommendations for MBS techniques do not align closely with those determined by expert surgical teams using a high success rate algorithm. Consequently, the study concludes that ChatGPT4 should not replace expert consultation in selecting MBS techniques. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",yes,"This study evaluates ChatGPT-4's ability to recommend surgical techniques for bariatric surgery based on patient data, comparing its recommendations to clinical guidelines and expert decisions. This is a direct clinical application of an LLM in healthcare decision-making.",10.1007/s11695-024-07279-1,,,,,
"Performance assessment of ChatGPT 4, ChatGPT 3.5, Gemini Advanced Pro 1.5 and Bard 2.0 to problem solving in pathology in French language","Digital teaching diversifies the ways of knowledge assessment, as natural language processing offers the possibility of answering questions posed by students and teachers. Objective: This study evaluated ChatGPT's, Bard's and Gemini's performances on second year of medical studies’ (DFGSM2) Pathology exams from the Health Sciences Center of Dijon (France) in 2018–2022. Methods: From 2018 to 2022, exam scores, discriminating powers and discordance rates were retrieved. Seventy questions (25 first-order single response questions and 45 second-order multiple response questions) were submitted on May 2023 to ChatGPT 3.5 and Bard 2.0, and on September 2024 to Gemini 1.5 and ChatGPT-4. Chatbot's and student's average scores were compared, as well as discriminating powers of questions answered by chatbots. The percentage of student–chatbot identical answers was retrieved, and linear regression analysis correlated the scores of chatbots with student's discordance rates. Chatbot's reliability was assessed by submitting the questions in four successive rounds and comparing score variability using a Fleiss’ Kappa and a Cohen's Kappa. Results: Newer chatbots outperformed both students and older chatbots as for the overall scores and multiple-response questions. All chatbots outperformed students on less discriminating questions. Oppositely, all chatbots were outperformed by students to questions with a high discriminating power. Chatbot's scores were correlated to student discordance rates. ChatGPT 4 and Gemini 1.5 provided variable answers, due to effects linked to prompt engineering. Conclusion: Our study in line with the literature confirms chatbot's moderate performance for questions requiring complex reasoning, with ChatGPT outperforming Google chatbots. The use of NLP software based on distributional semantics remains a challenge for the generation of questions in French. Drawbacks to the use of NLP software in generating questions include the generation of hallucinations and erroneous medical knowledge which have to be taken into count when using NLP software in medical education. © The Author(s) 2025.",yes,"This study evaluates the performance of several generative LLMs (ChatGPT-3.5, ChatGPT-4, Gemini, Bard) on medical pathology exam questions, comparing their results to those of medical students. The focus is on clinical knowledge assessment in a medical education context, specifically on the LLMs' ability to answer clinically relevant exam questions. This fits the inclusion criteria as it is a clinical evaluation of LLMs' performance in a healthcare/medical context.",10.1177/20552076241310630,,,,,
Evaluating Large Language Models in Echocardiography Reporting: Opportunities and Challenges,"Background The increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the quality and promptness of reports. While Large Language Models (LLMs) have proven effective in summarizing clinical texts, their application in echo remains underexplored. Aims To evaluate open-source LLMs in echo report summarization. Methods Adult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were categorized into two groups: development (all Mayo locations except Arizona) and Arizona validation sets. We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) using In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo report summarization from “Findings” to “Impressions.” Against cardiologist-generated Impressions, the models' performance was assessed both quantitatively with automatic metrics and qualitatively by cardiologists. Results The development dataset included 97,506 reports from 71,717 unique patients, predominantly male (55.4%), with an average age of 64.3±15.8 years. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other LLMs with win rates ranging from 87% to 99% in various automatic metrics, and produced reports comparable to cardiologists in qualitative review (significantly preferred in conciseness (p< 0.001), with no significant preference in completeness, correctness, and clinical utility). Correlations between automatic and human metrics were fair to modest, with the best being RadGraph F1 scores versus clinical utility (r=0.42) and automatic metrics showed insensitivity (0-5% drop) to changes in measurement numbers. Conclusions EchoGPT can generate draft reports for human review and approval, helping to streamline the workflow. However, scalable evaluation approaches dedicated to echo reports remains necessary.",yes,"This study evaluates several open-source LLMs (including Llama-2 and a fine-tuned variant called EchoGPT) for the clinical task of summarizing echocardiography reports, comparing their outputs to those of cardiologists. The focus is on a clinical application (report generation for diagnostic workflow), and the evaluation includes both automatic and expert human review. This fits the inclusion criteria as it assesses LLMs in a clinical context.",10.1101/2024.01.18.24301503,,,,,
Enhanced Artificial Intelligence Strategies in Renal Oncology: Iterative Optimization and Comparative Analysis of GPT 3.5 Versus 4.0,"Background: The rise of artificial intelligence (AI) in medicine has revealed the potential of ChatGPT as a pivotal tool in medical diagnosis and treatment. This study assesses the efficacy of ChatGPT versions 3.5 and 4.0 in addressing renal cell carcinoma (RCC) clinical inquiries. Notably, fine-tuning and iterative optimization of the model corrected ChatGPT’s limitations in this area. Methods: In our study, 80 RCC-related clinical questions from urology experts were posed three times to both ChatGPT 3.5 and ChatGPT 4.0, seeking binary (yes/no) responses. We then statistically analyzed the answers. Finally, we fine-tuned the GPT-3.5 Turbo model using these questions, and assessed its training outcomes. Results: We found that the average accuracy rates of answers provided by ChatGPT versions 3.5 and 4.0 were 67.08% and 77.50%, respectively. ChatGPT 4.0 outperformed ChatGPT 3.5, with a higher accuracy rate in responses (p < 0.05). By counting the number of correct responses to the 80 questions, we then found that although ChatGPT 4.0 performed better (p < 0.05), both versions were subject to instability in answering. Finally, by fine-tuning the GPT-3.5 Turbo model, we found that the correct rate of responses to these questions could be stabilized at 93.75%. Iterative optimization of the model can result in 100% response accuracy. Conclusion: We compared ChatGPT versions 3.5 and 4.0 in addressing clinical RCC questions, identifying their limitations. By applying the GPT-3.5 Turbo fine-tuned model iterative training method, we enhanced AI strategies in renal oncology. This approach is set to enhance ChatGPT’s database and clinical guidance capabilities, optimizing AI in this field. © Society of Surgical Oncology 2024.",yes,"This study directly evaluates the clinical performance of ChatGPT (GPT-3.5 and GPT-4.0) in answering renal cell carcinoma clinical questions, including accuracy and optimization through fine-tuning. The focus is on clinical application (diagnosis and treatment inquiries), not technical architecture or non-clinical tasks. Meets inclusion criteria.",10.1245/s10434-024-15107-0,,,,,
Appropriateness of Artificial Intelligence Chatbots in Diabetic Foot Ulcer Management,"Type 2 diabetes is a significant global health concern. It often causes diabetic foot ulcers (DFUs), which affect millions of people and increase amputation and mortality rates. Despite existing guidelines, the complexity of DFU treatment makes clinical decisions challenging. Large language models such as chat generative pretrained transformer (ChatGPT), which are adept at natural language processing, have emerged as valuable resources in the medical field. However, concerns about the accuracy and reliability of the information they provide remain. We aimed to assess the accuracy of various artificial intelligence (AI) chatbots, including ChatGPT, in providing information on DFUs based on established guidelines. Seven AI chatbots were asked clinical questions (CQs) based on the DFU guidelines. Their responses were analyzed for accuracy in terms of answers to CQs, grade of recommendation, level of evidence, and agreement with the reference, including verification of the authenticity of the references provided by the chatbots. The AI chatbots showed a mean accuracy of 91.2% in answers to CQs, with discrepancies noted in grade of recommendation and level of evidence. Claude-2 outperformed other chatbots in the number of verified references (99.6%), whereas ChatGPT had the lowest rate of reference authenticity (66.3%). This study highlights the potential of AI chatbots as tools for disseminating medical information and demonstrates their high degree of accuracy in answering CQs related to DFUs. However, the variability in the accuracy of these chatbots and problems like AI hallucinations necessitate cautious use and further optimization for medical applications. This study underscores the evolving role of AI in healthcare and the importance of refining these technologies for effective use in clinical decision-making and patient education. © The Author(s) 2024.",yes,"This study evaluates the accuracy of LLM-based chatbots (including ChatGPT and Claude-2) in answering clinical questions about diabetic foot ulcer management, based on established guidelines. The focus is on clinical information provision and decision support, which is a direct clinical application. The study does not focus on non-clinical tasks or technical architecture, and is not a review or opinion article.",10.1177/15347346241236811,,,,,
Exploring the Unknown: Evaluating ChatGPT's Performance in Uncovering Novel Aspects of Plastic Surgery and Identifying Areas for Future Innovation,"BACKGROUND: Artificial intelligence (AI) has emerged as a powerful tool in 
various medical fields, including plastic surgery. This study aims to evaluate 
the performance of ChatGPT, an AI language model, in elucidating historical 
aspects of plastic surgery and identifying potential avenues for innovation.
METHODS: A comprehensive analysis of ChatGPT's responses to a diverse range of 
plastic surgery-related inquiries was performed. The quality of the AI-generated 
responses was assessed based on their relevance, accuracy, and novelty. 
Additionally, the study examined the AI's ability to recognize gaps in existing 
knowledge and propose innovative solutions. ChatGPT's responses were analysed by 
specialist plastic surgeons with extensive research experience, and 
quantitatively analysed with a Likert scale.
RESULTS: ChatGPT demonstrated a high degree of proficiency in addressing a wide 
array of plastic surgery-related topics. The AI-generated responses were found 
to be relevant and accurate in most cases. However, it demonstrated convergent 
thinking and failed to generate genuinely novel ideas to revolutionize plastic 
surgery. Instead, it suggested currently popular trends that demonstrate great 
potential for further advancements. Some of the references presented were also 
erroneous as they cannot be validated against the existing literature.
CONCLUSION: Although ChatGPT requires major improvements, this study highlights 
its potential as an effective tool for uncovering novel aspects of plastic 
surgery and identifying areas for future innovation. By leveraging the 
capabilities of AI language models, plastic surgeons may drive advancements in 
the field. Further studies are needed to cautiously explore the integration of 
AI-driven insights into clinical practice and to evaluate their impact on 
patient outcomes.
LEVEL OF EVIDENCE V: This journal requires that authors assign a level of 
evidence to each article. For a full description of these Evidence-Based 
Medicine ratings, please refer to the Table of Contents or the online 
Instructions to Authors www.springer.com/00266.

© 2024. The Author(s).",yes,"This study evaluates ChatGPT's performance in answering plastic surgery-related questions, focusing on its ability to identify knowledge gaps and suggest innovations. While the primary focus is not direct clinical decision-making or patient care, it does assess ChatGPT's responses to clinical inquiries relevant to the field. The evaluation is not about technical architecture or non-clinical tasks, and it is not a review or editorial. Therefore, it fits the inclusion criteria as it assesses an LLM's performance in a medical context.",10.1007/s00266-024-03952-z,,,,,
Precision of artificial intelligence in paediatric cardiology multimodal image interpretation,"Multimodal imaging is crucial for diagnosis and treatment in paediatric cardiology. However, the proficiency of artificial intelligence chatbots, like ChatGPT-4, in interpreting these images has not been assessed. This cross-sectional study evaluates the precision of ChatGPT-4 in interpreting multimodal images for paediatric cardiology knowledge assessment, including echocardiograms, angiograms, X-rays, and electrocardiograms. One hundred multiple-choice questions with accompanying images from the textbook Pediatric Cardiology Board Review were randomly selected. The chatbot was prompted to answer these questions with and without the accompanying images. Statistical analysis was done using X 2, Fisher's exact, and McNemar tests. Results showed that ChatGPT-4 answered 41% of questions with images correctly, performing best on those with electrocardiograms (54%) and worst on those with angiograms (29%). Without the images, ChatGPT-4's performance was similar at 37% (difference = 4%, 95% confidence interval (CI) -9.4% to 17.2%, p = 0.56). The chatbot performed significantly better when provided the image of an electrocardiogram than without (difference = 18, 95% CI 4.0% to 31.9%, p < 0.04). In cases of incorrect answers, ChatGPT-4 was more inconsistent with an image than without (difference = 21%, 95% CI 3.5% to 36.9%, p < 0.02). In conclusion, ChatGPT-4 performed poorly in answering image-based multiple-choice questions in paediatric cardiology. Its accuracy in answering questions with images was similar to without, indicating limited multimodal image interpretation capabilities. Substantial training is required before clinical integration can be considered. Further research is needed to assess the clinical reasoning skills and progression of ChatGPT in paediatric cardiology for clinical and academic utility. © 2024 The Author(s).",yes,"This study evaluates ChatGPT-4's performance in interpreting multimodal images (e.g., echocardiograms, angiograms, X-rays, ECGs) in the context of pediatric cardiology board review questions. It directly assesses a generative LLM's clinical reasoning and diagnostic capabilities using clinically relevant material.",10.1017/S1047951124036035,,,,,
Can ChatGPT-4o really pass medical science exams? A pragmatic analysis using novel questions.,"ChatGPT apparently shows excellent performance on high level professional exams such as those involved in medical assessment and licensing. This has raised concerns that ChatGPT could be used for academic misconduct, especially in unproctored online exams. However, ChatGPT has also shown weaker performance on questions with pictures, and there have been concerns that ChatGPT’s performance may be artificially inflated by the public nature of the sample questions tested, meaning they likely formed part of the training materials for ChatGPT. This led to suggestions that cheating could be mitigated by using novel questions for every sitting of an exam and making extensive use of picture-based questions. These approaches remain untested. Here we tested the performance of ChatGPT-4o on existing medical licensing exams in the UK and USA, and on novel questions based on those exams. ChatGPT-4o scored 94% on the United Kingdom Medical Licensing Exam Applied Knowledge Test, and 89.9% on the United States Medical Licensing Exam Step 1. Performance was not diminished when the questions were rewritten into novel versions, or on completely novel questions which were not based on any existing questions. ChatGPT did show a slightly reduced performance on questions containing images, particularly when the answer options were added to an image as text labels. These data demonstrate that the performance of ChatGPT continues to improve and that online unproctored exams are an invalid form of assessment of the foundational knowledge needed for higher order learning.",yes,"This study evaluates the performance of ChatGPT-4o on medical licensing exams, including both existing and novel questions, and assesses its ability to answer clinically relevant exam content. This is a direct clinical evaluation of an LLM in a medical context.",10.1101/2024.06.29.24309595,,,,,
Evaluation of ChatGPT and Google Bard Using Prompt Engineering in Cancer Screening Algorithms,"Large language models (LLMs) such as ChatGPT and Bard have emerged as powerful tools in medicine, showcasing strong results in tasks such as radiology report translations and research paper drafting. While their implementation in clinical practice holds promise, their response accuracy remains variable. This study aimed to evaluate the accuracy of ChatGPT and Bard in clinical decision-making based on the American College of Radiology Appropriateness Criteria for various cancers. Both LLMs were evaluated in terms of their responses to open-ended (OE) and select-all-that-apply (SATA) prompts. Furthermore, the study incorporated prompt engineering (PE) techniques to enhance the accuracy of LLM outputs. The results revealed similar performances between ChatGPT and Bard on OE prompts, with ChatGPT exhibiting marginally higher accuracy in SATA scenarios. The introduction of PE also marginally improved LLM outputs in OE prompts but did not enhance SATA responses. The results highlight the potential of LLMs in aiding clinical decision-making processes, especially when guided by optimally engineered prompts. Future studies in diverse clinical situations are imperative to better understand the impact of LLMs in radiology. © 2024 The Association of University Radiologists",yes,"This study evaluates ChatGPT and Bard (both LLMs) in the context of clinical decision-making for cancer screening, specifically using established clinical guidelines (American College of Radiology Appropriateness Criteria). The focus is on the accuracy of LLMs in providing clinical recommendations, which is a direct clinical application. The use of prompt engineering to improve clinical output is also relevant.",10.1016/j.acra.2023.11.002,,,,,
Navigating ChatGPT's alignment with expert consensus on pediatric OSA management,"Objective: This study aimed to evaluate the potential integration of artificial intelligence (AI), specifically ChatGPT, into healthcare decision-making, focusing on its alignment with expert consensus statements regarding the management of persistent pediatric obstructive sleep apnea. Methods: We analyzed ChatGPT's responses to 52 statements from the 2024 expert consensus statement (ECS) on the management of pediatric persistent OSA after adenotonsillectomy. Each statement was input into ChatGPT using a 9-point Likert scale format, with each statement entered three times to calculate mean scores and standard deviations. Statistical analysis was performed using Excel. Results: ChatGPT's responses were within 1.0 of the consensus statement mean score for 63 % (33/52) of the statements. 13 % (7/52) were statements in which the ChatGPT mean response was different from the ECS mean by 2.0 or greater, the majority of which were in the categories of surgical and medical management. Statements with ChatGPT mean scores differing by more than 2.0 from the consensus mean highlighted the risk of disseminating incorrect information on established medical topics, with a notable variation in responses suggesting inconsistencies in ChatGPT's reliability. Conclusion: While ChatGPT demonstrated a promising ability to align with expert medical opinions in many cases, its inconsistencies and potential to propagate inaccuracies in contested areas raise important considerations for its application in clinical settings. The findings underscore the need for ongoing evaluation and refinement of AI tools in healthcare, emphasizing collaboration between AI developers, healthcare professionals, and regulatory bodies to ensure AI's safe and effective integration into medical decision-making processes. © 2024 Elsevier B.V.",yes,"This study evaluates ChatGPT's alignment with expert consensus in the clinical management of pediatric obstructive sleep apnea, directly assessing its potential use in healthcare decision-making. It tests ChatGPT's responses to clinical management statements, which is a clinical application.",10.1016/j.ijporl.2024.112131,,,,,
ChatGPT Provides Unsatisfactory Responses to Frequently Asked Questions Regarding Anterior Cruciate Ligament Reconstruction,"Purpose: To determine whether the free online artificial intelligence platform ChatGPT could accurately, adequately, and appropriately answer questions regarding anterior cruciate ligament (ACL) reconstruction surgery. Methods: A list of 10 questions about ACL surgery was created based on a review of frequently asked questions that appeared on websites of various orthopaedic institutions. Each question was separately entered into ChatGPT (version 3.5), and responses were recorded, scored, and graded independently by 3 authors. The reading level of the ChatGPT response was calculated using the WordCalc software package, and readability was assessed using the Flesch-Kincaid grade level, Simple Measure of Gobbledygook index, Coleman-Liau index, Gunning fog index, and automated readability index. Results: Of the 10 frequently asked questions entered into ChatGPT, 6 were deemed as unsatisfactory and requiring substantial clarification; 1, as adequate and requiring moderate clarification; 1, as adequate and requiring minor clarification; and 2, as satisfactory and requiring minimal clarification. The mean DISCERN score was 41 (inter-rater reliability, 0.721), indicating the responses to the questions were average. According to the readability assessments, a full understanding of the ChatGPT responses required 13.4 years of education, which corresponds to the reading level of a college sophomore. Conclusions: Most of the ChatGPT-generated responses were outdated and failed to provide an adequate foundation for patients’ understanding regarding their injury and treatment options. The reading level required to understand the responses was too advanced for some patients, leading to potential misunderstanding and misinterpretation of information. ChatGPT lacks the ability to differentiate and prioritize information that is presented to patients. Clinical Relevance: Recognizing the shortcomings in artificial intelligence platforms may equip surgeons to better set expectations and provide support for patients considering and preparing for ACL reconstruction. © 2024 Arthroscopy Association of North America",yes,"This study evaluates ChatGPT (an LLM) in its ability to answer patient questions about ACL reconstruction, assessing the clinical adequacy and readability of its responses. The focus is on clinical information provision to patients, which is a relevant clinical application.",10.1016/j.arthro.2024.01.017,,,,,
Performance of large language models on advocating the management of meningitis: a comparative qualitative stud,"Objectives We aimed to examine the adherence of large language models (LLMs) to bacterial meningitis guidelines using a hypothetical medical case, highlighting their utility and limitations in healthcare. Methods A simulated clinical scenario of a patient with bacterial meningitis secondary to mastoiditis was presented in three independent sessions to seven publicly accessible LLMs (Bard, Bing, Claude-2, GTP-3.5, GTP-4, Llama, PaLM). Responses were evaluated for adherence to good clinical practice and two international meningitis guidelines. Results A central nervous system infection was identified in 90% of LLM sessions. All recommended imaging, while 81% suggested lumbar puncture. Blood cultures and specific mastoiditis work-up were proposed in only 62% and 38% sessions, respectively. Only 38% of sessions provided the correct empirical antibiotic treatment, while antiviral treatment and dexamethasone were advised in 33% and 24%, respectively. Misleading statements were generated in 52%. No significant correlation was found between LLMs' text length and performance (r=0.29, p=0.20). Among all LLMs, GTP-4 demonstrated the best performance. Discussion Latest LLMs provide valuable advice on differential diagnosis and diagnostic procedures but significantly vary in treatment-specific information for bacterial meningitis when introduced to a realistic clinical scenario. Misleading statements were common, with performance differences attributed to each LLM's unique algorithm rather than output length. Conclusions Users must be aware of such limitations and performance variability when considering LLMs as a support tool for medical decision-making. Further research is needed to refine these models' comprehension of complex medical scenarios and their ability to provide reliable information.  © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.",yes,"This study evaluates multiple large language models (LLMs) including GPT-3.5, GPT-4, Bard, Claude-2, Llama, and PaLM in the context of a clinical scenario (bacterial meningitis management). It assesses their adherence to clinical guidelines and their utility and limitations in providing clinical decision support. This fits the inclusion criteria as it is a direct clinical evaluation of LLMs.",10.1136/bmjhci-2023-100978,,,,,
Accuracy of a ChatGPT in Diagnosing Urologic Conditions From Cross-sectional Imaging,"Objective: To evaluate ChatGPT's effectiveness in medical imaging interpretation within urology, addressing the critical need for safe AI application in healthcare by identifying its strengths and limitations as a diagnostic and educational resource. Material and Methods: Using publicly available cases from Radiopaedia.com, we entered 1-3 CT or MRI images into ChatGPT. A standard prompt instructed the model to provide a differential diagnosis ranked by probability. This task was repeated a second time with organ guidance (OG), which provided the organ of diagnostic interest to the model (eg, kidney). Primary outcomes included whether the model's top or differential diagnosis correctly identified the underlying pathology. Results: ChatGPT correctly identified the pathologic condition as its top diagnosis in 14% of CT (7/50) and 28% (14/50) of MRI cases (P = .08). OG increased the model's ability to recognize the top diagnosis by 18% (P = .03) when interpreting CT images, a benefit not shared when interpreting MRI images (P = .4). At baseline the differential diagnosis contained the final diagnosis for 30% and 56% of CT and MRI cases (P = .03). With the inclusion of OG, the model's differential diagnosis was able to correctly identify the underlying condition in 62% of both CT and MRI cases (CT: P = .001, MRI: P = .31). Conclusion: ChatGPT's effectiveness in medical imaging diagnostics is initially limited, yet it substantially benefits from the addition of user guidance. The study underscores AI's current shortcomings but also its considerable capacity to improve clinical operations when enriched with more data and expert direction. © 2025 The Authors",yes,"This study evaluates ChatGPT's ability to interpret cross-sectional medical images and provide differential diagnoses in a clinical urology context. It directly assesses the clinical diagnostic performance of a generative LLM, which fits the inclusion criteria.",10.1016/j.urology.2024.11.036,,,,,
Credibility of ChatGPT in the assessment of obesity in type 2 diabetes according to the guidelines,"Background: The Chat Generative Pre-trained Transformer (ChatGPT) allows students, researchers, and patients in the medical field to access information easily and has gained attention nowadays. We aimed to evaluate the credibility of ChatGPT according to the guidelines for the assessment of obesity in type 2 diabetes (T2D), which is one of the major concerns of this century. Materials and method: In this cross-sectional non-human subject study, experienced endocrinologists posed 20 questions to ChatGPT in subsections, which were assessments and different treatment options for obesity according to the American Diabetes Association and American Association of Clinical Endocrinology guidelines. The responses of ChatGPT were classified into four categories: compatible, compatible but insufficient, partially incompatible and incompatible with the guidelines. Results: ChatGPT demonstrated a systematic approach to answering questions and recommended consulting a healthcare provider to receive personalized advice based on the specific health needs and circumstances of patients. The compatibility of ChatGPT with the guidelines was 100% in the assessment of obesity in type 2 diabetes; however, it was lower in the therapy sections, which included nutritional, medical, and surgical approaches to weight loss. Furthermore, ChatGPT required additional prompts for responses that were evaluated as “compatible but insufficient” to provide all the information in the guidelines. Conclusion: The assessment and management of obesity in T2D are highly individualized. Despite ChatGPT’s comprehensive and understandable responses, it should not be used as a substitute for healthcare professionals’ patient-centered approach. © 2023, The Author(s), under exclusive licence to Springer Nature Limited.",yes,"This study evaluates ChatGPT's ability to answer clinical questions about obesity assessment and management in type 2 diabetes, comparing its responses to established clinical guidelines. The focus is on ChatGPT's clinical accuracy and credibility in a healthcare context.",10.1038/s41366-023-01410-5,,,,,
"Comparative Analysis of M4CXR, an LLM-Based Chest X-Ray Report Generation Model, and ChatGPT in Radiological Interpretation","Background/Objectives: This study investigated the diagnostic capabilities of two AI-based tools, M4CXR (research-only version) and ChatGPT-4o, in chest X-ray interpretation. M4CXR is a specialized cloud-based system using advanced large language models (LLMs) for generating comprehensive radiology reports, while ChatGPT, built on the GPT-4 architecture, offers potential in settings with limited radiological expertise. Methods: This study evaluated 826 anonymized chest X-ray images from Inha University Hospital. Two experienced radiologists independently assessed the performance of M4CXR and ChatGPT across multiple diagnostic parameters. The evaluation focused on diagnostic accuracy, false findings, location accuracy, count accuracy, and the presence of hallucinations. Interobserver agreement was quantified using Cohen’s kappa coefficient. Results: M4CXR consistently demonstrated superior performance compared to ChatGPT across all evaluation metrics. For diagnostic accuracy, M4CXR achieved approximately 60–62% acceptability ratings compared to ChatGPT’s 42–45%. Both systems showed high interobserver agreement rates, with M4CXR generally displaying stronger consistency. Notably, M4CXR showed better performance in anatomical localization (76–77.5% accuracy) compared to ChatGPT (36–36.5%) and demonstrated fewer instances of hallucination. Conclusions: The findings highlight the complementary potential of these AI technologies in medical diagnostics. While M4CXR shows stronger performance in specialized radiological analysis, the integration of both systems could potentially optimize diagnostic workflows. This study emphasizes the role of AI in augmenting human expertise rather than replacing it, suggesting that a combined approach leveraging both AI capabilities and clinical judgment could enhance patient care outcomes. © 2024 by the authors.",yes,"This study directly evaluates the clinical performance of two LLM-based systems (M4CXR and ChatGPT-4o) in interpreting chest X-rays and generating radiology reports, with assessment by radiologists. The focus is on diagnostic accuracy and clinical utility, which fits the inclusion criteria.",10.3390/jcm13237057,,,,,
Safety and quality of AI chatbots for drug-related inquiries: A real-world comparison with licensed pharmacists,"Introduction: Pharmacists play a pivotal role in ensuring patients are administered safe and effective medications; however, they encounter obstacles such as elevated workloads and a scarcity of qualified professionals. Despite the prospective utility of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), in addressing pharmaceutical inquiries, their applicability in real-world cases remains unexplored. Objective: To evaluate GPT-based chatbots’ accuracy in real-world drug-related inquiries, comparing their performance to licensed pharmacists. Methods: In this cross-sectional study, authors analyzed real-world drug inquiries from a Drug Information Inquiry Database. Two independent pharmacists evaluated the performance of GPT-based chatbots (GPT-3, GPT-3.5, GPT-4) against human pharmacists using accuracy, detail, and risk of harm criteria. Descriptive statistics described inquiry characteristics. Absolute proportion comparative analyses assessed accuracy, detail, and risk of harm. Stratified analyses were performed for different inquiry types. Results: Seventy inquiries were included. Most inquiries were received from physicians (41%) and pharmacists (44%). Inquiries type included dosage/administration (34.2%), drug interaction (12.8%) and pregnancy/lactation (15.7%). Majority of inquires included adults (83%) and female patients (54.3%). GPT-4 had 64.3% completely accurate responses, comparable to human pharmacists. GPT-4 and human pharmacists provided sufficiently detailed responses, with GPT-4 offering additional relevant details. Both GPT-4 and human pharmacists delivered 95% safe responses; however, GPT-4 provided proactive risk mitigation information in 70% of the instances, whereas similar information was included in 25.7% of human pharmacists’ responses. Conclusion: Our study showcased GPT-4's potential in addressing drug-related inquiries accurately and safely, comparable to human pharmacists. Current GPT-4-based chatbots could support healthcare professionals and foster global health improvements. © The Author(s) 2024.",yes,"This study directly evaluates GPT-based LLMs (GPT-3, GPT-3.5, GPT-4) in answering real-world drug-related clinical inquiries, comparing their performance to licensed pharmacists. The focus is on clinical accuracy, detail, and safety in a healthcare context, which fits the inclusion criteria.",10.1177/20552076241253523,,,,,
Online patient education in body contouring: A comparison between Google and ChatGPT,"Appropriate patient education and preparation prior to surgery represent a fundamental step in managing expectations, avoiding unnecessary encounters and eventually achieving optimal outcomes. Thus, the objective of this study is to evaluate ChatGPT's potential as a viable source for patient education by comparing its responses and provided references to frequently asked questions on body contouring, with Google's. A Google search was conducted on July 15th, 2023, using the search term “body contouring surgery”. The first 15 questions under the “People also ask” section and answers provided by Google were recorded. The 15 questions were then asked to ChatGPT-3.5. Four plastic surgeons evaluated the answers from 1 to 5 according to the Global Quality Scale. The mean score for responses given by Google was 2.55 ± 1.29, indicating poor quality but some information present, of very limited use to patients. The mean score for responses produced by ChatGPT was 4.38 ± 0.67, suggesting that the content was of good quality, useful to patients, and encompassed the most important topics. The difference was statistically significant (p = 0.001). Deficiencies in providing references represent one of the most evident weaknesses of ChatGPT. However, ChatGPT did not appear to spread misinformation, and the content of the generated responses was deemed of good quality and useful to patients. The integration of AI technology as a source for patient education has the potential to optimize patient queries on body contouring questions. © 2023 British Association of Plastic, Reconstructive and Aesthetic Surgeons",yes,"This study evaluates ChatGPT-3.5's ability to answer patient questions about body contouring surgery, comparing its responses to those found via Google. The focus is on the quality and usefulness of ChatGPT's clinical information for patient education, which is a direct clinical application relevant to healthcare.",10.1016/j.bjps.2023.10.091,,,,,
Artificial intelligence model GPT4 narrowly fails simulated radiological protection exam,"This study assesses the efficacy of Generative Pre-Trained Transformers (GPT) published by OpenAI in the specialised domains of radiological protection and health physics. Utilising a set of 1064 surrogate questions designed to mimic a health physics certification exam, we evaluated the models' ability to accurately respond to questions across five knowledge domains. Our results indicated that neither model met the 67% passing threshold, with GPT-3.5 achieving a 45.3% weighted average and GPT-4 attaining 61.7%. Despite GPT-4's significant parameter increase and multimodal capabilities, it demonstrated superior performance in all categories yet still fell short of a passing score. The study's methodology involved a simple, standardised prompting strategy without employing prompt engineering or in-context learning, which are known to potentially enhance performance. The analysis revealed that GPT-3.5 formatted answers more correctly, despite GPT-4's higher overall accuracy. The findings suggest that while GPT-3.5 and GPT-4 show promise in handling domain-specific content, their application in the field of radiological protection should be approached with caution, emphasising the need for human oversight and verification.",yes,"This study evaluates the performance of GPT-3.5 and GPT-4 on a simulated health physics (radiological protection) certification exam, which is a clinically relevant, domain-specific test. The focus is on the models' ability to answer questions in a specialized area of healthcare, aligning with the inclusion criteria for clinical evaluation of LLMs.",10.1088/1361-6498/ad1fdf,,,,,
The Role of Prompt Engineering for Multimodal LLM Glaucoma Diagnosis,"Background and Aim: This study evaluates the diagnostic performance of multimodal large language models (LLMs), GPT-4o and Claude Sonnet 3.5, in detecting glaucoma from fundus images. We specifically assess the impact of prompt engineering and the use of reference images on model performance. Methods: We utilized the ACRIMA public dataset, comprising 705 labeled fundus images, and designed four prompt types, ranging from simple instructions to more refined prompts with reference images. The two model were tested across 5640 API runs, with accuracy, sensitivity, specificity, PPV, and NPV assessed through nonparametric statistical tests. Results: Claude Sonnet 3.5 achieved a highest sensitivity of 94.92%, a specificity of 73.46%, and F1 score of 0.726. GPT-4o reached a highest sensitivity of 81.47%, a specificity of 50.49%, and F1 score of 0.645. The incorporation of prompt engineering and reference images improved GPT-4o's accuracy by 39.8% and Claude Sonnet 3.5's by 64.2%, significantly enhancing both models' performance. Conclusion: Multimodal LLMs demonstrated potential in diagnosing glaucoma, with Claude Sonnet 3.5 achieving a sensitivity of 94.92%, far exceeding the 22% sensitivity reported for primary care physicians in the literature. Prompt engineering, especially with reference images, significantly improved diagnostic performance. As LLMs become more integrated into medical practice, efficient prompt design may be key, and training doctors to use these tools effectively could enhance clinical outcomes.",yes,"This study evaluates the clinical diagnostic performance of frontier multimodal LLMs (GPT-4o and Claude Sonnet 3.5) in detecting glaucoma from fundus images, and assesses the impact of prompt engineering. The focus is on clinical diagnosis, using real patient data, and the models are directly compared to human performance. This fits the inclusion criteria for clinical evaluation of LLMs in medicine.",10.1101/2024.10.30.24316434,,,,,
Artificial Intelligence in Orthopaedics: Performance of ChatGPT on Text and Image Questions on a Complete AAOS Orthopaedic In-Training Examination (OITE),"OBJECTIVE: Artificial intelligence (AI) is capable of answering complex medical examination questions, offering the potential to revolutionize medical education and healthcare delivery. In this study we aimed to assess ChatGPT, a model that has demonstrated exceptional performance on standardized exams. Specifically, our focus was on evaluating ChatGPT's performance on the complete 2019 Orthopaedic In-Training Examination (OITE), including questions with an image component. Furthermore, we explored difference in performance when questions varied by text only or text with an associated image, including whether the image was described using AI or a trained orthopaedist. DESIGN And SETTING: Questions from the 2019 OITE were input into ChatGPT version 4.0 (GPT-4) using 3 response variants. As the capacity to input or interpret images is not publicly available in ChatGPT at the time of this study, questions with an image component were described and added to the OITE question using descriptions generated by Microsoft Azure AI Vision Studio or authors of the study. RESULTS: ChatGPT performed equally on OITE questions with or without imaging components, with an average correct answer choice of 49% and 48% across all 3 input methods. Performance dropped by 6% when using image descriptions generated by AI. When using single answer multiple-choice input methods, ChatGPT performed nearly double the rate of random guessing, answering 49% of questions correctly. The performance of ChatGPT was worse than all resident classes on the 2019 exam, scoring 4% lower than PGY-1 residents. DISCUSSION: ChatGT performed below all resident classes on the 2019 OITE. Performance on text only questions and questions with images was nearly equal if the image was described by a trained orthopaedic specialist but decreased when using an AI generated description. Recognizing the performance abilities of AI software may provide insight into the current and future applications of this technology into medical education. © 2024 Association of Program Directors in Surgery",yes,"This study evaluates ChatGPT (GPT-4) on a full orthopaedic in-training exam, including both text and image-based clinical questions (with images described for the model). The focus is on clinical knowledge and exam performance, which is a direct clinical application relevant to medical education.",10.1016/j.jsurg.2024.08.002,,,,,
"Comparison of the audiological knowledge of three chatbots – ChatGPT, Bing Chat, and Bard","Objective: The purpose of this study was to evaluate three chatbots – OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard – in terms of their responses to a defined set of audiological questions. Design: Each chatbot was presented with the same 10 questions. The authors rated the responses on a Likert scale ranging from 1 to 5. Additional features, such as the number of inaccuracies or errors and the provision of references, were also examined. Results: Most responses given by all three chatbots were rated as satisfactory or better. However all chatbots generated at least a few errors or inaccuracies. ChatGPT achieved the highest overall score, while Bard was the worst. Bard was also the only chatbot unable to provide a response to one of the questions. ChatGPT was the only chatbot that did not provide information about its sources. Conclusions: Chatbots are an intriguing tool that can be used to access basic information in a specialized area like audiology. Nevertheless, one needs to be careful, as correct information is not infrequently mixed in with errors that are hard to pick up unless the user is well versed in the field.",yes,"This study evaluates the clinical knowledge and accuracy of three LLM-based chatbots (ChatGPT, Bing Chat, Bard) in answering audiological questions. The focus is on their ability to provide correct information in a medical specialty, which aligns with the inclusion criteria for clinical evaluation of LLMs.",10.1101/2023.11.22.23298893,,,,,
Comparing the accuracy and referencing of ChatGPT’s responses to herbal medicine queries: A zero-shot versus roleplay prompting approach,"Objectives: This study aims to assess the accuracy of answers generated by ChatGPT-3.5 using zero-shot and roleplay prompts in response to questions about herbal medicine from clinical pharmacists. In addition, we aim to evaluate the quality of text responses when ChatGPT-3.5 is prompted to provide references for the given answers. Materials and Methods: ChatGPT-3.5, developed by OpenAI in San Francisco, is an advanced artificial intelligence chatbot that utilizes a large language model to generate text resembling human-like responses. In this study, a total of 90 questions were posed to ChatGPT-3.5, and the accuracy of its responses was compared using both zero-shot and roleplay prompts. These questions were evenly distributed among four categories related to herbal medicine clinical practice. To evaluate the correctness of the responses, they were cross-referenced with peer-reviewed and trusted references. The authenticity of the references provided by ChatGPT-3.5 was also assessed. Results: Using the roleplay prompt with ChatGPT-3.5 led to an 86.67% correct answer rate for herbal medicine questions, compared to 77.78% with zero-shot prompts. When references were requested, 77.48% (zero-shot) and 75.69% (roleplay) were identified as “true” references, but ChatGPT-3.5 itself generated many references. Only 38.37% (zero-shot) and 50.46% (Roleplay) of these “true’ references, provided a sufficient background for accurate answers. Conclusion: In this pilot study, ChatGPT-3.5 with the roleplay prompt shows promise as an alternative tool for addressing herbal medicine questions. However, improvements are needed to enhance reference accuracy and the availability of comprehensive background information.",yes,"This study evaluates ChatGPT-3.5's ability to answer clinical questions about herbal medicine posed by clinical pharmacists, comparing different prompting strategies and assessing the accuracy and referencing of its responses. The focus is on the LLM's clinical performance in providing information relevant to healthcare practice.",10.56808/3027-7922.2838,,,,,
ChatGPT-4 Assistance in Optimizing Emergency Department Radiology Referrals and Imaging Selection,"Purpose: The quality of radiology referrals influences patient management and imaging interpretation by radiologists. The aim of this study was to evaluate ChatGPT-4 as a decision support tool for selecting imaging examinations and generating radiology referrals in the emergency department (ED). Methods: Five consecutive clinical notes from the ED were retrospectively extracted, for each of the following pathologies: pulmonary embolism, obstructing kidney stones, acute appendicitis, diverticulitis, small bowel obstruction, acute cholecystitis, acute hip fracture, and testicular torsion. A total of 40 cases were included. These notes were entered into ChatGPT-4, requesting recommendations on the most appropriate imaging examinations and protocols. The chatbot was also asked to generate radiology referrals. Two independent radiologists graded the referral on a scale ranging from 1 to 5 for clarity, clinical relevance, and differential diagnosis. The chatbot's imaging recommendations were compared with the ACR Appropriateness Criteria (AC) and with the examinations performed in the ED. Agreement between readers was assessed using linear weighted Cohen's κ coefficient. Results: ChatGPT-4's imaging recommendations aligned with the ACR AC and ED examinations in all cases. Protocol discrepancies between ChatGPT and the ACR AC were observed in two cases (5%). ChatGPT-4-generated referrals received mean scores of 4.6 and 4.8 for clarity, 4.5 and 4.4 for clinical relevance, and 4.9 from both reviewers for differential diagnosis. Agreement between readers was moderate for clinical relevance and clarity and substantial for differential diagnosis grading. Conclusions: ChatGPT-4 has shown potential in aiding imaging study selection for select clinical cases. As a complementary tool, large language models may improve radiology referral quality. Radiologists should stay informed about this technology and be mindful of potential challenges and risks. © 2023 American College of Radiology",yes,"This study evaluates ChatGPT-4 as a decision support tool for selecting imaging examinations and generating radiology referrals in the emergency department, comparing its recommendations to established clinical criteria and expert review. It directly assesses the clinical application of a generative LLM in a healthcare setting.",10.1016/j.jacr.2023.06.009,,,,,
Performance of ChatGPT on the National Korean Occupational Therapy Licensing Examination,"Background: ChatGPT is an artificial intelligence-based large language model (LLM). ChatGPT has been widely applied in medicine, but its application in occupational therapy has been lacking. Objective: This study examined the accuracy of ChatGPT on the National Korean Occupational Therapy Licensing Examination (NKOTLE) and investigated its potential for application in the field of occupational therapy. Methods: ChatGPT 3.5 was used during the five years of the NKOTLE with Korean prompts. Multiple choice questions were entered manually by three dependent encoders, and scored according to the number of correct answers. Results: During the most recent five years, ChatGPT did not achieve a passing score of 60% accuracy and exhibited interrater agreement of 0.6 or higher. Conclusion: ChatGPT could not pass the NKOTLE but demonstrated a high level of agreement between raters. Even though the potential of ChatGPT to pass the NKOTLE is currently inadequate, it performed very close to the passing level even with only Korean prompts. © The Author(s) 2024.",yes,"This study evaluates the performance of ChatGPT-3.5 on a national clinical licensing exam (NKOTLE) relevant to occupational therapy, which is a clinical field. The study directly assesses the LLM's ability to answer clinically relevant exam questions, fitting the inclusion criteria.",10.1177/20552076241236635,,,,,
Retrieval-Augmented Generation for Extracting CHA(2DS2-VASc) Risk Factors from Unstructured Clinical Notes in Patients with Atrial Fibrillation,"Background: Assessment of stroke risk in patients with atrial fibrillation (AF) is crucial for guiding anticoagulation therapy. CHA(2DS2-VASc) is a widely used score for defining this risk, but current assessments rely on manual calculation by clinicians or approximations from structured EHR data elements. Unstructured clinical notes contain rich information that could enhance risk assessment. We developed and validated a Retrieval-Augmented Generation (RAG) approach to extract CHA(2DS2-VASc) risk factors from unstructured notes in patients with AF. Methods: We employed a RAG architecture paired with the large language model, Llama3.1, to extract features relevant to CHA(2DS2-VASc) scores from unstructured notes. The model was deployed on a random set of 1,000 clinical notes (934 AF patients) from Yale New Haven Health System (YNHHS). To establish a gold standard, 2 clinicians manually reviewed and labeled CHA(2DS2-VASc) risk factors in a random subset of 200 notes. The CHA(2DS2-VASc) scores were calculated for each patient using structured data alone and by incorporating risk factors identified with RAG. We assessed performance across risk factors using macro-averaged area under the receiver operating characteristic (AUROC). For external validation, we utilized 100 manually labeled clinical notes from the MIMIC-IV database. Results: The RAG model demonstrated robust performance in extracting risk factors from clinical notes. In the 1000 clinical notes, RAG identified several risk factors more frequently than structured elements, including hypertension (82.4% vs 26.2%), stroke/TIA (62.9% vs 45.5%), vascular disease (83.4% vs 56.6%), and diabetes (84.1% vs 47.2%). In the 200 expert-annotated notes, the RAG approach achieved high performance for various risk factors, with AUROCs ranging from 0.96 to 0.98 for hypertension, diabetes, and age ≥75 years. Incorporating risk factors identified by RAG increased CHA(2DS2-VASc) scores compared with using structured data alone. Conclusion: An LLM-optimized RAG can accurately extract CHA(2DS2-VASc) risk factors from unstructured clinical notes in AF patients. This approach can enable computable risk assessment and guide appropriate anticoagulation therapy.",yes,"This study uses a large language model (Llama3.1) in a Retrieval-Augmented Generation (RAG) framework to extract clinically relevant risk factors from unstructured clinical notes, directly supporting clinical risk assessment (CHA2DS2-VASc) in atrial fibrillation patients. The extracted information is used to guide anticoagulation therapy, which is a clinical application. Therefore, it meets the inclusion criteria.",10.1101/2024.09.19.24313992,,,,,
A multidisciplinary assessment of ChatGPT’s knowledge of amyloidosis,"Amyloidosis is a rare, multisystem disease with several subtypes including AA (secondary), AL (amyloid light chain), and ATTR (transthyretin amyloidosis). In addition to variable symptoms and multidisciplinary management, amyloidosis being a rare disease further contributes to patients being at risk for decreased health literacy regarding their condition. Increased access to education materials containing simple, plain language may bridge literacy gaps and improve outcomes for patients with rare diseases such as amyloidosis. The large language model (LLM), Chat Generative Pre-Trained Transformer (ChatGPT), may be a powerful tool for improving the availability of accurate and easy to understand education materials. Amyloidosis-related questions from cardiology, gastroenterology, and neurology were sourced from esteemed medical societies and institutions along with amyloidosis Facebook support groups and inputted into ChatGPT-3.5 and GPT-4. Answers were graded on 4-point scale with both models responding to the majority of questions with either “comprehensive” or “correct but inadequate” answers with only 1 (1.2%) answer by GPT-3.5 graded as “completely inaccurate”. When assessing reproducibility, GPT-3.5 scored reliably on more than 83.3% of responses, while GPT-4 produced above 98.2% consistent answers. Our findings show that ChatGPT can potentially serve as a supplemental tool in disseminating vital health education to patients living with amyloidosis.",yes,"This study evaluates ChatGPT (GPT-3.5 and GPT-4) by having it answer amyloidosis-related questions from multiple clinical specialties, with responses graded for accuracy and reproducibility. The focus is on the LLM's ability to provide patient education in a clinical context, which fits the inclusion criteria.",10.1101/2023.07.17.23292780,,,,,
Quality of ChatGPT Responses to Questions Related to Pancreatic Cancer and its Surgical Care,"2795. Ann Surg Oncol. 2023 Oct;30(11):6284-6286. doi: 10.1245/s10434-023-13777-w. Epub 
2023 Jun 22.

Quality of ChatGPT Responses to Questions Related to Pancreatic Cancer and its 
Surgical Care.

Moazzam Z(1), Cloyd J(1), Lima HA(1), Pawlik TM(2).

Author information:
(1)Department of Surgery, The Ohio State University Wexner Medical Center and 
James Comprehensive Cancer Center, Columbus, OH, USA.
(2)Department of Surgery, The Ohio State University Wexner Medical Center and 
James Comprehensive Cancer Center, Columbus, OH, USA. tim.pawlik@osumc.edu.",yes,"This study evaluates the quality of ChatGPT's responses to questions about pancreatic cancer and its surgical care, which is a clinical application relevant to patient education and decision support.",10.1245/s10434-023-13777-w,,,,,
Effectiveness of Generative Artificial Intelligence-Driven Responses to Patient Concerns in Long-Term Opioid Therapy: Cross-Model Assessment,"Background: While long-term opioid therapy is a widely utilized strategy for managing chronic pain, many patients have understandable questions and concerns regarding its safety, efficacy, and potential for dependency and addiction. Providing clear, accurate, and reliable information is essential for fostering patient understanding and acceptance. Generative artificial intelligence (AI) applications offer interesting avenues for delivering patient education in healthcare. This study evaluates the reliability, accuracy, and comprehensibility of ChatGPT’s responses to common patient inquiries about opioid long-term therapy. Methods: An expert panel selected thirteen frequently asked questions regarding long-term opioid therapy based on the authors’ clinical experience in managing chronic pain patients and a targeted review of patient education materials. Questions were prioritized based on prevalence in patient consultations, relevance to treatment decision-making, and the complexity of information typically required to address them comprehensively. We assessed comprehensibility by implementing the multimodal generative AI Copilot (Microsoft 365 Copilot Chat). Spanning three domains—pre-therapy, during therapy, and post-therapy—each question was submitted to GPT-4.0 with the prompt “If you were a physician, how would you answer a patient asking…”. Ten pain physicians and two non-healthcare professionals independently assessed the responses using a Likert scale to rate reliability (1–6 points), accuracy (1–3 points), and comprehensibility (1–3 points). Results: Overall, ChatGPT’s responses demonstrated high reliability (5.2 ± 0.6) and good comprehensibility (2.8 ± 0.2), with most answers meeting or exceeding predefined thresholds. Accuracy was moderate (2.7 ± 0.3), with lower performance on more technical topics like opioid tolerance and dependency management. Conclusions: While AI applications exhibit significant potential as a supplementary tool for patient education on opioid long-term therapy, limitations in addressing highly technical or context-specific queries underscore the need for ongoing refinement and domain-specific training. Integrating AI systems into clinical practice should involve collaboration between healthcare professionals and AI developers to ensure safe, personalized, and up-to-date patient education in chronic pain management. © 2025 by the authors.",yes,"This study evaluates ChatGPT (GPT-4.0) and Copilot's ability to answer patient questions about long-term opioid therapy, focusing on reliability, accuracy, and comprehensibility. The LLMs are assessed in a clinical context (patient education about a specific therapy), and the evaluation is performed by clinicians. This fits the inclusion criteria as it is a clinical evaluation of a generative LLM in healthcare.",10.3390/biomedicines13030636,,,,,
Large language model doctor: assessing the ability of ChatGPT-4 to deliver interventional radiology procedural information to patients during the consent process,"Purpose: The study aims to evaluate how current interventional radiologists view ChatGPT in the context of informed consent for interventional radiology (IR) procedures. Methods: ChatGPT-4 was instructed to outline the risks, benefits, and alternatives for IR procedures. The outputs were reviewed by IR physicians to assess if outputs were 1) accurate, 2) comprehensive, 3) easy to understand, 4) written in a conversational tone, and 5) if they were comfortable providing the output to the patient. For each criterion, outputs were measured on a 5-point scale. Mean scores and percentage of physicians rating output as sufficient (4 or 5 on 5-point scale) were measured. A linear regression correlated mean rating with number of years in practice. Intraclass correlation coefficient (ICC) measured agreement among physicians. Results: The mean rating of the ChatGPT responses was 4.29, 3.85, 4.15, 4.24, 3.82 for accuracy, comprehensiveness, readability, conversational tone, and physician comfort level, respectively. Percentage of physicians rating outputs as sufficient was 84%, 71%, 85%, 85%, and 67% for accuracy, comprehensiveness, readability, conversational tone, and physician comfort level, respectively. There was an inverse relationship between years in training and output score (coeff = -0.03413, p = 0.0128); ICC measured 0.39 (p = 0.003). Conclusions: GPT-4 produced outputs that were accurate, understandable, and in a conversational tone. However, GPT-4 had a decreased capacity to produce a comprehensive output leading some physicians to be uncomfortable providing the output to patients. Practicing IRs should be aware of these limitations when counseling patients as ChatGPT-4 continues to develop into a clinically usable AI tool. © The Author(s) 2024.",yes,"This study evaluates ChatGPT-4's ability to provide clinical procedural information to patients during the informed consent process for interventional radiology. It directly assesses the LLM's clinical communication and information delivery, with physician review of its outputs for accuracy, comprehensiveness, and suitability for patient counseling. This fits the inclusion criteria as it is a clinical evaluation of an LLM in a healthcare context.",10.1186/s42155-024-00477-z,,,,,
DIAGNOSIS IN BYTES: COMPARING THE DIAGNOSTIC ACCURACY OF GOOGLE AND CHATGPT 3.5 AS DIAGNOSTIC SUPPORT TOOLS,"Objective: Adopting digital technologies as diagnostic support tools in medicine is unquestionable. However, the accuracy in suggesting diagnoses remains controversial and underexplored. We aimed to evaluate and compare the diagnostic accuracy of two primary and accessible internet search tools: Google and ChatGPT 3.5. Method: We used 60 clinical cases related to urological pathologies to evaluate both platforms. These cases were divided into two groups: one with common conditions (constructed from the most frequent symptoms, following EAU and UpToDate guidelines) and another with rare disorders - based on case reports published between 2022 and 2023 in Urology Case Reports. Each case was inputted into Google Search and ChatGPT 3.5, and the results were categorized as ""correct diagnosis,"" ""likely differential diagnosis,"" or ""incorrect diagnosis."" A team of researchers evaluated the responses blindly and randomly. Results: In typical cases, Google achieved 53.3% accuracy, offering a likely differential diagnosis in 23.3% and errors in the rest. ChatGPT 3.5 exhibited superior performance, with 86.6% accuracy, and suggested a reasonable differential diagnosis in 13.3%, without mistakes. In rare cases, Google did not provide correct diagnoses but offered a likely differential diagnosis in 20%. ChatGPT 3.5 achieved 16.6% accuracy, with 50% differential diagnoses. Conclusion: ChatGPT 3.5 demonstrated higher diagnostic accuracy than Google in both contexts. The platform showed acceptable accuracy in common cases; however, limitations in rare cases remained evident.",yes,"This study directly evaluates the diagnostic accuracy of ChatGPT 3.5 in clinical scenarios (urological cases), comparing it to Google. It uses clinical vignettes and assesses the LLM's ability to provide correct or differential diagnoses, which is a clinical application.",10.1101/2023.11.10.23294668,,,,,
Using Natural Language Processing (GPT-4) for Computed Tomography Image Analysis of Cerebral Hemorrhages in Radiology: Retrospective Analysis,"BACKGROUND: Cerebral hemorrhage is a critical medical condition that 
necessitates a rapid and precise diagnosis for timely medical intervention, 
including emergency operation. Computed tomography (CT) is essential for 
identifying cerebral hemorrhage, but its effectiveness is limited by the 
availability of experienced radiologists, especially in resource-constrained 
regions or when shorthanded during holidays or at night. Despite advancements in 
artificial intelligence-driven diagnostic tools, most require technical 
expertise. This poses a challenge for widespread adoption in radiological 
imaging. The introduction of advanced natural language processing (NLP) models 
such as GPT-4, which can annotate and analyze images without extensive 
algorithmic training, offers a potential solution.
OBJECTIVE: This study investigates GPT-4's capability to identify and annotate 
cerebral hemorrhages in cranial CT scans. It represents a novel application of 
NLP models in radiological imaging.
METHODS: In this retrospective analysis, we collected 208 CT scans with 6 types 
of cerebral hemorrhages at Ren Ji Hospital, Shanghai Jiao Tong University School 
of Medicine, between January and September 2023. All CT images were mixed 
together and sequentially numbered, so each CT image had its own corresponding 
number. A random sequence from 1 to 208 was generated, and all CT images were 
inputted into GPT-4 for analysis in the order of the random sequence. The 
outputs were subsequently examined using Photoshop and evaluated by experienced 
radiologists on a 4-point scale to assess identification completeness, accuracy, 
and success.
RESULTS: The overall identification completeness percentage for the 6 types of 
cerebral hemorrhages was 72.6% (SD 18.6%). Specifically, GPT-4 achieved higher 
identification completeness in epidural and intraparenchymal hemorrhages (89.0%, 
SD 19.1% and 86.9%, SD 17.7%, respectively), yet its identification completeness 
percentage in chronic subdural hemorrhages was very low (37.3%, SD 37.5%). The 
misidentification percentages for complex hemorrhages (54.0%, SD 28.0%), 
epidural hemorrhages (50.2%, SD 22.7%), and subarachnoid hemorrhages (50.5%, SD 
29.2%) were relatively high, whereas they were relatively low for acute subdural 
hemorrhages (32.6%, SD 26.3%), chronic subdural hemorrhages (40.3%, SD 27.2%), 
and intraparenchymal hemorrhages (26.2%, SD 23.8%). The identification 
completeness percentages in both massive and minor bleeding showed no 
significant difference (P=.06). However, the misidentification percentage in 
recognizing massive bleeding was significantly lower than that for minor 
bleeding (P=.04). The identification completeness percentages and 
misidentification percentages for cerebral hemorrhages at different locations 
showed no significant differences (all P>.05). Lastly, radiologists showed 
relative acceptance regarding identification completeness (3.60, SD 0.54), 
accuracy (3.30, SD 0.65), and success (3.38, SD 0.64).
CONCLUSIONS: GPT-4, a standout among NLP models, exhibits both promising 
capabilities and certain limitations in the realm of radiological imaging, 
particularly when it comes to identifying cerebral hemorrhages in CT scans. This 
opens up new directions and insights for the future development of NLP models in 
radiology.
TRIAL REGISTRATION: ClinicalTrials.gov NCT06230419; 
https://clinicaltrials.gov/study/NCT06230419.

©Daiwen Zhang, Zixuan Ma, Ru Gong, Liangliang Lian, Yanzhuo Li, Zhenghui He, 
Yuhan Han, Jiyuan Hui, Jialin Huang, Jiyao Jiang, Weiji Weng, Junfeng Feng. 
Originally published in the Journal of Medical Internet Research 
(https://www.jmir.org), 26.09.2024.",yes,"This study evaluates GPT-4, a generative large language model, for its ability to identify and annotate cerebral hemorrhages in CT scans—a direct clinical application in radiology. The model's performance is assessed by radiologists, focusing on clinical diagnostic tasks. This fits the inclusion criteria as it tests an LLM in a healthcare context for clinical decision support.",10.2196/58741,,,,,
In the face of confounders: Atrial fibrillation detection – Practitioners vs. ChatGPT,"Introduction: Atrial fibrillation (AF) is the most common arrhythmia in clinical practice, yet interpretation concerns among healthcare providers persist. Confounding factors contribute to false-positive and false-negative AF diagnoses, leading to potential omissions. Artificial intelligence advancements show promise in electrocardiogram (ECG) interpretation. We sought to examine the diagnostic accuracy of ChatGPT-4omni (GPT-4o), equipped with image evaluation capabilities, in interpreting ECGs with confounding factors and compare its performance to that of physicians. Methods: Twenty ECG cases, divided into Group A (10 cases of AF or atrial flutter) and Group B (10 cases of sinus or another atrial rhythm), were crafted into multiple-choice questions. Total of 100 practitioners (25 from each: emergency medicine, internal medicine, primary care, and cardiology) were tasked to identify the underlying rhythm. Next, GPT-4o was prompted in five separate sessions. Results: GPT-4o performed inadequately, averaging 3 (±2) in Group A questions and 5.40 (±1.34) in Group B questions. Upon examining the accuracy of the total ECG questions, no significant difference was found between GPT-4o, internists, and primary care physicians (p = 0.952 and = 0.852, respectively). Cardiologists outperformed other medical disciplines and GPT-4o (p < 0.001), while emergency physicians followed in accuracy, though comparison to GPT-4o only indicated a trend (p = 0.068). Conclusion: GPT-4o demonstrated suboptimal accuracy with significant under- and over-recognition of AF in ECGs with confounding factors. Despite its potential as a supportive tool for ECG interpretation, its performance did not surpass that of medical practitioners, underscoring the continued importance of human expertise in complex diagnostics. © 2024",yes,This study directly evaluates the diagnostic accuracy of a generative LLM (ChatGPT-4o) in interpreting ECGs with confounding factors and compares its performance to that of physicians. The clinical context (arrhythmia diagnosis) and use of a frontier LLM model meet the inclusion criteria.,10.1016/j.jelectrocard.2024.153851,,,,,
Online Patient Education in Obstructive Sleep Apnea: ChatGPT versus Google Search,"The widespread implementation of artificial intelligence technologies provides an appealing alternative to traditional search engines for online patient healthcare education. This study assessed ChatGPT-3.5’s capabilities as a source of obstructive sleep apnea (OSA) information, using Google Search as a comparison. Ten frequently searched questions related to OSA were entered into Google Search and ChatGPT-3.5. The responses were assessed by two independent researchers using the Global Quality Score (GQS), Patient Education Materials Assessment Tool (PEMAT), DISCERN instrument, CLEAR tool, and readability scores (Flesch Reading Ease and Flesch–Kincaid Grade Level). ChatGPT-3.5 significantly outperformed Google Search in terms of GQS (5.00 vs. 2.50, p < 0.0001), DISCERN reliability (35.00 vs. 29.50, p = 0.001), and quality (11.50 vs. 7.00, p = 0.02). The CLEAR tool scores indicated that ChatGPT-3.5 provided excellent content (25.00 vs. 15.50, p < 0.001). PEMAT scores showed higher understandability (60–91% vs. 44–80%) and actionability for ChatGPT-3.5 (0–40% vs. 0%). Readability analysis revealed that Google Search responses were easier to read (FRE: 56.05 vs. 22.00; FKGL: 9.00 vs. 14.00, p < 0.0001). ChatGPT-3.5 delivers higher quality and more comprehensive OSA information compared to Google Search, although its responses are less readable. This suggests that while ChatGPT-3.5 can be a valuable tool for patient education, efforts to improve readability are necessary to ensure accessibility and utility for all patients. Healthcare providers should be aware of the strengths and weaknesses of various healthcare information resources and emphasize the importance of critically evaluating online health information, advising patients on its reliability and relevance. © 2024 by the authors.",yes,"This study evaluates ChatGPT-3.5's ability to provide patient education on obstructive sleep apnea, comparing its responses to those from Google Search. The assessment uses established quality and readability metrics, focusing on the clinical relevance of information provided to patients. Since it directly tests an LLM's performance in a healthcare context (patient education), it meets the inclusion criteria.",10.3390/healthcare12171781,,,,,
Large Language Models Fail to Reproduce Level I Recommendations for Breast Radiotherapy,"This study evaluates the reliability of the largest public-facing large language models in providing accurate breast cancer radiotherapy recommendations. We assessed ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, Claude 3.5 Sonnet, and ChatGPT o1 in three common clinical scenarios. The clinical cases are as follows: post-lumpectomy radiotherapy in a 40 year old woman, (2) postmastectomy radiation in a 40 year old woman with 4+ lymph nodes, and (3) postmastectomy radiation in an 80 year old woman with early stage tumor and negative axillary dissection. Each case was designed to be unambiguous with respect to the Level I evidence and clinical guideline-supported approach. The evidence-supported radiation treatments are as follows: (1) Whole breast with boost (2) Regional nodal irradiation (3) Omission of post-operative radiotherapy. Each prompt is presented to each LLM multiple times to ensure reproducibility. Results indicate that the free, public-facing models often fail to provide accurate treatment recommendations, particularly when omission of radiotherapy was the correct course of action. Many recommendations suggested by the LLMs increase morbidity and mortality in patients. Models only accessible through paid subscription (ChatGPT o1 and o1-mini) demonstrated greatly improved accuracy. Some prompt-engineering techniques, rewording and chain-of-reasoning, enhanced the accuracy of the LLMs, while true/false questioning significantly worsened results. While public-facing LLMs show potential for medical applications, their current reliability is unsuitable for clinical decision-making.",yes,"This study directly evaluates multiple frontier LLMs (ChatGPT 3.5, 4, 4o, Claude 3.5 Sonnet, etc.) in providing clinical recommendations for breast cancer radiotherapy in specific patient scenarios. The focus is on the accuracy and reliability of LLMs in making evidence-based clinical decisions, which fits the inclusion criteria.",10.1101/2024.11.11.24317123,,,,,
Evaluating a generative artificial intelligence accuracy in providing medication instructions from smartphone images,"Background: The Food and Drug Administration mandates patient labeling materials like the Medication Guide (MG) and Instructions for Use (IFU) to support appropriate medication use. However, challenges such as low health literacy and difficulties navigating these materials may lead to incorrect medication usage, resulting in therapy failure or adverse outcomes. The rise of generative AI, presents an opportunity to provide scalable, personalized patient education through image recognition and text generation. Objective: This study aimed to evaluate the accuracy and safety of medication instructions generated by ChatGPT based on user-provided drug images, compared to the manufacturer's standard instructions. Methods: Images of 12 medications requiring multiple steps for administration were uploaded to ChatGPT's image recognition function. ChatGPT's responses were compared to the official IFU and MG using text classifiers, Count Vectorization (CountVec), and Term Frequency-Inverse Document Frequency (TF-IDF). The clinical accuracy was further evaluated by independent pharmacists to determine if ChatGPT responses were valid for patient instruction. Results: ChatGPT correctly identified all medications and generated patient instructions. CountVec outperformed TF-IDF in text similarity analysis, with an average similarity score of 76%. However, clinical evaluation revealed significant gaps in the instructions, particularly for complex administration routes, where ChatGPT's guidance lacked essential details, leading to lower clinical accuracy scores. Conclusion: While ChatGPT shows promise in generating patient-friendly medication instructions, its effectiveness varies based on the complexity of the medication. The findings underscore the need for further refinement and clinical oversight to ensure the safety and accuracy of AI-generated medical guidance, particularly for medications with complex administration processes. © 2024 American Pharmacists Association®",yes,"This study evaluates ChatGPT's ability to generate clinically relevant medication instructions from user-provided drug images, comparing its output to official instructions and assessing clinical accuracy with pharmacist review. The focus is on clinical application (patient education and medication safety), and the LLM is used in a healthcare context.",10.1016/j.japh.2024.102284,,,,,
Exploring the potential of large language models in identifying metabolic dysfunction-associated steatotic liver disease: A comparative study of non-invasive tests and artificial intelligence-generated responses,"Background and Aims: This study sought to assess the capabilities of large language models (LLMs) in identifying clinically significant metabolic dysfunction-associated steatotic liver disease (MASLD). Methods: We included individuals from NHANES 2017–2018. The validity and reliability of MASLD diagnosis by GPT-3.5 and GPT-4 were quantitatively examined and compared with those of the Fatty Liver Index (FLI) and United States FLI (USFLI). A receiver operating characteristic curve was conducted to assess the accuracy of MASLD diagnosis via different scoring systems. Additionally, GPT-4V's potential in clinical diagnosis using ultrasound images from MASLD patients was evaluated to provide assessments of LLM capabilities in both textual and visual data interpretation. Results: GPT-4 demonstrated comparable performance in MASLD diagnosis to FLI and USFLI with the AUROC values of.831 (95% CI.796–.867),.817 (95% CI.797–.837) and.827 (95% CI.807–.848), respectively. GPT-4 exhibited a trend of enhanced accuracy, clinical relevance and efficiency compared to GPT-3.5 based on clinician evaluation. Additionally, Pearson's r values between GPT-4 and FLI, as well as USFLI, were.718 and.695, respectively, indicating robust and moderate correlations. Moreover, GPT-4V showed potential in understanding characteristics from hepatic ultrasound imaging but exhibited limited interpretive accuracy in diagnosing MASLD compared to skilled radiologists. Conclusions: GPT-4 achieved performance comparable to traditional risk scores in diagnosing MASLD and exhibited improved convenience, versatility and the capacity to offer user-friendly outputs. The integration of GPT-4V highlights the capacities of LLMs in handling both textual and visual medical data, reinforcing their expansive utility in healthcare practice. © 2024 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd.",yes,"This study directly evaluates the clinical performance of large language models (GPT-3.5, GPT-4, GPT-4V) in diagnosing metabolic dysfunction-associated steatotic liver disease (MASLD), comparing their accuracy to established non-invasive tests and assessing their ability to interpret both textual and visual (ultrasound) data. The focus is on clinical diagnosis and decision support, which fits the inclusion criteria.",10.1111/liv.16112,,,,,
"What’s Going On With Me and How Can I Better Manage My Health? The Potential of GPT-4 to Transform Discharge Letters Into Patient-Centered Letters to Enhance Patient Safety: Prospective, Exploratory Study","Background: For hospitalized patients, the discharge letter serves as a crucial source of medical information, outlining important discharge instructions and health management tasks. However, these letters are often written in professional jargon, making them difficult for patients with limited medical knowledge to understand. Large language models, such as GPT, have the potential to transform these discharge summaries into patient-friendly letters, improving accessibility and understanding. Objective: This study aims to use GPT-4 to convert discharge letters into more readable patient-centered letters. We evaluated how effectively and comprehensively GPT-4 identified and transferred patient safety–relevant information from the discharge letters to the transformed patient letters. Methods: Three discharge letters were created based on common medical conditions, containing 72 patient safety–relevant pieces of information, referred to as “learning objectives.” GPT-4 was prompted to transform these discharge letters into patient-centered letters. The resulting patient letters were analyzed for medical accuracy, patient centricity, and the ability to identify and translate the learning objectives. Bloom’s taxonomy was applied to analyze and categorize the learning objectives. Results: GPT-4 addressed the majority (56/72, 78%) of the learning objectives from the discharge letters. However, 11 of the 72 (15%) learning objectives were not included in the majority of the patient-centered letters. A qualitative analysis based on Bloom’s taxonomy revealed that learning objectives in the “Understand” category (9/11) were more frequently omitted than those in the “Remember” category (2/11). Most of the missing learning objectives were related to the content field of “prevention of complications.” By contrast, learning objectives regarding “lifestyle” and “organizational” aspects were addressed more frequently. Medical errors were found in a small proportion of sentences (31/787, 3.9%). In terms of patient centricity, the patient-centered letters demonstrated better readability than the discharge letters. Compared with discharge letters, they included fewer medical terms (132/860, 15.3%, vs 165/273, 60/4%), fewer abbreviations (43/860, 5%, vs 49/273, 17.9%), and more explanations of medical terms (121/131, 92.4%, vs 0/165, 0%). Conclusions: Our study demonstrates that GPT-4 has the potential to transform discharge letters into more patient-centered communication. While the readability and patient centricity of the transformed letters are well-established, they do not fully address all patient safety–relevant information, resulting in the omission of key aspects. Further optimization of prompt engineering may help address this issue and improve the completeness of the transformation. ©Felix Eisinger, Friederike Holderried, Moritz Mahling, Christian Stegemann–Philipps, Anne Herrmann–Werner, Eric Nazarenus, Alessandra Sonanini, Martina Guthoff, Carsten Eickhoff, Martin Holderried.",yes,"This study evaluates GPT-4's ability to transform clinical discharge letters into patient-centered versions, focusing on the accurate and comprehensive transfer of patient safety–relevant information. The use of GPT-4 is directly assessed in a clinical communication context, with outcomes relevant to patient understanding and safety. This fits the inclusion criteria as it evaluates a generative LLM in a healthcare application.",10.2196/67143,,,,,
Evaluation of Large Language Models' Concordance With Guidelines on Olfaction,"Objective: To assess the concordance of artificial intelligence (AI)-generated information with the 2022 International Consensus Statement on Allergy and Rhinology: Olfaction (ICAR-O). Methods: Forty-two guidelines were extracted from the ICAR-O. Each guideline was converted into a question, which was presented to ChatGPT version 4.o and Google Gemini. Concordance was deemed an agreement between the AI response and the clinical recommendation. Credibility was granted if the AI platform provided a credible resource. Accuracy was graded on a Likert scale (0: entirely inaccurate information, 1: mix of accurate and inaccurate information, 2: entirely accurate information). Statistical analysis was performed. Results: A total of 84 responses were generated. The mean accuracy of the ChatGPT and Gemini responses was 1.85 and 1.48 out of 2, respectively, indicating that the responses contained a mix of accurate and inaccurate information. ChatGPT responses were significantly more accurate than Gemini responses (p = 0.001). Of the ChatGPT responses, 78.57% (N = 33) were concordant with the ICAR-O guidelines and 100% (N = 42) cited a credible resource. Of the Gemini responses, 66.67% (N = 28) were concordant and 97.62% (N = 41) cited a credible resource. There were no significant differences in concordance (p = 0.22) or credibility (p = 0.31) between the AI platforms. Conclusion: ChatGPT provided more accurate information than Gemini on olfaction. However, overall, both platforms did not consistently align with clinical guidelines. AI platforms require further evaluation before clinical implementation or use as educational adjuncts. Level of Evidence: N/A. © 2025 The Author(s). Laryngoscope Investigative Otolaryngology published by Wiley Periodicals LLC on behalf of The Triological Society.",yes,This study evaluates the performance of two large language models (ChatGPT-4o and Google Gemini) in providing information concordant with clinical guidelines (ICAR-O) in the context of olfaction. The LLMs are assessed for their accuracy and ability to cite credible resources in response to clinical guideline-based questions. This is a direct clinical evaluation of LLMs.,10.1002/lio2.70130,,,,,
Assessment of ChatGPT-3.5's Knowledge in Oncology: Comparative Study with ASCO-SEP Benchmarks,"Background: ChatGPT (Open AI) is a state-of-the-art large language model that uses artificial intelligence (AI) to address questions across diverse topics. The American Society of Clinical Oncology Self-Evaluation Program (ASCO-SEP) created a comprehensive educational program to help physicians keep up to date with the many rapid advances in the field. The question bank consists of multiple choice questions addressing the many facets of cancer care, including diagnosis, treatment, and supportive care. As ChatGPT applications rapidly expand, it becomes vital to ascertain if the knowledge of ChatGPT-3.5 matches the established standards that oncologists are recommended to follow. Objective: This study aims to evaluate whether ChatGPT-3.5’s knowledge aligns with the established benchmarks that oncologists are expected to adhere to. This will furnish us with a deeper understanding of the potential applications of this tool as a support for clinical decision-making. Methods: We conducted a systematic assessment of the performance of ChatGPT-3.5 on the ASCO-SEP, the leading educational and assessment tool for medical oncologists in training and practice. Over 1000 multiple choice questions covering the spectrum of cancer care were extracted. Questions were categorized by cancer type or discipline, with subcategorization as treatment, diagnosis, or other. Answers were scored as correct if ChatGPT-3.5 selected the answer as defined by ASCO-SEP. Results: Overall, ChatGPT-3.5 achieved a score of 56.1% (583/1040) for the correct answers provided. The program demonstrated varying levels of accuracy across cancer types or disciplines. The highest accuracy was observed in questions related to developmental therapeutics (8/10; 80% correct), while the lowest accuracy was observed in questions related to gastrointestinal cancer (102/209; 48.8% correct). There was no significant difference in the program’s performance across the predefined subcategories of diagnosis, treatment, and other (P=.16, which is greater than.05). Conclusions: This study evaluated ChatGPT-3.5’s oncology knowledge using the ASCO-SEP, aiming to address uncertainties regarding AI tools like ChatGPT in clinical decision-making. Our findings suggest that while ChatGPT-3.5 offers a hopeful outlook for AI in oncology, its present performance in ASCO-SEP tests necessitates further refinement to reach the requisite competency levels. Future assessments could explore ChatGPT’s clinical decision support capabilities with real-world clinical scenarios, its ease of integration into medical workflows, and its potential to foster interdisciplinary collaboration and patient engagement in health care settings. © Roupen Odabashian, Donald Bastin, Georden Jones, Maria Manzoor, Sina Tangestaniapour, Malke Assad, Sunita Lakhani, Maritsa Odabashian, Sharon McGee.",yes,"This study evaluates ChatGPT-3.5's performance on the ASCO-SEP, a clinically relevant oncology exam, to assess its knowledge in cancer care (diagnosis, treatment, supportive care). The focus is on clinical knowledge and decision-making benchmarks, which aligns with the inclusion criteria.",10.2196/50442,,,,,
"Comparison of the Performance of ChatGPT, Claude and Bard in Support of Myopia Prevention and Control","Purpose: Chatbots, which are based on large language models, are increasingly being used in public health. However, the effectiveness of chatbot responses has been debated, and their performance in myopia prevention and control has not been fully explored. This study aimed to evaluate the effectiveness of three well-known chatbots—ChatGPT, Claude, and Bard—in responding to public health questions about myopia. Methods: Nineteen public health questions about myopia (including three topics of policy, basics and measures) were responded individually by three chatbots. After shuffling the order, each chatbot response was independently rated by 4 raters for comprehensiveness, accuracy and relevance. Results: The study’s questions have undergone reliable testing. There was a significant difference among the word count responses of all 3 chatbots. From most to least, the order was ChatGPT, Bard, and Claude. All 3 chatbots had a composite score above 4 out of 5. ChatGPT scored the highest in all aspects of the assessment. However, all chatbots exhibit shortcomings, such as giving fabricated responses. Conclusion: Chatbots have shown great potential in public health, with ChatGPT being the best. The future use of chatbots as a public health tool will require rapid development of standards for their use and monitoring, as well as continued research, evaluation and improvement of chatbots. © 2024 Wang et al. This work is published and licensed by Dove Medical Press Limited.",yes,"This study evaluates the performance of three LLM-based chatbots (ChatGPT, Claude, Bard) in answering public health questions about myopia prevention and control. The evaluation focuses on the accuracy, comprehensiveness, and relevance of their responses to clinical/public health questions. This fits the inclusion criteria as it assesses LLMs in a healthcare context, specifically their ability to provide information relevant to disease prevention and control.",10.2147/JMDH.S473680,,,,,
Can ChatGPT-4 perform as a competent physician based on the Chinese critical care examination?,"Background: The use of ChatGPT in medical applications is of increasing interest. However, its efficacy in critical care medicine remains uncertain. This study aims to assess ChatGPT-4's performance in critical care examination, providing insights into its potential as a tool for clinical decision-making. Methods: A dataset from the Chinese Health Professional Technical Qualification Examination for Critical Care Medicine, covering four components—fundamental knowledge, specialized knowledge, professional practical skills, and related medical knowledge—was utilized. ChatGPT-4 answered 600 questions, which were evaluated by critical care experts using a standardized rubric. Results: ChatGPT-4 achieved a 73.5 % success rate, surpassing the 60 % passing threshold in four components, with the highest accuracy in fundamental knowledge (81.94 %). ChatGPT-4 performed significantly better on single-choice questions than on multiple-choice questions (76.72 % vs. 51.32 %, p < 0.001), while no significant difference was observed between case-based and non-case-based questions. Conclusion: ChatGPT demonstrated notable strengths in critical care examination, highlighting its potential for supporting clinical decision-making, information retrieval, and medical education. However, caution is required regarding its potential to generate inaccurate responses. Its application in critical care must therefore be carefully supervised by medical professionals to ensure both the accuracy of the information and patient safety. © 2024 Elsevier Inc.",yes,"This study evaluates ChatGPT-4's performance on a critical care medicine board-style examination, which is a clinically relevant assessment. The results provide insight into the model's potential for clinical decision support and medical education, meeting the inclusion criteria.",10.1016/j.jcrc.2024.155010,,,,,
Can Artificial Intelligence aid communication? Considering the possibilities of GPT-3 in Palliative care,"Objectives: This article reviews the developments in artificial intelligence (AI) technologies and their current and prospective applications in end-of-life communications. It uses Open AI’s generative pre-trained transformer 3 (GPT-3) as a case study to understand the possibilities of AI-aided communication in Palliative Care. Material and Methods: Open AI’s GPT-3 was taken as a case study where responses were generated through the GPT-3 beta playground (Davinci engine) and were scrutinised by six mental health professionals (MHPs) working in a palliative care setting in India. They were tasked to evaluate the responses generated by the AI (the identity was not revealed until a part of the study was completed) in a simulated palliative care conversation with another MHP posing as a patient. The aim was to undermine whether the professionals were able to detect that the responses were indeed generated by a machine and did they approve or disapprove of the responses. Results: The GPT-3 playground with the right prompts produced remarkable, often surprising texts and responses that imitated human interaction. However, glitches such as redundancy were noticed along with strongly held opinions in certain questions related to faith, death, and life after death. Conclusion: AI-assisted communication in palliative care could be used to train professionals in the palliative care field using it as a simulation in training. It could also be used as a therapeutic intervention for the purpose of engagement and philosophical dialogue after certain modifications. However, it would have its own limitations such as it cannot replace a human agent just yet. ©2023 Published by Scientific Scholar on behalf of Indian Journal of Palliative Care.",yes,"This study evaluates GPT-3's ability to generate responses in simulated palliative care conversations, with mental health professionals assessing the quality and detectability of the AI's responses. The focus is on the clinical application of LLMs in communication within a healthcare context, specifically palliative care, and discusses potential uses in training and therapeutic interventions.",10.25259/IJPC_155_2023,,,,,
Comparative Analysis of Large Language Models in Traditional Chinese Medicine,"Background: Traditional Chinese Medicine (TCM), with its unique framework and linguistic characteristics, presents challenges for large language models (LLMs). Identifying LLMs’ strengths and limitations is crucial to exploring their potential in the TCM domain. Herein, we evaluated the performance of multiple LLMs in TCM, including their foundational knowledge, diagnostic capabilities, and herbal prescription generation. Methods: This comparative research was conducted in two phases: (1) assessing LLMs’ mastery of TCM foundational knowledge with two question banks (1911 questions in Question Bank A and 100 questions in Question Bank B) that were accessed from a paywalled online platform. (May–June 2024); and (2) evaluating LLMs’ syndrome differentiation and prescription generation using 100 real-world clinical cases collected from the First Affiliated Hospital of Sun Yat-sen University (July–August 2024). Eight LLMs (ChatGPT 3.5, ChatGPT 4o, Gemini, Gemini Advanced, ERNIE 3.5, ERNIE 4, GLM 3, and GLM 4) were evaluated. High-performing models advanced to the second phase, and their outputs were compared with diagnoses and prescriptions from TCM practitioners. LLMs’ accuracy was assessed by percentage scores and consistency by Intraclass Correlation Coefficient (ICC). TCM syndrome diagnoses, decomposed into disease location and nature elements, were evaluated by the Dice Similarity Coefficient (DSC), and herbal prescription quality was assessed by expert review.Findings: In the examination, ERNIE 4 and ERNIE 3.5 scored the highest (78.6%), followed by ChatGPT 4o (76.6%). ERNIE 4 achieved the highest ICC (0.92), with ERNIE 3.5 (0.89) and ChatGPT 4o (0.78) also demonstrating strong repeatability. In clinical cases, ChatGPT 4o outperformed ERNIE 3.5 in syndrome differentiation for both disease location (DSC: 0.74 vs. 0.63, p<.05) and disease nature (DSC: 0.73 vs. 0.64, p<.05), and surpassed ERNIE 4 in disease nature differentiation (DSC: 0.73 vs. 0.64, p<.05). ChatGPT 4o also achieved the highest prescription score (119), though only 45% of its prescriptions fully matched the reference.Interpretation: LLMs excelled in TCM knowledge and diagnosis but need improvement in generating effective herbal prescriptions. Funding: National Nature Science Foundation of China (NO: 82205248 and NO: 82272076) and Guangdong Natural Science Foundation (NO：2022A1515012133).Declaration of Interest: The authors declare no competing interests.Ethical Approval: The retrospective analysis obtained ethical approval (No.2021464) and waived the informed consent requirement.",yes,"This study evaluates multiple generative LLMs (including ChatGPT 3.5, ChatGPT 4o, Gemini, etc.) on their clinical knowledge, diagnostic capabilities, and prescription generation in Traditional Chinese Medicine, using real-world clinical cases and expert comparison. The focus is on clinical application (diagnosis and prescription), not just technical or non-clinical tasks.",10.2139/ssrn.5050324,,,,,
QuickPic AAC: An AI-Based Application to Enable Just-in-Time Generation of Topic-Specific Displays for Persons Who Are Minimally Speaking,"As artificial intelligence (AI) makes significant headway in various arenas, the field of speech–language pathology is at the precipice of experiencing a transformative shift towards automation. This study introduces QuickPic AAC, an AI-driven application designed to generate topic-specific displays from photographs in a “just-in-time” manner. Using QuickPic AAC, this study aimed to (a) determine which of two AI algorithms (NLG-AAC and GPT-3.5) results in greater specificity of vocabulary (i.e., percentage of vocabulary kept/deleted by clinician relative to vocabulary generated by QuickPic AAC; percentage of vocabulary modified); and to (b) evaluate perceived usability of QuickPic AAC among practicing speech–language pathologists. Results revealed that the GPT-3.5 algorithm consistently resulted in greater specificity of vocabulary and that speech–language pathologists expressed high user satisfaction for the QuickPic AAC application. These results support continued study of the implementation of QuickPic AAC in clinical practice and demonstrate the possibility of utilizing topic-specific displays as just-in-time supports. © 2024 by the authors.",yes,"This study evaluates the use of GPT-3.5 (an LLM) in generating topic-specific vocabulary displays for minimally speaking individuals, a clinical application relevant to speech–language pathology. The study compares GPT-3.5 to another AI algorithm and assesses clinical usability, fitting the inclusion criteria for LLM evaluation in a healthcare context.",10.3390/ijerph21091150,,,,,
Utilizing large language models for detecting hospital-acquired conditions: an empirical study on pulmonary embolism,"OBJECTIVES: Adverse event detection from Electronic Medical Records (EMRs) is challenging due to the low incidence of the event, variability in clinical documentation, and the complexity of data formats. Pulmonary embolism as an adverse event (PEAE) is particularly difficult to identify using existing approaches. This study aims to develop and evaluate a Large Language Model (LLM)-based framework for detecting PEAE from unstructured narrative data in EMRs. MATERIALS AND METHODS: We conducted a chart review of adult patients (aged 18-100) admitted to tertiary-care hospitals in Calgary, Alberta, Canada, between 2017-2022. We developed an LLM-based detection framework consisting of three modules: evidence extraction (implementing both keyword-based and semantic similarity-based filtering methods), discharge information extraction (focusing on six key clinical sections), and PEAE detection. Four open-source LLMs (Llama3, Mistral-7B, Gemma, and Phi-3) were evaluated using positive predictive value, sensitivity, specificity, and F1-score. Model performance for population-level surveillance was assessed at yearly, quarterly, and monthly granularities. RESULTS: The chart review included 10 066 patients, with 40 cases of PEAE identified (0.4% prevalence). All four LLMs demonstrated high sensitivity (87.5-100%) and specificity (94.9-98.9%) across different experimental conditions. Gemma achieved the highest F1-score (28.11%) using keyword-based retrieval with discharge summary inclusion, along with 98.4% specificity, 87.5% sensitivity, and 99.95% negative predictive value. Keyword-based filtering reduced the median chunks per patient from 789 to 310, while semantic filtering further reduced this to 9 chunks. Including discharge summaries improved performance metrics across most models. For population-level surveillance, all models showed strong correlation with actual PEAE trends at yearly granularity (r=0.92-0.99), with Llama3 achieving the highest correlation (0.988). DISCUSSION: The results of our method for PEAE detection using EMR notes demonstrate high sensitivity and specificity across all four tested LLMs, indicating strong performance in distinguishing PEAE from non-PEAE cases. However, the low incidence rate of PEAE contributed to a lower PPV. The keyword-based chunking approach consistently outperformed semantic similarity-based methods, achieving higher F1 scores and PPV, underscoring the importance of domain knowledge in text segmentation. Including discharge summaries further enhanced performance metrics. Our population-based analysis revealed better performance for yearly trends compared to monthly granularity, suggesting the framework's utility for long-term surveillance despite dataset imbalance. Error analysis identified contextual misinterpretation, terminology confusion, and preprocessing limitations as key challenges for future improvement. CONCLUSIONS: Our proposed method demonstrates that LLMs can effectively detect PEAE from narrative EMRs with high sensitivity and specificity. While these models serve as effective screening tools to exclude non-PEAE cases, their lower PPV indicates they cannot be relied upon solely for definitive PEAE identification. Further chart review remains necessary for confirmation. Future work should focus on improving contextual understanding, medical terminology interpretation, and exploring advanced prompting techniques to enhance precision in adverse event detection from EMRs.",yes,This study evaluates multiple large language models (LLMs) for the clinical task of detecting hospital-acquired pulmonary embolism from unstructured EMR data. The LLMs are directly applied to a clinical problem (adverse event detection) and their performance is measured using clinical metrics. This fits the inclusion criteria as it is a clinical evaluation of LLMs in healthcare.,10.1093/jamia/ocaf048,,,,,
Bariatric Evaluation Through AI: a Survey of Expert Opinions Versus ChatGPT-4 (BETA-SEOV),"Background: Recent advancements in artificial intelligence, such as OpenAI’s ChatGPT-4, are revolutionizing various sectors, including healthcare. This study investigates the use of ChatGPT-4 in identifying suitable candidates for bariatric surgery and providing surgical recommendations to improve decision-making in obesity treatment amid the global obesity epidemic. Methods: We devised ten patient scenarios, thoughtfully encompassing a spectrum that spans from uncomplicated cases to more complex ones. Our objective was to delve into the decision-making process regarding the recommendation of bariatric surgery. From July 29th to August 10th, 2023, we conducted a voluntary online survey involving thirty prominent bariatric surgeons, ensuring that there was no predetermined bias in the selection of a specific type of bariatric surgery. This survey was designed to collect their insights on these scenarios and gain a deeper understanding of their professional experience and background in the field of bariatric surgery. Additionally, we consulted ChatGPT-4 in two separate conversations to evaluate its alignment with expert opinions on bariatric surgery options. Results: In 40% of the scenarios, disparities were identified between the two conversations with ChatGPT-4. It matched expert opinions in 30% of cases. Differences were noted in cases like gastrointestinal metaplasia and gastric adenocarcinoma, but there was alignment with conditions like endometriosis and GERD. Conclusion: The evaluation of ChatGPT-4’s role in determining bariatric surgery suitability uncovered both potential and shortcomings. Its alignment with experts was inconsistent, and it often overlooked key factors, emphasizing human expertise’s value. Its current use requires caution, and further refinement is needed for clinical application. Graphical Abstract: [Figure not available: see fulltext.] © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",yes,This study evaluates ChatGPT-4's ability to make clinical recommendations regarding bariatric surgery by comparing its responses to those of expert bariatric surgeons across patient scenarios. The focus is on clinical decision-making in a healthcare context using a generative LLM.,10.1007/s11695-023-06903-w,,,,,
"ChatGPT's Attitude, Knowledge, and Clinical Application in Geriatrics Practice and Education: Exploratory Observational Study","Background: The increasing use of ChatGPT in clinical practice and medical education necessitates the evaluation of its reliability, particularly in geriatrics. Objective: This study aimed to evaluate ChatGPT’s trustworthiness in geriatrics through 3 distinct approaches: evaluating ChatGPT’s geriatrics attitude, knowledge, and clinical application with 2 vignettes of geriatric syndromes (polypharmacy and falls). Methods: We used the validated University of California, Los Angeles, geriatrics attitude and knowledge instruments to evaluate ChatGPT’s geriatrics attitude and knowledge and compare its performance with that of medical students, residents, and geriatrics fellows from reported results in the literature. We also evaluated ChatGPT’s application to 2 vignettes of geriatric syndromes (polypharmacy and falls). Results: The mean total score on geriatrics attitude of ChatGPT was significantly lower than that of trainees (medical students, internal medicine residents, and geriatric medicine fellows; 2.7 vs 3.7 on a scale from 1-5; 1=strongly disagree; 5=strongly agree). The mean subscore on positive geriatrics attitude of ChatGPT was higher than that of the trainees (medical students, internal medicine residents, and neurologists; 4.1 vs 3.7 on a scale from 1 to 5 where a higher score means a more positive attitude toward older adults). The mean subscore on negative geriatrics attitude of ChatGPT was lower than that of the trainees and neurologists (1.8 vs 2.8 on a scale from 1 to 5 where a lower subscore means a less negative attitude toward aging). On the University of California, Los Angeles geriatrics knowledge test, ChatGPT outperformed all medical students, internal medicine residents, and geriatric medicine fellows from validated studies (14.7 vs 11.3 with a score range of –18 to +18 where +18 means that all questions were answered correctly). Regarding the polypharmacy vignette, ChatGPT not only demonstrated solid knowledge of potentially inappropriate medications but also accurately identified 7 common potentially inappropriate medications and 5 drug-drug and 3 drug-disease interactions. However, ChatGPT missed 5 drug-disease and 1 drug-drug interaction and produced 2 hallucinations. Regarding the fall vignette, ChatGPT answered 3 of 5 pretests correctly and 2 of 5 pretests partially correctly, identified 6 categories of fall risks, followed fall guidelines correctly, listed 6 key physical examinations, and recommended 6 categories of fall prevention methods. Conclusions: This study suggests that ChatGPT can be a valuable supplemental tool in geriatrics, offering reliable information with less age bias, robust geriatrics knowledge, and comprehensive recommendations for managing 2 common geriatric syndromes (polypharmacy and falls) that are consistent with evidence from guidelines, systematic reviews, and other types of studies. ChatGPT’s potential as an educational and clinical resource could significantly benefit trainees, health care providers, and laypeople. Further research using GPT-4o, larger geriatrics question sets, and more geriatric syndromes is needed to expand and confirm these findings before adopting ChatGPT widely for geriatrics education and practice. © Huai Yong Cheng.",yes,"This study evaluates ChatGPT's knowledge, attitude, and clinical application in geriatrics, including its performance on clinical vignettes (polypharmacy and falls) and comparison to human trainees. It directly assesses ChatGPT's clinical utility and educational value in a medical context, meeting inclusion criteria.",10.2196/63494,,,,,
The Potential of Chat-Based Artificial Intelligence Models in Differentiating Between Keloid and Hypertrophic Scars: A Pilot Study,"Background: Lasting scars such as keloids and hypertrophic scars adversely affect a patient’s quality of life. However, these scars are frequently underdiagnosed because of the complexity of the current diagnostic criteria and classification systems. This study aimed to explore the application of Large Language Models (LLMs) such as ChatGPT in diagnosing scar conditions and to propose a more accessible and straightforward diagnostic approach. Methods: In this study, five artificial intelligence (AI) chatbots, including ChatGPT-4 (GPT-4), Bing Chat (Precise, Balanced, and Creative modes), and Bard, were evaluated for their ability to interpret clinical scar images using a standardized set of prompts. Thirty mock images of various scar types were analyzed, and each chatbot was queried five times to assess the diagnostic accuracy. Results: GPT-4 had a significantly higher accuracy rate in diagnosing scars than Bing Chat. The overall accuracy rates of GPT-4 and Bing Chat were 36.0% and 22.0%, respectively (P = 0.027), with GPT-4 showing better performance in terms of specificity for keloids (0.6 vs. 0.006) and hypertrophic scars (0.72 vs. 0.0) than Bing Chat. Conclusions: Although currently available LLMs show potential for use in scar diagnostics, the current technology is still under development and is not yet sufficient for clinical application standards, highlighting the need for further advancements in AI for more accurate medical diagnostics. Level of Evidence IV: This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online instructions to authors www.springer.com/00266. © Springer Science+Business Media, LLC, part of Springer Nature and International Society of Aesthetic Plastic Surgery 2024.",yes,"This study evaluates the diagnostic accuracy of LLM-based chatbots (including ChatGPT-4 and Bard) in differentiating between keloid and hypertrophic scars using clinical images. The focus is on clinical application (diagnosis), and frontier LLMs are tested. This fits the inclusion criteria.",10.1007/s00266-024-04380-9,,,,,
Socio-Demographic Bias in Large Language Models Alters Ethical Decision-Making in Healthcare,"Objective Large language models (LLMs) are increasingly applied in healthcare. However, their ethical alignment remains uncertain. We tested whether LLMs shift ethical priorities in health-related scenarios under different socio-demographic modifiers, focusing on autonomy, beneficence, nonmaleficence, justice, and utilitarianism. Methods We created 100 clinical scenarios, each requiring a yes/no choice between two conflicting principles. We tested nine LLMs, each with and without 53 socio-demographic modifiers, repeating each scenario-modifier combination 10 times per model (for a total of 0.5M prompts). We measured how often each principle was chosen. Results All models changed their responses when socio-demographic details were introduced (p<0.001). Justice and nonmaleficence were each prioritized in over 30% of responses. Utilitarianism ranged from 6.7% (CI: 6.2-7.2%) for a “Black Transgender woman (she/her)” to 17.0% (CI: 16.3-17.8%) for “Billionaires”, across models and modifiers. High-income modifiers increased utilitarian choices while lowering beneficence and nonmaleficence. Marginalized-group modifiers raised autonomy prioritization. Some models were more consistent than others, but none maintained consistency across all scenarios. Conclusions LLMs can be influenced by socio-demographic cues and do not always maintain stable ethical priorities, with the greatest shifts seen in utilitarian choices. Our findings reveal that socio-demographic cues systematically alter LLM ethical decision-making, raising concerns about algorithmic fairness in healthcare.",yes,"This study evaluates multiple LLMs in the context of healthcare by testing their ethical decision-making in clinical scenarios, specifically examining how socio-demographic modifiers affect their choices. The scenarios are clinical and the LLMs' responses are assessed in a way that is directly relevant to clinical decision support and ethical considerations in medicine.",10.1101/2025.02.01.25321523,,,,,
The doc versus the bot: A pilot study to assess the quality and accuracy of physician and chatbot responses to clinical questions in gynecologic oncology,"Artificial intelligence (AI) applications to medical care are currently under investigation. We aimed to evaluate and compare the quality and accuracy of physician and chatbot responses to common clinical questions in gynecologic oncology. In this cross-sectional pilot study, ten questions about the knowledge and management of gynecologic cancers were selected. Each question was answered by a recruited gynecologic oncologist, ChatGPT (Generative Pretreated Transformer) AI platform, and Bard by Google AI platform. Five recruited gynecologic oncologists who were blinded to the study design were allowed 15 min to respond to each of two questions. Chatbot responses were generated by inserting the question into a fresh session in September 2023. Qualifiers and language identifying the response source were removed. Three gynecologic oncology providers who were blinded to the response source independently reviewed and rated response quality using a 5-point Likert scale, evaluated each response for accuracy, and selected the best response for each question. Overall, physician responses were judged to be best in 76.7 % of evaluations versus ChatGPT (10.0 %) and Bard (13.3 %; p < 0.001). The average quality of responses was 4.2/5.0 for physicians, 3.0/5.0 for ChatGPT and 2.8/5.0 for Bard (t-test for both and ANOVA p < 0.001). Physicians provided a higher proportion of accurate responses (86.7 %) compared to ChatGPT (60 %) and Bard (43 %; p < 0.001 for both). Physicians provided higher quality responses to gynecologic oncology clinical questions compared to chatbots. Patients should be cautioned against non-validated AI platforms for medical advice; larger studies on the use of AI for medical advice are needed. © 2024 The Authors",yes,"This study directly evaluates the clinical performance of LLMs (ChatGPT and Bard) by comparing their answers to clinical questions in gynecologic oncology with those of physicians. The focus is on the quality and accuracy of responses to clinical questions, which is a clinical application of LLMs.",10.1016/j.gore.2024.101477,,,,,
"Accuracy of ChatGPT 3.5, 4.0, 4o and Gemini in diagnosing oral potentially malignant lesions based on clinical case reports and image recognition","Background: The accurate and timely diagnosis of oral potentially malignant lesions (OPMLs) is crucial for effective management and prevention of oral cancer. Recent advancements in artificial intelligence technologies indicates its potential to assist in clinical decision-making. Hence, this study was carried out with the aim to evaluate and compare the diagnostic accuracy of ChatGPT 3.5, 4.0, 4o and Gemini in identifying OPMLs. Material and Methods: The analysis was carried out using 42 case reports from PubMed, Scopus and Google Scholar and images from two datasets, corresponding to different OPMLs. The reports were inputted separately for text description-based diagnosis in GPT 3.5, 4.0, 4o and Gemini, and for image recognition-based diagnosis in GPT 4o and Gemini. Two subject-matter experts independently reviewed the reports and offered their evaluations. Results: For text-based diagnosis, among LLMs, GPT 4o got the maximum number of correct responses (27/42), followed by GPT 4.0 (20/42), GPT 3.5 (18/42) and Gemini (15/42). In identifying OPMLs based on image, GPT 4o demonstrated better performance than Gemini. There was fair to moderate agreement found between Large Language Models (LLMs) and subject experts. None of the LLMs matched the accuracy of the subject experts in identifying the correct number of lesions. Conclusions: The results point towards cautious optimism with respect to commonly used LLMs in diagnosing OPMLs. While their potential in diagnostic applications is undeniable, their integration should be approached judiciously. © Medicina Oral S. L. C.I.F. B.",yes,"This study directly evaluates the diagnostic accuracy of several frontier LLMs (ChatGPT 3.5, 4.0, 4o, and Gemini) in identifying oral potentially malignant lesions using both clinical case reports and image recognition. The LLMs are tested in a clinical diagnostic context, which fits the inclusion criteria.",10.4317/medoral.26824,,,,,
Blepharoptosis Consultation with Artificial Intelligence: Aesthetic Surgery Advice and Counseling from Chat Generative Pre-Trained Transformer (ChatGPT),"Background: Chat generative pre-trained transformer (ChatGPT) is a publicly available extensive artificial intelligence (AI) language model that leverages deep learning to generate text that mimics human conversations. In this study, the performance of ChatGPT was assessed by offering insightful and precise answers to a series of fictional questions and emulating a preliminary consultation on blepharoplasty. Methods: ChatGPT was posed with questions derived from a blepharoplasty checklist provided by the American Society of Plastic Surgeons. Board-certified plastic surgeons and non-medical staff members evaluated the responses for accuracy, informativeness, and accessibility. Results: Nine questions were used in this study. Regarding informativeness, the average score given by board-certified plastic surgeons was significantly lower than that given by non-medical staff members (2.89 ± 0.72 vs 4.41 ± 0.71; p = 0.042). No statistically significant differences were observed in accuracy (p = 0.56) or accessibility (p = 0.11). Conclusions: Our results emphasize the effectiveness of ChatGPT in simulating doctor–patient conversations during blepharoplasty. Non-medical individuals found its responses more informative compared with the surgeons. Although limited in terms of specialized guidance, ChatGPT offers foundational surgical information. Further exploration is warranted to elucidate the broader role of AI in esthetic surgical consultations. Level of Evidence V: Observational study under respected authorities. This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266. © Springer Science+Business Media, LLC, part of Springer Nature and International Society of Aesthetic Plastic Surgery 2024.",yes,"This study evaluates ChatGPT's performance in simulating preliminary clinical consultations for blepharoplasty, with responses assessed by both clinicians and laypersons for accuracy and informativeness. The use of ChatGPT in a clinical consultation context aligns with the inclusion criteria.",10.1007/s00266-024-04002-4,,,,,
Assessing the diagnostic capability of ChatGPT through clinical case scenarios in dermatology,,yes,"The title indicates that the study evaluates ChatGPT's diagnostic performance using clinical case scenarios in dermatology, which is a direct clinical application of an LLM. No abstract is provided, but the title alone meets the inclusion criteria.",10.25259/IJDVL_1267_2023,,,,,
Accuracy and comprehensibility of chat-based artificial intelligence for patient information on atrial fibrillation and cardiac implantable electronic devices,"Aims Natural language processing chatbots (NLPC) can be used to gather information for medical content. However, these tools contain a potential risk of misinformation. This study aims to evaluate different aspects of responses given by different NLPCs on questions about atrial fibrillation (AF) and clinical implantable electronic devices (CIED) Methods Questions were entered into three different NLPC interfaces. Responses were evaluated with regard to appropriateness, and results comprehensibility, appearance of confabulation, absence of relevant content, and recommendations given for clinically relevant decisions. Moreover, readability was assessed by calculating word count and Flesch Reading Ease score. 52, 60, and 84% of responses on AF and 16, 72, and 88% on CIEDs were evaluated to be appropriate for all responses given by Google Bard, (GB) Bing Chat (BC) and ChatGPT Plus (CGP), respectively. Assessment of comprehensibility showed that 96, 88, and 92% of responses on AF and 92 and 88%, and 100% on CIEDs were comprehensible for all responses created by GB, BC, and CGP, respectively. Readability varied between different NLPCs. Relevant aspects were missing in 52% (GB), 60% (BC), and 24% (CGP) for AF, and in 92% (GB), 88% (BC), and 52% (CGP) for CIEDs Conclusion Responses generated by an NLPC are mostly easy to understand with varying readability between the different NLPCs. The appropriateness of responses is limited and varies between different NLPCs. Important aspects are often missed to be mentioned. Thus, chatbots should be used with caution to gather medical information about cardiac arrhythmias and devices. © The Author(s) 2023. Published by Oxford University Press on behalf of the European Society of Cardiology.",yes,"This study evaluates the accuracy and comprehensibility of responses from LLM-based chatbots (ChatGPT Plus, Google Bard, Bing Chat) to patient questions about atrial fibrillation and cardiac devices. It assesses clinical appropriateness, comprehensibility, and recommendations for clinically relevant decisions, which fits the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1093/europace/euad369,,,,,
Automated Diagnostic Reports from Images of Electrocardiograms at the Point-of-Care,"Timely and accurate assessment of electrocardiograms (ECGs) is crucial for diagnosing, triaging, and clinically managing patients. Current workflows rely on a computerized ECG interpretation using rule-based tools built into the ECG signal acquisition systems with limited accuracy and flexibility. In low-resource settings, specialists must review every single ECG for such decisions, as these computerized interpretations are not available. Additionally, high-quality interpretations are even more essential in such low-resource settings as there is a higher burden of accuracy for automated reads when access to experts is limited. Artificial Intelligence (AI)-based systems have the prospect of greater accuracy yet are frequently limited to a narrow range of conditions and do not replicate the full diagnostic range. Moreover, these models often require raw signal data, which are unavailable to physicians and necessitate costly technical integrations that are currently limited. To overcome these challenges, we developed and validated a format-independent vision encoder-decoder model – ECG-GPT – that can generate free-text, expert-level diagnosis statements directly from ECG images. The model shows robust performance, validated on 2.6 million ECGs across 6 geographically distinct health settings: (1) 2 large and diverse US health systems- Yale-New Haven and Mount Sinai Health Systems, (2) a consecutive ECG dataset from a central ECG repository from Minas Gerais, Brazil, (3) the prospective cohort study, UK Biobank, (4) a Germany-based, publicly available repository, PTB-XL, and (5) a community hospital in Missouri. The model demonstrated consistently high performance (AUROC≥0.81) across a wide range of rhythm and conduction disorders. This can be easily accessed via a web-based application capable of receiving ECG images and represents a scalable and accessible strategy for generating accurate, expert-level reports from images of ECGs, enabling accurate triage of patients globally, especially in low-resource settings.",yes,"The study describes the development and validation of ECG-GPT, a vision encoder-decoder model that generates free-text diagnostic reports from ECG images. The model is named 'ECG-GPT' and generates expert-level diagnosis statements, suggesting it is a generative large language model (LLM) or at least incorporates LLM technology for clinical reporting. The application is directly clinical: generating diagnostic reports and supporting triage and management decisions from ECGs. The evaluation is on clinical data across multiple health systems.",10.1101/2024.02.17.24302976,,,,,
ChatGPT yields low accuracy in determining LI-RADS scores based on free-text and structured radiology reports in German language,"Background: To investigate the feasibility of the large language model (LLM) ChatGPT for classifying liver lesions according to the Liver Imaging Reporting and Data System (LI-RADS) based on MRI reports, and to compare classification performance on structured vs. unstructured reports. Methods: LI-RADS classifiable liver lesions were included from German written structured and unstructured MRI reports with report of size, location, and arterial phase contrast enhancement as minimum inclusion requirements. The findings sections of the reports were propagated to ChatGPT (GPT-3.5), which was instructed to determine LI-RADS scores for each classifiable liver lesion. Ground truth was established by two radiologists in consensus. Agreement between ground truth and ChatGPT was assessed with Cohen's kappa. Test-retest reliability was assessed by passing a subset of n = 50 lesions five times to ChatGPT, using the intraclass correlation coefficient (ICC). Results: 205 MRIs from 150 patients were included. The accuracy of ChatGPT at determining LI-RADS categories was poor (53% and 44% on unstructured and structured reports). The agreement to the ground truth was higher (k = 0.51 and k = 0.44), the mean absolute error in LI-RADS scores was lower (0.5 ± 0.5 vs. 0.6 ± 0.7, p < 0.05), and the test-retest reliability was higher (ICC = 0.81 vs. 0.50), in free-text compared to structured reports, respectively, although structured reports comprised the minimum required imaging features significantly more frequently (Chi-square test, p < 0.05). Conclusions: ChatGPT attained only low accuracy when asked to determine LI-RADS scores from liver imaging reports. The superior accuracy and consistency throughout free-text reports might relate to ChatGPT's training process. Clinical relevance statement: Our study indicates both the necessity of optimization of LLMs for structured clinical data input and the potential of LLMs for creating machine-readable labels based on large free-text radiological databases. 2024 Fervers, Hahnfeldt, Kottlors, Wagner, Maintz, Pinto dos Santos, Lennartz and Persigehl.",yes,"This study evaluates ChatGPT (GPT-3.5), a generative LLM, for a clinical task: classifying liver lesions according to LI-RADS based on radiology reports. The study assesses the model's accuracy and reliability in a real clinical context, comparing performance on structured and unstructured reports. This fits the inclusion criteria as it is a clinical evaluation of an LLM.",10.3389/fradi.2024.1390774,,,,,
"Revolutionizing teledermatology: Exploring the integration of artificial intelligence, including Generative Pre-trained Transformer chatbots for artificial intelligence-driven anamnesis, diagnosis, and treatment plans","The integration of teledermatology and artificial intelligence (AI) marks a significant advancement in dermatologic care. This study examines the synergistic interplay between these two domains, highlighting their collective impact on enhancing the accuracy, accessibility, and efficiency of teledermatologic services. Teledermatology expands dermatologic care to remote and underserved areas, and AI technologies show considerable potential in analyzing dermatologic images and performing various tasks involved in teledermatology consultations. Such integration facilitates rapid, precise diagnoses, personalized treatment plans, and data-driven insights. Our explorative study involved designing a GPT-based chatbot named “Dr. DermBot” and exploring its performance in a teledermatologic consultation process. The design phase focused on the chatbot's ability to conduct consultations autonomously. The subsequent testing phase assessed its performance against the backdrop of current teledermatologic practices, exploring the potential of AI and chatbots to simulate and potentially enhance teledermatologic health care. Our study demonstrates the promising future of combining teledermatology with AI. It also brings to light ethical and legal concerns, including the protection of patient data privacy and adherence to regulatory standards. The union of teledermatology and AI not only aims to enhance the precision of teledermatologic diagnoses but also broadens the accessibility of dermatologic services to previously underserved populations, benefiting patients, health care providers, and the overall health care system. © 2024 Elsevier Inc.",yes,"This study describes the design and testing of a GPT-based chatbot ('Dr. DermBot') for conducting teledermatology consultations, including anamnesis, diagnosis, and treatment planning. It evaluates the chatbot's performance in a clinical context, which aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.clindermatol.2024.06.020,,,,,
Artificial intelligence with ChatGPT 4: a large language model in support of ocular oncology cases,"Purpose: To evaluate ChatGPT’s ability to analyze comprehensive case descriptions of patients with uveal melanoma and provide recommendations for the most appropriate management. Design: Retrospective analysis of ocular oncology patients’ medical records. Subjects. Forty patients treated for uveal melanoma between May 2019 and October 2023. Methods: We uploaded each case description into the ChatGPT interface (version 4.0) and asked the model to provide realistic treatment options by asking the question, “What type of treatment do you recommend?” The accuracy of decisions produced by ChatGPT was compared to those recorded in patients’ files and the treatment recommendations provided by three ocular oncologists, each with more than 10 years of experience. Main outcome measures. The primary objective of this research was to assess the accuracy of ChatGPT replies in ocular oncology cases, analyzing its competence in both straightforward and intricate situations. Our secondary purpose was to assess the concordance between the responses of ChatGPT and those of ocular oncology specialists when faced with analogous clinical scenarios. Results: ChatGPT’s surgical choices matched those in patients’ files in 55% of cases (22 out of 40). ChatGPT options were agreed upon by 50%, 55%, and 57% of the three ocular oncology specialists. The investigation revealed significant differences between ChatGPT’s responses and those of the three cancer specialists when compared to patients’ files (p = 0.003, p = 0.001, and p = 0.001). ChatGPT’s surgical responses matched with patient data in 18 out of 24 cases (75%), excluding enucleation cases. The decisions matched with the three ocular oncology specialists in 17/24, 18/24, and 18/24 cases, reflecting agreements of 70%, 75%, and 75%, respectively. The decisions made by ChatGPT were not significantly different from those of the three professionals in this cohort (p = 0.50, p = 0.36, and p = 0.36 for ChatGPT compared to specialists 1, 2, and 3). Conclusion: ChatGPT exhibited a level of proficiency that was comparable to that of trained ocular oncology specialists. However, it exhibited its limitations when evaluating more complex scenarios, such as extrascleral extension or infiltration of the optic nerve, when a comprehensive evaluation of the patient is therefore necessary. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.",yes,"This study directly evaluates ChatGPT-4's ability to provide clinical management recommendations for ocular oncology cases, comparing its performance to that of human specialists and actual patient records. The focus is on clinical decision-making in a real-world medical context, which fits the inclusion criteria.",10.1007/s10792-024-03399-w,,,,,
Performance of ChatGPT on Stage 1 of the Taiwanese medical licensing exam,"Introduction: Since its release by OpenAI in November 2022, numerous studies have subjected ChatGPT to various tests to evaluate its performance in medical exams. The objective of this study is to evaluate ChatGPT's accuracy and logical reasoning across all 10 subjects featured in Stage 1 of Senior Professional and Technical Examinations for Medical Doctors (SPTEMD) in Taiwan, with questions that encompass both Chinese and English. Methods: In this study, we tested ChatGPT-4 to complete SPTEMD Stage 1. The model was presented with multiple-choice questions extracted from three separate tests conducted in February 2022, July 2022, and February 2023. These questions encompass 10 subjects, namely biochemistry and molecular biology, anatomy, embryology and developmental biology, histology, physiology, microbiology and immunology, parasitology, pharmacology, pathology, and public health. Subsequently, we analyzed the model's accuracy for each subject. Result: In all three tests, ChatGPT achieved scores surpassing the 60% passing threshold, resulting in an overall average score of 87.8%. Notably, its best performance was in biochemistry, where it garnered an average score of 93.8%. Conversely, the performance of the generative pre-trained transformer (GPT)-4 assistant on anatomy, parasitology, and embryology was not as good. In addition, its scores were highly variable in embryology and parasitology. Conclusion: ChatGPT has the potential to facilitate not only exam preparation but also improve the accessibility of medical education and support continuous education for medical professionals. In conclusion, this study has demonstrated ChatGPT's potential competence across various subjects within the SPTEMD Stage 1 and suggests that it could be a helpful tool for learning and exam preparation for medical students and professionals. © The Author(s) 2024.",yes,"This study evaluates ChatGPT-4's performance on a medical licensing exam, specifically assessing its accuracy and reasoning across multiple clinical subjects. This fits the inclusion criteria as it is a clinical evaluation of an LLM's performance on a board exam relevant to medicine.",10.1177/20552076241233144,,,,,
The impact of prompt engineering in large language model performance: a psychiatric example,"Large language models (LLMs) are increasing in prevalence, and the use of these tools in academic medical research is increasing. ChatGPT is one specific example of an LLM. Many research studies utilize ChatGPT to perform a task (take a medical exam, for instance), and the researchers will assess the model’s ability to perform such a task. They often look at the results and determine whether the model is effective at this task or not. However, little attention is paid to the prompts that are input into the model. OpenAI, ChatGPT’s parent company, provides a free course for developers on prompt engineering, which is the process of asking the LLM to perform a task. The purpose of this course is predicated on the fact that the construction of a prompt dictates the performance of the model. Therefore, in medical research studies, we should be paying more attention to prompt construction as a variable in a model’s performance. This investigation utilized ChatGPT 4.0 to answer a common question patients ask in mental healthcare: “How can I feel happier?” The model was given varying levels of prompting to answer this question, and it significantly impacted the responses. Furthermore, specific prompts resulted in the LLM suggesting potentially harmful answers to the proposed question. This highlights the importance of prompt engineering in achieving desired results from LLMs, especially in the realm of mental healthcare where context and subjectivity play larger roles than in most of medicine. © Journal of Medical Artificial Intelligence. All rights reserved.",yes,"This study evaluates ChatGPT 4.0's responses to a clinically relevant patient question in mental healthcare, specifically examining how prompt engineering affects the quality and safety of those responses. The focus is on the LLM's clinical output in response to patient queries, which fits the inclusion criteria.",10.21037/jmai-23-71,,,,,
Use of GPT-4 to Analyze Medical Records of Patients With Extensive Investigations and Delayed Diagnosis,"Plain Language Summary: This case series investigates whether analysis of 
clinical history via a language model system improves diagnostic accuracy in 
patients with complex and delayed diagnoses.",yes,"The study evaluates GPT-4, a large language model, for analyzing clinical histories to improve diagnostic accuracy in patients with complex and delayed diagnoses. This is a direct clinical application of an LLM in healthcare.",10.1001/jamanetworkopen.2023.25000,,,,,
Accuracy of latest large language models in answering multiple choice questions in dentistry: A comparative study,"449. PLoS One. 2025 Jan 29;20(1):e0317423. doi: 10.1371/journal.pone.0317423. 
eCollection 2025.

Accuracy of latest large language models in answering multiple choice questions 
in dentistry: A comparative study.

Nguyen HC(1), Dang HP(1), Nguyen TL(1), Hoang V(2), Nguyen VA(1).

Author information:
(1)Faculty of Dentistry, PHENIKAA University, Hanoi, Vietnam.
(2)Faculty of Dentistry, Van Lang University, Ho Chi Minh City, Vietnam.

OBJECTIVES: This study aims to evaluate the performance of the latest large 
language models (LLMs) in answering dental multiple choice questions (MCQs), 
including both text-based and image-based questions.
MATERIAL AND METHODS: A total of 1490 MCQs from two board review books for the 
United States National Board Dental Examination were selected. This study 
evaluated six of the latest LLMs as of August 2024, including ChatGPT 4.0 omni 
(OpenAI), Gemini Advanced 1.5 Pro (Google), Copilot Pro with GPT-4 Turbo 
(Microsoft), Claude 3.5 Sonnet (Anthropic), Mistral Large 2 (Mistral AI), and 
Llama 3.1 405b (Meta). χ2 tests were performed to determine whether there were 
significant differences in the percentages of correct answers among LLMs for 
both the total sample and each discipline (p < 0.05).
RESULTS: Significant differences were observed in the percentage of accurate 
answers among the six LLMs across text-based questions, image-based questions, 
and the total sample (p<0.001). For the total sample, Copilot (85.5%), Claude 
(84.0%), and ChatGPT (83.8%) demonstrated the highest accuracy, followed by 
Mistral (78.3%) and Gemini (77.1%), with Llama (72.4%) exhibiting the lowest.
CONCLUSIONS: Newer versions of LLMs demonstrate superior performance in 
answering dental MCQs compared to earlier versions. Copilot, Claude, and ChatGPT 
achieved high accuracy on text-based questions and low accuracy on image-based 
questions. LLMs capable of handling image-based questions demonstrated superior 
performance compared to LLMs limited to text-based questions.
CLINICAL RELEVANCE: Dental clinicians and students should prioritize the most 
up-to-date LLMs when supporting their learning, clinical practice, and research.

Copyright: © 2025 Nguyen et al. This is an open access article distributed under 
the terms of the Creative Commons Attribution License, which permits 
unrestricted use, distribution, and reproduction in any medium, provided the 
original author and source are credited.",yes,"This study evaluates the performance of several frontier LLMs (ChatGPT 4.0 omni, Gemini Advanced, Copilot Pro with GPT-4 Turbo, Claude 3.5 Sonnet, Mistral Large 2, Llama 3.1) on answering multiple choice questions from the US National Board Dental Examination, including both text-based and image-based clinical questions. The focus is on clinical knowledge and exam performance, which is a direct clinical application relevant to healthcare and medical education.",10.1371/journal.pone.0317423,,,,,
Application of multimodal large language models for safety indicator calculation and contraindication prediction in laser vision correction,"This study demonstrates the potential of multimodal large language models in calculating safety indicators and predicting contraindications for laser vision correction. ChatGPT-4 effectively analyzed ocular data, calculated key indicators, generated calculator codes, and outperformed traditional machine learning models and indicators in handling unstructured data and corneal topography. Its modality-independent system enabled efficient and accurate data analysis. Despite longer processing times, ChatGPT-4’s performance highlights its potential as a decision-support tool, offering advancements in improving safety. © The Author(s) 2025.",yes,"This study evaluates ChatGPT-4, a frontier LLM, in a clinical context—specifically for calculating safety indicators and predicting contraindications in laser vision correction. The LLM is used for clinical decision support, directly addressing clinical questions and outperforming traditional models. This fits the inclusion criteria.",10.1038/s41746-025-01487-4,,,,,
MedicalGLM: A Pediatric Medical Question Answering Model with a quality evaluation mechanism,"Objective: Large Language models (LLMs) have a wide range of medical applications, especially in scenarios such as question-answering. However, existing models face the challenge of accurately assessing the quality of information when generating medical information, which may lead to the inability to effectively distinguish beneficial and harmful information, thus affecting the quality of question-answering. This study aims to improve the information quality and practicability of medical question-answering. Methods: This study proposes MedicalGLM, a fine-tuning model based on a quality evaluation mechanism. Specifically, MedicalGLM contains a reward model for assessing the quality of medical QA. It adjusts its training process by returning the assessment scores to the QA model as penalties through a quality score loss function. Results: The experimental results indicate that MedicalGLM achieved the highest scores among the evaluated models in the Rouge-1, Rouge-2, Rouge-L, and BLEU metrics, with values of 54.90, 28.02, 44.50, and 32.61, respectively. Its proficiency in generating responses for the pediatric medical quiz task is notably superior to other prevailing LLMs in the medical domain. Conclusion: MedicalGLM significantly improves the quality and practicability of the generated information of the medical question-answering model by introducing a quality evaluation mechanism, which provides an effective improvement idea for researching medical large language models. Our code and model are publicly available for further research on https://github.com/wangxinwwang/MedicalGLM. © 2025 Elsevier Inc.",yes,"This study evaluates a generative large language model (MedicalGLM) specifically for pediatric medical question answering, including its performance on clinical QA tasks. The focus is on improving and assessing the quality of clinical information generated by the LLM, which aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.1016/j.jbi.2025.104793,,,,,
Comparative analysis of BERT-based and generative large language models for detecting suicidal ideation: a performance evaluation study; [Análise comparativa de modelos de linguagem baseados em BERT e generativos amplos para detecção de ideação suicida: um estudo de avaliação de desempenho]; [Análisis comparativo de modelos de lenguaje basados en BERT y generativos amplios para la detección de ideación suicida: un estudio de evaluación del desempeño],"Artificial intelligence can detect suicidal ideation manifestations in texts. Studies demonstrate that BERT-based models achieve better performance in text classification problems. Large language models (LLMs) answer free-text queries without being specifically trained. This work aims to compare the performance of three variations of BERT models and LLMs (Google Bard, Microsoft Bing/GPT-4, and OpenAI ChatGPT-3.5) for identifying suicidal ideation from nonclinical texts written in Brazilian Portuguese. A dataset labeled by psychologists consisted of 2,691 sentences without suicidal ideation and 1,097 with suicidal ideation, of which 100 sentences were selected for testing. We applied data preprocessing techniques, hyperparameter optimization, and hold-out cross-validation for training and testing BERT models. When evaluating LLMs, we used zero-shot prompting engineering. Each test sentence was labeled if it contained suicidal ideation, according to the chatbot’s response. Bing/GPT-4 achieved the best performance, with 98% across all metrics. Fine-tuned BERT models outperformed the other LLMs: BERTimbau-Large performed the best with a 96% accuracy, followed by BERTimbau-Base with 94%, and BERT-Multilingual with 87%. Bard performed the worst with 62% accuracy, whereas ChatGPT-3.5 achieved 81%. The high recall capacity of the models suggests a low misclassification rate of at-risk patients, which is crucial to prevent missed interventions by professionals. However, despite their potential in supporting suicidal ideation detection, these models have not been validated in a patient monitoring clinical setting. Therefore, caution is advised when using the evaluated models as tools to assist healthcare professionals in detecting suicidal ideation. © 2024 Fundacao Oswaldo Cruz. All rights reserved.",yes,"This study evaluates generative LLMs (Bard, Bing/GPT-4, ChatGPT-3.5) for detecting suicidal ideation in text, comparing them to BERT-based models. Although the dataset is not from a clinical setting, the task (identifying suicidal ideation) is directly relevant to healthcare and could inform clinical decision support. The study is not a review or opinion piece and directly assesses LLMs' performance in a health-related context.",10.1590/0102-311XEN028824,,,,,
Clinical Knowledge and Reasoning Abilities of AI Large Language Models in Pharmacy: A Comparative Study on the NAPLEX Exam,"Objective: This study aims to evaluate the capabilities and limitations of three large language models (LLMs) – GPT-3, GPT-4, and Bard, in the field of pharmaceutical sciences by assessing their pharmaceutical reasoning abilities on a sample North American Pharmacist Licensure Examination (NAPLEX). We also analyze the potential impacts of LLMs on pharmaceutical education and practice. Methods: A sample NAPLEX exam consisting of 137 multiple-choice questions was obtained from an online source. GPT-3, GPT-4, and Bard were used to answer the questions by inputting them into the LLMs’ user interface. The answers provided by the LLMs were then compared with the answer key. Results: GPT-4 exhibited superior performance compared to GPT-3 and Bard, answering 78.8% of the questions correctly. This score was 11% higher than Bard and 27.7% higher than GPT-3. However, when considering questions that required multiple selections, the performance of each LLM decreased significantly. GPT-4, GPT-3, and Bard only correctly answered 53.6%, 13.9%, and 21.4% of these questions, respectively. Conclusion: Among the three LLMs evaluated, GPT-4 was the only model capable of passing the NAPLEX exam. Nevertheless, given the continuous evolution of LLMs, it is reasonable to anticipate that future models will effortlessly pass the exam. This highlights the significant potential of LLMs to impact the pharmaceutical field. Hence, we must evaluate both the positive and negative implications associated with the integration of LLMs in pharmaceutical education and practice.",yes,"This study evaluates the clinical reasoning and knowledge abilities of LLMs (GPT-3, GPT-4, Bard) by testing their performance on the NAPLEX, a clinically relevant pharmacy licensure exam. This fits the inclusion criteria as it assesses LLMs in a clinical/medical context.",10.1101/2023.06.07.544055,,,,,
Currently Available Large Language Models Do Not Provide Musculoskeletal Treatment Recommendations That Are Concordant With Evidence-Based Clinical Practice Guidelines,"Purpose: To determine whether several leading, commercially available large language models (LLMs) provide treatment recommendations concordant with evidence-based clinical practice guidelines (CPGs) developed by the American Academy of Orthopaedic Surgeons (AAOS). Methods: All CPGs concerning the management of rotator cuff tears (n = 33) and anterior cruciate ligament injuries (n = 15) were extracted from the AAOS. Treatment recommendations from Chat-Generative Pretrained Transformer version 4 (ChatGPT-4), Gemini, Mistral-7B, and Claude-3 were graded by 2 blinded physicians as being concordant, discordant, or indeterminate (i.e., neutral response without definitive recommendation) with respect to AAOS CPGs. The overall concordance between LLM and AAOS recommendations was quantified, and the comparative overall concordance of recommendations among the 4 LLMs was evaluated through the Fisher exact test. Results: Overall, 135 responses (70.3%) were concordant, 43 (22.4%) were indeterminate, and 14 (7.3%) were discordant. Inter-rater reliability for concordance classification was excellent (κ = 0.92). Concordance with AAOS CPGs was most frequently observed with ChatGPT-4 (n = 38, 79.2%) and least frequently observed with Mistral-7B (n = 28, 58.3%). Indeterminate recommendations were most frequently observed with Mistral-7B (n = 17, 35.4%) and least frequently observed with Claude-3 (n = 8, 6.7%). Discordant recommendations were most frequently observed with Gemini (n = 6, 12.5%) and least frequently observed with ChatGPT-4 (n = 1, 2.1%). Overall, no statistically significant difference in concordant recommendations was observed across LLMs (P = .12). Of all recommendations, only 20 (10.4%) were transparent and provided references with full bibliographic details or links to specific peer-reviewed content to support recommendations. Conclusions: Among leading commercially available LLMs, more than 1-in-4 recommendations concerning the evaluation and management of rotator cuff and anterior cruciate ligament injuries do not reflect current evidence-based CPGs. Although ChatGPT-4 showed the highest performance, clinically significant rates of recommendations without concordance or supporting evidence were observed. Only 10% of responses by LLMs were transparent, precluding users from fully interpreting the sources from which recommendations were provided. Clinical Relevance: Although leading LLMs generally provide recommendations concordant with CPGs, a substantial error rate exists, and the proportion of recommendations that do not align with these CPGs suggests that LLMs are not trustworthy clinical support tools at this time. Each off-the-shelf, closed-source LLM has strengths and weaknesses. Future research should evaluate and compare multiple LLMs to avoid bias associated with narrow evaluation of few models as observed in the current literature. © 2024 Arthroscopy Association of North America",yes,"This study evaluates several leading LLMs (ChatGPT-4, Gemini, Mistral-7B, Claude-3) on their ability to provide musculoskeletal treatment recommendations in line with evidence-based clinical practice guidelines. The focus is on clinical decision support, directly assessing LLMs' clinical utility and accuracy in a healthcare context.",10.1016/j.arthro.2024.07.040,,,,,
An experimental study of integrating fine-tuned large language models and prompts for enhancing mental health support chatbot system,"Background: Conversational mental healthcare support plays a crucial role in aiding individuals with mental health concerns. Large language models (LLMs) like GPT and BERT show potential in enhancing chatbot-based therapy responses. Despite their potential, there are recognised limitations in directly deploying these LLMs for therapeutic interactions as they are trained in general context and knowledge data. The overarching aim of this study is to integrate the capabilities of both GPT and BERT with the use of specialised mental health dataset methodologies. Its goal is to enhance mental health conversations, limiting the risk and increasing quality. Methods: To achieve these aims, we will review existing chatbot methodologies from rule-based systems to advanced approaches based on cognitive behavioural therapy (CBT) principles. The study introduces a unique method which integrates a fine-tuned DialoGPT model along with the real-time capabilities of the ChatGPT 3.5 application programming interface (API). This blended combination aims to leverage the contextual awareness of LLMs and the precision of mental health-focused training. The evaluation involves a case study whereby our hybrid model is compared to traditional and standalone LLM-based chatbots. The performance is assessed using metrics such as perplexity and BLEU (Bilingual Evaluation Understudy) scores, along with subjective evaluations from end-users and mental health carers. Results: Our combined model outperforms others in conversational quality and relevance in mental healthcare. The positive feedback from patients and mental healthcare professionals is evidence of this. However, vital limitations highlight the need for further development in next-generation mental health support systems. Addressing these challenges is crucial for such technologies’ practical application and effectiveness. Conclusions: With the rise of digital mental health tools, integrating models such as LLMs transforms conversational support. The study presents a promising approach combining state-of-the-art LLMs with domain-specific fine-tuned model principles. Results suggest our combined model offers affordable and better everyday support, validated by positive feedback from patients and professionals. Our research emphasises the potential of LLMs and points towards shaping responsible and effective policies for chatbot deployment in mental healthcare. These findings will contribute to future mental healthcare chatbot development and policy guidelines, emphasising the need for balanced and effective integration of advanced models and traditional therapeutic principles. © Journal of Medical Artificial Intelligence. All rights reserved.",yes,"This study evaluates the integration of fine-tuned LLMs (DialoGPT and ChatGPT 3.5) for use in a mental health support chatbot, with performance assessed through both objective metrics and subjective feedback from patients and mental health professionals. The focus is on clinical conversational support, making it a direct clinical application of LLMs in healthcare.",10.21037/jmai-23-136,,,,,
ChatGPT: Artificial Intelligence as a Potential Tool for Parents Seeking Information About Autism,"Autism Spectrum Disorder has seen a drastic increase in prevalence over the past two decades, along with discourse rife with debates and misinformation. This discourse has primarily taken place online, the main source of information for parents seeking information about autism. One potential tool for navigating information is ChatGPT-4, an artificial intelligence question and answer-style communication program. Although ChatGPT shows great promise, no empirical work has evaluated its viability as a tool for providing information about autism to caregivers. The current study evaluated answers provided by ChatGPT, including basic information about autism, myths/misconceptions, and resources. Our results suggested that ChatGPT was largely correct, concise, and clear, but did not provide much actionable advice, which was further limited by inaccurate references and hyperlinks. The authors conclude that ChatGPT-4 is a viable tool for parents seeking accurate information about autism, with opportunities for improvement in actionability and reference accuracy.",yes,"This study empirically evaluates ChatGPT-4's ability to answer parent questions about autism, including accuracy and clarity of information provided. The focus is on clinical information provision to caregivers, which is a relevant healthcare application.",10.1089/cyber.2023.0202,,,,,
"Evaluating Artificial Intelligence-Driven Responses to Acute Liver Failure Queries: A Comparative Analysis Across Accuracy, Clarity, and Relevance","Introduction: Recent advancements in Artificial Intelligence (AI), particularly through the deployment of Large Language Models (LLMs), have profoundly impacted healthcare. This study assesses five LLMs—ChatGPT 3.5, ChatGPT 4, BARD, CLAUDE, and COPILOT—on their response accuracy, clarity, and relevance to queries concerning acute liver failure (ALF). We subsequently compare these results with Chat GPT4 enhanced with Retrieval Augmented Generation (RAG) technology. Methods: Based on real-world clinical use and the American College of Gastroenterology guidelines, we formulated 16 ALF questions or clinical scenarios to explore LLMs' ability to handle different clinical questions. Using the ""New Chat"" functionality, each query was processed individually across the models to reduce any bias. Additionally, we employed the RAG functionality of GPT-4, which integrates external sources as references to ground the results. All responses were evaluated on a Likert scale from 1 to 5 for accuracy, clarity, and relevance by four independent investigators to ensure impartiality. Result: ChatGPT 4, augmented with RAG, demonstrated superior performance compared to others, consistently scoring the highest (4.70, 4.89, 4.78) across all three domains. ChatGPT 4 exhibited notable proficiency, with scores of 3.67 in accuracy, 4.04 in clarity, and 4.01 in relevance. In contrast, CLAUDE achieved 3.04 in clarity, 3.6 in relevance, and 3.65 in accuracy. Meanwhile, BARD and COPILOT exhibited lower performance levels; BARD recorded scores of 2.01 in accuracy and 3.03 in relevance, while COPILOT obtained 2.26 in accuracy and 3.12 in relevance. Conclusion: The study highlights Chat GPT 4 +RAG's superior performance compared to other LLMs. By integrating RAG with LLMs, the system combines generative language skills with accurate, up-to-date information. This improves response clarity, relevance, and accuracy, making them more effective in healthcare. However, AI models must continually evolve and align with medical practices for successful healthcare integration. Copyright © 2024 by The American College of Gastroenterology.",yes,"This study directly evaluates multiple generative LLMs (ChatGPT 3.5, ChatGPT 4, Bard, Claude, Copilot, and ChatGPT 4 + RAG) on their ability to answer clinical questions about acute liver failure, using real-world clinical scenarios and established guidelines. The focus is on clinical accuracy, clarity, and relevance, which aligns with the inclusion criteria for clinical evaluation of LLMs in healthcare.",10.14309/ajg.0000000000003255,,,,,
Evaluating LLMs for Diagnosis Summarization,"During a patient's hospitalization, extensive information is documented in clinical notes. The efficient summarization of this information is vital for keeping healthcare professionals abreast of the patient's status. This paper proposes a methodology to assess the efficacy of six large language models (LLMs) in automating the task of diagnosis summarization, particularly in discharge summaries. Our approach involves defining an automatic metric based on LLMs, highly correlated with human assessments. We evaluate the performance of the six models using the F1-Score and compare the results with those of healthcare specialists. The experiments reveal that there is room for improvement in the medical knowledge and diagnostic capabilities of LLMs. The source code and data for these experiments are available on the project's GitHub page. © 2024 IEEE.",yes,"This study evaluates the performance of six LLMs in automating diagnosis summarization in discharge summaries, a clinically relevant task that supports healthcare professionals. The focus is on the clinical application of LLMs for summarizing diagnoses, which is directly related to patient care and clinical workflow.",10.1109/EMBC53108.2024.10782231,,,,,
The role of artificial intelligence in gynecologic and obstetric emergencies,"Objective: To investigate the potential of artificial intelligence (AI) in emergency medicine, focusing on its utility in triaging and managing acute gynecologic and obstetric emergencies. Methods and Materials: This feasibility study assessed Chat-GPT's performance in triaging and recommending management interventions for gynecologic and obstetric emergencies, using ten fictive cases. Five common conditions were modeled for each specialty. Chat-GPT was tasked with proposing triage classifications and providing immediate management recommendations. Human experts independently reviewed each case, classified triage categories, and proposed management. Following this, experts evaluated Chat-GPT's recommendations, rating the AI's responses on accuracy and clinical applicability. Results: Chat-GPT's recommendations demonstrated high concordance with human evaluators. Chat-GPT's triage classifications matched those of human experts in most cases, though minor discrepancies in urgency ratings were observed. The AÍs suggestions were mostly rated as “very good” to “excellent.” While Chat-GPT consistently delivered appropriate responses, some human evaluators noted slight differences in perceived urgency. Conclusions: This study highlights Chat-GPT's potential as a clinical support tool in emergency medicine. Chat-GPT provided structured, evidence-based recommendations comparable to those of experienced clinicians, especially for high-stakes gynecologic and obstetric emergencies. Although encouraging, these results highlight the value of utilizing AI in addition to human knowledge, as variations in urgency ratings and management nuances highlight the necessity of human supervision in crucial decision-making. © 2025",yes,"This study directly evaluates ChatGPT, a generative large language model, in the clinical context of triaging and managing gynecologic and obstetric emergencies. It assesses the LLM's performance compared to human experts, focusing on clinical decision-making and recommendations. This fits the inclusion criteria.",10.1016/j.ejogrb.2025.01.007,,,,,
Google Bard: Utility in Drug Interactions,Emergency medicine is a busy department with a huge flow of patients with multiple complaints. The department caters to a large cohort of patients simultaneously as well as administers many drugs. This can lead to potential drug interactions. With the advent of artificial intelligence (AI) and language learning model (LLM) we can use these AI enabled programs to help us be cautious yet judicious in our drug delivery and also be a careful vigilante to prevent drug interactions in these patients. © 2023 Jacob.,yes,"The study discusses the potential utility of Google Bard (an LLM) in identifying drug interactions in an emergency medicine context. While the abstract is brief and does not detail a specific evaluation, it implies a clinical application of an LLM for drug interaction vigilance. If the full text contains an actual evaluation or test of Bard's performance in this clinical task, it would be included. Based on the title and abstract, it appears to fit the inclusion criteria.",10.5937/scriptamed54-45110,,,,,
ChatGPT and Bard in Plastic Surgery: Hype or Hope?,"Online artificial intelligence (AI) tools have recently gained in popularity. So-called “generative AI” chatbots unlock new opportunities to access vast realms of knowledge when being prompted by users. Here, we test the capabilities of two such AIs in order to determine the benefits for plastic surgery while also assessing the potential risks. Future developments are outlined. We used the online portals of OpenAI’s ChatGPT (version 3.5) and Google’s Bard to ask a set of questions and give specific commands. The results provided by the two tools were compared and analyzed by a committee. For professional plastic surgeons, we found that ChatGPT and Bard can be of help when it comes to conducting scientific reviews and helping with scientific writing but are of limited use due to the superficiality of their answers in specific domains. For medical students, in addition to the above, they provide useful educational material with respect to surgical methods and exam preparation. For patients, they can help when it comes to preparing for an intervention, weighing the risks and benefits, while providing guidance on optimal post-operative care. ChatGPT and Bard open widely accessible data to every internet user. While they might create a sense of “magic” due to their chatbot interfaces, they nonetheless can help to increase productivity. For professional surgeons, they produce superficial answers—for now—albeit providing help with scientific writing and literature reviews. For medical students, they are great tools to deepen their knowledge about specific topics such as surgical methods and exam preparation. For patients, they can help in translating complicated medical jargon into understandable lingo and provide support for pre-operative as well as post-operative care. Such AI tools should be used cautiously, as their answers are not always precise or accurate, and should always be used in combination with expert medical guidance. © 2024 by the authors.",yes,"This study evaluates ChatGPT and Bard in several contexts, including providing educational material for medical students, answering patient questions about pre- and post-operative care, and supporting professional surgeons. While some of the focus is on non-clinical tasks (e.g., scientific writing), the abstract clearly states that the LLMs were tested for their ability to help patients prepare for interventions, weigh risks and benefits, and provide post-operative guidance—direct clinical applications. Therefore, it meets the inclusion criteria.",10.3390/surgeries5010006,,,,,
Surgeons vs ChatGPT: Assessment and Feedback Performance Based on Real Surgical Scenarios,"INTRODUCTION: Artificial intelligence tools are being progressively integrated into medicine and surgical education. Large language models, such as ChatGPT, could provide relevant feedback aimed at improving surgical skills. The purpose of this study is to assess ChatGPT´s ability to provide feedback based on surgical scenarios. METHODS: Surgical situations were transformed into texts using a neutral narrative. Texts were evaluated by ChatGPT 4.0 and 3 surgeons (A, B, C) after a brief instruction was delivered: identify errors and provide feedback accordingly. Surgical residents were provided with each of the situations and feedback obtained during the first stage, as written by each surgeon and ChatGPT, and were asked to assess the utility of feedback (FCUR) and its quality (FQ). As control measurement, an Education-Expert (EE) and a Clinical-Expert (CE) were asked to assess FCUR and FQ. RESULTS: Regarding residents’ evaluations, 96.43% of times, outputs provided by ChatGPT were considered useful, comparable to what surgeons’ B and C obtained. Assessing FQ, ChatGPT and all surgeons received similar scores. Regarding EE's assessment, ChatGPT obtained a significantly higher FQ score when compared to surgeons A and B (p = 0.019; p = 0.033) with a median score of 8 vs. 7 and 7.5, respectively; and no difference respect surgeon C (score of 8; p = 0.2). Regarding CE´s assessment, surgeon B obtained the highest FQ score while ChatGPT received scores comparable to that of surgeons A and C. When participants were asked to identify the source of the feedback, residents, CE, and EE perceived ChatGPT's outputs as human-provided in 33.9%, 28.5%, and 14.3% of cases, respectively. CONCLUSION: When given brief written surgical situations, ChatGPT was able to identify errors with a detection rate comparable to that of experienced surgeons and to generate feedback that was considered useful for skill improvement in a surgical context performing as well as surgical instructors across assessments made by general surgery residents, an experienced surgeon, and a nonsurgeon feedback expert. © 2024 Association of Program Directors in Surgery",yes,"This study evaluates ChatGPT's ability to provide feedback on surgical scenarios, comparing its performance to that of human surgeons. The feedback is assessed for utility and quality by surgical residents and experts, directly relating to clinical education and decision support. The use of LLMs in a clinical training context fits the inclusion criteria.",10.1016/j.jsurg.2024.03.012,,,,,
Are artificial intelligence large language models a reliable tool for difficult differential diagnosis? An a posteriori analysis of a peculiar case of necrotizing otitis externa,"Key Clinical Message: Large language models have made artificial intelligence readily available to the general public and potentially have a role in healthcare; however, their use in difficult differential diagnosis is still limited, as demonstrated by a case of necrotizing otitis externa. Abstract: This case report presents a peculiar case of necrotizing otitis externa (NOE) with skull base involvement which proved diagnostically challenging. The initial patient presentation and the imaging performed on the 78-year-old patient suggested a neoplastic rhinopharyngeal lesion and only after several unsuccessful biopsies the patient was transferred to our unit. Upon re-evaluation of the clinical picture, a clinical hypothesis of NOE with skull base erosion was made and confirmed by identifying Pseudomonas aeruginosa in biopsy specimens of skull base bone and external auditory canal skin. Upon diagnosis confirmation, the patient was treated with culture-oriented long-term antibiotics with complete resolution of the disease. Given the complex clinical presentation, we chose to submit a posteriori this NOE case to two large language models (LLM) to test their ability to handle difficult differential diagnoses. LLMs are easily approachable artificial intelligence tools that enable human-like interaction with the user relying upon large information databases for analyzing queries. The LLMs of choice were ChatGPT-3 and ChatGPT-4 and they were requested to analyze the case being provided with only objective clinical and imaging data.",yes,"This study evaluates the ability of two LLMs (ChatGPT-3 and ChatGPT-4) to handle a difficult clinical differential diagnosis by providing them with real patient data from a complex case. The focus is on the clinical application of LLMs in diagnosis, which fits the inclusion criteria.",10.1002/ccr3.7933,,,,,
Enhancing Large Language Models with Human Expertise for Disease Detection in Electronic Health Records,"Electronic health records (EHR) are widely available to complement administrative data-based disease surveillance and healthcare performance evaluation. Defining conditions from EHR is labour-intensive, requiring advanced medical informatics knowledge. We linked a cardiac registry cohort in 2015 with an EHR system in a city in Canada. We developed a throughput pipeline that leverages a generative large language model (LLM) to analyze, understand, and interpret EHR notes through clinical experts' designed prompts and rules. The pipeline was applied to detect diabetes, hypertension, and acute myocardial infarction from the notes. The performance was compared against clinician-validated diagnoses as the reference standard.  © 2024 IEEE.",yes,"This study uses a generative LLM to analyze and interpret EHR notes for disease detection (diabetes, hypertension, acute myocardial infarction), comparing its performance to clinician-validated diagnoses. This is a direct clinical application of LLMs in disease detection.",10.1109/ICDH62654.2024.00031,,,,,
Large Language Models Improve Alzheimer's Disease Diagnosis Using Multi-Modality Data,"In diagnosing challenging conditions such as Alzheimer's disease (AD), while imaging is an important reference, non-image data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use image data, and cannot make comprehensive use of non-image data. We based our approach on an existing pretrained large language model (LLM) to enhance the model's ability to utilize non-image data on diagnosing AD. Our test results on the ADNI dataset show that the approached achieved state-of-the-art (SOTA) performance.  © 2023 IEEE.",yes,"This study uses a pretrained large language model (LLM) to improve Alzheimer's disease diagnosis by integrating non-image data (e.g., patient info, genetics, cognitive tests) with imaging. The focus is on clinical diagnosis, and the LLM is applied in a clinical context.",10.1109/MedAI59581.2023.00016,,,,,
Step into the era of large multimodal models: a pilot study on ChatGPT-4V(ision)'s ability to interpret radiological images,"BACKGROUND: The introduction of ChatGPT-4V's 'Chat with images' feature represents the beginning of the era of large multimodal models (LMMs), which allows ChatGPT to process and answer questions based on uploaded images. This advancement has the potential to transform how surgical teams utilize radiographic data, as radiological interpretation is crucial for surgical planning and postoperative care. However, a comprehensive evaluation of ChatGPT-4V's capabilities in interpret radiological images and formulating treatment plans remains to be explored. PATIENTS AND METHODS: Three types of questions were collected: (1) 87 USMLE-style questions, submitting only the question stems and images without providing options to assess ChatGPT's diagnostic capability. For questions involving treatment plan formulations, a five-point Likert scale was used to assess ChatGPT's proposed treatment plan. The 87 questions were then adapted by removing detailed patient history to assess its contribution to diagnosis. The diagnostic performance of ChatGPT-4V was also tested when only medical history was provided. (2) We randomly selected 100 chest radiography from the ChestX-ray8 database to test the ability of ChatGPT-4V to identify abnormal chest radiography. (3) Cases from the 'Diagnose Please' section in the Radiology journal were collected to evaluate the performance of ChatGPT-4V in diagnosing complex cases. Three responses were collected for each question. RESULTS: ChatGPT-4V achieved a diagnostic accuracy of 77.01% for USMLE-style questions. The average score of ChatGPT-4V's treatment plans was 3.97 (Interquartile Range: 3.33-4.67). Removing detailed patient history dropped the diagnostic accuracy to 19.54% (P<0.0001). ChatGPT-4V achieved an AUC of 0.768 (95% CI: 0.684-0.851) in detecting abnormalities in chest radiography, but could not specify the exact disease due to the lack of detailed patient history. For cases from 'Diagnose Please' ChatGPT provided diagnoses consistent with or very similar to the reference answers. CONCLUSION: ChatGPT-4V demonstrated an impressive ability to combine patient history with radiological images to make diagnoses and directly design treatment plans based on images, suggesting its potential for future application in clinical practice. Copyright © 2024 The Author(s). Published by Wolters Kluwer Health, Inc.",yes,"This study evaluates ChatGPT-4V(ision), a large multimodal language model, in interpreting radiological images and formulating treatment plans—a direct clinical application. The study uses USMLE-style questions, real chest radiographs, and complex clinical cases to assess diagnostic and treatment planning performance, which fits the inclusion criteria.",10.1097/JS9.0000000000001359,,,,,
Assessing the Soft Tissue Infection Expertise of ChatGPT and Bard Compared to IDSA Recommendations,"The aim of the study was to evaluate whether ChatGPT-3.5 and Bard provide safe and reliable medical answers to common topics related to soft tissue infections and their management according to the guidelines provided by the Infectious Disease Society of America (IDSA). IDSA’s abridged recommendations for soft tissue infections were identified on the IDSA official website. Twenty-five queries were entered into the LLMs as they appear on the IDSA website. To assess the concordance and precision of the LLMs’ responses with the IDSA guidelines, two infectious disease physicians independently compared and evaluated each response. This was done using a 5-point Likert scale, with 1 representing poor concordance and 5 excellent concordance, as adapted from the validated Global Quality Scale. The mean ± SD score for ChatGPT-generated responses was 4.34 ± 0.74, n = 25. This indicates that raters found the answers were good to excellent quality with the most important topics covered. Although some topics were not covered, the answers were in good concordance with the IDSA guidelines. The mean ± SD score for Bard-generate responses was 3.5 ± 1.2, n = 25, indicating moderate quality. Despite LLMs did not appear to provide wrong recommendations and covered most of the topics, the responses were often found to be generic, rambling, missing some details, and lacking actionability. As AI continues to evolve and researchers feed it with more extensive and diverse medical knowledge, it may be inching closer to becoming a reliable aid for clinicians, ultimately enhancing the accuracy of infectious disease diagnosis and management in the future.",yes,"This study directly evaluates the clinical performance of two LLMs (ChatGPT-3.5 and Bard) in answering questions about soft tissue infections, comparing their responses to established IDSA guidelines. The focus is on clinical accuracy, safety, and reliability in a medical context, which fits the inclusion criteria.",10.1007/s10439-023-03372-1,,,,,
Evaluating ChatGPT for neurocognitive disorder diagnosis: a multicenter study,"Objective: To evaluate the accuracy and reliability of ChatGPT 4 Omni in diagnosing neurocognitive disorders using comprehensive clinical data and compare its performance to previous versions of ChatGPT. Method: This project utilized a two-part design: Study 1 examined diagnostic agreement between ChatGPT 4 Omni and clinicians using a few-shot prompt approach, and Study 2 compared the diagnostic performance of ChatGPT models using a zero-shot prompt approach using data from the National Alzheimer’s Coordinating Center (NACC) Uniform Data Set 3. Study 1 included 12,922 older adults (Mage = 69.13, SD = 9.87), predominantly female (57%) and White (80%). Study 2 involved 537 older adults (Mage = 67.88, SD = 9.52), majority female (57%) and White (81%). Diagnoses included no cognitive impairment, amnestic mild cognitive impairment (MCI), nonamnestic MCI, and dementia. Results: In Study 1, ChatGPT 4 Omni showed fair association with clinician diagnoses (χ2 (9) = 6021.96, p <.001; κ =.33). Notable predictive measures of agreement included the MoCA and memory recall tests. ChatGPT 4 Omni demonstrated high internal reliability (α =.96). In Study 2, no significant diagnostic agreement was found between ChatGPT versions and clinicians. Conclusions: Although ChatGPT 4 Omni shows potential in aligning with clinician diagnoses, its diagnostic accuracy is insufficient for clinical application without human oversight. Continued refinement and comprehensive training of AI models are essential to enhance their utility in neuropsychological assessment. With rapidly developing technological innovations, integrating AI tools in clinical practice could soon improve diagnostic efficiency and accessibility to neuropsychological services. © 2025 Informa UK Limited, trading as Taylor & Francis Group.",yes,"This study directly evaluates the diagnostic performance of ChatGPT (specifically ChatGPT 4 Omni) in diagnosing neurocognitive disorders using real clinical data, comparing its results to clinicians and previous ChatGPT versions. The focus is on clinical application (diagnosis), not technical development or non-clinical tasks.",10.1080/13854046.2025.2475567,,,,,
Dr. ChatGPT: Utilizing Artificial Intelligence in Surgical Education,"Objective: This study sought to explore the unexamined capabilities of ChatGPT in describing the surgical steps of a specialized operation, the Fisher cleft lip repair. Design: A chat log within ChatGPT was created to generate the procedural steps of a cleft lip repair utilizing the Fisher technique. A board certified craniomaxillofacial (CMF) surgeon then wrote the Fisher repair in his own words blinded to the ChatGPT response. Using both responses, a voluntary survey questionnaire was distributed to residents of plastic and reconstructive surgery (PRS), general surgery (GS), internal medicine (IM), and medical students at our institution in a blinded study. Setting: Authors collected information from residents (PRS, GS, IM) and medical students at one institution. Main Outcome Measures: Primary outcome measures included understanding, preference, and author identification of the procedural prompts. Results: Results show PRS residents were able to detect more inaccuracies of the ChatGPT response as well as prefer the CMF surgeon's prompt in performing the surgery. Residents with less expertise in the procedure not only failed to detect who wrote what procedure, but preferred the ChatGPT response in explaining the concept and chose it to perform the surgery. Conclusions: In applications to surgical education, ChatGPT was found to be effective in generating easy to understand procedural steps that can be followed by medical personnel of all specialties. However, it does not have expert capabilities to provide the minute detail of measurements and specific anatomy required to perform medical procedures. © 2023, American Cleft Palate Craniofacial Association.",yes,"This study evaluates ChatGPT's ability to generate surgical procedural steps and compares its output to that of a board-certified surgeon, with medical trainees assessing the responses. The focus is on ChatGPT's utility in surgical education, specifically in describing clinical procedures. This fits the inclusion criteria as it assesses a generative LLM (ChatGPT) in a clinical education context relevant to medicine.",10.1177/10556656231193966,,,,,
Large language models as a diagnostic support tool in neuropathology,"The WHO guidelines for classifying central nervous system (CNS) tumours are changing considerably with each release. The classification of CNS tumours is uniquely complex among most other solid tumours as it incorporates not just morphology, but also genetic and epigenetic features. Keeping current with these changes across medical fields can be challenging, even for clinical specialists. Large language models (LLMs) have demonstrated their ability to parse and process complex medical text, but their utility in neuro-oncology has not been systematically tested. We hypothesised that LLMs can effectively diagnose neuro-oncology cases from free-text histopathology reports according to the latest WHO guidelines. To test this hypothesis, we evaluated the performance of ChatGPT-4o, Claude-3.5-sonnet, and Llama3 across 30 challenging neuropathology cases, which each presented a complex mix of morphological and genetic information relevant to the diagnosis. Furthermore, we integrated these models with the latest WHO guidelines through Retrieval-Augmented Generation (RAG) and again assessed their diagnostic accuracy. Our data show that LLMs equipped with RAG, but not without RAG, can accurately diagnose the neuropathological tumour subtype in 90% of the tested cases. This study lays the groundwork for a new generation of computational tools that can assist neuropathologists in their daily reporting practice. © 2024 The Author(s). The Journal of Pathology: Clinical Research published by The Pathological Society of Great Britain and Ireland and John Wiley & Sons Ltd.",yes,"This study directly evaluates the clinical performance of multiple frontier LLMs (ChatGPT-4o, Claude-3.5-sonnet, Llama3) in diagnosing neuropathology cases from real-world histopathology reports, using up-to-date WHO guidelines. The focus is on clinical diagnostic support, which fits the inclusion criteria.",10.1002/2056-4538.70009,,,,,
Caution! AI Bot Has Entered the Patient Chat: ChatGPT Has Limitations in Providing Accurate Urologic Healthcare Advice,"Objective: To conduct the first study examining the accuracy of ChatGPT, an artificial intelligence (AI) chatbot, derived patient counseling responses based on clinical care guidelines in urology using a validated questionnaire. Methods: We asked ChatGPT a set of 13 urological guideline-based questions three times. Answers were evaluated for appropriateness and using Brief DISCERN (BD), a validated healthcare information assessment questionnaire. Data analysis included descriptive statistics and Student's t test (SAS Studio). Results: 60% (115/195) of ChatGPT responses were deemed appropriate. Variability existed between responses to the same prompt, with 25% of the 13 question sets having discordant appropriateness designations. The average BD score was 16.8 ± 3.59. Only 7 (54%) of 13 topics and 21 (54%) of 39 responses met the BD cut-off score of ≥16 to denote good-quality content. Appropriateness was associated with higher overall and Relevance domain scores (both P < .01). The lowest BD domain scores were for Source categories, since ChatGPT does not provide references by default. With prompting, 92.3% had ≥1 incorrect, misinterpreted, or nonfunctional citations. Conclusion: While ChatGPT provides appropriate responses to urological questions more than half of the time, it misinterprets clinical care guidelines, dismisses important contextual information, conceals its sources, and provides inappropriate references. Chatbot models hold great promise, but users should be cautious when interpreting healthcare-related advice from existing AI models. Additional training and modifications are needed before these AI models will be ready for reliable use by patients and providers. © 2023 Elsevier Inc.",yes,"This study evaluates ChatGPT's accuracy in providing patient counseling responses based on urological clinical care guidelines, using a validated questionnaire. It directly assesses the clinical utility and reliability of an LLM in a healthcare context.",10.1016/j.urology.2023.07.010,,,,,
Exploring AI-chatbots’ capability to suggest surgical planning in ophthalmology: ChatGPT versus Google Gemini analysis of retinal detachment cases,"Background We aimed to define the capability of three different publicly available large language models, Chat Generative Pretrained Transformer (ChatGPT-3.5), ChatGPT-4 and Google Gemini in analysing retinal detachment cases and suggesting the best possible surgical planning. Methods Analysis of 54 retinal detachments records entered into ChatGPT and Gemini’s interfaces. After asking’Specify what kind of surgical planning you would suggest and the eventual intraocular tamponade.’ and collecting the given answers, we assessed the level of agreement with the common opinion of three expert vitreoretinal surgeons. Moreover, ChatGPT and Gemini answers were graded 1–5 (from poor to excellent quality), according to the Global Quality Score (GQS). Results After excluding 4 controversial cases, 50 cases were included. Overall, ChatGPT-3.5, ChatGPT-4 and Google Gemini surgical choices agreed with those of vitreoretinal surgeons in 40/50 (80%), 42/50 (84%) and 35/50 (70%) of cases. Google Gemini was not able to respond in five cases. Contingency analysis showed significant differences between ChatGPT-4 and Gemini (p=0.03). ChatGPT’s GQS were 3.9±0.8 and 4.2±0.7 for versions 3.5 and 4, while Gemini scored 3.5±1.1. There was no statistical difference between the two ChatGPTs (p=0.22), while both outperformed Gemini scores (p=0.03 and p=0.002, respectively). The main source of error was endotamponade choice (14% for ChatGPT-3.5 and 4, and 12% for Google Gemini). Only ChatGPT-4 was able to suggest a combined phacovitrectomy approach. Conclusion In conclusion, Google Gemini and ChatGPT evaluated vitreoretinal patients’ records in a coherent manner, showing a good level of agreement with expert surgeons. According to the GQS, ChatGPT’s recommendations were much more accurate and precise. © Author(s) (or their employer(s)) 2024.",yes,"This study directly evaluates the clinical performance of LLMs (ChatGPT-3.5, ChatGPT-4, and Google Gemini) in suggesting surgical planning for retinal detachment cases, comparing their recommendations to those of expert surgeons. It uses real clinical cases and assesses agreement and quality of recommendations, which fits the inclusion criteria.",10.1136/bjo-2023-325143,,,,,
Performance of ChatGPT on Responding to Common Online Questions Regarding Key Information Gaps in Glaucoma,"2803. J Glaucoma. 2024 Jul 1;33(7):e54-e56. doi: 10.1097/IJG.0000000000002409. Epub 
2024 Apr 23.

Performance of ChatGPT on Responding to Common Online Questions Regarding Key 
Information Gaps in Glaucoma.

Wu JH(1), Nishida T, Moghimi S, Weinreb RN.

Author information:
(1)Hamilton Glaucoma Center, Shiley Eye Institute, Viterbi Family Department of 
Ophthalmology, University of California, San Diego, La Jolla, CA.",yes,"This study evaluates ChatGPT's performance in answering common online questions about glaucoma, which is a clinical application involving patient information and education. It directly assesses the LLM's ability to address key information gaps in a medical context.",10.1097/IJG.0000000000002409,,,,,
Exploring the Potential of Claude 3 Opus in Renal Pathological Diagnosis: Performance Evaluation,"Background: Artificial intelligence (AI) has shown great promise in assisting medical diagnosis, but its application in renal pathology remains limited. Objective: We evaluated the performance of an advanced AI language model, Claude 3 Opus (Anthropic), in generating diagnostic descriptions for renal pathological images. Methods: We carefully curated a dataset of 100 representative renal pathological images from the Diagnostic Atlas of Renal Pathology (3rd edition). The image selection aimed to cover a wide spectrum of common renal diseases, ensuring a balanced and comprehensive dataset. Claude 3 Opus generated diagnostic descriptions for each image, which were scored by 2 pathologists on clinical relevance, accuracy, fluency, completeness, and overall value. Results: Claude 3 Opus achieved a high mean score in language fluency (3.86) but lower scores in clinical relevance (1.75), accuracy (1.55), completeness (2.01), and overall value (1.75). Performance varied across disease types. Interrater agreement was substantial for relevance (κ=0.627) and overall value (κ=0.589) and moderate for accuracy (κ=0.485) and completeness (κ=0.458). Conclusions: Claude 3 Opus shows potential in generating fluent renal pathology descriptions but needs improvement in accuracy and clinical value. The AI’s performance varied across disease types. Addressing the limitations of single-source data and incorporating comparative analyses with other AI approaches are essential steps for future research. Further optimization and validation are needed for clinical applications. ©Xingyuan Li, Ke Liu, Yanlin Lang, Zhonglin Chai, Fang Liu.",yes,"This study evaluates the performance of the Claude 3 Opus LLM in generating diagnostic descriptions for renal pathology images, with clinical relevance and accuracy assessed by pathologists. The use of a frontier LLM (Claude 3 Opus) for a clinical diagnostic task (renal pathology) fits the inclusion criteria.",10.2196/65033,,,,,
Assessing the performance of ChatGPT in bioethics: a large language model's moral compass in medicine,"Chat Generative Pre-Trained Transformer (ChatGPT) has been a growing point of interest in medical education yet has not been assessed in the field of bioethics. This study evaluated the accuracy of ChatGPT-3.5 (April 2023 version) in answering text-based, multiple choice bioethics questions at the level of US third-year and fourth-year medical students. A total of 114 bioethical questions were identified from the widely utilised question banks UWorld and AMBOSS. Accuracy, bioethical categories, difficulty levels, specialty data, error analysis and character count were analysed. We found that ChatGPT had an accuracy of 59.6%, with greater accuracy in topics surrounding death and patient-physician relationships and performed poorly on questions pertaining to informed consent. Of all the specialties, it performed best in paediatrics. Yet, certain specialties and bioethical categories were under-represented. Among the errors made, it tended towards content errors and application errors. There were no significant associations between character count and accuracy. Nevertheless, this investigation contributes to the ongoing dialogue on artificial intelligence's (AI) role in healthcare and medical education, advocating for further research to fully understand AI systems' capabilities and constraints in the nuanced field of medical bioethics.  © 2023 BMJ Publishing Group. All rights reserved.",yes,"This study evaluates ChatGPT-3.5's performance on bioethics questions at the level of medical students, using clinical question banks. The focus is on the LLM's ability to answer questions relevant to medical education and clinical decision-making in bioethics, which is a clinically relevant domain. The study is not a review, editorial, or technical report, and it directly assesses the LLM's clinical reasoning in a medical context.",10.1136/jme-2023-109366,,,,,
Quality of the Information provided by ChatGPT for Patients in Breast Plastic Surgery: Are we already in the future?,"Introduction: In recent years, artificial intelligence (AI) has gained popularity, even in the field of plastic surgery. It is increasingly common for patients to use the internet to gather information about plastic surgery, and AI-based chatbots, such as ChatGPT, could be employed to answer patients' questions. The aim of this study was to evaluate the quality of medical information provided by ChatGPT regarding three of the most common procedures in breast plastic surgery: breast reconstruction, breast reduction, and augmentation mammaplasty. Methods: The quality of information was evaluated through the expanded EQIP scale. Responses were collected from a pool made by ten resident doctors in plastic surgery and then processed by SPSS software ver. 28.0. Results: The analysis of the contents provided by ChatGPT revealed sufficient quality of information across all selected topics, with a high bias in terms of distribution of the score between the different items. There was a critical lack in the “Information data field” (0/6 score in all the 3 investigations) but a very high overall evaluation concerning the “Structure data” (>7/11 in all the 3 investigations). Conclusion: Currently, AI serves as a valuable tool for patients; however, engineers and developers must address certain critical issues. It is possible that models like ChatGPT will play an important role in improving patient's consciousness about medical procedures and surgical interventions in the future, but their role must be considered ancillary to that of surgeons. © 2024 The Author(s)",yes,"This study evaluates ChatGPT's ability to provide medical information to patients about breast plastic surgery procedures, assessing the quality of its responses to patient questions. This is a direct clinical application of an LLM in a healthcare context, specifically patient education and information provision.",10.1016/j.jpra.2024.02.001,,,,,
Evaluating the Diagnostic and Treatment Recommendation Capabilities of GPT-4 Vision in Dermatology,"Background: The integration of artificial intelligence (AI) in dermatology presents a promising frontier for enhancing diagnostic accuracy and treatment planning. However, general purpose AI models require rigorous evaluation before being applied to real-world medical cases. Objective: This project specifically evaluates GPT-4V's performance in accurately diagnosing and generating treatment plans for common dermatological conditions, comparing its assessment of textual versus image data and its performance with multimodal inputs. Beyond the immediate scope, this study contributes to the broader trajectory of integrating AI in healthcare, highlighting the limitations of these technologies, as well as their potential to enhance efficiency, and education within medical training and practice. Methods: A dataset of 102 images representing nine common dermatological conditions was compiled from open-access websites. Fifty-four images were ultimately selected by two board-certified dermatologists as being representative and typical of the common conditions. Additionally, nine clinical scenarios corresponding to these conditions were developed. GPT-4V's diagnostic capabilities were assessed in three setups: Image Prompt (image-based), Scenario Prompt (text-based), and Image and Scenario Prompt (combining both modalities). The model's performance was evaluated based on diagnostic accuracy, differential diagnosis, and treatment recommendations. Results: In the Image Prompt setup, GPT-4V correctly identified the primary diagnosis for 29 of 54 images. The Scenario Prompt setup showed a higher accuracy rate of 89% in identifying the primary diagnosis. The multimodal Image and Scenario Prompt setup also achieved an 89% accuracy rate. However, a notable bias towards textual data over visual data was observed. Treatment recommendations were evaluated by the same two dermatologists, using a modified Entrustment Scale, showing competent but not expert-level performance. Conclusion: GPT-4V demonstrates promising capabilities in dermatological diagnosis and treatment recommendations, particularly in text-based scenarios. However, its performance in image-based diagnosis and integration of multimodal data highlights areas for improvement. The study underscores the potential of AI in augmenting dermatological practice, emphasizing the need for further development, and fine-tuning of such models to ensure their efficacy and reliability in clinical settings.",yes,"This study directly evaluates GPT-4 Vision (GPT-4V), a frontier LLM, in the clinical context of dermatology by assessing its ability to diagnose and recommend treatments for real dermatological cases using both text and image inputs. The evaluation is clinical, focusing on diagnosis and treatment, and uses real-world scenarios and images. This fits the inclusion criteria perfectly.",10.1101/2024.01.24.24301743,,,,,
Prompt engineering on leveraging large language models in generating response to InBasket messages,"Objectives: Large Language Models (LLMs) have been proposed as a solution to address high volumes of Patient Medical Advice Requests (PMARs). This study addresses whether LLMs can generate high quality draft responses to PMARs that satisfies both patients and clinicians with prompt engineering. Materials and Methods: We designed a novel human-involved iterative processes to train and validate prompts to LLM in creating appropriate responses to PMARs. GPT-4 was used to generate response to the messages. We updated the prompts, and evaluated both clinician and patient acceptance of LLM-generated draft responses at each iteration, and tested the optimized prompt on independent validation data sets. The optimized prompt was implemented in the electronic health record production environment and tested by 69 primary care clinicians. Results: After 3 iterations of prompt engineering, physician acceptance of draft suitability increased from 62% to 84% (P <.001) in the validation dataset (N = 200), and 74% of drafts in the test dataset were rated as ""helpful.""Patients also noted significantly increased favorability of message tone (78%) and overall quality (80%) for the optimized prompt compared to the original prompt in the training dataset, patients were unable to differentiate human and LLM-generated draft PMAR responses for 76% of the messages, in contrast to the earlier preference for human-generated responses. Majority (72%) of clinicians believed it can reduce cognitive load in dealing with InBasket messages. Discussion and Conclusion: Informed by clinician and patient feedback synergistically, tuning in LLM prompt alone can be effective in creating clinically relevant and useful draft responses to PMARs.  © 2024 The Author(s).",yes,"This study evaluates the use of GPT-4, a large language model, to generate draft responses to patient medical advice requests (PMARs) in a clinical setting. It measures both clinician and patient acceptance of the LLM-generated responses, and tests the system in a real-world electronic health record environment. The focus is on clinical communication and decision support, meeting the inclusion criteria.",10.1093/jamia/ocae172,,,,,
Uncovering Language Disparity of ChatGPT on Retinal Vascular Disease Classification: Cross-Sectional Study,"Background: Benefiting from rich knowledge and the exceptional ability to understand text, large language models like ChatGPT have shown great potential in English clinical environments. However, the performance of ChatGPT in non-English clinical settings, as well as its reasoning, have not been explored in depth. Objective: This study aimed to evaluate ChatGPT's diagnostic performance and inference abilities for retinal vascular diseases in a non-English clinical environment. Methods: In this cross-sectional study, we collected 1226 fundus fluorescein angiography reports and corresponding diagnoses written in Chinese and tested ChatGPT with 4 prompting strategies (direct diagnosis or diagnosis with a step-by-step reasoning process and in Chinese or English). Results: Compared with ChatGPT using Chinese prompts for direct diagnosis that achieved an F1-score of 70.47%, ChatGPT using English prompts for direct diagnosis achieved the best diagnostic performance (80.05%), which was inferior to ophthalmologists (89.35%) but close to ophthalmologist interns (82.69%). As for its inference abilities, although ChatGPT can derive a reasoning process with a low error rate (0.4 per report) for both Chinese and English prompts, ophthalmologists identified that the latter brought more reasoning steps with less incompleteness (44.31%), misinformation (1.96%), and hallucinations (0.59%) (all P < .001). Also, analysis of the robustness of ChatGPT with different language prompts indicated significant differences in the recall (P = .03) and F1-score (P = .04) between Chinese and English prompts. In short, when prompted in English, ChatGPT exhibited enhanced diagnostic and inference capabilities for retinal vascular disease classification based on Chinese fundus fluorescein angiography reports. Conclusions: ChatGPT can serve as a helpful medical assistant to provide diagnosis in non-English clinical environments, but there are still performance gaps, language disparities, and errors compared to professionals, which demonstrate the potential limitations and the need to continually explore more robust large language models in ophthalmology practice. © 2024 Journal of Medical Internet Research. All rights reserved.",yes,"This study evaluates ChatGPT's diagnostic performance and reasoning abilities in classifying retinal vascular diseases using clinical reports, directly assessing its clinical utility in a non-English (Chinese) environment. The focus is on clinical diagnosis and inference, which fits the inclusion criteria.",10.2196/51926,,,,,
PERFORMANCE ASSESSMENT OF AN ARTIFICIAL INTELLIGENCE CHATBOT IN CLINICAL VITREORETINAL SCENARIOS,"2322. Retina. 2024 Jun 1;44(6):954-964. doi: 10.1097/IAE.0000000000004053.

PERFORMANCE ASSESSMENT OF AN ARTIFICIAL INTELLIGENCE CHATBOT IN CLINICAL 
VITREORETINAL SCENARIOS.

Maywood MJ(1), Parikh R(2)(3), Deobhakta A(4), Begaj T(1)(5).

Author information:
(1)Department of Ophthalmology, Corewell Health William Beaumont University 
Hospital, Royal Oak, Michigan.
(2)Manhattan Retina and Eye Consultants, New York, New York.
(3)Department of Ophthalmology, New York University School of Medicine, New 
York, New York.
(4)Icahn School of Medicine of Mount Sinai, New York, New York.
(5)Associated Retinal Consultants, Royal Oak, Michigan .

PURPOSE: To determine how often ChatGPT is able to provide accurate and 
comprehensive information regarding clinical vitreoretinal scenarios. To assess 
the types of sources ChatGPT primarily uses and to determine whether they are 
hallucinated.
METHODS: This was a retrospective cross-sectional study. The authors designed 40 
open-ended clinical scenarios across four main topics in vitreoretinal disease. 
Responses were graded on correctness and comprehensiveness by three blinded 
retina specialists. The primary outcome was the number of clinical scenarios 
that ChatGPT answered correctly and comprehensively. Secondary outcomes included 
theoretical harm to patients, the distribution of the type of references used by 
the chatbot, and the frequency of hallucinated references.
RESULTS: In June 2023, ChatGPT answered 83% of clinical scenarios (33/40) 
correctly but provided a comprehensive answer in only 52.5% of cases (21/40). 
Subgroup analysis demonstrated an average correct score of 86.7% in neovascular 
age-related macular degeneration, 100% in diabetic retinopathy, 76.7% in retinal 
vascular disease, and 70% in the surgical domain. There were six incorrect 
responses with one case (16.7%) of no harm, three cases (50%) of possible harm, 
and two cases (33.3%) of definitive harm.
CONCLUSION: ChatGPT correctly answered more than 80% of complex open-ended 
vitreoretinal clinical scenarios, with a reduced capability to provide a 
comprehensive response.",yes,"This study evaluates ChatGPT's performance in answering clinical vitreoretinal scenarios, including accuracy, comprehensiveness, and potential for patient harm. It directly assesses a generative LLM (ChatGPT) in a clinical context, meeting inclusion criteria.",10.1097/IAE.0000000000004053,,,,,
"Current safeguards, risk mitigation, and transparency measures of large language models against the generation of health disinformation: repeated cross sectional analysis","OBJECTIVES To evaluate the effectiveness of safeguards to prevent large language models (LLMs) from being misused to generate health disinformation, and to evaluate the transparency of artificial intelligence (AI) developers regarding their risk mitigation processes against observed vulnerabilities. DESIGN Repeated cross sectional analysis. SETTING Publicly accessible LLMs. METHODS In a repeated cross sectional analysis, four LLMs (via chatbots/assistant interfaces) were evaluated: OpenAI’s GPT-4 (via ChatGPT and Microsoft’s Copilot), Google’s PaLM 2 and newly released Gemini Pro (via Bard), Anthropic’s Claude 2 (via Poe), and Meta’s Llama 2 (via HuggingChat). In September 2023, these LLMs were prompted to generate health disinformation on two topics: sunscreen as a cause of skin cancer and the alkaline diet as a cancer cure. Jailbreaking techniques (ie, attempts to bypass safeguards) were evaluated if required. For LLMs with observed safeguarding vulnerabilities, the processes for reporting outputs of concern were audited. 12 weeks after initial investigations, the disinformation generation capabilities of the LLMs were re-evaluated to assess any subsequent improvements in safeguards. MAIN OUTCOME MEASURES The main outcome measures were whether safeguards prevented the generation of health disinformation, and the transparency of risk mitigation processes against health disinformation. RESULTS Claude 2 (via Poe) declined 130 prompts submitted across the two study timepoints requesting the generation of content claiming that sunscreen causes skin cancer or that the alkaline diet is a cure for cancer, even with jailbreaking attempts. GPT-4 (via Copilot) initially refused to generate health disinformation, even with jailbreaking attempts—although this was not the case at 12 weeks. In contrast, GPT-4 (via ChatGPT), PaLM 2/Gemini Pro (via Bard), and Llama 2 (via HuggingChat) consistently generated health disinformation blogs. In September 2023 evaluations, these LLMs facilitated the generation of 113 unique cancer disinformation blogs, totalling more than 40 000 words, without requiring jailbreaking attempts. The refusal rate across the evaluation timepoints for these LLMs was only 5% (7 of 150), and as prompted the LLM generated blogs incorporated attention grabbing titles, authentic looking (fake or fictional) references, fabricated testimonials from patients and clinicians, and they targeted diverse demographic groups. Although each LLM evaluated had mechanisms to report observed outputs of concern, the developers did not respond when observations of vulnerabilities were reported. CONCLUSIONS This study found that although effective safeguards are feasible to prevent LLMs from being misused to generate health disinformation, they were inconsistently implemented. Furthermore, effective processes for reporting safeguard problems were lacking. Enhanced regulation, transparency, and routine auditing are required to help prevent LLMs from contributing to the mass generation of health disinformation. © 2024 BMJ Publishing Group. All rights reserved.",yes,"This study evaluates the ability of several frontier LLMs (GPT-4, Gemini, Claude, Llama 2) to generate health disinformation and assesses the effectiveness of their safeguards. The focus is on the clinical risk of LLMs being misused to produce harmful health content, which is a relevant clinical application (misinformation in healthcare). The study directly tests LLMs in a healthcare context, not just technical aspects or non-clinical tasks.",10.1136/bmj-2023-078538,,,,,
Large Language Models for Individualized Psychoeducational Tools for Psychosis: A cross-sectional study,"Importance: In mental healthcare, the potential of Large Language Models (LLMs) to enhance psychoeducation is a burgeoning field. This study explored the potential of ChatGPT as an individualized psychoeducational support tool specifically for psychosis education. Objective: The study aims to evaluate psychosis-related questions to provide accurate, clear, and clinically relevant individualized information for patients and caregivers. Design: This cross-sectional study uses a qualitative analysis design. The researchers specifically employed a question-answering system (GPT-4 via ChatGPT) to generate responses to common questions about psychosis. Experts in the field then evaluated these responses to assess their quality for use in a clinical setting. Primary Outcome: Researchers presented ChatGPT with 20 common questions frequently asked by patients' caregivers and relatives. Two experts in psychosis then assessed the quality of the responses using six criteria: accuracy (1-3), clarity (1-3), inclusivity (1-3), completeness (0-1), clinical utility (1-5) and an overall score (1-4). Results: The evaluation yielded positive results overall. Responses were rated as accurate (M±SD= 2.89±0.22) and clear (mean score of 2.93±0.18). There was potential for improvement in terms of inclusivity (mean score of 2.30±0.41), suggesting a need to incorporate more diverse perspectives. Completeness received high ratings (mean score of 0.93±0.18), indicating responses addressed all aspects of the questions. Most importantly, the responses were deemed clinically useful (mean score of 4.35±0.52). Conclusions: In summary, this study underscores the significant promise of ChatGPT as a psychoeducational tool for patients with psychosis, their relatives, and their caregivers. The experts' findings affirm that the information delivered by ChatGPT is not only accurate and clinically relevant but also conveyed conversationally, enhancing its accessibility and usability. The initial performance of ChatGPT as a psychoeducational tool in the context of psychosis education is undeniably positive.",yes,"This study evaluates ChatGPT (GPT-4) as a tool for providing individualized psychoeducational information to patients and caregivers about psychosis. The LLM is assessed for its ability to answer clinical questions relevant to patient care, and its responses are rated by clinical experts for accuracy, clarity, and clinical utility. This fits the inclusion criteria as it is a clinical evaluation of an LLM in a healthcare context.",10.1101/2024.07.26.24311075,,,,,
Exploring the role of an artificial intelligence chatbot on appendicitis management: an experimental study on ChatGPT,"Background: Appendicitis is a common surgical condition that requires urgent medical attention. Recent advancements in artificial intelligence and large language processing, such as ChatGPT, have demonstrated potential in supporting healthcare management and scientific research. This study aims to evaluate the accuracy and comprehensiveness of ChatGPT's knowledge on appendicitis management. Methods: Six questions related to appendicitis management were created by experienced RACS qualified general surgeons to assess ChatGPT's ability to provide accurate information. The criteria of ChatGPT answers' accuracy were compared with current healthcare guidelines for appendicitis and subjective evaluation by two RACS qualified General Surgeons. Additionally, ChatGPT was then asked to provide five high level evidence references to support its responses. Results: ChatGPT provided clinically relevant information on appendicitis management, however, was inconsistent in doing so and often provided superficial information. Further to this, ChatGPT encountered difficulties in generating relevant references, with some being either non-existent or incorrect. Conclusion: ChatGPT has the potential to provide timely and comprehensible medical information on appendicitis management to laypersons. However, its issue of inaccuracy in information and production of non-existent or erroneous references presents a challenge for researchers and clinicians who may inadvertently employ such information in their research or healthcare. Therefore, clinicians should exercise caution when using ChatGPT for these purposes. © 2023 Royal Australasian College of Surgeons.",yes,"This study evaluates ChatGPT's ability to provide accurate and comprehensive clinical information on appendicitis management, comparing its responses to clinical guidelines and expert opinion. The focus is on clinical content relevant to patient care, not on non-clinical tasks or technical aspects. Therefore, it meets the inclusion criteria.",10.1111/ans.18736,,,,,
A Novel Knowledge Enhanced Large Language Model Augmented Framework for Medical Question Answering,"Leveraging domain-specific knowledge from pre-trained large language models and knowledge graphs for reasoning in the medical question answering task has emerged as a prominent research field. However, the accuracy of the inference results is restricted by multiple factors, including the quality of analyzed topic entities, the selected inference path in the knowledge graph, and the absence of mutual updating for embedding representations from large language models and knowledge graphs. In this paper, we propose a novel medical question answering framework based on the domain-specific large language model, aiming to enhance the quality of topic entities by implementing the retrieval augmentation technique. Inspired by the concept of chain-of-thought reasoning, we introduce a joint reasoning approach based on analyzed topic entities to facilitate the generation of accurate inference paths. Additionally, we design a unified embedding mechanism that combines representations from both the large language model and graph neural networks, incorporating a pooling operation for predicting answers to input questions. To the best of our knowledge, this work signifies the pioneering efforts in implementing the retrieval augmentation technique and the joint reasoning approach within the context of the medical question answering task. Experimental results on public benchmark datasets demonstrate that the introduced method outperforms state-of-the-art baseline approaches. © 2024 IEEE.",yes,"This study proposes and evaluates a framework that uses a domain-specific large language model (LLM) for medical question answering, which is a clinical application. The focus is on improving the LLM's performance in answering medical questions, and experimental results are reported. This fits the inclusion criteria as it evaluates an LLM in a healthcare context.",10.1109/BIBM62325.2024.10822706,,,,,
"Assessing the quality of AI information from ChatGPT regarding oral surgery, preventive dentistry, and oral cancer: An exploration study","Aim: Evaluation of the quality of dental information produced by the ChatGPT artificial intelligence language model within the context of oral surgery, preventive dentistry, and oral cancer. Methodology: This study adopted quantitative methods approach. The experts prepared 50 questions (including dimensions of, risk factors, preventive measures, diagnostic methods, and treatment options) that would be presented to ChatGPT, and its responses were rated for their accuracy, completeness, relevance, clarity or comprehensibility, and possible risks using a standardized rubric. To carry out the assessment of the responses by ChatGPT, a standardized scoring rubric was used. Evaluation process included feedback concerning the strengths, weaknesses, and potential areas of improvement in the responses provided by ChatGPT. Results: While achieving the highest score for preventive dentistry at 4.3/5 and being able to communicate the complex information coherently, the tool showed lower accuracy for oral surgery and oral cancer, scoring 3.9/5 and 3.6/5, respectively, with several gaps for post-operative instructions, personalized risk assessments, and specialized diagnostic methods. Potential risks, such as a lack of individualized advice, were shown in 53% of the oral cancer and in 40% of the oral surgery. While showing promise in some domains, ChatGPT had important limitations in specialized areas that require nuanced expertise. Conclusion: The findings point to the need for professional supervision while using AI-generated information and ongoing evaluation as capabilities evolve, for the assurance of responsible implementation in the best interest of patient care.",yes,"This study evaluates ChatGPT's ability to provide clinical information in the context of oral surgery, preventive dentistry, and oral cancer by assessing the quality of its responses to expert-generated clinical questions. The focus is on the accuracy, completeness, and risks of ChatGPT's answers to clinical queries, which is a direct clinical application relevant to patient care.",10.1016/j.sdentj.2024.09.009,,,,,
Comparative analysis of diagnostic accuracy in adult congenital heart disease: A study of three physician groups and ChatGPT,"This research letter evaluates the diagnostic accuracy of three physician groups—pediatric cardiologists, adult congenital heart disease (ACHD) specialists, and general cardiologists—and compares it to the performance of an AI (Artificial Intelligence) LLM model. Based on eight targeted questions in adult congenital heart disease, the analysis highlights significant disparities in correct answer rates, with congenital specialists achieving the highest accuracy and the AI model – the lowest. © 2024 The Authors",yes,"This study directly compares the diagnostic accuracy of ChatGPT (an LLM) to physician groups in the context of adult congenital heart disease, which is a clinical application. It evaluates the LLM's performance in a medical diagnostic task.",10.1016/j.ijcchd.2024.100547,,,,,
Evaluating Large Language Model-Assisted Emergency Triage: A Comparison of Acuity Assessments by GPT-4 and Medical Experts,"Aim: To evaluate the accuracy of the Emergency Severity Index (ESI) assignments by GPT-4, a large language model (LLM), compared to senior emergency department (ED) nurses and physicians. Method: An observational study of 100 consecutive adult ED patients was conducted. ESI scores assigned by GPT-4, triage nurses, and by a senior clinician. Both model and human experts were provided the same patient data. Results: GPT-4 assigned a lower median ESI score (2.0) compared to human evaluators (median 3.0; p < 0.001), suggesting a potential overestimation of patient severity by the LLM. The results showed differences in the triage assessment approaches between GPT-4 and the human evaluators, including variations in how patient age and vital signs were considered in the ESI assignments. Conclusion: While GPT-4 offers a novel methodology for patient triage, its propensity to overestimate patient severity highlights the necessity for further development and calibration of LLM tools in clinical environments. The findings underscore the potential and limitations of LLM in clinical decision-making, advocating for cautious integration of LLMs in healthcare settings. Reporting Method: This study adhered to relevant EQUATOR guidelines for reporting observational studies. © 2024 The Author(s). Journal of Clinical Nursing published by John Wiley & Sons Ltd.",yes,"This study directly evaluates the clinical performance of GPT-4, a large language model, in the context of emergency department triage by comparing its acuity assessments to those of medical experts. The focus is on a clinical decision-making task (ESI assignment), which fits the inclusion criteria.",10.1111/jocn.17490,,,,,
Artificial intelligence in the medical field: diagnostic capabilities of GPT-4 in comparison with physicians,"Background: GPT-4 is an extensive language model designed to understand and generate human-like text, with the ability to accomplish complex tasks such as data analysis and decision-making. Methods: We conducted a retrospective observational study, which involved preprocessing 340 clinical cases, the contents of which included medical history, physical findings, laboratory, and instrumental data. We provided this information to GPT-4, which was instructed to give us the top five most likely differential diagnoses. The discrepancies between artificial intelligence (AI)’s five differentials and the physician’s final diagnosis were analyzed regarding different specialties and laboratory/instrument data inclusion. Results: Before we integrated laboratory data, GPT-4 showed a 60% diagnostic match with physicians on the first differential and 86% in all five. After we integrated the instrumental and laboratory findings, these percentages grew to 72% and 92%, respectively. The integration of laboratory and instrumental data increased GPT-4’s diagnostic accuracy, evidenced by an odds ratio factor of 2.1 and McNemar’s test chi-squared value of 10.76, highlighting the substantial impact of this data on AI’s diagnostic precision. Conclusions: This study highlighted the potential of using GPT-4 in medical diagnostics, provided a basis for integrating AI tools with clinical judgment, and opened avenues for future research, particularly in developing AI models tailored to the diagnostic needs of different medical fields. © AME Publishing Company.",yes,"This study directly evaluates GPT-4, a large language model, in the clinical context by comparing its diagnostic capabilities to those of physicians using real clinical cases. The focus is on clinical decision-making (differential diagnosis), which fits the inclusion criteria.",10.21037/jmai-24-276,,,,,
Diagnostic Accuracy of Differential-Diagnosis Lists Generated by Generative Pretrained Transformer 3 Chatbot for Clinical Vignettes with Common Chief Complaints: A Pilot Study,"The diagnostic accuracy of differential diagnoses generated by artificial intelligence (AI) chatbots, including the generative pretrained transformer 3 (GPT-3) chatbot (ChatGPT-3) is unknown. This study evaluated the accuracy of differential-diagnosis lists generated by ChatGPT-3 for clinical vignettes with common chief complaints. General internal medicine physicians created clinical cases, correct diagnoses, and five differential diagnoses for ten common chief complaints. The rate of correct diagnosis by ChatGPT-3 within the ten differential-diagnosis lists was 28/30 (93.3%). The rate of correct diagnosis by physicians was still superior to that by ChatGPT-3 within the five differential-diagnosis lists (98.3% vs. 83.3%, p = 0.03). The rate of correct diagnosis by physicians was also superior to that by ChatGPT-3 in the top diagnosis (53.3% vs. 93.3%, p < 0.001). The rate of consistent differential diagnoses among physicians within the ten differential-diagnosis lists generated by ChatGPT-3 was 62/88 (70.5%). In summary, this study demonstrates the high diagnostic accuracy of differential-diagnosis lists generated by ChatGPT-3 for clinical cases with common chief complaints. This suggests that AI chatbots such as ChatGPT-3 can generate a well-differentiated diagnosis list for common chief complaints. However, the order of these lists can be improved in the future. © 2023 by the authors.",yes,"This study directly evaluates the diagnostic accuracy of differential-diagnosis lists generated by ChatGPT-3 for clinical vignettes, comparing its performance to that of physicians. The use of ChatGPT-3 in generating clinical differential diagnoses is a clear clinical application of an LLM.",10.3390/ijerph20043378,,,,,
Performance of Chatgpt in ophthalmology exam; human versus AI,"Purpose: This cross-sectional study focuses on evaluating the success rate of ChatGPT in answering questions from the ‘Resident Training Development Exam’ and comparing these results with the performance of the ophthalmology residents. Methods: The 75 exam questions, across nine sections and three difficulty levels, were presented to ChatGPT. The responses and explanations were recorded. The readability and complexity of the explanations were analyzed and The Flesch Reading Ease (FRE) score (0–100) was recorded using the program named Readable. Residents were categorized into four groups based on their seniority. The overall and seniority-specific success rates of the residents were compared separately with ChatGPT. Results: Out of 69 questions, ChatGPT answered 37 correctly (53.62%). The highest success was in Lens and Cataract (77.77%), and the lowest in Pediatric Ophthalmology and Strabismus (0.00%). Of 789 residents, overall accuracy was 50.37%. Seniority-specific accuracy rates were 43.49%, 51.30%, 54.91%, and 60.05% for 1st to 4th-year residents. ChatGPT ranked 292nd among residents. Difficulty-wise, 11 questions were easy, 44 moderate, and 14 difficult. ChatGPT's accuracy for each level was 63.63%, 54.54%, and 42.85%, respectively. The average FRE score of responses generated by ChatGPT was found to be 27.56 ± 12.40. Conclusion: ChatGPT correctly answered 53.6% of questions in an exam for residents. ChatGPT has a lower success rate on average than a 3rd year resident. The readability of responses provided by ChatGPT is low, and they are difficult to understand. As difficulty increases, ChatGPT's success decreases. Predictably, these results will change with more information loaded into ChatGPT. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.",yes,"This study evaluates ChatGPT's performance on a clinically relevant ophthalmology exam and compares its results to those of human residents. The focus is on clinical knowledge assessment, which is a direct clinical application of LLMs in medical education.",10.1007/s10792-024-03353-w,,,,,
Performance of GPT-4 Vision on kidney pathology exam questions,"Objectives: ChatGPT (OpenAI, San Francisco, CA) has shown impressive results across various medical examinations, but its performance in kidney pathology is not yet established. This study evaluated proficiencies of GPT-4 Vision (GPT-4V), an updated version of the platform with the ability to analyze images, on kidney pathology questions and compared its responses with those of nephrology trainees. Methods: Thirty-nine questions (19 text-based questions and 20 with various kidney biopsy images) designed specifically for the training of nephrology fellows were employed. Results: GPT-4V displayed comparable accuracy rates in the first and second runs (67% and 72%, respectively, P =. 50). The aggregated accuracy, however - particularly, the consistent accuracy - of GPT-4V was lower than that of trainees (72% and 67% vs 79%). Both GPT-4V and trainees displayed comparable accuracy in responding to image-based and text-only questions (55% vs 79% and 81% vs 78%, P =. 11 and P =. 67, respectively). The consistent accuracy in image-based, directly asked questions for GPT-4V was 29%, much lower than its 88% consistency on text-only, directly asked questions (P =. 02). In contrast, trainees maintained similar accuracy in directly asked image-based and text-based questions (80% vs 77%, P =. 65). Although the aggregated accuracy for correctly interpreting images was 69%, the consistent accuracy across both runs was only 39%. The accuracy of GPT-4V in answering questions with correct image interpretation was significantly higher than for questions with incorrect image interpretation (100% vs 0% and 100% vs 33% for the first and second runs, P =. 001 and P =. 02, respectively). Conclusions: The performance of GPT-4V in handling kidney pathology questions, especially those including images, is limited. There is a notable need for enhancement in GPT-4V proficiency in interpreting images.  © 2024 The Author(s). Published by Oxford University Press on behalf of American Society for Clinical Pathology. All rights reserved.",yes,"This study evaluates the clinical performance of GPT-4 Vision (an LLM with image analysis capabilities) on kidney pathology exam questions, including both text and image-based questions, and compares its results to those of nephrology trainees. The focus is on clinical knowledge and diagnostic interpretation, which is a direct clinical application.",10.1093/ajcp/aqae030,,,,,