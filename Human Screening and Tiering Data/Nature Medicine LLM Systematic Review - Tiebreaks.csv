Title,Abstract,Comments 1,Comments 2,Screener 1,Screener 2,Eunice,Final Decision
Putting ChatGPT’s Medical Advice to the (Turing) Test,"Importance: Chatbots could play a role in answering patient questions, but patients’ ability to distinguish between provider and chatbot responses, and patients’ trust in chatbots’ functions are not well established. Objective: To assess the feasibility of using ChatGPT or a similar AI-based chatbot for patient-provider communication. Design: Survey in January 2023 Setting: Survey Participants: A US representative sample of 430 study participants aged 18 and above was recruited on Prolific, a crowdsourcing platform for academic studies. 426 participants filled out the full survey. After removing participants who spent less than 3 minutes on the survey, 392 respondents remained. 53.2% of respondents analyzed were women; their average age was 47.1. Exposure(s): Ten representative non-administrative patient-provider interactions were extracted from the EHR. Patients’ questions were placed in ChatGPT with a request for the chatbot to respond using approximately the same word count as the human provider’s response. In the survey, each patient’s question was followed by a provider- or ChatGPT-generated response. Participants were informed that five responses were provider-generated and five were chatbot-generated. Participants were asked, and incentivized financially, to correctly identify the response source. Participants were also asked about their trust in chatbots’ functions in patient-provider communication, using a Likert scale of 1-5. Main Outcome(s) and Measure(s): Main outcome: Proportion of responses correctly classified as provider- vs chatbot-generated. Secondary outcomes: Average and standard deviation of responses to trust questions. Results: The correct classification of responses ranged between 49.0% to 85.7% for different questions. On average, chatbot responses were correctly identified 65.5% of the time, and provider responses were correctly distinguished 65.1% of the time. On average, responses toward patients’ trust in chatbots’ functions were weakly positive (mean Likert score: 3.4), with lower trust as the health-related complexity of the task in questions increased. Conclusions and Relevance: ChatGPT responses to patient questions were weakly distinguishable from provider responses. Laypeople appear to trust the use of chatbots to answer lower risk health questions. It is important to continue studying patient-chatbot interaction as chatbots move from administrative to more clinical roles in healthcare.",This is a silly study but fits the inclusion criteria,I think yes because it is a survey but testing if an LLM's responses to patients are distinguisahbe from providers.,Yes,Maybe?,"Clinical task is responding to patient questions, LLM is ChatGPT",Yes
Patient2Trial: From patient to participant in clinical trials using large language models,"Purpose: Large language models (LLMs) exhibit promising language understanding and generation capabilities and have been adopted for various clinical use cases. Investigating the feasibility of leveraging LLMs in building a clinical trial retrieval system for patients is crucial as it can greatly enhance the patient enrollment process by prioritizing the most suitable trials pertaining to a patient. In this work, we develop an LLM-assisted system focused on a patient-initiated approach, allowing patients with specific conditions to directly find eligible trials by completing disorder-specific questionnaires. Methods: We obtained clinical trial eligibility criteria (from ClinicalTrials.gov) and simulated patient questionnaires (or topics) from the Text REtrieval Conference (TREC) 2023 Clinical Trials Track conducted by the National Institute of Standards and Technology (NIST), in which we also participated. These topics cover eight disorders across diverse domains, namely glaucoma, anxiety, chronic obstructive pulmonary disease, breast cancer, Covid-19, rheumatoid arthritis, sickle cell anemia, and type 2 diabetes. A Generative Pre-trained Transformer model (GPT-4) was employed for system development. We conducted both quantitative and qualitative evaluation using 37 patient topics. Results: The system achieved an overall Precision@10 (proportion of relevant trials) of 0.7351 and NDCG@10 (considers ranking order of relevant trials) of 0.8109, indicating its effectiveness in retrieving ranked lists of suitable trials for patients. Notably, for eight out of 37 patient topics, all the top 10 retrieved trials were relevant. The system scored the highest on breast cancer (NDCG@10 = 0.9347, Precision@10 = 0.84) and the lowest on type 2 diabetes (NDCG@10 = 0.61, Precision@10 = 0.475). One probable reason could be that the information in breast cancer topics is relatively straightforward to match. Qualitative error analysis classified errors into four categories (e.g., difficulty in correctly matching inclusion criteria) and further highlighted strengths (e.g., ability to make clinical inference). Conclusion: We demonstrated the feasibility of integrating LLMs in identifying and ranking suitable trials for patients across multiple disorders. Further work is required to assess the system's generalizability on other disorders and patient information sources. This system has the potential to expedite the patient-trial matching process by suggesting a ranked list of applicable trials to patients and clinicians. © 2025",This is a cool LLM application but not really a super clinical one,I'm mixed... it is clinical but it isnt something a doctor reall would do in a clinical setting. Recommend nah,No,Maybe?,"Agree - cool use case, but this isn't a task that the average physician is doing. Moeover, angle seems to be ""patient initiated"" trial finding",No
An Opportunity to Standardize and Enhance Intelligent Virtual Assistant-Delivered Layperson Cardiopulmonary Resuscitation Instructions,"Importance: Intelligent virtual assistants (IVAs) are ubiquitous and hold the potential to provide bystander cardiopulmonary resuscitation (CPR) instructions during an emergency. Objective: To evaluate the quality of CPR instructions provided by IVAs. Design: We evaluated the appropriateness of responses of four IVAs (Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana) to eight CPR-related questions. We also evaluated text-based responses provided by OpenAI ChatGPT, a recently developed artificial intelligence large language model. Results: Out of 32 responses provided by IVAs, only 19 (59%) were related to CPR, 9 (28%) suggested calling emergency services, and 4 (12%) provided verbal CPR instructions. All responses provided by ChatGPT were related to CPR and suggested calling emergency services. Among responses related to CPR, the answers provided varied significantly in the utility of information provided. Conclusions and Relevance: These results highlight the need for the technology industry to partner with the medical community to improve and standardize bystander CPR instruction provided by IVAs.",,,No,Maybe?,"If we're going by ""is this a task that the average physician in that specialty would need to do?” I would say no. Bystander CPR is the preclinical setting, and the average EM doc isn't instructing layperson CPR",No
ChatGPT-o1 and the Pitfalls of Familiar Reasoning in Medical Ethics,"Large language models (LLMs) like ChatGPT often exhibit Type 1 thinking—fast, intuitive reasoning that relies on familiar patterns—which can be dangerously simplistic in complex medical or ethical scenarios requiring more deliberate analysis. In our recent explorations, we observed that LLMs frequently default to well-known answers, failing to recognize nuances or twists in presented situations. For instance, when faced with modified versions of the classic ""Surgeon's Dilemma"" or medical ethics cases where typical dilemmas were resolved, LLMs still reverted to standard responses, overlooking critical details. Even models designed for enhanced analytical reasoning, such as ChatGPT-o1, did not consistently overcome these limitations. This suggests that despite advancements toward fostering Type 2 thinking, LLMs remain heavily influenced by familiar patterns ingrained during training. As LLMs are increasingly integrated into clinical practice, it is crucial to acknowledge and address these shortcomings to ensure reliable and contextually appropriate AI assistance in medical decision-making.","This is a tough call, but they actually do try to evaluate clinical judgement, specifically from an ethics standpoint here",potentially clinically relevant,Yes,Maybe?,Eval LLMs on clinical ethics cases,Yes
Effects of Different Prompts on the Quality of GPT-4 Responses to Dementia Care Questions,"Evidence suggests that different prompts lead large language models (LLMs) to generate responses with varying quality. Yet, little is known about prompts' effects on response quality in health care domains. In this exploratory study, we address this gap, focusing on a specific healthcare domain: dementia caregiving. We first developed an innovative prompt template with three components: (1) system prompts (SPs) featuring 4 different roles; (2) an initialization prompt; and (3) task prompts (TPs) specifying different levels of details, totaling 12 prompt combinations. Next, we selected 3 social media posts containing complicated, real-world questions about dementia caregivers' challenges in 3 areas: memory loss and confusion, aggression, and driving. We then entered these posts into G PT-4, with our 12 prompts, to generate 12 responses per post, totaling 36 responses. We compared the word count of the 36 responses to explore potential differences in response length. Two experienced dementia care clinicians on our team assessed the response quality using a rating scale with 5 quality indicators: factual, interpretation, application, synthesis, and comprehensiveness (scoring range: 0-5; higher scores indicate higher quality). Both clinicians rated the responses from 3 to 5, with 75% agreement. Consensus was reached through discussion. Overall, 44% of responses (16/36) were rated as 5; another 44% (16/36), as 4; the remaining 4 (11 %), as 3. We found no interaction effect of system and task prompts or main effect of system prompts on response length. Task prompts had a statistically significant effect on response length: F(2,24) = 82.784, p <.001. Post hoc analysis showed that the significant difference in responses was due to TP3, which led to significantly longer responses. There was no interaction or main effect of system and task prompts on response quality. Our clinicians' qualitative feedback provided further insight: (1) system prompts with the different professional roles (neuropsychologist and social worker) did not lead to noticeable differences in response content (that is, there were no neuropsychology- and social work-versions of GPT-4 responses); and (2) TP3, while producing longer responses statistically, might not necessarily have produced higher quality responses clinically: at times the details contained in the lengthy responses seem unnecessary from a clinical perspective. We discuss study limitations and future research directions. © 2024 IEEE.",,,Yes,Maybe?,Answering caregiver questions is within clinician task scope,Yes
The Artificial Intelligence Shoulder Arthroplasty Score: development and validation of a tool for large language model responses to common patient questions regarding total shoulder arthroplasty,"Background and Hypothesis: While research into artificial intelligence, specifically large language model (LLM), ability to respond to patient questions regarding specific orthopedic pathologies continues to grow, no tool presently exists to systematically and comprehensively evaluate the quality of LLM responses. The present study seeks to develop and validate the Artificial Intelligence Shoulder Arthroplasty Score (AISAS) to create a comprehensive, standardized, and reproducible system for evaluating artificial intelligence responses to patient questions regarding their orthopedic pathology. Methods: The novel scoring tool, AISAS, was developed to include four equally weighted components related to accuracy, completeness, clarity, and readability. Fifteen common patient questions on glenohumeral arthritis were asked one by one to three of the most used LLMs: ChatGPT (version 3.5), Claude (version 3.5) Sonnet, and Gemini. Ten shoulder and elbow fellowship trained orthopedic surgeons used the proposed framework to evaluate each of the 45 responses. Inter-rater reliability was calculated via Cohen's kappa and rater-score correlation was calculated via Cronbach's alpha. Results: AISAS use for Claude and ChatGPT produced moderate agreement (k = 0.55 and 0.43) while Gemini produced substantial reliability among raters ((k = 0.66). Cronbach's alpha scores demonstrated excellent correlation of Gemini ratings (⍺ = 0.91) and acceptable correlation of the Claude and ChatGPT ratings (⍺ = 0.79 and 0.75). Discussion and Conclusion: AISAS use enables systematic assessment of the overall quality of an LLM response, as well as the individual components of a response that may vary in quality to enable easy comparisons for LLM responses. Furthermore, it offers a tool to trend the progress of LLMs in ability to respond to patient questions. Establishing such a framework to guide areas of improvement for LLMs will serve to optimize LLMs as a patient tool, identify areas for improvement, and allow physicians to better direct patients on how to utilize these tools for optimal use. Conclusion: The AISAS is a comprehensive and reproducible tool for evaluating LLM responses, with high levels of inter-rater reliability. AISAS use can help to evaluate responses to patient questions to guide growth and improvement of LLMs for use in the orthopedic setting. © 2025 American Shoulder and Elbow Surgeons",,This is the development of a tool to assess how good AI is,Yes,No,"Results are re the viability of the scoring method, but not on the nature of the responses outputed by the LLMs used ",No
Assessing the performance of ChatGPT-4 and ChatGPT-4o in lung cancer diagnoses,,,Yet another letter LMAO,Yes,No,Letter to the editor,No
Generative artificial intelligence responses to patient messages in the electronic health record: early lessons learned,"Background: Electronic health record (EHR)-based patient messages can contribute to burnout. Messages with a negative tone are particularly challenging to address. In this perspective, we describe our initial evaluation of large language model (LLM)-generated responses to negative EHR patient messages and contend that using LLMs to generate initial drafts may be feasible, although refinement will be needed. Methods: A retrospective sample (n = 50) of negative patient messages was extracted from a health system EHR, de-identified, and inputted into an LLM (ChatGPT). Qualitative analyses were conducted to compare LLM responses to actual care team responses. Results: Some LLM-generated draft responses varied from human responses in relational connection, informational content, and recommendations for next steps. Occasionally, the LLM draft responses could have potentially escalated emotionally charged conversations. Conclusion: Further work is needed to optimize the use of LLMs for responding to negative patient messages in the EHR. Lay Summary Doctors and other clinicians are receiving a growing number of messages from patients through electronic health records systems. This workload is contributing to clinician burnout. Some messages can be very negative or emotionally charged. These messages often require a lot of time or effort to respond to. In this article, we discuss results from a preliminary evaluation we conducted using large language models (like ChatGPT). We analyzed whether these models could help provide starting drafts to respond to patient messages, with a focus on negative messages, since these can be particularly difficult. We found that ChatGPT provided reasonable starting drafts in many cases, but that there were also issues in the drafts that would require further editing. These issues included sometimes not drafting the text from the perspective of a clinician, using overly broad or generic language, inappropriate escalation (eg, instructing the patient to file complaints to the medical board), and inconsistent recommendations for in-person follow-up visits. Based on this evaluation, we highlight not only the promise and possibilities of this technology but also considerations and challenges that need to be addressed for optimizing its future use. # The Author(s) 2024.",,LLM answering patient Qs; just not quantitative...,Yes,Maybe?,Qualitative analysis,No
An Explainable Artificial Intelligence Text Classifier for Suicidality Prediction in Youth Crisis Text Line Users: Development and Validation Study,"Background: Suicide represents a critical public health concern, and machine learning (ML) models offer the potential for identifying at-risk individuals. Recent studies using benchmark datasets and real-world social media data have demonstrated the capability of pretrained large language models in predicting suicidal ideation and behaviors (SIB) in speech and text. Objective: This study aimed to (1) develop and implement ML methods for predicting SIBs in a real-world crisis helpline dataset, using transformer-based pretrained models as a foundation; (2) evaluate, cross-validate, and benchmark the model against traditional text classification approaches; and (3) train an explainable model to highlight relevant risk-associated features. Methods: We analyzed chat protocols from adolescents and young adults (aged 14-25 years) seeking assistance from a German crisis helpline. An ML model was developed using a transformer-based language model architecture with pretrained weights and long short-term memory layers. The model predicted suicidal ideation (SI) and advanced suicidal engagement (ASE), as indicated by composite Columbia-Suicide Severity Rating Scale scores. We compared model performance against a classical word-vector-based ML model. We subsequently computed discrimination, calibration, clinical utility, and explainability information using a Shapley Additive Explanations value-based post hoc estimation model. Results: The dataset comprised 1348 help-seeking encounters (1011 for training and 337 for testing). The transformer-based classifier achieved a macroaveraged area under the curve (AUC) receiver operating characteristic (ROC) of 0.89 (95% CI 0.81-0.91) and an overall accuracy of 0.79 (95% CI 0.73-0.99). This performance surpassed the word-vector-based baseline model (AUC-ROC=0.77, 95% CI 0.64-0.90; accuracy=0.61, 95% CI 0.61-0.80). The transformer model demonstrated excellent prediction for nonsuicidal sessions (AUC-ROC=0.96, 95% CI 0.96-0.99) and good prediction for SI and ASE, with AUC-ROCs of 0.85 (95% CI 0.97-0.86) and 0.87 (95% CI 0.81-0.88), respectively. The Brier Skill Score indicated a 44% improvement in classification performance over the baseline model. The Shapley Additive Explanations model identified language features predictive of SIBs, including self-reference, negation, expressions of low self-esteem, and absolutist language. Conclusions: Neural networks using large language model–based transfer learning can accurately identify SI and ASE. The post hoc explainer model revealed language features associated with SI and ASE. Such models may potentially support clinical decision-making in suicide prevention services. Future research should explore multimodal input features and temporal aspects of suicide risk. ©Julia Thomas, Antonia Lucht, Jacob Segler, Richard Wundrack, Marcel Miché, Roselind Lieb, Lars Kuchinke, Gunther Meinlschmidt.",This is not an LLM,"Two questions: (1) its a transformer, but unclear if it is an LLM; i opt to say no... (2) is suicide risk prediction really a clinical task? i think yes... so two conflicting perspectives lol",No,Maybe?,Not an LLM,No
Can ChatGPT write radiology reports?,"These case examples exemplify the utility of ChatGPT in augmenting the radiology report drafting process, thereby contributing to the efficiency of report generation. © The Author(s) 2024.",,"this is a ""short communication"" Although it does represent a single case it isnt really doing a robust analysis",Yes,No,Qualitative analysis,Yes
Parental Perception on Usage of AI Chatbot to Understand Paediatric Otorhinolaryngology Condition: A Survey,"Artificial intelligence (AI) chatbots such as ChatGPT have the potential to assist parents and caregivers in understanding their child’s general health condition. Despite the potential benefits research on parents perceptions of using AI chatbots is still limited. This study explored parents’ and caregivers’ perceptions of AI chatbots to understand their child’s Otorhinolaryngology (ORL) condition. A cross-sectional survey was conducted among parents or caregivers of children with ORL conditions attending Paediatric ORL clinic over one month. The survey explored the familiarity with AI chatbots, perception, openness, and factors influencing their usage to understand their child’s ORL condition. 38 responses were obtained. Awareness of AI chatbots among parents/ caregivers was considered average, with only 52.63% (n = 20) having heard about the technology. A smaller subset used AI chatbots, i.e., ChatGPT (70.00%, n = 14), with 21.43% (n = 3) used ChatGPT specifically for pediatric health-related information. 68.42% preferred the explanation provided by doctors compared to the AI chatbot (n = 26), although the majority agreed that the AI chatbot might help them to understand health information better than traditional resources like web search engines, with 60.53% (n = 23). Most participants (60.53% n = 23) expressed interest in using AI chatbots. High openness was shown when using an AI chatbot to obtain general information about the condition (76.32%, n = 29) and symptom identification (71.05%, n = 7). The participants expressed their interest in adopting ChatGPT for health-related use, such as ease of understanding (68.42%, n = 26), accurate information (65.79%, n = 25), and easing communication with doctors (60.53%, n = 23). We found a high preference for medical consultation with doctors compared to AI chatbots (73.68%, n = 28). Familiarity with AI chatbots and their potential role in healthcare is still scarce in the community. Future research should continue exploring the potential of ChatGPT as a health AI chatbot and increase its accuracy in delivering health-related information. © Association of Otolaryngologists of India 2025.",,"This study surveyed *humans* on their familiarity with chatbots, not evaluated chatbots.",Yes,No,"Technically, most of the paper is on how parents feel using a chatbot, so would say no. There is a smaller part of the study comparing ChatGPT responses to health questions versus a doctor's responses, which could be perceived as a ""clinical task,"" but it's not the main focus of the paper so would lean no.",No
"Assessing the reliability of ChatGPT: a content analysis of self-generated and self-answered questions on clear aligners, TADs and digital imaging","INTRODUCTION: Artificial Intelligence (AI) is a tool that is already part of our 
reality, and this is an opportunity to understand how it can be useful in 
interacting with patients and providing valuable information about orthodontics.
OBJECTIVE: This study evaluated the accuracy of ChatGPT in providing accurate 
and quality information to answer questions on Clear aligners, Temporary 
anchorage devices and Digital imaging in orthodontics.
METHODS: forty-five questions and answers were generated by the ChatGPT 4.0, and 
analyzed separately by five orthodontists. The evaluators independently rated 
the quality of information provided on a Likert scale, in which higher scores 
indicated greater quality of information (1 = very poor; 2 = poor; 3 = 
acceptable; 4 = good; 5 = very good). The Kruskal-Wallis H test (p< 0.05) and 
post-hoc pairwise comparisons with the Bonferroni correction were performed.
RESULTS: From the 225 evaluations of the five different evaluators, 11 (4.9%) 
were considered as very poor, 4 (1.8%) as poor, and 15 (6.7%) as acceptable. The 
majority were considered as good [34 (15,1%)] and very good [161 (71.6%)]. 
Regarding evaluators' scores, a slight agreement was perceived, with Fleiss's 
Kappa equal to 0.004.
CONCLUSIONS: ChatGPT has proven effective in providing quality answers related 
to clear aligners, temporary anchorage devices, and digital imaging within the 
context of interest of orthodontics.

INTRODUÇÃO: A Inteligência Artificial (IA) é uma ferramenta que já faz parte de 
nossa realidade, e esta é uma oportunidade de entendermos como ela pode ser útil 
na interação com os pacientes e no fornecimento de informações valiosas sobre 
Ortodontia.
OBJETIVO: O objetivo deste estudo foi avaliar a precisão do ChatGPT em responder 
a perguntas sobre Alinhadores transparentes, Dispositivos de ancoragem 
temporária, e Imagens digitais em Ortodontia.
MÉTODOS: 45 perguntas e respostas foram geradas pelo ChatGPT 4.0 e analisadas 
separadamente por cinco ortodontistas que, de forma independente, avaliaram a 
qualidade das informações fornecidas, usando uma escala de Likert, na qual 
pontuações mais altas indicavam uma maior qualidade das informações (1 = muito 
ruim; 2 = ruim; 3 = aceitável; 4 = bom; 5 = muito bom). Aplicou-se o teste H de 
Kruskal-Wallis (p < 0,05) e comparações pareadas post-hoc com correção de 
Bonferroni.
RESULTADOS: Das 225 avaliações dos cinco avaliadores diferentes, 11 (4,9%) foram 
consideradas como muito ruins, 4 (1,8%) como ruins, e 15 (6,7%) como aceitáveis. 
A maioria foi considerada boa [34 (15,1%)] ou muito boa [161 (71,6%)]. Com 
relação às pontuações dos avaliadores, percebeu-se uma leve concordância, com o 
Kappa de Fleiss igual a 0,004.
CONCLUSÕES: O ChatGPT mostrou eficácia em fornecer respostas de qualidade para 
questões relacionadas a Alinhadores transparentes, Dispositivos de ancoragem 
temporária e Imagens digitais.",,I am torn on this one as it makes GPT generate both questions and answers - Anton - I concluded that making questions is not clinical,Yes,No,"Also torn, my rationale for Yes is that even though they used GPT to generate the questions (not clinical), it feels like it's because the researchers were lazy and it's a bad study. I think the core intent though was to evaluate the ability to answer clinical questions like those that could be asked to an orthodontist. I'm a bad tiebreaker :^(",Yes
Use of Natural Language Processing to Infer Sites of Metastatic Disease From Radiology Reports at Scale,"PURPOSE To evaluate natural language processing (NLP) methods to infer metastatic sites from radiology reports. METHODS A set of 4,522 computed tomography (CT) reports of 550 patients with 14 types of cancer was used to fine-tune four clinical large language models (LLMs) for multilabel classification of metastatic sites. We also developed an NLP information extraction (IE) system (on the basis of named entity recognition, assertion status detection, and relation extraction) for comparison. Model performances were measured by F1 scores on test and three external validation sets. The best model was used to facilitate analysis of metastatic frequencies in a cohort study of 6,555 patients with 53,838 CT reports. RESULTS The RadBERT, BioBERT, GatorTron-base, and GatorTron-medium LLMs achieved F1 scores of 0.84, 0.87, 0.89, and 0.91, respectively, on the test set. The IE system performed best, achieving an F1 score of 0.93. F1 scores of the IE system by individual cancer type ranged from 0.89 to 0.96. The IE system attained F1 scores of 0.89, 0.83, and 0.81, respectively, on external validation sets including additional cancer types, positron emission tomography-CT,and magnetic resonance imaging scans, respectively. In our cohort study, we found that for colorectal cancer, liver-only metastases were higher in de novo stage IV versus recurrent patients (29.7% v 12.2%; P <.001). Conversely, lung-only metastases were more frequent in recurrent versus de novo stage IV patients (17.2% v 7.3%; P <.001). CONCLUSION We developed an IE system that accurately infers metastatic sites in multiple primary cancers from radiology reports. It has explainable methods and performs better than some clinical LLMs. The inferred metastatic phenotypes could enhance cancer research databases and clinical trial matching, and identify potential patients for oligometastatic interventions.  © 2024 American Society of Clinical Oncology.",Data extraction task,"A study evaluating *non-generative* LLMs at infering metastatic sites from the pathology reports. In my understanding this does not pass Sully's initial conditions, but satisfies the broadened ones of any clinical task and any model, including non-generative. Need to confirm - posted in slack",No,Maybe?,Not a clinical task,No
A comparative study of large language model-based zero-shot inference and task-specific supervised classification of breast cancer pathology reports,"Objective: Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs could reduce the need for large-scale data annotations. Materials and Methods: We curated a dataset of 769 breast cancer pathology reports, manually labeled with 12 categories, to compare zero-shot classification capability of the following LLMs: GPT-4, GPT-3.5, Starling, and ClinicalCamel, with task-specific supervised classification performance of 3 models: random forests, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Results: Across all 12 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, LSTM-Att (average macro F1-score of 0.86 vs 0.75), with advantage on tasks with high label imbalance. Other LLMs demonstrated poor performance. Frequent GPT-4 error categories included incorrect inferences from multiple samples and from history, and complex task design, and several LSTM-Att errors were related to poor generalization to the test set. Discussion: On tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of data labeling. However, if the use of LLMs is prohibitive, the use of simpler models with large annotated datasets can provide comparable results. Conclusions: GPT-4 demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for large annotated datasets. This may increase the utilization of NLP-based variables and outcomes in clinical studies.  © 2024 The Author(s).",,"Compare LLMs with supervised methods in classifying breast cancer pathology reports. - Again, to confirm on Slack.",Yes,Maybe?,This is a data extraction NER task. Not clinical,No
Exposing Vulnerabilities in Clinical LLMs Through Data Poisoning Attacks: Case Study in Breast Cancer,"Training Large Language Models (LLMs) with billions of parameters on a dataset and publishing the model for public access is the standard practice currently. Despite their transformative impact on natural language processing, public LLMs present notable vulnerabilities given the source of training data is often web-based or crowdsourced, and hence can be manipulated by perpetrators. We delve into the vulnerabilities of clinical LLMs, particularly BioGPT which is trained on publicly available biomedical literature and clinical notes from MIMIC-III, in the realm of data poisoning attacks. Exploring susceptibility to data poisoning-based attacks on de-identified breast cancer clinical notes, our approach is the first one to assess the extent of such attacks and our findings reveal successful manipulation of LLM outputs. Through this work, we emphasize on the urgency of comprehending these vulnerabilities in LLMs, and encourage the mindful and responsible usage of LLMs in the clinical domain.",,"eval of ""poisoning"" of an LLM",No,Yes,Would argue that the task being poisoned is a clinical task (answering q's related to treatment + side effects). Eval of a vulnerability of the LLM in performing that clinical task,Yes
Simulate Scientific Reasoning with Multiple Large Language Models: An Application to Alzheimer’s Disease Combinatorial Therapy,"Motivation This study aims to develop an AI-driven framework that leverages large language models (LLMs) to simulate scientific reasoning and peer review to predict efficacious combinatorial therapy when data-driven prediction is infeasible. Results Our proposed framework achieved a significantly higher accuracy (0.74) than traditional knowledge-based prediction (0.52). An ablation study highlighted the importance of high quality few-shot examples, external knowledge integration, self-consistency, and review within the framework. The external validation with private experimental data yielded an accuracy of 0.82, further confirming the framework's ability to generate high-quality hypotheses in biological inference tasks. Our framework offers an automated knowledge-driven hypothesis generation approach when data-driven prediction is not a viable option. Availability and implementation Our source code and data are available at https://github.com/QidiXu96/Coated-LLM",,Evaluating LLMs accuracy in predicting combinatorial therapy for Alzheimer's Disease compared to traditional knowledge-based prediction (??? whatever that means),No,Yes,This isn't a clinical context (generates hypotheses for in vitro experiments),No
"Assessing ChatGPT’s Potential in HIV Prevention Communication: A Comprehensive Evaluation of Accuracy, Completeness, and Inclusivity","With the advancement of artificial intelligence(AI), platforms like ChatGPT have gained traction in different fields, including Medicine. This study aims to evaluate the potential of ChatGPT in addressing questions related to HIV prevention and to assess its accuracy, completeness, and inclusivity. A team consisting of 15 physicians, six members from HIV communities, and three experts in gender and queer studies designed an assessment of ChatGPT. Queries were categorized into five thematic groups: general HIV information, behaviors increasing HIV acquisition risk, HIV and pregnancy, HIV testing, and the prophylaxis use. A team of medical doctors was in charge of developing questions to be submitted to ChatGPT. The other members critically assessed the generated responses regarding level of expertise, accuracy, completeness, and inclusivity. The median accuracy score was 5.5 out of 6, with 88.4% of responses achieving a score ≥ 5. Completeness had a median of 3 out of 3, while the median for inclusivity was 2 out of 3. Some thematic groups, like behaviors associated with HIV transmission and prophylaxis, exhibited higher accuracy, indicating variable performance across different topics. Issues of inclusivity were identified, notably the use of outdated terms and a lack of representation for some communities. ChatGPT demonstrates significant potential in providing accurate information on HIV-related topics. However, while responses were often scientifically accurate, they sometimes lacked the socio-political context and inclusivity essential for effective health communication. This underlines the importance of aligning AI-driven platforms with contemporary health communication strategies and ensuring the balance of accuracy and inclusivity. © The Author(s) 2024.",,,No,Yes,Definitely an ID/primary care clinical task,Yes
Large language models outperform mental and medical health care professionals in identifying obsessive-compulsive disorder,"Despite the promising capacity of large language model (LLM)-powered chatbots to diagnose diseases, they have not been tested for obsessive-compulsive disorder (OCD). We assessed the diagnostic accuracy of LLMs in OCD using vignettes and found that LLMs outperformed medical and mental health professionals. This highlights the potential benefit of LLMs in assisting in the timely and accurate diagnosis of OCD, which usually entails a long delay in diagnosis and treatment. © The Author(s) 2024.",,,Yes,Maybe?,Diagnostic,Yes
"Comparison of ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro, and Ophthalmologists in the management of uveitis and ocular inflammation: A comparative study of large language models; [Comparaison de ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro et des ophtalmologues dans la gestion de l'uvéite et de l'inflammation oculaire : une étude comparative sur de grands modèles linguistiques]","Purpose: The aim of this study was to compare the latest large language models (LLMs) ChatGPT-4o, Google Gemini 1.5 Pro and Microsoft Copilot Pro developed by three different companies, with each other and with a group of ophthalmologists, to reveal the strengths and weaknesses of LLMs against each other and against ophthalmologists in the field of uveitis and ocular inflammation. Methods: Using a personal OphthoQuestions (www.ophthoquestions.com) account, a total of 100 questions from 201 questions on uveitis and ocular inflammation out of a total of 4551 questions on OphthoQuestions, including questions involving multimodal imaging, were included in the study using the randomization feature of the website. In November 2024, ChatGPT-4o, Microsoft Copilot Pro, and Google Gemini 1.5 Pro were asked the same 100 questions: 80 multiple-choice and 20 open-ended questions. Each question was categorized as either true or false. A statistical comparison of the accuracy rates was performed. Results: Among the 100 questions, ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro, and the human group (ophthalmologists) answered 80 (80.00%), 81 (81.00%), 80 (80.00%) and 72 (72.00%) questions, respectively, correctly. In the statistical comparisons between the groups for multiple-choice questions, no significant difference was found between the correct and incorrect response rates of the three LLMs and the human group (P = 0.207, Cochran's Q test). In the statistical comparisons of responses to open-ended questions, there was no significant difference between the correct and incorrect response rates of the three LLMs and the human group (P = 0.392, Cochran's Q test). Conclusion: Although ChatGPT-4o, Google Gemini 1.5 Pro, and Microsoft Copilot Pro answered higher percentages of questions correctly than the human group, the LLMs were not statistically superior to each other or to the human group in the management of uveitis and ocular inflammation. © 2025 Elsevier Masson SAS; Objectif: L'objectif de cette étude était de comparer les derniers grands modèles de langage (LLM) ChatGPT-4o, Google Gemini 1.5 Pro et Microsoft Copilot Pro développés par trois sociétés différentes, entre eux et avec un groupe d'ophtalmologistes, afin de révéler les forces et les faiblesses des LLM les uns par rapport aux autres et par rapport aux ophtalmologistes dans le domaine de l'uvéite et de l'inflammation oculaire. Méthodes: Par l'usage d'un compte personnel OphthoQuestions (www.ophthoquestions.com), un total de 100 questions parmi 201 questions sur l'uvéite et l'inflammation oculaire sur un total de 4551 questions sur OphthoQuestions, y compris les questions impliquant l'imagerie multimodale, ont été incluses dans l’étude en utilisant la fonction de randomisation du site Web. En novembre 2024, les mêmes 100 questions ont été posées à ChatGPT-4o, Microsoft Copilot Pro et Google Gemini 1.5 Pro : 80 questions à choix multiples et 20 questions ouvertes. Chaque question était classée comme vraie ou fausse. Une comparaison statistique des taux de précision a été effectuée. Résultats: Parmi les 100 questions, ChatGPT-4o, Google Gemini 1.5 Pro, Microsoft Copilot Pro et le groupe humain (ophtalmologues) ont répondu correctement à 80 (80,00 %), 81 (81,00 %), 80 (80,00 %) et 72 (72,00 %) d'entre elles, respectivement. Dans les comparaisons statistiques entre les groupes pour les questions à choix multiples, aucune différence significative n'a été trouvée entre les taux de réponses correctes et incorrectes des trois LLM et du groupe humain (p = 0,207, test Q de Cochran). Dans les comparaisons statistiques des réponses aux questions ouvertes, il n'y avait pas de différence significative entre les taux de réponses correctes et incorrectes des trois LLM et du groupe humain (p = 0,392, test Q de Cochran). Conclusion: Bien que ChatGPT-4o, Google Gemini 1.5 Pro et Microsoft Copilot Pro aient répondu correctement à un pourcentage plus élevé de questions que le groupe humain, les LLM n’étaient pas statistiquement supérieurs les uns aux autres ou au groupe humain dans la gestion de l'uvéite et de l'inflammation oculaire. © 2025 Elsevier Masson SAS",,Direct use of LLM to try and manage uveitis and ocular inflamm; however not in english? exclde?,Yes,Maybe?,,Yes
"Accuracy, satisfaction, and impact of custom GPT in acquiring clinical knowledge: Potential for AI-assisted medical education","BACKGROUND: Recent advancements in artificial intelligence (AI) have enabled the customization of large language models to address specific domains such as medical education. This study investigates the practical performance of a custom GPT model in enhancing clinical knowledge acquisition for medical students and physicians. METHODS: A custom GPT was developed by incorporating the latest readily available teaching resources. Its accuracy in providing clinical knowledge was evaluated using a set of clinical questions, and responses were compared against established medical guidelines. Satisfaction was assessed through surveys involving medical students and physicians at different stages and from various types of hospitals. The impact of the custom GPT was further evaluated by comparing its role in facilitating clinical knowledge acquisition with traditional learning methods. RESULTS: The custom GPT demonstrated higher accuracy (83.6%) compared to general AI models (65.5%, 69.1%) and was comparable to a professionally developed AI (Glass Health, 83.6%). Residents reported the highest satisfaction compared to clerks and physicians, citing improved learning independence, motivation, and confidence (p < 0.05). Physicians, especially those from teaching hospitals, showed greater eagerness to develop a custom GPT compared to clerks and residents (p < 0.05). The impact analysis revealed that residents using the custom GPT achieved better test scores compared to those using traditional resources (p < 0.05), though fewer perfect scores were obtained. CONCLUSIONS: The custom GPT demonstrates significant promise as an innovative tool for advancing medical education, particularly for residents. Its capability to deliver accurate, tailored information complements traditional teaching methods, aiding educators in promoting personalized and consistent training. However, it is essential for both learners and educators to remain critical in evaluating AI-generated information. With continued development and thoughtful integration, AI tools like custom GPTs have the potential to significantly enhance the quality and accessibility of medical education.[Box: see text].",,This is use of GPT as a learning tool,Yes,No,Eval is on how the tool improves learning,No
Quality of Information in Carpal Tunnel Syndrome: Social Media Platforms Versus Large Language Models,"Introduction Carpal tunnel syndrome (CTS) is the most common peripheral nerve entrapment disease, and it is a subject of great interest and concern to medical professionals and the general public. Our study aims to analyze and compare the quality and accuracy of the information related to CTS provided by social media platforms (SMPs) and the new large language models (LLM). Methods On YouTube, the first 20 videos in English and the first 20 videos in Spanish when searching for ""carpal tunnel syndrome""and ""síndrome túnel carpo""were selected. On Instagram, the first 20 videos with the hashtag #carpaltunnelsyndrome and #tunelcarpiano were chosen (in total 80 videos). Duration, number of likes, number of views, number of followers, upload date, and author category (medical specialist, patient, etc) were evaluated. Three specific questions about CTS were asked to 2 new LLMs (ChatGPT and Google Bard). The quality of information was analyzed and compared by two independent board-certified plastic surgeons using the Journal of American Medical Association (JAMA) and DISCERN scales. Results LLMs showed a significant higher quality of information when compared with SMPs based on the DISCERN scores (P < 0.05). Average DISCERN scores for answers given by ChatGPT and Google Bard were 52.83 and 57.83, respectively (good quality). In YouTube and Instagram, the average score for the 80 videos based on the JAMA scale was 1.92 (low reliability) and 25.18 (very low quality) on the DISCERN scale. Videos created by medical professionals in SMPs were associated with a higher JAMA and DISCERN scores (P < 0.05). 53.8% of the videos were made by a nonmedical author. Conclusions The quality of information from LLMs was good and significantly better than in SMP. A low participation of board-certified surgeons in SMP was found. Board-certified surgeons should be more involved in LLM and SMPs to increase leadership, improve education, and spread knowledge of peripheral nerve surgery.  © 2025 Wolters Kluwer Health, Inc. All rights reserved.",This is sentiment analysis applied to clinical medicine basically,looking at how LLM does at advice; but it isnt realy the doctors job to be doing social media i guess? i'm torn,No,Maybe?,Not a clinical task,No
Exploring the Potential of Code-Free Custom GPTs in Ophthalmology: An Early Analysis of GPT Store and User-Creator Guidance,"Introduction: OpenAI recently introduced the ability to create custom generative pre-trained transformers (cGPTs) using text-based instruction and/or external documents using retrieval-augmented generation (RAG) architecture without coding knowledge. This study aimed to analyze the features of ophthalmology-related cGPTs and explore their potential utilities. Methods: Data collection took place on January 20 and 21, 2024, and custom GPTs were found by entering ophthalmology keywords into the “Explore GPTS” section of the website. General and specific features of cGPTs were recorded, such as knowledge other than GPT-4 training data. The instruction and description sections were analyzed for compatibility using the Likert scale. We analyzed two custom GPTs with the highest Likert score in detail. We attempted to create a convincingly presented yet potentially harmful cGPT to test safety features. Results: We analyzed 22 ophthalmic cGPTs, of which 55% were for general use and the most common subspecialty was glaucoma (18%). Over half (55%) contained knowledge other than GPT-4 training data. The representation of the instructions through the description was between “Moderately representative” and “Very representative” with a median Likert score of 3.5 (IQR 3.0–4.0). The instruction word count was significantly associated with Likert scores (P = 0.03). Tested cGPTs demonstrated potential for specific conversational tone, information, retrieval and combining knowledge from an uploaded source. With these safety settings, creating a malicious GPT was possible. Conclusions: This is the first study to our knowledge to examine the GPT store for a medical field. Our findings suggest that these cGPTs can be immediately implemented in practice and may offer more targeted and effective solutions compared to the standard GPT-4. However, further research is necessary to evaluate their capabilities and limitations comprehensively. The safety features currently appear to be rather limited. It may be helpful for the user to review the instruction section before using a cGPT. © The Author(s) 2024.",,This is a comparison of early GPTs; it is a look at the GPT store more than an actual test,Yes,No,Not a great study but technically meets criteria. Does report some quantitative metrics regarding LLM response correctness,Yes
Development and Preliminary Evaluation of Remote Pacemaker Monitoring System Using Large Language Model,"A remote monitoring system that periodically transmits information stored in the pacemaker from patients' homes to a hospital is now in widespread use. However, the system requires access to the vendor's cloud server via a browser and consists of date-by-date PDF files, making the creation of aggregate data a significant burden. Since the release of commercially available systems such as ChatG PT, various large language models (LLMs) have been widely used, leading to that semantic search, which can perform searches that take into account the meaning of language, has attracted attention. In this study, we constructed an LLM - based remote monitoring system. Then, as a preliminary evaluation, we examined its effectiveness for RM operations based on the accuracy of RM data aggregation and work time. © 2024 IEEE.",,"This seems more like data collection, but could MAYBE be considered clinical context because it is diagnosing a problem i guess?",No,Maybe?,"This doesn't meet the ""average physician litmus task"" test",No
"Letter to the Editor Regarding the Article: ""Exploring the Capacities of ChatGPT: A Comprehensive Evaluation of Its Accuracy and Repeatability in Addressing Helicobacter pylori-Related Queries""","3082. Helicobacter. 2024 Jul-Aug;29(4):e13131. doi: 10.1111/hel.13131.

Letter to the Editor Regarding the Article: ""Exploring the Capacities of 
ChatGPT: A Comprehensive Evaluation of Its Accuracy and Repeatability in 
Addressing Helicobacter pylori-Related Queries"".

Yang P(1), Jiang J(2).

Author information:
(1)Department of Laboratory Medicine, People's Hospital of Qiannan Prefecture, 
Guizhou, China.
(2)Department of Hepatopancreatobiliary Surgery, Sichuan Cancer Hospital and 
Institute, Sichuan Cancer Center, School of Medicine, University of Electronic 
Science and Technology of China, Chengdu, China.",,This is a letter to the editor on another manuscript around ChatGPT,Yes,No,Letter to the editor,No
Diagnostic Accuracy of ChatGPT for Patients’ Triage; a Systematic Review and Meta-Analysis,"Introduction: Artificial intelligence (AI), particularly ChatGPT developed by OpenAI, has shown the potential to improve diagnostic accuracy and efficiency in emergency department (ED) triage. This study aims to evaluate the diagnostic performance and safety of ChatGPT in prioritizing patients based on urgency in ED settings. Methods: A systematic review and meta-analysis were conducted following PRISMA guidelines. Comprehensive literature searches were performed in Scopus, Web of Science, PubMed, and Embase. Studies evaluating ChatGPT’s diagnostic performance in ED triage were included. Quality assessment was conducted using the QUADAS-2 tool. Pooled accuracy estimates were calculated using a random-effects model, and heterogeneity was assessed with the I2 statistic. Results: Fourteen studies with a total of 1, 412 patients or scenarios were included. ChatGPT 4.0 demonstrated a pooled accuracy of 0.86 (95% CI: 0.64–0.98) with substantial heterogeneity (I2 = 93%). ChatGPT 3.5 showed a pooled accuracy of 0.63 (95% CI: 0.43–0.81) with significant heterogeneity (I2 = 84%). Funnel plots indicated potential publication bias, particularly for ChatGPT 3.5. Quality assessments revealed varying levels of risk of bias and applicability concerns. Conclusions: ChatGPT, especially version 4.0, shows promise in improving ED triage accuracy. However, significant variability and potential biases highlight the need for further evaluation and enhancement. This open-access article distributed under the terms of the Creative Commons Attribution NonCommercial 3.0 License (CC BY-NC 3.0). Downloaded from: https://journals.sbmu.ac.ir/aaem/index.php/AAEM/index",,This is a systematic review,Yes,No,Systematic review,No
MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,"As mental health issues such as anxiety and depression are increasingly prevalent nowadays, we introduce MENTALER, an advanced multi-role collaboration framework specifically designed to enhance large language models (LLMs) in the diagnosis and treatment of mental health issues. In the MENTALER framework, the mental health support process comprises three specialized roles: Analyzer, Knowledge-Collector, and Strategy-Planner. Analyzer guides LLMs to achieve a in-depth diagnosis via multi-stage chain-of-thought prompting. Knowledge-Collector focus on involving domain-specific knowledge from exemplars retrieval. Strategy-Planner integrates professional support strategies into the generation process to further improve the professionalism among the generated texts. Through extensive automatic and human evaluations, we have validated that the mental health support counseling texts generated by MENTALER demonstrate a high degree of fluency and professionalism, closely aligning with real professional counseling texts. Our research advances the application of LLMs in the field of mental health support, providing an innovative and effective tool for those with psychological problems. © 2024 IEEE.",,"This is a framework, not really an LLM",Yes,No,,No
EpiPathAI: Using Large Language Models to Explore Mechanisms of Life Course Exposure-Outcome Associations,"Large language models (LLMs) enhanced with Graph Retrieval-Augmented Generation (GRAG) are promising for life-course epidemiology, which typically depends on costly and incomplete cohort data. Inspired by the epidemiological pathway model, we introduce EpiPathAI, which combines literature-derived causal knowledge graphs with LLMs to mine bridging variables and synthesize potential mechanisms between gestational diabetes and dementia. We test four GRAG strategies on GPT-4 and evaluate the identified mediators with clinical experts and three other LLM reviewers. The knowledge graph identifies 118 bridging variables, including coronary heart disease and chronic kidney disease, previously validated in our data-driven approach through the UK Biobank. EpiPathAI has identified additional clinically meaningful mediators, including high-level low-density lipoprotein (9.8% of effect, 95% CI: 3.7%-23.2%), and depression, which is a reasonable but statistically non-significant mediator in UK Biobank. EpiPathAI serves as a knowledge-driven mechanism mining agent that complements the data-driven approach, providing a compelling foundation for investigating other mediating pathways in future longitudinal cohort studies.",,,No,Yes,Not a clinical task,No
"Aligning, Autoencoding and Prompting Large Language Models for Novel Disease Reporting","Given radiology images, automatic radiology report generation aims to produce informative text that reports diseases. It can benefit current clinical practice in diagnostic radiology. Existing methods typically rely on large-scale medical datasets annotated by clinicians to train desirable models. However, for novel diseases, sufficient training data are typically not available. We propose a prompt-based deep learning framework, i.e., PromptLLM, to align, autoencode, and prompt the (large) language model to generate reports for novel diseases accurately and efficiently. Our method includes three major steps: (1) aligning visual images and textual reports to learn general knowledge across modalities from diseases where labeled data are sufficient, (2) autoencoding the LLM using unlabeled data of novel diseases to learn the specific knowledge and writing styles of the novel disease, and (3) prompting the LLM with learned knowledge and writing styles to report the novel diseases contained in the radiology images. Through the above three steps, with limited labels on novel diseases, we show that PromptLLM can rapidly learn the corresponding knowledge for accurate novel disease reporting. The experiments on COVID-19 and diverse thorax diseases show that our approach, utilizing 1% of the training data, achieves desirable performance compared to previous methods. It shows that our approach allows us to relax the reliance on labeled data that is common to existing methods. It could have a real-world impact on data analysis during the early stages of novel diseases. Our code and data are available at https://github.com/ai-in-health/PromptLLM.",,,Maybe?,Yes,,Yes
Can large language models predict antimicrobial peptide activity and toxicity?,"Antimicrobial peptides (AMPs) are naturally occurring or designed peptides up to a few tens of amino acids which may help address the antimicrobial resistance crisis. However, their clinical development is limited by toxicity to human cells, a parameter which is very difficult to control. Given the similarity between peptide sequences and words, large language models (LLMs) might be able to predict AMP activity and toxicity. To test this hypothesis, we fine-tuned LLMs using data from the Database of Antimicrobial Activity and Structure of Peptides (DBAASP). GPT-3 performed well but not reproducibly for activity prediction and hemolysis, taken as a proxy for toxicity. The later GPT-3.5 performed more poorly and was surpassed by recurrent neural networks (RNN) trained on sequence-activity data or support vector machines (SVM) trained on MAP4C molecular fingerprint-activity data. These simpler models are therefore recommended, although the rapid evolution of LLMs warrants future re-evaluation of their prediction abilities.",,Evaluating the accuracy of LLMs in predicting antimicrobial efficacy and toxicity,No,Yes,Not a clinical task,No
Medical Text Prediction and Suggestion Using Generative Pretrained Transformer Models with Dental Medical Notes,"Background  Generative pretrained transformer (GPT) models are one of the latest large pretrained natural language processing models that enables model training with limited datasets and reduces dependency on large datasets, which are scarce and costly to establish and maintain. There is a rising interest to explore the use of GPT models in health care. Objective  We investigate the performance of GPT-2 and GPT-Neo models for medical text prediction using 374,787 free-text dental notes. Methods  We fine-tune pretrained GPT-2 and GPT-Neo models for next word prediction on a dataset of over 374,000 manually written sections of dental clinical notes. Each model was trained on 80% of the dataset, validated on 10%, and tested on the remaining 10%. We report model performance in terms of next word prediction accuracy and loss. Additionally, we analyze the performance of the models on different types of prediction tokens for categories. For comparison, we also fine-tuned a non-GPT pretrained neural network model, XLNet (large), for next word prediction. We annotate each token in 100 randomly sampled notes by category (e.g., names, abbreviations, clinical terms, punctuation, etc.) and compare the performance of each model by token category. Results  Models present acceptable accuracy scores (GPT-2: 76%; GPT-Neo: 53%), and the GPT-2 model also performs better in manual evaluations, especially for names, abbreviations, and punctuation. Both GPT models outperformed XLNet in terms of accuracy. The results suggest that pretrained models have the potential to assist medical charting in the future. We share the lessons learned, insights, and suggestions for future implementations. Conclusion  The results suggest that pretrained models have the potential to assist medical charting in the future. Our study presented one of the first implementations of the GPT model used with medical notes. © 2022 Georg Thieme Verlag. All rights reserved.",This is a weird study,,No,Yes,"Agree weird study, I think in a weird roundabout way it's trying to do note writing...",Yes
Application of a general LLM-based classification system to retrieve information about oncological trials,"Purpose: The automated classification of clinical trials and medical literature is increasingly relevant, particularly in oncology, as the volume of publications and trial reports continues to expand. Large Language Models (LLMs) may provide new opportunities for automated diverse classification tasks. In this study, we developed a general-purpose text classification framework using LLMs and evaluated its performance on oncological trial classification tasks. Methods and Materials: A general text classification framework with adaptable prompt, model and categories for the classification was developed. The framework was tested with four datasets comprising nine binary classification questions related to oncological trials. Evaluation was conducted using a locally hosted version of Mixtral-8x7B-Instruct v0.1 and three cloud-based LLMs: Mixtral-8x7BInstruct v0.1, Llama3.1-70B-Instruct, and Qwen-2.5-72B. Results: The system consistently produced valid responses with the local Mixtral-8x7B-Instruct model and the Llama3.1-70B-Instruct model. It achieved a response validity rate of 99.70% and 99.88% for the cloud-based Mixtral and Qwen models, respectively. Across all models, the framework achieved an overall accuracy of >94%, precision of >92%, recall of >90%, and an F1-score of >92%. Question-specific accuracy ranged from 86.33% to 99.83% for the local Mixtral model, 85.49% to 99.83% for the cloud-based Mixtral model, 90.50% to 99.83% for the Llama3.1 model, and 77.13% to 99.83% for the Qwen model. Conclusions: The LLM-based classification framework exhibits robust accuracy and adaptability across various oncological trial classification tasks. The findings highlight the potential of automated, LLM-driven trial classification systems, which may become increasingly used in oncology.",,,No,Yes,Trial classification is not a clinical task,No
A Future of Self-Directed Patient Internet Research: Large Language Model-Based Tools Versus Standard Search Engines,"Purpose: As generalist large language models (LLMs) become more commonplace, patients will inevitably increasingly turn to these tools instead of traditional search engines. Here, we evaluate publicly available LLM-based chatbots as tools for patient education through physician review of responses provided by Google, Bard, GPT-3.5 and GPT-4 to commonly searched queries about prevalent chronic health conditions in the United States. Methods: Five distinct commonly Google-searched queries were selected for (i) hypertension, (ii) hyperlipidemia, (iii) diabetes, (iv) anxiety, and (v) mood disorders and prompted into each model of interest. Responses were assessed by board-certified physicians for accuracy, comprehensiveness, and overall quality on a five-point Likert scale. The Flesch-Kincaid Grade Levels were calculated to assess readability. Results: GPT-3.5 (4.40 ± 0.48, 4.29 ± 0.43) and GPT-4 (4.35 ± 0.30, 4.24 ± 0.28) received higher ratings in comprehensiveness and quality than Bard (3.79 ± 0.36, 3.87 ± 0.32) and Google (1.87 ± 0.42, 2.11 ± 0.47), all p < 0.05. However, Bard (9.45 ± 1.35) and Google responses (9.92 ± 5.31) had a lower average Flesch-Kincaid Grade Level compared to GPT-3.5 (14.69 ± 1.57) and GPT-4 (12.88 ± 2.02), indicating greater readability. Conclusion: This study suggests that publicly available LLM-based tools may provide patients with more accurate responses to queries on chronic health conditions than answers provided by Google search. These results provide support for the use of these tools in place of traditional search engines for health-related queries.",,Answering patient's questions,Yes,Maybe?,,Yes
Differentiating between GPT-generated and human-written feedback for radiology residents,"Purpose: Recent competency-based medical education (CBME) implementation within Canadian radiology programs has required faculty to conduct more assessments. The rise of narrative feedback in CBME, coinciding with the rise of large language models (LLMs), raises questions about the potential of these models to generate informative comments matching human experts and associated challenges. This study compares human-written feedback to GPT-3.5-generated feedback for radiology residents, and how well raters can differentiate between these sources. Methods: Assessments were completed by 28 faculty members for 10 residents within a Canadian Diagnostic Radiology program (2019–2023). Comments were extracted from Elentra, de-identified, and parsed into sentences, of which 110 were randomly selected for analysis. 11 of these comments were entered into GPT-3.5, generating 110 synthetic comments that were mixed with actual comments. Two faculty raters and GPT-3.5 read each comment to predict whether it was human-written or GPT-generated. Results: Actual comments from humans were often longer and more specific than synthetic comments, especially when describing clinical procedures and patient interactions. Source differentiation was more difficult when both feedback types were similarly vague. Low agreement (k=-0.237) between responses provided by GPT-3.5 and humans was observed. Human raters were also more accurate (80.5 %) at identifying actual and synthetic comments than GPT-3.5 (50 %). Conclusion: Currently, GPT-3.5 cannot match human experts in delivering specific, nuanced feedback for radiology residents. Compared to humans, GPT-3.5 also performs worse in distinguishing between actual and synthetic comments. These insights could guide the development of more sophisticated algorithms to produce higher-quality feedback, supporting faculty development. © 2025 The Author(s)",Not doing a patient care related task,"Not sure what the point of using LLM to regurgitate formative assessments about residents is. it's one thing if the LLM is able to output high quality constructive feedback based on an input of all the resident's notes or clinical encounters or other metrics of perfomance, but that doesn't seem to be the case here",No,Maybe?,Not a clinical task,No
Ubie Symptom Checker: A Clinical Vignette Simulation Study,"Background AI-driven symptom checkers (SC) are increasingly adopted in healthcare for their potential to provide users with accessible and immediate preliminary health education. These tools, powered by advanced artificial intelligence algorithms, assist patients in quickly assessing their symptoms. Previous studies using clinical vignette approaches have evaluated SC accuracy, highlighting both strengths and areas for improvement. Objective This study aims to evaluate the performance of the Ubie Symptom Checker (Ubie SC) using an innovative large language model-assisted (LLM) simulation method. Methods The study employed a three-phase methodology: gathering 400 publicly available clinical vignettes, medical entity linking these vignettes to the Ubie SC using large language models and physician supervision, and evaluation of accuracy metrics. The analysis focused on 328 vignettes that were within the scope of the Ubie SC with accuracy measured by Top-5 hit rates. Results Ubie achieved a Top-5 hit accuracy of 63.4% and a Top-10 hit accuracy of 71.6%, indicating its effectiveness in providing relevant information based on symptom input. The system performed particularly well in domains such as the nervous system and respiratory conditions, though variability in accuracy was observed across different ICD groupings, highlighting areas for further refinement. When compared to physicians and comparator SC’s that used the same clinical vignettes set, Ubie compared favorably to the median physician hit accuracy. Conclusions The Ubie Symptom Checker shows considerable promise as a supportive education tool in healthcare. While the study highlights the system's strengths, it also identifies areas for improvement suggesting continued refinement and real-world testing are essential to fully realize Ubie's potential in AI-assisted healthcare.",,seems like they're using LLM to create vignettes for simulation testing of Ubie Symptom Checker (which is AI based but don't think it's LLM),Yes,Maybe?,"Study is an evaluation of the Ubie Symptom Checker (not an LLM), only uses an LLM to run ""simulations"" to test the Ubie tool. ",No
"Performance assessment of ChatGPT 4, ChatGPT 3.5, Gemini Advanced Pro 1.5 and Bard 2.0 to problem solving in pathology in French language","Digital teaching diversifies the ways of knowledge assessment, as natural language processing offers the possibility of answering questions posed by students and teachers. Objective: This study evaluated ChatGPT's, Bard's and Gemini's performances on second year of medical studies’ (DFGSM2) Pathology exams from the Health Sciences Center of Dijon (France) in 2018–2022. Methods: From 2018 to 2022, exam scores, discriminating powers and discordance rates were retrieved. Seventy questions (25 first-order single response questions and 45 second-order multiple response questions) were submitted on May 2023 to ChatGPT 3.5 and Bard 2.0, and on September 2024 to Gemini 1.5 and ChatGPT-4. Chatbot's and student's average scores were compared, as well as discriminating powers of questions answered by chatbots. The percentage of student–chatbot identical answers was retrieved, and linear regression analysis correlated the scores of chatbots with student's discordance rates. Chatbot's reliability was assessed by submitting the questions in four successive rounds and comparing score variability using a Fleiss’ Kappa and a Cohen's Kappa. Results: Newer chatbots outperformed both students and older chatbots as for the overall scores and multiple-response questions. All chatbots outperformed students on less discriminating questions. Oppositely, all chatbots were outperformed by students to questions with a high discriminating power. Chatbot's scores were correlated to student discordance rates. ChatGPT 4 and Gemini 1.5 provided variable answers, due to effects linked to prompt engineering. Conclusion: Our study in line with the literature confirms chatbot's moderate performance for questions requiring complex reasoning, with ChatGPT outperforming Google chatbots. The use of NLP software based on distributional semantics remains a challenge for the generation of questions in French. Drawbacks to the use of NLP software in generating questions include the generation of hallucinations and erroneous medical knowledge which have to be taken into count when using NLP software in medical education. © The Author(s) 2025.","Could see an arguemnt either way for this, but these questions for MS2 are likely patient care related concepts. ",,Maybe?,Yes,Agree that second-year path exam is probably similar enough to information pathologist would have to know,Yes
Building a Human Digital Twin (HDTwin) Using Large Language Models for Cognitive Diagnosis: Algorithm Development and Validation,"BACKGROUND: Human digital twins have the potential to change the practice of 
personalizing cognitive health diagnosis because these systems can integrate 
multiple sources of health information and influence into a unified model. 
Cognitive health is multifaceted, yet researchers and clinical professionals 
struggle to align diverse sources of information into a single model.
OBJECTIVE: This study aims to introduce a method called HDTwin, for unifying 
heterogeneous data using large language models. HDTwin is designed to predict 
cognitive diagnoses and offer explanations for its inferences.
METHODS: HDTwin integrates cognitive health data from multiple sources, 
including demographic, behavioral, ecological momentary assessment, n-back test, 
speech, and baseline experimenter testing session markers. Data are converted 
into text prompts for a large language model. The system then combines these 
inputs with relevant external knowledge from scientific literature to construct 
a predictive model. The model's performance is validated using data from 3 
studies involving 124 participants, comparing its diagnostic accuracy with 
baseline machine learning classifiers.
RESULTS: HDTwin achieves a peak accuracy of 0.81 based on the automated 
selection of markers, significantly outperforming baseline classifiers. On 
average, HDTwin yielded accuracy=0.77, precision=0.88, recall=0.63, and Matthews 
correlation coefficient=0.57. In comparison, the baseline classifiers yielded 
average accuracy=0.65, precision=0.86, recall=0.35, and Matthews correlation 
coefficient=0.36. The experiments also reveal that HDTwin yields superior 
predictive accuracy when information sources are fused compared to single 
sources. HDTwin's chatbot interface provides interactive dialogues, aiding in 
diagnosis interpretation and allowing further exploration of patient data.
CONCLUSIONS: HDTwin integrates diverse cognitive health data, enhancing the 
accuracy and explainability of cognitive diagnoses. This approach outperforms 
traditional models and provides an interface for navigating patient information. 
The approach shows promise for improving early detection and intervention 
strategies in cognitive health.

©Gina Sprint, Maureen Schmitter-Edgecombe, Diane Cook. Originally published in 
JMIR Formative Research (https://formative.jmir.org), 23.12.2024.",,comparing an LLM-integrated method with traditional ML classifiers for diagnostic accuracy in cognitive health,No,Yes,"Kind of a weird one, but (1) prediction of ""cognitive health"" i.e. degree of MCI seems a sufficiently clinical task, (2) LLM integration with a chatbot feature",Yes
Cardiac auscultation predicts mortality in elderly patients admitted for COVID-19,"Introduction: COVID-19 has had a great impact on the elderly population. All admitted patients underwent cardiac auscultation at the Emergency Department. However, to our knowledge, there is no literature that explains the implications of cardiac auscultation at the Emergency Department. Material and methods: Data collection from our hospital records. Our cohort consists of 300 admissions with a mean age of 81.6 years and 50.7% men. Results: Pathological cardiac auscultation at the Emergency Department was a risk factor for in-hospital mortality (RR = 1.9; 95% CI 1.3–2.8), heart failure (RR = 3.2; 95% CI = 1.8–5.6), respiratory failure (RR = 1.8; 95% CI = 1.3–2.5), acute kidney injury (RR = 2.6; 95% CI = 2–3.2), and ICU admission (RR = 3.3; 95% CI = 1.3–8.2). The findings in patients with pathological cardiac auscultation were that oxygen saturation in the Emergency Department, arterial pH, and HCO3− were significantly lower, and the ALT/GPT, LDH, and lactate determinations were significantly higher, which is compatible and correlates with the fact that the main variable is indeed a risk factor for a more severe clinical course. Among the findings from pathological auscultation, arrhythmic tone/arrhythmia was the most frequent (50%) and a risk factor for in-hospital mortality (RR = 2.3; 95% CI = 1.6–3.4). Logistic regression was performed from a multivariate analysis that showed that the initial ex novo arrhythmia correlated with pathological cardiac auscultation is an independent risk factor for in-hospital mortality. Conclusion: Continuous rhythm monitoring makes it possible to detect ex novo arrhythmias and act proactively, and to offer greater care and attention to these patients who have a higher risk of in-hospital mortality and a worse prognosis. Cardiac auscultation can alert us in order to perform more electrocardiograms in these patients and thus have better monitoring. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",,Covid is a hoax. Exclude.,No,Yes,LLM where? ,No
ChatGPT as a teaching tool,,Editorial,"sounds like an editorial piece, can't find it on Google Search. please advise, many thanks!",No,Maybe?,Editorial,No
Exploring artificial intelligence literacy and the use of ChatGPT and copilot in instruction on nursing academic report writing,"BACKGROUND: Nursing education increasingly emphasizes academic writing and 
communication, critical for delivering quality patient care and professional 
advancement. Rapidly emerging artificial intelligence (AI) tools such as ChatGPT 
and Copilot are transforming educational methodologies, and a focus is being 
placed on embedding AI literacy to effectively bridge the gap between 
theoretical knowledge and clinical practice. These technologies have the 
potential to reshape nursing education in a technology-driven health-care 
landscape.
AIM: This study investigated the effectiveness of AI literacy and the 
application of ChatGPT and Copilot in academic nursing report writing. It 
assessed the level of AI literacy of nursing students, examined the integration 
of basic AI concepts into a curriculum, and analyzed the impact of these tools 
compared with traditional teaching methods.
METHODS: The study adopted a sample of 203 senior nursing students from Southern 
Taiwan to compare an AI-enhanced teaching approach using ChatGPT and Copilot 
with conventional methods. The curriculum, centered on the ""Writing Case Reports 
and Seminars"" course, employed the Analyze, Design, Develop, Implement, Evaluate 
model and incorporated scaffolding techniques to synergistically integrate 
clinical skills with academic learning. AI literacy was measured using the Meta 
AI Literacy Scale (MAILS). Summative assessments, adhering to the Taiwan Nursing 
Association standards, focused on individual and group case report evaluations.
FINDINGS: Following an 18-week AI intervention, the experimental group 
demonstrated significant improvements in all dimensions of the MAILS. A ChatGPT 
usage of 100 % was found, with a notable enhancement discovered in the ""Nursing 
Plan"" section of case reports. Although the experimental group outperformed the 
control group in overall case report evaluations, the connections between 
identified problems and proposed plans were weaker and nursing interventions 
tended to be less individualized for the experimental group.
CONCLUSIONS: The incorporation of AI tools such as ChatGPT and Copilot into a 
scaffolding teaching framework significantly boosted students' AI literacy and 
performance in summative assessments. Effective AI training for students, 
supervised use of these tools, and continuous professional development for 
educators are paramount to successful implementation. Addressing the current 
limitations of AI has the potential to further improve academic writing, foster 
critical thinking, and ensure responsible application in patient care, 
ultimately leading to higher-quality and more effective nursing education.

Copyright © 2025 Elsevier Ltd. All rights reserved.",,"explores integration of LLM in nursing education, could argue that it's like those USMLE LLM studies",No,Maybe?,"The homework task being evaluated is writing of a case report, not clinical",No
Explainability to Business: Demystify Transformer Models with Attention-based Explanations,"Recently, many companies are relying on Natural Language Processing (NLP) techniques to understand the text data generated daily. It has become very critical to deal with this data because finding the sentiments of text and summarizing them will help the company understand the pain points of the customers posting reviews on social media or understand the experience of the customer. These requirements have increasingly demanded many advanced algorithms to deal the text data. The introduction of Transformers led to businesses adopting NLP methods more and more to keep up with their needs. Models like Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT), state-of-the-art results were achieved with billions of parameters learned. Although these advancements improved the accuracy and expanded the use of algorithms to a wide range of NLP tasks like language translation, text summarization, and language modeling. Businesses are more interested in the Explainability of the model compared to its accuracy. Explainable Artificial Intelligence (XAI) plays an important role to comprehend the complexities of the model as well as the influence of weights on predictions. In this paper, the complexities of the transformer model are unraveled by presenting a straightforward method for computing explainable predictions. The DistilBERT model is chosen as an example to implement the explainable system due to its lighter nature. Combining the strengths of a Posthoc expla-nation with those of a self-learning neural network, the method makes it simple to scale it to other algorithms to implement. With technologies like python, PyTorch, and Hugging Face, a detailed step-by-step algorithmic computation is demonstrated to explain the predictions from the attention-based explanations. © 2023 IEEE.",,Something with business and nothing with clinical evals of LLMs,No,Yes,Completely unrelated to medicine,No
A comparison of drug information question responses by a drug information center and by ChatGPT,"DISCLAIMER: In an effort to expedite the publication of articles, AJHP is posting manuscripts online as soon as possible after acceptance. Accepted manuscripts have been peer-reviewed and copyedited, but are posted online before technical formatting and author proofing. These manuscripts are not the final version of record and will be replaced with the final article (formatted per AJHP style and proofed by the authors) at a later time. PURPOSE: A study was conducted to assess the accuracy and ability of Chat Generative Pre-trained Transformer (ChatGPT) to systematically respond to drug information inquiries relative to responses of a drug information center (DIC). METHODS: Ten drug information questions answered by the DIC in 2022 or 2023 were selected for analysis. Three pharmacists created new ChatGPT accounts and submitted each question to ChatGPT at the same time. Each question was submitted twice to identify consistency in responses. Two days later, the same process was conducted by a fourth pharmacist. Phase 1 of data analysis consisted of a drug information pharmacist assessing all 84 ChatGPT responses for accuracy relative to the DIC responses. In phase 2, 10 ChatGPT responses were selected to be assessed by 3 blinded reviewers. Reviewers utilized an 8-question predetermined rubric to evaluate the ChatGPT and DIC responses. RESULTS: When comparing the ChatGPT responses (n = 84) to the DIC responses, ChatGPT had an overall accuracy rate of 50%. Accuracy across the different question types varied. In regards to the overall blinded score, ChatGPT responses scored higher than the responses by the DIC according to the rubric (overall scores of 67.5% and 55.0%, respectively). The DIC responses scored higher in the categories of references mentioned and references identified. CONCLUSION: Responses generated by ChatGPT have been found to be better than those created by a DIC in clarity and readability; however, the accuracy of ChatGPT responses was lacking. ChatGPT responses to drug information questions would need to be carefully reviewed for accuracy and completeness.",,Comparing accuracy and qualtiy of ChatGPT-generated responses to gold standard (drug information center) for answering questions about drugs/pharmacy,No,Yes,"Presuming per the slack channel that ""clinical task"" means physician, then no",No
Integrating Sensor Technologies with Conversational AI: Enhancing Context-Sensitive Interaction Through Real-Time Data Fusion,"This article examines how sensor technologies (such as environmental sensors, biometric sensors, and IoT devices) intersect with conversational AI models like ChatGPT 4.0. In particular, this article explores how data from different sensors in real time can improve AI models' comprehension of surroundings, user contexts, and physical conditions. Lastly, this article delves into the scientific principles supporting sensor technologies, data processing methods, and their fusion with generative models such as ChatGPT to develop adaptable, dynamic systems that engage with humans intelligently in real time. Some of the specific topics that are investigated include the science behind sensor networks and acquiring real-time data, how ChatGPT can analyze sensor data to generate dialogue that is sensitive to context, instances in healthcare (such as using wearable sensors along with AI chatbots for patient treatment), and smart homes (interaction with AI assistants driven by sensors). These subjects will prove advantageous for researchers in sensor technology as well as AI development, showcasing interdisciplinary progress in smart systems.",,"Sounds like a review paper, given the lack of tangible results provided in the abstract",No,Maybe?,Review,No
Integrative diagnosis of psychiatric conditions using ChatGPT and fMRI data,"BACKGROUND: Traditional diagnostic methods for psychiatric disorders often rely 
on subjective assessments, leading to inconsistent diagnoses. Integrating 
advanced natural language processing (NLP) techniques with neuroimaging data may 
improve diagnostic accuracy.
METHODS: We propose a novel approach that uses ChatGPT to conduct interactive 
patient interviews, capturing nuanced emotional and psychological data. By 
analyzing these dialogues using NLP, we generate a comprehensive feature matrix. 
This matrix, combined with 4D fMRI data, is input into a neural network to 
predict psychiatric diagnoses. We conducted comparative analysis with 
survey-based and app-based methods, providing detailed statistical validation.
RESULTS: Our model achieved an accuracy of 85.7%, significantly outperforming 
traditional methods. Statistical analysis confirmed the superiority of the 
ChatGPT-based approach in capturing nuanced patient information, with p-values 
indicating significant improvements over baseline models.
CONCLUSIONS: Integrating NLP-driven patient interactions with fMRI data offers a 
promising approach to psychiatric diagnosis, enhancing precision and 
reliability. This method could advance clinical practice by providing a more 
objective and comprehensive diagnostic tool, although more research is needed to 
generalize these findings.

© 2025. The Author(s).",Technically should be included but oh my gosh this is so sus,"custom model, just used ChatGPT for extraction",Yes,No,"LLM generates conversation with patient, but actual task of diagnostic prediction is done by a custom model on embeddings from that conversation ",No
Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis,"Background: As large language models (LLMs) are becoming increasingly integrated into different aspects of health care, questions about the implications for medical academic literature have begun to emerge. Key aspects such as authenticity in academic writing are at stake with artificial intelligence (AI) generating highly linguistically accurate and grammatically sound texts. Objective: The objective of this study is to compare human-written with AI-generated scientific literature in orthopedics and sports medicine. Methods: Five original abstracts were selected from the PubMed database. These abstracts were subsequently rewritten with the assistance of 2 LLMs with different degrees of proficiency. Subsequently, researchers with varying degrees of expertise and with different areas of specialization were asked to rank the abstracts according to linguistic and methodological parameters. Finally, researchers had to classify the articles as AI generated or human written. Results: Neither the researchers nor the AI-detection software could successfully identify the AI-generated texts. Furthermore, the criteria previously suggested in the literature did not correlate with whether the researchers deemed a text to be AI generated or whether they judged the article correctly based on these parameters. Conclusions: The primary finding of this study was that researchers were unable to distinguish between LLM-generated and human-written texts. However, due to the small sample size, it is not possible to generalize the results of this study. As is the case with any tool used in academic research, the potential to cause harm can be mitigated by relying on the transparency and integrity of the researchers. With scientific integrity at stake, further research with a similar study design should be conducted to determine the magnitude of this issue. © 2024 JMIR Publications Inc.. All rights reserved.",,comparing LLM-generated medical/scientific articles versus human written,No,Yes,Research != clinical task,No
Applications of Large Language Models (LLMs) in Breast Cancer Care,"Purpose: Recently introduced Large Language Models (LLMs) such as ChatGPT have already shown promising results in natural language processing in healthcare. The aim of this study is to systematically review the literature on the applications of LLMs in breast cancer diagnosis and care. Methods: A literature search was conducted using MEDLINE, focusing on studies published up to October 22nd, 2023, using the following terms: “large language models”, “LLM”, “GPT”, “ChatGPT”, “OpenAI”, and “breast”. Results: Five studies met our inclusion criteria. All studies were published in 2023, focusing on ChatGPT-3.5 or GPT-4 by OpenAI. Applications included information extraction from clinical notes, question-answering based on guidelines, and patients’ management recommendations. The rate of correct answers varied from 64-98%, with the highest accuracy (88-98%) observed in information extraction and question-answering tasks. Notably, most studies utilized real patient data rather than data sourced from the internet. Limitations included inconsistent accuracy, prompt sensitivity, and overlooked clinical details, highlighting areas for cautious LLM integration into clinical practice. Conclusion: LLMs demonstrate promise in text analysis tasks related to breast cancer care, including information extraction and guideline-based question-answering. However, variations in accuracy and the occurrence of erroneous outputs necessitate validation and oversight. Future works should focus on improving reliability of LLMs within clinical workflow.",,Review,Yes,No,Review,No
Development of a Human Evaluation Framework and Correlation with Automated Metrics for Natural Language Generation of Medical Diagnoses,"In the evolving landscape of clinical Natural Language Generation (NLG), assessing abstractive text quality remains challenging, as existing methods often overlook generative task complexities. This work aimed to examine the current state of automated evaluation metrics in NLG in healthcare. To have a robust and well-validated baseline with which to examine the alignment of these metrics, we created a comprehensive human evaluation framework. Employing ChatGPT-3.5-turbo generative output, we correlated human judgments with each metric. None of the metrics demonstrated high alignment; however, the SapBERT score—a Unified Medical Language System (UMLS)- showed the best results. This underscores the importance of incorporating domain-specific knowledge into evaluation efforts. Our work reveals the deficiency in quality evaluations for generated text and introduces our comprehensive human evaluation framework as a baseline. Future efforts should prioritize integrating medical knowledge databases to enhance the alignment of automated metrics, particularly focusing on refining the SapBERT score for improved assessments.",,method to evaluate quality of NLG texts,Yes,No,"Not an evaluation of LLM capabilities, an evaluation of metrics to evaluate LLMs",No
Artificial intelligence in neurology; [Künstliche Intelligenz in der Neurologie],"Ever since the publication of ChatGPT everyone is talking about Artificial intelligence (AI). Every AI-algorithm is based on the analysis of data. In neurology, digitalization has created sufficiently large amounts of data to be analyzed. This article aims to provide an overview of AI models, as well as current research and applications of AI in the field of neurology. It will also briefly highlight potential problems in the integration of AI into clinical practice and provide an outlook for the future. © 2023 Georg Thieme Verlag. All rights reserved.",,"Anyone here reads german? I presume inapplicable given the tiny abstract, but i still would check in this case.",No,Maybe?,Seems like a review,No
Extracting social support and social isolation information from clinical psychiatry notes: comparing a rule-based natural language processing system and a large language model,"Objectives: Social support (SS) and social isolation (SI) are social determinants of health (SDOH) associated with psychiatric outcomes. In electronic health records (EHRs), individual-level SS/SI is typically documented in narrative clinical notes rather than as structured coded data. Natural language processing (NLP) algorithms can automate the otherwise labor-intensive process of extraction of such information. Materials and Methods: Psychiatric encounter notes from Mount Sinai Health System (MSHS, n = 300) and Weill Cornell Medicine (WCM, n = 225) were annotated to create a gold-standard corpus. A rule-based system (RBS) involving lexicons and a large language model (LLM) using FLAN-T5-XL were developed to identify mentions of SS and SI and their subcategories (eg, social network, instrumental support, and loneliness). Results: For extracting SS/SI, the RBS obtained higher macroaveraged F1-scores than the LLM at both MSHS (0.89 versus 0.65) and WCM (0.85 versus 0.82). For extracting the subcategories, the RBS also outperformed the LLM at both MSHS (0.90 versus 0.62) and WCM (0.82 versus 0.81). Discussion and Conclusion: Unexpectedly, the RBS outperformed the LLMs across all metrics. An intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM. The RBS was designed and refined to follow the same specific rules as the gold-standard annotations. Conversely, the LLM was more inclusive with categorization and conformed to common English-language understanding. Both approaches offer advantages, although additional replication studies are warranted. © The Author(s) 2024. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.",,"using LLM to extract specific verbage from EHR, not directy relevant to clinical/medical care as it focuses on social support and social isolation themes",Yes,No,"Agree, task is data extraction",No
LMCG-Net: Integrating LLMs and ECG for LVEF-Based Classification for Pacing Patients,"Detecting left ventricular systolic dysfunction (LVSD) traditionally relies on expensive and specialized echocardiography, limiting accessibility for many patients. To address this, researchers explore the potential of electrocardiography (ECG), a more affordable and widely available alternative, despite its historically limited performance in cardiac dysfunction detection. In this study, we present a novel approach that integrates a 1D convolutional neural networks (CNNs) with a large-scale language model (LLM) to simultaneously analyze sequential ECG data and non-sequential clinical metadata. To validate our model's effectiveness, we conducted rigorous comparative experiments on both specially collected clinical data and public datasets, achieving an impressive AUROC of 0.97 across both. Our findings underscore the capability of ECG-based AI to accurately predict LVSD including pacemaker patients, offering a rapid and cost-effective alternative to traditional echocardiography. This innovative approach could significantly enhance early diagnosis and management of cardiac dysfunction. © 2024 IEEE.",,"employs an AI model that combines 1D CNN w/ LLM, not sure if Sully wants pure LLM stuff or not. this one is a hybrid",No,Maybe?,I mean technically does use a LLM for a clinical task - though combining with the CNN is tough to sort out,Yes
Health literacy in ChatGPT: exploring the potential of the use of artificial intelligence to produce academic text; [A literacia em saúde no ChatGPT: explorando o potencial de uso de inteligência artificial para a elaboração de textos acadêmicos Artigo Temático],"The aim of this study was to identify and analyze the main constituent elements of text generated by ChatGPT in response to questions on an emerging topic in the academic literature in Portuguese – health literacy – and discuss how the evidence produced can contribute to improv-ing our understanding of the limits and challenges of using artificial intelligence (AI) in academic writing. We conducted an exploratory descriptive study based on responses to five consecutive questions in Portuguese and English with increasing levels of complexity put to ChatGPT. Our findings reveal the potential of the use of widely avail-able, unrestricted access AI-based technologies like ChatGPT for academic writing. Featuring a simple and intuitive interface, the tool generated structured and coherent text using natural-like language. Considering that academic productiv-ism is associated with a growing trend in profes-sional misconduct, especially plagiarism, there is a need too take a careful look at academic writing and scientific knowledge dissemination processes mediated by AI technologies. © 2024, Associacao Brasileira de Pos - Graduacao em Saude Coletiva. All rights reserved.",,"specifically explores the pitfalls of widely accessible AI in scientific literature, from the lens of identifying AI-generated text for plagarism screening",No,Maybe?,not a clinical task,No
General-Purpose Large Language Models Versus a Domain-Specific Natural Language Processing Tool for Label Extraction From Chest Radiograph Reports,"Plain Language Summary: GPT-4 outperformed a radiology domain-specific natural 
language processing model in classifying imaging findings from chest radiograph 
reports, both with and without predefined labels. Prompt engineering for context 
further improved performance. The findings indicate a role for large language 
models to accelerate artificial intelligence model development in radiology by 
automating data annotation.",,LLM-augmented extraction of labels from radiology reports to automate data annotation,Yes,Maybe?,"Technically I think this structured data extraction from a radiology report, which is more the ""data processing"" than the ""clinical task"" bucket",No
From Revisions to Insights: Converting Radiology Report Revisions into Actionable Educational Feedback Using Generative AI Models,"2871. J Imaging Inform Med. 2025 Apr;38(2):1265-1279. doi: 10.1007/s10278-024-01233-4. 
Epub 2024 Aug 19.

From Revisions to Insights: Converting Radiology Report Revisions into 
Actionable Educational Feedback Using Generative AI Models.

Lyo S(1), Mohan S(2), Hassankhani A(2), Noor A(2), Dako F(2), Cook T(2).

Author information:
(1)Department of Radiology, Hospital of the University of Pennsylvania, 
Philadelphia, PA, USA. shawn.kt.lyo@gmail.com.
(2)Department of Radiology, Hospital of the University of Pennsylvania, 
Philadelphia, PA, USA.

Expert feedback on trainees' preliminary reports is crucial for radiologic 
training, but real-time feedback can be challenging due to non-contemporaneous, 
remote reading and increasing imaging volumes. Trainee report revisions contain 
valuable educational feedback, but synthesizing data from raw revisions is 
challenging. Generative AI models can potentially analyze these revisions and 
provide structured, actionable feedback. This study used the OpenAI GPT-4 Turbo 
API to analyze paired synthesized and open-source analogs of preliminary and 
finalized reports, identify discrepancies, categorize their severity and type, 
and suggest review topics. Expert radiologists reviewed the output by grading 
discrepancies, evaluating the severity and category accuracy, and suggested 
review topic relevance. The reproducibility of discrepancy detection and maximal 
discrepancy severity was also examined. The model exhibited high sensitivity, 
detecting significantly more discrepancies than radiologists (W = 19.0, 
p < 0.001) with a strong positive correlation (r = 0.778, p < 0.001). Interrater 
reliability for severity and type were fair (Fleiss' kappa = 0.346 and 0.340, 
respectively; weighted kappa = 0.622 for severity). The LLM achieved a weighted 
F1 score of 0.66 for severity and 0.64 for type. Generated teaching points were 
considered relevant in ~ 85% of cases, and relevance correlated with the maximal 
discrepancy severity (Spearman ρ = 0.76, p < 0.001). The reproducibility was 
moderate to good (ICC (2,1) = 0.690) for the number of discrepancies and 
substantial for maximal discrepancy severity (Fleiss' kappa = 0.718; weighted 
kappa = 0.94). Generative AI models can effectively identify discrepancies in 
report revisions and generate relevant educational feedback, offering promise 
for enhancing radiology training.

© 2024. The Author(s).",,"seems very niche, doesn't have broad clinical applicability nor does it seem compelling to other specialties",No,Maybe?,,No
AI-driven translations for kidney transplant equity in Hispanic populations,"Health equity and accessing Spanish kidney transplant information continues being a substantial challenge facing the Hispanic community. This study evaluated ChatGPT's capabilities in translating 54 English kidney transplant frequently asked questions (FAQs) into Spanish using two versions of the AI model, GPT-3.5 and GPT-4.0. The FAQs included 19 from Organ Procurement and Transplantation Network (OPTN), 15 from National Health Service (NHS), and 20 from National Kidney Foundation (NKF). Two native Spanish-speaking nephrologists, both of whom are of Mexican heritage, scored the translations for linguistic accuracy and cultural sensitivity tailored to Hispanics using a 1-5 rubric. The inter-rater reliability of the evaluators, measured by Cohen's Kappa, was 0.85. Overall linguistic accuracy was 4.89 ± 0.31 for GPT-3.5 versus 4.94 ± 0.23 for GPT-4.0 (non-significant p = 0.23). Both versions scored 4.96 ± 0.19 in cultural sensitivity (p = 1.00). By source, GPT-3.5 linguistic accuracy was 4.84 ± 0.37 (OPTN), 4.93 ± 0.26 (NHS), 4.90 ± 0.31 (NKF). GPT-4.0 scored 4.95 ± 0.23 (OPTN), 4.93 ± 0.26 (NHS), 4.95 ± 0.22 (NKF). For cultural sensitivity, GPT-3.5 scored 4.95 ± 0.23 (OPTN), 4.93 ± 0.26 (NHS), 5.00 ± 0.00 (NKF), while GPT-4.0 scored 5.00 ± 0.00 (OPTN), 5.00 ± 0.00 (NHS), 4.90 ± 0.31 (NKF). These high linguistic and cultural sensitivity scores demonstrate Chat GPT effectively translated the English FAQs into Spanish across systems. The findings suggest Chat GPT's potential to promote health equity by improving Spanish access to essential kidney transplant information. Additional research should evaluate its medical translation capabilities across diverse contexts/languages. These English-to-Spanish translations may increase access to vital transplant information for underserved Spanish-speaking Hispanic patients.",,"I've put a maybe just to attract attention but i would lean towards a No, since translation is not something physicians would typically do, but i can imagine in some narrow circumstance would. Thoughts?",No,Maybe?,,No
Our journey with an AI assistant offering narrative therapy on WhatsApp,"This study examines the integration of AI chatbots into social work, with a focus on democratising AI for use in Narrative Therapy (NT). As AI becomes more accessible, social work encounters new opportunities to utilise these tools in practice, but also faces significant challenges. The study explores the feasibility of using customisable GPT platforms to develop an AI chatbot for NT sessions and assesses its performance. An autoethnographic case study was conducted using a chatbot configured with the OpenAI Assistant API to perform NT on WhatsApp. Findings suggest that while practitioners can configure the chatbot to guide users through NT stages and adapt to interactions, there are still limitations, such as the need for basic coding and technical knowledge for deployment and enhancement. The study highlights both the potential and the challenges of AI in social work, advocating for enhanced training, interdisciplinary collaboration, and the cautious development of AI-assisted practices. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",,focus on social work. seems out of scope imo,No,Maybe?,,No
GPT-4 and histopathological image detection and classification of colorectal adenomas,"3213. J Clin Pathol. 2024 May 17;77(6):383. doi: 10.1136/jcp-2024-209405.

GPT-4 and histopathological image detection and classification of colorectal 
adenomas.

Daungsupawong H(1), Wiwanitkit V(2).

Author information:
(1)Private Academic Consultant, Vientiane, Lao People's Democratic Republic 
hinpetchdaung@gmail.com.
(2)DY Patil Vidhyapeeth, Pune, India.",,"This is a correspondence, which wouldn't be obvious from the extracted abstract, but obvious on the journal's website.",Yes,No,,No
Using AI to Translate and Simplify Spanish Orthopedic Medical Text: Instrument Validation Study,"Background: Language barriers contribute significantly to health care disparities in the United States, where a sizable proportion of patients are exclusively Spanish speakers. In orthopedic surgery, such barriers impact both patients’ comprehension of and patients’ engagement with available resources. Studies have explored the utility of large language models (LLMs) for medical translation but have yet to robustly evaluate artificial intelligence (AI)–driven translation and simplification of orthopedic materials for Spanish speakers. Objective: This study used the bilingual evaluation understudy (BLEU) method to assess translation quality and investigated the ability of AI to simplify patient education materials (PEMs) in Spanish. Methods: PEMs (n=78) from the American Academy of Orthopaedic Surgery were translated from English to Spanish, using 2 LLMs (GPT-4 and Google Translate). The BLEU methodology was applied to compare AI translations with professionally human-translated PEMs. The Friedman test and Dunn multiple comparisons test were used to statistically quantify differences in translation quality. A readability analysis and feature analysis were subsequently performed to evaluate text simplification success and the impact of English text features on BLEU scores. The capability of an LLM to simplify medical language written in Spanish was also assessed. Results: As measured by BLEU scores, GPT-4 showed moderate success in translating PEMs into Spanish but was less successful than Google Translate. Simplified PEMs demonstrated improved readability when compared to original versions (P<.001) but were unable to reach the targeted grade level for simplification. The feature analysis revealed that the total number of syllables and average number of syllables per sentence had the highest impact on BLEU scores. GPT-4 was able to significantly reduce the complexity of medical text written in Spanish (P<.001). Conclusions: Although Google Translate outperformed GPT-4 in translation accuracy, LLMs, such as GPT-4, may provide significant utility in translating medical texts into Spanish and simplifying such texts. We recommend considering a dual approach—using Google Translate for translation and GPT-4 for simplification—to improve medical information accessibility and orthopedic surgery education among Spanish-speaking patients. © Saman Andalib, Aidin Spina, Bryce Picton, Sean S Solomon, John A Scolaro, Ariana M Nelson.",,LLM for translation of medical information to patients to mitigate the effect of language barriers in clinical care (orthopedics),No,Yes,I think consensus is that translation is not an average clinical task,No
Extracting symptoms from free-text responses using ChatGPT among COVID-19 cases in Hong Kong,"Objectives: To investigate the feasibility and performance of Chat Generative Pretrained Transformer (ChatGPT) in converting symptom narratives into structured symptom labels. Methods: We extracted symptoms from 300 deidentified symptom narratives of COVID-19 patients by a computer-based matching algorithm (the standard), and prompt engineering in ChatGPT. Common symptoms were those with a prevalence >10% according to the standard, and similarly less common symptoms were those with a prevalence of 2–10%. The precision of ChatGPT was compared with the standard using sensitivity and specificity with 95% exact binomial CIs (95% binCIs). In ChatGPT, we prompted without examples (zero-shot prompting) and with examples (few-shot prompting). Results: In zero-shot prompting, GPT-4 achieved high specificity (0.947 [95% binCI: 0.894–0.978]—1.000 [95% binCI: 0.965–0.988, 1.000]) for all symptoms, high sensitivity for common symptoms (0.853 [95% binCI: 0.689–0.950]—1.000 [95% binCI: 0.951–1.000]), and moderate sensitivity for less common symptoms (0.200 [95% binCI: 0.043–0.481]—1.000 [95% binCI: 0.590–0.815, 1.000]). Few-shot prompting increased the sensitivity and specificity. GPT-4 outperformed GPT-3.5 in response accuracy and consistent labelling. Discussion: This work substantiates ChatGPT's role as a research tool in medical fields. Its performance in converting symptom narratives to structured symptom labels was encouraging, saving time and effort in compiling the task-specific training data. It potentially accelerates free-text data compilation and synthesis in future disease outbreaks and improves the accuracy of symptom checkers. Focused prompt training addressing ambiguous descriptions impacts medical research positively. © 2023 European Society of Clinical Microbiology and Infectious Diseases",,"exploring utility of ChatGPT in extracting relevant symptoms from patient narratives, with the overarching goal of helping physicians narrow down diagnosis",No,Maybe?,structured data extraction,No
Instructional support on first aid in choking by an artificial intelligence-powered chatbot,"716. Am J Emerg Med. 2023 Aug;70:200-202. doi: 10.1016/j.ajem.2023.06.010. Epub 2023 
Jun 10.

Instructional support on first aid in choking by an artificial 
intelligence-powered chatbot.

Birkun AA(1), Gautam A(2).

Author information:
(1)Department of General Surgery, Anesthesiology, Resuscitation and Emergency 
Medicine, Medical Academy named after S.I. Georgievsky of V.I. Vernadsky Crimean 
Federal University, Lenin Blvd, 5/7, Simferopol 295051, Russian Federation. 
Electronic address: birkunalexei@gmail.com.
(2)Regional Government Hospital, Una (H.P.), 174303, India.",,Insufficient results provided in Abstract to make a reasonable assessment,No,Maybe?,,No
"Retraction Note: Diagnostic power of ChatGPT 4 in distal radius fracture detection through wrist radiographs (Archives of Orthopaedic and Trauma Surgery, (2024), 144, 5, (2461-2467), 10.1007/s00402-024-05298-2)","Authors have retracted this article because they lack proper permissions for X-rays and radiological reports employed in this study. Wolfram Demmer, Sinan Mert, Benedikt Fuchs, Elisabeth M. Hass-Lützenberger, Riccardo E. Giunta, Patrick Stoerzer, and Johannes Brauer agree to this retraction. Tim Nuernberger has not responded to correspondence from the publisher about this retraction.",,article was retracted... idk if that should affect anything? - Anton,No,Maybe?,Retracted,No
Bridging the Literacy Gap for Surgical Consents: An AI-Human Expert Collaborative Approach,"Background: Despite the importance of informed consent in healthcare, the readability and specificity of consent forms often impedes patients’ comprehension. Health literacy is linked to patient outcomes, making it essential to address these issues. This study investigates the use of GPT-4 to simplify surgical consent forms and introduces an AI-human expert collaborative approach to validate content appropriateness. Methods: Consent forms from multiple institutions were assessed for readability and simplified using GPT-4, with pre- and post-simplification readability metrics compared using nonparametric tests. Independent reviews by medical authors and a malpractice defense attorney were conducted. Finally, GPT-4's potential for generating de novo procedure-specific consent forms was assessed, with forms evaluated using a validated 8-item rubric and expert subspecialty surgeon review. Results: Analysis of 15 academic medical centers' consent forms revealed significant reductions in average reading time, word rarity, and passive sentence frequency (all P<0.05) following GPT-4-faciliated simplification. Readability improved from an average college freshman to an 8th-grade level (P=0.004), matching the average American's reading level. Medical and legal sufficiency consistency was confirmed. GPT-4 generated procedure-specific consent forms for five varied surgical procedures at an average 6th-grade reading level. These forms received perfect scores on a standardized consent form rubric and withstood scrutiny upon expert subspeciality surgeon review. Conclusions: This study demonstrates the first AI-human expert collaboration to enhance surgical consent forms, significantly improving readability without sacrificing clinical detail. Our framework could be extended to other patient communication materials, emphasizing clear communication and mitigating disparities related to health literacy barriers. Ensuring AI technologies are safely incorporated into clinical practice is crucial to reach a wide range of patients, including the most vulnerable.",,,No,Yes,Simplification of consent forms to a 6th grade level isn't really a clinical task,No
Large Language Models to Help Appeal Denied Radiotherapy Services,"PURPOSELarge language model (LLM) artificial intelligences may help physicians appeal insurer denials of prescribed medical services, a task that delays patient care and contributes to burnout. We evaluated LLM performance at this task for denials of radiotherapy services.METHODSWe evaluated generative pretrained transformer 3.5 (GPT-3.5; OpenAI, San Francisco, CA), GPT-4, GPT-4 with internet search functionality (GPT-4web), and GPT-3.5ft. The latter was developed by fine-Tuning GPT-3.5 via an OpenAI application programming interface with 53 examples of appeal letters written by radiation oncologists. Twenty test prompts with simulated patient histories were programmatically presented to the LLMs, and output appeal letters were scored by three blinded radiation oncologists for language representation, clinical detail inclusion, clinical reasoning validity, literature citations, and overall readiness for insurer submission.RESULTSInterobserver agreement between radiation oncologists' scores was moderate or better for all domains (Cohen's kappa coefficients: 0.41-0.91). GPT-3.5, GPT-4, and GPT-4web wrote letters that were on average linguistically clear, summarized provided clinical histories without confabulation, reasoned appropriately, and were scored useful to expedite the insurance appeal process. GPT-4 and GPT-4web letters demonstrated superior clinical reasoning and were readier for submission than GPT-3.5 letters (P <.001). Fine-Tuning increased GPT-3.5ft confabulation and compromised performance compared with other LLMs across all domains (P <.001). All LLMs, including GPT-4web, were poor at supporting clinical assertions with existing, relevant, and appropriately cited primary literature.CONCLUSIONWhen prompted appropriately, three commercially available LLMs drafted letters that physicians deemed would expedite appealing insurer denials of radiotherapy services. LLMs may decrease this task's clerical workload on providers. However, LLM performance worsened when fine-Tuned with a task-specific, small training data set.  © 2024 by American Society of Clinical Oncology.",,seems like a problem that physicians struggle with and not directly related to patient care or clinical outcomes imo,No,Maybe?,"Edge case, but agree that writing an appeal letter isn't fully a ""clinical task"" ",No
Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models,"3900. Bioinformatics. 2024 Jun 28;40(Suppl 1):i119-i129. doi: 
10.1093/bioinformatics/btae238.

Improving medical reasoning through retrieval and self-reflection with 
retrieval-augmented large language models.

Jeong M(1), Sohn J(1), Sung M(2), Kang J(1)(3).

Author information:
(1)Department of Computer Science, Korea University, Seoul 02841, Republic of 
Korea.
(2)Department of Software Convergence, School of Computing, Kyung Hee 
University, Republic of Korea.
(3)AIGEN Sciences, Seoul 04778, Republic of Korea.

SUMMARY: Recent proprietary large language models (LLMs), such as GPT-4, have 
achieved a milestone in tackling diverse challenges in the biomedical domain, 
ranging from multiple-choice questions to long-form generations. To address 
challenges that still cannot be handled with the encoded knowledge of LLMs, 
various retrieval-augmented generation (RAG) methods have been developed by 
searching documents from the knowledge corpus and appending them unconditionally 
or selectively to the input of LLMs for generation. However, when applying 
existing methods to different domain-specific problems, poor generalization 
becomes apparent, leading to fetching incorrect documents or making inaccurate 
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for 
biomedical text that specializes in generating explanations, retrieving 
domain-specific documents, and self-reflecting generated responses. We utilize 
84k filtered biomedical instruction sets to train Self-BioRAG that can assess 
its generated explanations with customized reflective tokens. Our work proves 
that domain-specific components, such as a retriever, domain-related document 
corpus, and instruction sets are necessary for adhering to domain-related 
instructions. Using three major medical question-answering benchmark datasets, 
experimental results of Self-BioRAG demonstrate significant performance gains by 
achieving a 7.2% absolute improvement on average over the state-of-the-art 
open-foundation model with a parameter size of 7B or less. Similarly, 
Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient 
answers on two long-form question-answering benchmarks on average. Overall, we 
analyze that Self-BioRAG finds the clues in the question, retrieves relevant 
documents if needed, and understands how to answer with information from 
retrieved documents and encoded knowledge as a medical expert does. We release 
our data and code for training our framework components and model weights (7B 
and 13B) to enhance capabilities in biomedical and clinical domains.
AVAILABILITY AND IMPLEMENTATION: Self-BioRAG is available at 
https://github.com/dmis-lab/self-biorag.

© The Author(s) 2024. Published by Oxford University Press.",,,No,Yes,"RAG system uses GPT4, testing on MedQA + similar datasets",Yes
Symptom-BERT: Enhancing Cancer Symptom Detection in EHR Clinical Notes,"Context: Extracting cancer symptom documentation allows clinicians to develop highly individualized symptom prediction algorithms to deliver symptom management care. Leveraging advanced language models to detect symptom data in clinical narratives can significantly enhance this process. Objective: This study uses a pretrained large language model to detect and extract cancer symptoms in clinical notes. Methods: We developed a pretrained language model to identify cancer symptoms in clinical notes based on a clinical corpus from the Enterprise Data Warehouse for Research at a healthcare system in the Midwestern United States. This study was conducted in 4 phases:1 pretraining a Bio-Clinical BERT model on one million unlabeled clinical documents,2 fine-tuning Symptom-BERT for detecting 13 cancer symptom groups within 1112 annotated clinical notes,3 generating 180 synthetic clinical notes using ChatGPT-4 for external validation, and4 comparing the internal and external performance of Symptom-BERT against a non-pretrained version and six other BERT implementations. Results: The Symptom-BERT model effectively detected cancer symptoms in clinical notes. It achieved results with a micro-averaged F1-score of 0.933, an AUC of 0.929 internally, and 0.831 and 0.834 externally. Our analysis shows that physical symptoms, like Pruritus, are typically identified with higher performance than psychological symptoms, such as anxiety. Conclusion: This study underscores the transformative potential of specialized pretraining on domain-specific data in boosting the performance of language models for medical applications. The Symptom-BERT model's exceptional efficacy in detecting cancer symptoms heralds a groundbreaking stride in patient-centered AI technologies, offering a promising path to elevate symptom management and cultivate superior patient self-care outcomes. © 2024 American Academy of Hospice and Palliative Medicine",,Data extraction from EHR Clinical Notes,Yes,No,Data extraction,No
"Chat GPT, Gemini or Meta AI: A comparison of AI platforms as a tool for answering higher-order questions in microbiology","Introduction: Artificial intelligence (AI) platforms have achieved a noteworthy role in various fields of medical sciences, ranging from medical education to clinical diagnostics and treatment. ChatGPT, Gemini, and Meta AI are some large language models (LLMs) that have gained immense popularity among students for solving questions from different branches of education. Materials and Methods: A cross-sectional study was conducted in the Department of Microbiology to assess the performance of ChatGPT, Gemini, and Meta AI in answering higher-order questions from various competencies of the microbiology curriculum (MI 1 to 8), according to CBME guidelines. Sixty higher-order questions were compiled from university question papers of two universities. Their responses were assessed by three faculty members from the department. Results: The mean rank scores of ChatGPT, Gemini, and Meta AI were found to be 102.76, 108.5, and 60.23 by Evaluator 1; 106.03, 88.5, and 76.95 by Evaluator 2; and 104.85, 85.6, and 81.04, respectively, indicating lowest overall mean rank score for Meta AI. ChatGPT had the highest mean score in MI 2,3,5,6,7, and 8 competencies, while Gemini had a higher score for MI 1 and 4 competencies. A qualitative assessment of the three platforms was also performed. ChatGPT provided elaborative responses, some responses from Gemini lacked certain significant points, and Meta AI gave answers in bullet points. Conclusions: Both ChatGPT and Gemini have created vast databases to correctly respond to higher-order queries in medical microbiology in comparison to Meta AI. Our study is the first of its kind to compare these three popular LLM platforms for microbiology.  © 2025 Journal of Postgraduate Medicine.",,"Torn because while microbiology is tested on exams, it is only useful for a small subset of doctors..... ",No,Maybe?,Bad study but I think technically meets inclusion criteria—medical microbiology exam taken in by med students seems like a clinically relevant exam ,Yes