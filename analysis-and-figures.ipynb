{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02437929",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_studies = []\n",
    "\n",
    "with open(\"final_processed_studies.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        included_studies.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50928a",
   "metadata": {},
   "source": [
    "# Quickly Regenerate the Most Specific Dates\n",
    "The reason we must do this is because some studies are duplicated across databases (which we removed in the deduplication step), but some databases have more specific dates than others. We want the most specific date possible for our analysis, so we must go through each database and try to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to open up all the database files to try our hardest to match a date to each study if it doesn't have one\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def get_year(row):\n",
    "    for key in [\"Publication Year\", \"Year\"]:\n",
    "        if key in row and pd.notnull(row[key]):\n",
    "            try:\n",
    "                return int(row[key])\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "unique_id_cols = {\n",
    "    \"PMID\",\n",
    "    \"PMCID\",\n",
    "    \"NIHMS ID\",\n",
    "    \"DOI\",\n",
    "    \"EID\",\n",
    "    \"Clinical Trial Numbers\"\n",
    "}\n",
    "\n",
    "embase = pd.read_csv(\"embase-export-9-6-25.csv\", sep=\",\").to_dict(orient='records')\n",
    "pubmed = pd.read_csv(\"pubmed-export-9-6-25.csv\", sep=\",\").to_dict(orient='records')\n",
    "scopus = pd.read_csv(\"scopus-export-9-6-25.csv\", sep=\",\").to_dict(orient='records')\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# define the cutoff once\n",
    "_CUTOFF = datetime(2025, 9, 6)\n",
    "\n",
    "def find_most_specific_date(title, return_raw_candidates=False):\n",
    "    # Gather all date‐strings from the three sources\n",
    "    candidates = []\n",
    "    for item in embase:\n",
    "        if title.lower() in str(item.get(\"Title\", \"\")).lower():\n",
    "            candidates.append(item.get(\"Date of Publication\"))\n",
    "    for item in pubmed:\n",
    "        if title.lower() in str(item.get(\"Title\", \"\")).lower():\n",
    "            candidates.append(item.get(\"Create Date\"))\n",
    "    for item in scopus:\n",
    "        if title.lower() in str(item.get(\"Title\", \"\")).lower():\n",
    "            candidates.append(item.get(\"Year\"))\n",
    "    \n",
    "    if return_raw_candidates:\n",
    "        return candidates\n",
    "    \n",
    "    # Define the formats and their specificity levels\n",
    "    formats = [\n",
    "        ('%d %b %Y', 3),   # e.g. '9 Jul 2023'\n",
    "        ('%d %B %Y', 3),   # e.g. '9 July 2023'\n",
    "        ('%m/%d/%y', 3),   # e.g. '9/1/23'\n",
    "        ('%m/%d/%Y', 3),   # e.g. '9/1/2023'\n",
    "        ('%b %Y',    2),   # e.g. 'Jul 2023'\n",
    "        ('%B %Y',    2),   # e.g. 'July 2023'\n",
    "        ('%Y',       1),   # e.g. '2023'\n",
    "    ]\n",
    "    \n",
    "    best_dt   = None\n",
    "    best_spec = 0\n",
    "    \n",
    "    # Try parsing each candidate\n",
    "    for raw in candidates:\n",
    "        if not raw:\n",
    "            continue\n",
    "        s = str(raw).strip()\n",
    "        parsed = False\n",
    "        \n",
    "        for fmt, spec in formats:\n",
    "            try:\n",
    "                dt = datetime.strptime(s, fmt)\n",
    "                parsed = True\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        if not parsed:\n",
    "            # skip anything we don’t recognize\n",
    "            continue\n",
    "\n",
    "        # given that we pulled this data before this cutoff date, we can be confident that any date in the future is incorrect\n",
    "        if dt > _CUTOFF:\n",
    "            continue\n",
    "        \n",
    "        # Keep the one with highest specificity, tiebreaker = most recent\n",
    "        if spec > best_spec or (\n",
    "           spec == best_spec and (best_dt is None or dt > best_dt)\n",
    "        ):\n",
    "            best_spec = spec\n",
    "            best_dt   = dt\n",
    "    \n",
    "    return best_dt\n",
    "\n",
    "for study in tqdm(included_studies):\n",
    "    date = find_most_specific_date(study[\"Title\"])\n",
    "    study[\"Date\"] = date\n",
    "\n",
    "with open(\"final_processed_studies_dated.jsonl\", \"w\") as f:\n",
    "    for study in included_studies:\n",
    "        f.write(json.dumps(study, default=str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d79083",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_processed_studies_dated.jsonl\", \"r\") as f:\n",
    "    included_studies = [json.loads(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9806a647",
   "metadata": {},
   "source": [
    "# Get Model Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_counts = {}\n",
    "for study in included_studies:\n",
    "    for model in study[\"processed_data\"][\"model_categories\"]:\n",
    "        if model == \"\":\n",
    "            model_counts[\"Unknown\"] = model_counts.get(\"Unknown\", 0) + 1\n",
    "        if model not in model_counts:\n",
    "            model_counts[model] = 0\n",
    "        \n",
    "        # We only include \"Other\" if it is the only model category\n",
    "        if \"Other\" in study[\"processed_data\"][\"model_categories\"] and len(study[\"processed_data\"][\"model_categories\"]) == 1 and model == \"Other\":\n",
    "            model_counts[\"Other\"] += 1\n",
    "        elif model != \"Other\":\n",
    "            model_counts[model] += 1\n",
    "\n",
    "sorted_model_counts = sorted(model_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the longest model name\n",
    "max_model_length = max(len(model) for model, _ in sorted_model_counts)\n",
    "\n",
    "# get win rate against humans based on model name, level of training, and tier\n",
    "win_rates = {}\n",
    "\n",
    "# Optional header\n",
    "print(f\"{'Model'.ljust(max_model_length)} | Studies\")\n",
    "print('-' * (max_model_length + 10))\n",
    "\n",
    "# Print formatted model counts\n",
    "for model, count in sorted_model_counts:\n",
    "    print(f\"{model.ljust(max_model_length)} | {str(count).rjust(7)}\")\n",
    "\n",
    "# sum total studies\n",
    "total_studies = sum(count for _, count in sorted_model_counts)\n",
    "print(f\"\\nTotal studies: {total_studies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(515+302+128+21+23+6)/7623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_open_vs_closed(sorted_model_counts):\n",
    "    # Define which models are open source\n",
    "    open_source_models = {\n",
    "        \"LLaMA\", \"LLaMA 2\", \"LLaMA 3.x\",\n",
    "        \"Mistral\", \"Mixtral\", \"Gemma\", \"Qwen\", \"GLM / ChatGLM\",\n",
    "        \"LLaVA\", \"Baichuan\", \"Vicuna\", \"WizardLM\", \"BLOOM\",\n",
    "        \"Falcon\", \"Yi\", \"Nous Hermes\", \"MPT\", \"Zephyr\",\n",
    "        \"DeepSeek\", \"MiniCPM\"\n",
    "    }\n",
    "\n",
    "    open_count = 0\n",
    "    closed_count = 0\n",
    "    unknown_count = 0\n",
    "\n",
    "    for model, count in sorted_model_counts:\n",
    "        if model in open_source_models:\n",
    "            open_count += count\n",
    "        elif model == \"Other\" or model == \"Unknown\":\n",
    "            unknown_count += count\n",
    "        else:\n",
    "            closed_count += count\n",
    "\n",
    "    return {\n",
    "        \"open_source\": open_count,\n",
    "        \"closed_source\": closed_count,\n",
    "        \"unknown\": unknown_count,\n",
    "        \"total\": open_count + closed_count + unknown_count\n",
    "    }\n",
    "\n",
    "results = compute_open_vs_closed(sorted_model_counts)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae5270",
   "metadata": {},
   "source": [
    "# Get Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451852d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_types_counts = {}\n",
    "num_tasks = []  # Make sure this list is initialized\n",
    "\n",
    "for study in included_studies:\n",
    "    if len(study[\"processed_data\"][\"task_types\"]) == 0:\n",
    "        task_types_counts[\"Unknown\"] = task_types_counts.get(\"Unknown\", 0) + 1\n",
    "    num_tasks.append(len(study[\"processed_data\"][\"task_types\"]))\n",
    "    for task in study[\"processed_data\"][\"task_types\"]:\n",
    "        if task not in task_types_counts:\n",
    "            task_types_counts[task] = 0\n",
    "        task_types_counts[task] += 1\n",
    "\n",
    "sorted_task_types_counts = sorted(task_types_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the longest task type name\n",
    "max_task_length = max(len(task) for task, _ in sorted_task_types_counts)\n",
    "\n",
    "# Compute total for percentage calculation\n",
    "total_studies = sum(task_types_counts.values())\n",
    "\n",
    "# Optional header\n",
    "print(f\"{'Task Type'.ljust(max_task_length)} | Studies | Percent\")\n",
    "print('-' * (max_task_length + 22))\n",
    "\n",
    "# Print formatted task types counts with percentages\n",
    "for task, count in sorted_task_types_counts:\n",
    "    percentage = (count / total_studies) * 100\n",
    "    print(f\"{task.ljust(max_task_length)} | {str(count).rjust(7)} | {percentage:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b3ee7",
   "metadata": {},
   "source": [
    "# Get Dataset Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1051b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_used_counts = {}\n",
    "studies_that_have_datasets = 0\n",
    "num_types = {}\n",
    "num_datasets = []\n",
    "for study in included_studies:\n",
    "    found = False\n",
    "\n",
    "    for item in study[\"processed_data\"][\"dataset_types\"]:\n",
    "        category = item.get(\"category\", \"Unknown\")\n",
    "        access_type = item.get(\"access\", \"unsure\")\n",
    "\n",
    "        if category not in datasets_used_counts:\n",
    "            datasets_used_counts[category] = 0\n",
    "        datasets_used_counts[category] += 1\n",
    "\n",
    "        if access_type not in num_types:\n",
    "            num_types[access_type] = 0\n",
    "        num_types[access_type] += 1\n",
    "\n",
    "        found = True\n",
    "    \n",
    "    num_datasets.append(len(study[\"processed_data\"][\"dataset_types\"]))\n",
    "\n",
    "    if found:\n",
    "        studies_that_have_datasets += 1\n",
    "    else:\n",
    "        datasets_used_counts[\"Undefined in Abstract\"] = datasets_used_counts.get(\"Undefined in Abstract\", 0) + 1\n",
    "        \n",
    "\n",
    "sorted_datasets_used_counts = sorted(datasets_used_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the longest dataset name\n",
    "max_dataset_length = max(len(dataset) for dataset, _ in sorted_datasets_used_counts)\n",
    "# Optional header\n",
    "print(f\"{'Dataset'.ljust(max_dataset_length)} | Studies | %\")\n",
    "print('-' * (max_dataset_length + 10))\n",
    "\n",
    "# Print formatted datasets used counts\n",
    "for dataset, count in sorted_datasets_used_counts:\n",
    "    print(f\"{dataset.ljust(max_dataset_length)} | {str(count).rjust(7)} | {count / len(included_studies) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Print dataset access types\n",
    "print(\"Dataset Access Types:\")\n",
    "for access_type, count in num_types.items():\n",
    "    print(f\"{access_type.ljust(12)} : {str(count).rjust(3)}\")\n",
    "\n",
    "# print total studies that used datasets\n",
    "print(f\"\\nTotal studies that used datasets: {studies_that_have_datasets} out of {len(included_studies)}\")\n",
    "print(f\"Total number of datasets used: {sum(datasets_used_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'figure.facecolor': 'white',\n",
    "})\n",
    "\n",
    "labels = [\n",
    "    \"Clinician Board & Self-Assessment Questions\",\n",
    "    \"Patient-Facing Q&A & FAQs\",\n",
    "    \"Clinical Vignettes & Case Reports\",\n",
    "    \"Clinical Guidelines & Consensus Statements\",\n",
    "    \"Imaging Data & Reports\",\n",
    "    \"Real-World Electronic Health Records\",\n",
    "    \"Survey Instruments & Psychometric Scales\",\n",
    "    \"Educational & Reference Texts\",\n",
    "    \"Dialogue & Chat Corpora\",\n",
    "    \"Synthetic / Simulated Clinical Data\",\n",
    "    \"Social-Media & Search-Query Data\",\n",
    "    \"Research Literature & Abstract Corpora\",\n",
    "    \"Other\",\n",
    "    \"Regulatory & Administrative Documents\",\n",
    "    \"Pharmacology & Medication Knowledge\",\n",
    "    \"Physiological Signals & Wearables\",\n",
    "    \"Laboratory & Pathology Data\",\n",
    "    \"Knowledge Graphs / Ontologies / Terminologies\",\n",
    "    \"Clinical Trial Eligibility & Structured Cohorts\",\n",
    "    \"Imaging Challenges & Benchmarks\",\n",
    "    \"Genomics & -Omics\",\n",
    "    \"Benchmark evaluation items\",\n",
    "]\n",
    "\n",
    "counts = [\n",
    "    1047, 691, 652, 579, 472, 423, 394, 307, 235, 194, 178, 142,\n",
    "    87, 68, 62, 61, 57, 51, 41, 38, 30, 1\n",
    "]\n",
    "\n",
    "# sort descending\n",
    "order = np.argsort(counts)[::-1]\n",
    "labels_sorted = [labels[i] for i in order]\n",
    "counts_sorted = [counts[i] for i in order]\n",
    "y = np.arange(len(labels_sorted))\n",
    "\n",
    "# alternating greys\n",
    "color1, color2 = '0.2', '0.4'\n",
    "bar_colors = [color1 if i % 2 == 0 else color2 for i in range(len(labels_sorted))]\n",
    "\n",
    "# --- plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(y, counts_sorted, height=0.7, color=bar_colors)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(labels_sorted)\n",
    "for tick, c in zip(ax.get_yticklabels(), bar_colors):\n",
    "    tick.set_color(c)\n",
    "\n",
    "ax.invert_yaxis()  # biggest at top\n",
    "\n",
    "# value labels to the right of each bar\n",
    "pad = max(counts_sorted) * 0.01 + 6\n",
    "for i, v in enumerate(counts_sorted):\n",
    "    ax.text(v + pad, i, f\"{v}\", va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel(\"Number of Studies\")\n",
    "ax.set_title(\"Study Dataset Types\", pad=10)\n",
    "\n",
    "# light grid on x to match style\n",
    "ax.xaxis.grid(True, linestyle='-', linewidth=0.3, color='0.85')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"study_dataset_types.svg\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'figure.facecolor': 'white',\n",
    "})\n",
    "\n",
    "models = [\n",
    "    \"ChatGPT-unspecified\", \"GPT-4\", \"GPT-4o\", \"ChatGPT-4\", \"GPT-3.5\", \"Gemini\",\n",
    "    \"ChatGPT-3.5\", \"Claude\", \"Bard\", \"LLaMA 3.x\", \"Bing Chat\",\n",
    "    \"DeepSeek\", \"Gemini 1.5\", \"LLaMA 2\", \"Perplexity\", \"Mistral\", \"GPT-unspecified\",\n",
    "    \"LLaMA\", \"Qwen\", \"Other\"\n",
    "]\n",
    "\n",
    "studies = [\n",
    "    1292, 1252, 753, 626, 600, 515, 381, 325, 302,\n",
    "    182, 167, 132, 128, 95, 87, 70, 70, 59, 57, 530\n",
    "] # Pulled from the above tables!\n",
    "\n",
    "# Sort descending\n",
    "order = np.argsort(studies)[::-1]\n",
    "sorted_models = [models[i] for i in order]\n",
    "sorted_studies = [studies[i] for i in order]\n",
    "y_pos = np.arange(len(sorted_models))\n",
    "\n",
    "# Alternating subtle greys\n",
    "color1 = '0.2'\n",
    "color2 = '0.4'\n",
    "bar_colors = [color1 if i % 2 == 0 else color2 for i in range(len(sorted_models))]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Spread bars: height=0.7\n",
    "ax.barh(y_pos, sorted_studies, color=bar_colors, height=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(sorted_models)\n",
    "\n",
    "# Alternate y-tick label colors to match bars\n",
    "for tick_label, color in zip(ax.get_yticklabels(), bar_colors):\n",
    "    tick_label.set_color(color)\n",
    "\n",
    "ax.invert_yaxis()  # top-down\n",
    "\n",
    "# Annotate bar values\n",
    "for i, v in enumerate(sorted_studies):\n",
    "    ax.text(v + 10, i, f\"{v}\", va='center', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Number of Studies')\n",
    "ax.set_title('LLM Usage in Studies', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "# save at 300 dpi as an svg\n",
    "plt.savefig('llm_usage_studies.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23572ba",
   "metadata": {},
   "source": [
    "# Get Breakdown by Specialty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a444e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty_counts = {}\n",
    "specialty_tier_counts = {}\n",
    "total_vals = 0\n",
    "num_terms = []\n",
    "subspecialties_only = False\n",
    "broad_specialties_only = True\n",
    "\n",
    "assert not (subspecialties_only and broad_specialties_only), \"Cannot set both subspecialties_only and broad_specialties_only to True\"\n",
    "\n",
    "for study in included_studies:\n",
    "    total_vals += len(study[\"processed_data\"][\"specialties\"])\n",
    "    for i, specialty in enumerate(study[\"processed_data\"][\"specialties\"]):\n",
    "        num_terms.append(len(specialty))\n",
    "        if len(specialty) < 2 and subspecialties_only:\n",
    "            continue\n",
    "        if specialty[-1] == \"Internal Medicine\" and subspecialties_only:\n",
    "            continue\n",
    "        if subspecialties_only:\n",
    "            list_to_process = specialty[-1:]\n",
    "        elif broad_specialties_only:\n",
    "            list_to_process = specialty[:1]\n",
    "        else:\n",
    "            list_to_process = specialty\n",
    "        for subspecialty in list_to_process:\n",
    "            if subspecialty == \"Neurosurgery\":\n",
    "                subspecialty = \"Neurological Surgery\"\n",
    "            if subspecialty == \"Dentistry\":\n",
    "                subspecialty = \"Dentistry and Oral and Maxillofacial Surgery\"\n",
    "            if subspecialty == \"Surgery (American Board of Surgery)\":\n",
    "                subspecialty = \"General Surgery\"\n",
    "\n",
    "            if subspecialty == \"Radiology\":\n",
    "                subspecialty = \"Interventional Radiology and Diagnostic Radiology\"\n",
    "\n",
    "            if subspecialty == \"Public Health and General Preventive Medicine\":\n",
    "                subspecialty = \"Preventive Medicine\"\n",
    "\n",
    "            if subspecialty == \"Neurology (Adult)\":\n",
    "                subspecialty = \"Neurology\"\n",
    "\n",
    "            if subspecialty == \"Pediatrics (General)\":\n",
    "                subspecialty = \"Pediatrics\"\n",
    "\n",
    "            if subspecialty == \"Hospitals and Palliative Medicine\":\n",
    "                subspecialty = \"Hospice and Palliative Medicine\"\n",
    "\n",
    "            if subspecialty == \"Psychiatry and Neurology (Shared/Multi-board subs)\":\n",
    "                subspecialty = \"Neuropsychiatry\"\n",
    "\n",
    "            if subspecialty == \"Pediatric Orthopedics\" or subspecialty == \"Pediatric Orthopaediology\" or subspecialty == \"Pediatric Orthopaedic Surgery\":\n",
    "                subspecialty = \"Pediatric Orthopaedics\"\n",
    "\n",
    "            if subspecialty == \"Pathology - Medical Microbiology\" or subspecialty == \"Medical Microbiology Pathology\":\n",
    "                subspecialty = \"Medical Microbiology (Pathology)\"\n",
    "\n",
    "            if subspecialty == \"Urogynecology and Reconstructive Pelvic Surgery\":\n",
    "                subspecialty = \"Female Pelvic Medicine and Reconstructive Surgery\"\n",
    "\n",
    "            if subspecialty == \"Child Neurology\":\n",
    "                subspecialty = \"Pediatric Neurology\"\n",
    "\n",
    "            if subspecialty == \"Pathology\":\n",
    "                subspecialty = \"Pathology - Anatomic/Clinical (AP/CP)\"\n",
    "\n",
    "            if subspecialty == \"Internal Medicine-Critical Care Medicine\":\n",
    "                subspecialty = \"Critical Care Medicine\"\n",
    "            \n",
    "            if subspecialty == \"Pathology - Anatomic/Clinical (AP/CP)\":\n",
    "                subspecialty = \"Pathology\"\n",
    "\n",
    "            if subspecialty == \"Otolaryngology - Head and Neck Surgery\":\n",
    "                subspecialty = \"Otolaryngology\"\n",
    "\n",
    "            # now use `subspecialty` downstream\n",
    "\n",
    "\n",
    "            if subspecialty not in specialty_counts:\n",
    "                specialty_counts[subspecialty] = 0\n",
    "\n",
    "            if subspecialty == \"\":\n",
    "                subspecialty = \"Unknown\"\n",
    "    \n",
    "            if subspecialty not in specialty_tier_counts:\n",
    "                specialty_tier_counts[subspecialty] = {\n",
    "                    \"S\": 0,\n",
    "                    \"I\": 0,\n",
    "                    \"II\": 0,\n",
    "                    \"III\": 0,\n",
    "                }\n",
    "\n",
    "            tier = study[\"LLM-tier\"][\"Tier\"]\n",
    "            specialty_tier_counts[subspecialty][tier] += 1\n",
    "            \n",
    "            specialty_counts[subspecialty] += 1\n",
    "            #break\n",
    "\n",
    "# Determine the longest specialty name for formatting\n",
    "max_specialty_length = max(len(s) for s in specialty_counts.keys())\n",
    "\n",
    "# Header (optional)\n",
    "print(f\"{'Specialty'.ljust(max_specialty_length)} | {'Total':>5} | Tier S | Tier I | Tier II | Tier III\")\n",
    "print('-' * (max_specialty_length + 50))\n",
    "\n",
    "# sort specialties by count\n",
    "specialty_counts = dict(sorted(specialty_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print formatted rows\n",
    "residual = 0\n",
    "residual_count = {\"S\": 0, \"I\": 0, \"II\": 0, \"III\": 0}\n",
    "for specialty, count in specialty_counts.items():\n",
    "    tiers = specialty_tier_counts[specialty]\n",
    "    if count/len(included_studies)*100 > 0.21:\n",
    "        print(f\"{specialty.ljust(max_specialty_length)} | {(str(count/len(included_studies)*100)[:4] + \"%\").rjust(5)} | \"\n",
    "            f\"{str(tiers['S']).rjust(6)} | {str(tiers['I']).rjust(6)} | \"\n",
    "            f\"{str(tiers['II']).rjust(7)} | {str(tiers['III']).rjust(8)} | {tiers['S'] + tiers['I'] + tiers['II'] + tiers['III']}\")\n",
    "    else:\n",
    "        residual += count/len(included_studies)*100\n",
    "        residual_count[\"S\"] += tiers[\"S\"]\n",
    "        residual_count[\"I\"] += tiers[\"I\"]\n",
    "        residual_count[\"II\"] += tiers[\"II\"]\n",
    "        residual_count[\"III\"] += tiers[\"III\"]\n",
    "\n",
    "print(f\"Residual: {residual:.2f}% | Count: {residual_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'figure.facecolor': 'white',\n",
    "})\n",
    "\n",
    "total = len(included_studies)\n",
    "\n",
    "spec_df = (pd.DataFrame({\n",
    "    'Specialty': list(specialty_counts.keys()),\n",
    "    'Count':     list(specialty_counts.values())\n",
    "})\n",
    "  .assign(Percent=lambda df: df['Count']/total)\n",
    "  .sort_values('Count', ascending=False)\n",
    "  .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "labels = spec_df['Specialty']\n",
    "percent = spec_df['Percent']\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "# include only the first N specialties to avoid clutter\n",
    "N = 30\n",
    "if len(labels) > N:\n",
    "    labels = labels[:N]\n",
    "    percent = percent[:N]\n",
    "    y_pos = y_pos[:N]\n",
    "\n",
    "# alternating greys\n",
    "color1 = '0.2'\n",
    "color2 = '0.4'\n",
    "bar_colors = [color1 if i%2==0 else color2 for i in range(len(labels))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "ax.barh(y_pos, percent, color=bar_colors, height=0.7)\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "\n",
    "# invert so highest on top\n",
    "ax.invert_yaxis()\n",
    "# Set y-tick labels with alternating colors\n",
    "ax.set_yticks(y_pos)\n",
    "for i, label in enumerate(labels):\n",
    "    tick_color = color1 if i % 2 == 0 else color2\n",
    "    ax.text(-0.01, i, label, va='center', ha='right', fontsize=10, color=tick_color,\n",
    "            transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "ax.set_yticklabels(['' for _ in labels])  # Hide default ticks\n",
    "\n",
    "ax.set_xlabel('Percentage of Studies')\n",
    "ax.set_title('Study Distribution by Specialty')\n",
    "\n",
    "# annotate percentages\n",
    "for i, (p, cnt) in enumerate(zip(percent, spec_df['Count'])):\n",
    "    ax.text(p + 0.01, i, f\"{p:.1%} (n={cnt})\", va='center', fontsize=9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162077a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labels = spec_df['Specialty']\n",
    "percent = spec_df['Percent']\n",
    "x_pos = np.arange(len(labels))\n",
    "\n",
    "# include only the first N specialties to avoid clutter\n",
    "N = 22\n",
    "if len(labels) > N:\n",
    "    labels = labels[:N]\n",
    "    percent = percent[:N]\n",
    "    x_pos = x_pos[:N]\n",
    "\n",
    "# Set alternating colors\n",
    "color1 = '0.2'\n",
    "color2 = '0.4'\n",
    "bar_colors = [color1 if i % 2 == 0 else color2 for i in range(len(labels))]\n",
    "\n",
    "# ── Plot ──\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(x_pos, percent, color=bar_colors, width=0.7)\n",
    "\n",
    "ax.set_ylabel('Percentage of Studies')\n",
    "ax.set_title('Proportion of Studies by Specialty (first subspecialty)')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "\n",
    "# Annotate bars with percentages\n",
    "for i, (p, cnt) in enumerate(zip(percent, spec_df['Count'])):\n",
    "    ax.text(i, p + 0.01, f\"{p:.1%}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Alternating tick label colors\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "for tick_label, i in zip(ax.get_xticklabels(), range(len(labels))):\n",
    "    tick_label.set_color(color1 if i % 2 == 0 else color2)\n",
    "\n",
    "plt.tight_layout()\n",
    "# save at 300 dpi as an svg\n",
    "plt.savefig('study_specialties_bar_chart.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618aed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the get specialty counts code first with these flags:\n",
    "# subspecialties_only = True\n",
    "# broad_specialties_only = False\n",
    "\n",
    "specialty_counts = {}\n",
    "specialty_tier_counts = {}\n",
    "total_vals = 0\n",
    "detected_studies = 0\n",
    "num_terms = []\n",
    "\n",
    "ABMS_SURGICAL_SPECIALTIES = [\n",
    "    \"Colon and Rectal Surgery\",\n",
    "    \"Neurological Surgery\",\n",
    "    \"Obstetrics and Gynecology\",\n",
    "    \"Opthalmology\",\n",
    "    \"Orthopaedic Surgery\",\n",
    "    \"Otolaryngology - Head and Neck Surgery\",\n",
    "    \"Plastic Surgery\",\n",
    "    \"General Surgery\",\n",
    "    \"Vascular Surgery\",\n",
    "    \"Thoracic Surgery\",\n",
    "    \"Urology\",\n",
    "]\n",
    "\n",
    "ABIM_IM_SPECIALTIES = [\n",
    "    \"Adolescent Medicine\",\n",
    "    \"Adult Congenital Heart Disease\",\n",
    "    \"Advanced Heart Failure and Transplant Cardiology\",\n",
    "    \"Cardiovascular Disease\",\n",
    "    \"Clinical Cardiac Electrophysiology\",\n",
    "    \"Critical Care Medicine\",\n",
    "    \"Endocrinology, Diabetes and Metabolism\",\n",
    "    \"Gastroenterology\",\n",
    "    \"Geriatric Medicine\",\n",
    "    \"Hematology\",\n",
    "    \"Hospice and Palliative Medicine\",\n",
    "    \"Infectious Disease\",\n",
    "    \"Interventional Cardiology\",\n",
    "    \"Medical Oncology\",\n",
    "    \"Nephrology\",\n",
    "    \"Neurocritical Care\",\n",
    "    \"Pulmonary Disease\",\n",
    "    \"Rheumatology\",\n",
    "    \"Sleep Medicine\",\n",
    "    \"Sports Medicine\",\n",
    "    \"Transplant Hepatology\"\n",
    "]\n",
    "\n",
    "categories_of_interest = ABIM_IM_SPECIALTIES\n",
    "\n",
    "\n",
    "for study in included_studies:\n",
    "    found = False\n",
    "    total_vals += len(study[\"processed_data\"][\"specialties\"])\n",
    "    for i, specialty in enumerate(study[\"processed_data\"][\"specialties\"]):\n",
    "        num_terms.append(len(specialty))\n",
    "        if not any(cat in specialty for cat in categories_of_interest):\n",
    "            continue\n",
    "        for subspecialty in specialty:\n",
    "            if subspecialty not in categories_of_interest:\n",
    "                continue\n",
    "            found = True\n",
    "            #if \"surgery\" in specialty[0].lower() and \"surgery\" not in subspecialty.lower():\n",
    "            #    subspecialty = \"Surgery: \" + subspecialty\n",
    "            \n",
    "            if subspecialty not in specialty_counts:\n",
    "                specialty_counts[subspecialty] = 0\n",
    "\n",
    "            if subspecialty == \"\":\n",
    "                subspecialty = \"Unknown\"\n",
    "    \n",
    "            if subspecialty not in specialty_tier_counts:\n",
    "                specialty_tier_counts[subspecialty] = {\n",
    "                    \"S\": 0,\n",
    "                    \"I\": 0,\n",
    "                    \"II\": 0,\n",
    "                    \"III\": 0,\n",
    "                }\n",
    "\n",
    "            tier = study[\"LLM-tier\"][\"Tier\"]\n",
    "            specialty_tier_counts[subspecialty][tier] += 1\n",
    "            \n",
    "            specialty_counts[subspecialty] += 1\n",
    "        \n",
    "    if found:\n",
    "        detected_studies += 1\n",
    "\n",
    "# Determine the longest specialty name for formatting\n",
    "max_specialty_length = max(len(s) for s in specialty_counts.keys())\n",
    "\n",
    "# Header (optional)\n",
    "print(f\"{'Specialty'.ljust(max_specialty_length)} | {'Total':>5} | Tier S | Tier I | Tier II | Tier III\")\n",
    "print('-' * (max_specialty_length + 50))\n",
    "\n",
    "# sort specialties by count\n",
    "specialty_counts = dict(sorted(specialty_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print formatted rows\n",
    "for specialty, count in specialty_counts.items():\n",
    "    tiers = specialty_tier_counts[specialty]\n",
    "    print(f\"{specialty.ljust(max_specialty_length)} | {(str(count/detected_studies*100)[:5] + \"%\").rjust(5)} | \"\n",
    "          f\"{str(tiers['S']).rjust(6)} | {str(tiers['I']).rjust(6)} | \"\n",
    "          f\"{str(tiers['II']).rjust(7)} | {str(tiers['III']).rjust(8)} | {tiers['S'] + tiers['I'] + tiers['II'] + tiers['III']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b0ce4",
   "metadata": {},
   "source": [
    "# Sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ed2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) build your extractor:\n",
    "NUM      = r'[≈~]?\\d+(?:,\\d{3})*(?:\\+)?'\n",
    "first_re = re.compile(rf'^\\s*({NUM})')\n",
    "\n",
    "def extract_first(s: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return the first sample-size–like token in s (with commas, ±, trailing +),\n",
    "    or None if nothing matches.\n",
    "    \"\"\"\n",
    "    m = first_re.match(s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "sample_sizes = []\n",
    "studies_with_sample_sizes = []\n",
    "for study in included_studies:\n",
    "    if (raw := study[\"extracted_data\"][\"sample_size\"]):\n",
    "        if (num := extract_first(raw)):\n",
    "            sample_sizes.append(int(num.replace('≈', '').replace(',', '').replace('+', '').replace('~', '')))\n",
    "            studies_with_sample_sizes.append(study)\n",
    "\n",
    "# get sample size by tier\n",
    "sample_size_by_tier = {\n",
    "    \"S\": [],\n",
    "    \"I\": [],\n",
    "    \"II\": [],\n",
    "    \"III\": []\n",
    "}\n",
    "for study in included_studies:\n",
    "    if (raw := study[\"extracted_data\"][\"sample_size\"]):\n",
    "        if (num := extract_first(raw)):\n",
    "            sample_size_by_tier[study[\"LLM-tier\"][\"Tier\"]].append(int(num.replace('≈', '').replace(',', '').replace('+', '').replace('~', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0550817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure, we will print some random studies and check if the extracted sample sizes match!\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "random_study = random.choice(studies_with_sample_sizes)\n",
    "idx = studies_with_sample_sizes.index(random_study)\n",
    "print(random_study[\"extracted_data\"][\"sample_size\"])\n",
    "print(sample_sizes[idx])\n",
    "print(random_study['Title'])\n",
    "print()\n",
    "print(\"\\n\".join(textwrap.wrap(random_study['Abstract'], width=80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f141395",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_studies_per_tier = {\n",
    "    \"I\": 0,\n",
    "    \"II\": 0,\n",
    "    \"III\": 0\n",
    "}\n",
    "\n",
    "for study in included_studies:\n",
    "    tier = study[\"LLM-tier\"][\"Tier\"]\n",
    "    if tier == \"S\":\n",
    "        num_studies_per_tier[\"I\"] += 1 # S is counted as I due to small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11455350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tier = \"all\"\n",
    "\n",
    "if tier is not \"all\":\n",
    "    # filter the data\n",
    "    filtered = np.array([x for x in sample_size_by_tier[tier] if 0 < x <= 1000])\n",
    "    n = len(sample_size_by_tier[tier])\n",
    "    log_vals = np.log10(filtered)\n",
    "else:\n",
    "    filtered = np.array([x for x in sample_sizes if 0 < x <= 1000])\n",
    "    n = len(sample_sizes)\n",
    "    log_vals = np.log10(filtered)\n",
    "\n",
    "# build the empirical CDF\n",
    "log_sorted = np.sort(log_vals)\n",
    "cum_percent = np.arange(1, n + 1) / n\n",
    "\n",
    "# ── 3.  plot as a line instead of bars ─────────────────────────────────\n",
    "plt.plot(log_sorted, cum_percent[:len(log_sorted)], lw=2)     # continuous line\n",
    "plt.fill_between(log_sorted, cum_percent[:len(log_sorted)], alpha=0.15)  # optional shading\n",
    "\n",
    "# ── 4.  cosmetic details (same as before) ──────────────────────────────\n",
    "plt.title(\"Cumulative percentile (outliers above 1000 removed)\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Cumulative Percentile\")\n",
    "plt.gca().yaxis.set_major_formatter(\n",
    "    plt.FuncFormatter(lambda y, _: f\"{y*100:.0f}%\"))\n",
    "\n",
    "# major x-ticks\n",
    "major_vals = [1, 10, 100, 1000]\n",
    "plt.xticks(np.log10(major_vals), [f\"{v:,}\" for v in major_vals])\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# minor x-ticks\n",
    "minor_vals = [d * m for d in [1, 10, 100] for m in range(2, 10)]\n",
    "plt.gca().set_xticks(np.log10(minor_vals), minor=True)\n",
    "plt.gca().set_xticklabels([], minor=True)\n",
    "plt.gca().set_yticks(np.linspace(0, 1, 11), minor=True)\n",
    "\n",
    "# grid\n",
    "plt.grid(which='major', linestyle='-', color='black', linewidth=1.2)\n",
    "plt.grid(which='minor', linestyle=':', color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"cumulative_percentile_{tier}.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd551f9",
   "metadata": {},
   "source": [
    "# Plot number of studies by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03427a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "# style setup\n",
    "grey_palette = [\n",
    "    \"#333333\",  # darkest\n",
    "    \"#666666\",\n",
    "    \"#999999\",\n",
    "    \"#CCCCCC\",  # lightest\n",
    "]\n",
    "mpl.rcParams.update({\n",
    "    'axes.prop_cycle': cycler('color', grey_palette),\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.color': '#cccccc',   # light grey instead of black\n",
    "    'grid.alpha': 0.6,         # make them partially transparent\n",
    "    'grid.linewidth': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'xtick.major.size': 4,\n",
    "    'ytick.major.size': 4,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica'],\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.titlesize': 10,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'axes.linewidth': 1.1,\n",
    "})\n",
    "\n",
    "df = pd.DataFrame(included_studies)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# cumulative total and by‐tier\n",
    "monthly = (\n",
    "    df.assign(year_month=lambda x: x['Date'].dt.to_period('M').dt.to_timestamp())\n",
    "      .groupby('year_month').size()\n",
    "      .sort_index()\n",
    ")\n",
    "cum_total = monthly.cumsum()\n",
    "start = cum_total.ne(0).idxmax()\n",
    "cum_total = cum_total.loc[start:]\n",
    "\n",
    "tiers = ['I', 'II', 'III', 'S']\n",
    "tier_cums = {}\n",
    "for t in tiers:\n",
    "    df_t = df[df['LLM-tier'].apply(lambda d: d['Tier'] == t)]\n",
    "    mon = (\n",
    "        df_t.assign(year_month=lambda x: x['Date'].dt.to_period('M').dt.to_timestamp())\n",
    "            .groupby('year_month').size()\n",
    "            .reindex(cum_total.index, fill_value=0)\n",
    "    )\n",
    "    tier_cums[t] = mon.cumsum()\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "ax.stackplot(\n",
    "    cum_total.index,\n",
    "    [tier_cums[t].values for t in tiers],\n",
    "    labels=[f'Tier {t}' for t in tiers],\n",
    "    colors=grey_palette,\n",
    "    edgecolor='white', linewidth=0.5,\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# final polish\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative studies')\n",
    "\n",
    "# readable monthly ticks\n",
    "tick_dates = pd.date_range(cum_total.index.min(), cum_total.index.max(), periods=12)\n",
    "ax.set_xticks(tick_dates)\n",
    "ax.set_xticklabels([d.strftime('%b %Y') for d in tick_dates], rotation=45, ha='right')\n",
    "\n",
    "ax.legend(title='LLM Tier', fontsize=8, title_fontsize=9, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# save at 300 dpi as an svg\n",
    "plt.savefig('cumulative_study_counts_by_tier.svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import binomtest\n",
    "from itertools import combinations\n",
    "\n",
    "# 1) Count studies in Tiers I, II, III\n",
    "df = pd.DataFrame(included_studies)\n",
    "tiers = ['S', 'I','II','III']\n",
    "counts = {\n",
    "    tier: df['LLM-tier'].apply(lambda d: d['Tier']==tier).sum()\n",
    "    for tier in tiers\n",
    "}\n",
    "\n",
    "# 2) Prepare the pairwise comparisons\n",
    "pairs = list(combinations(tiers, 2))\n",
    "\n",
    "# 3) Run tests & collect raw p-values\n",
    "results = []\n",
    "for a, b in pairs:\n",
    "    n_a = counts[a]\n",
    "    n_b = counts[b]\n",
    "    total = n_a + n_b\n",
    "    actual_prop = n_a / total\n",
    "    expected_prop = 0.5\n",
    "    \n",
    "    test = binomtest(n_a, n=total, p=expected_prop, alternative='two-sided')\n",
    "    p_raw = test.pvalue\n",
    "    \n",
    "    results.append({\n",
    "        'pair': f\"{a} vs {b}\",\n",
    "        'n_a': n_a,\n",
    "        'n_b': n_b,\n",
    "        'actual_prop': actual_prop,\n",
    "        'p_raw': p_raw\n",
    "    })\n",
    "\n",
    "# 4) Bonferroni correction & print\n",
    "m = len(results)\n",
    "print(f\"Pairwise 2‑tier tests with Bonferroni correction (m={m}):\\n\")\n",
    "for res in results:\n",
    "    p_adj = min(res['p_raw'] * m, 1.0)\n",
    "    print(f\"{res['pair']}:\")\n",
    "    print(f\"  Observed counts     = ({res['n_a']}, {res['n_b']})\")\n",
    "    print(f\"  Expected proportion = {expected_prop}\")\n",
    "    print(f\"  Actual proportion   = {res['actual_prop']}\")\n",
    "    print(f\"  raw p‑value         = {res['p_raw']}\")\n",
    "    print(f\"  Bonferroni p‑value  = {p_adj}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "df = pd.DataFrame(included_studies)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "monthly = (\n",
    "    df.assign(year_month=lambda x: x['Date'].dt.to_period('M').dt.to_timestamp())\n",
    "      .groupby('year_month').size()\n",
    "      .sort_index()\n",
    ")\n",
    "adj = monthly.copy()\n",
    "for ts, val in monthly.items():\n",
    "    if ts.month == 1 and val > 0:\n",
    "        adj.loc[ts] = 0\n",
    "        spread = val / 12.0\n",
    "        same_year = adj.index.year == ts.year\n",
    "        adj.loc[same_year] += spread\n",
    "\n",
    "# regression\n",
    "x_num = mdates.date2num(adj.index)\n",
    "y = adj.values\n",
    "m, b = np.polyfit(x_num, y, 1)\n",
    "y_fit = m * x_num + b\n",
    "r2 = np.corrcoef(y, y_fit)[0, 1]**2\n",
    "\n",
    "# slope per month (30.4375 days)\n",
    "slope_mo = m * 30.4375\n",
    "# proper sign formatting for intercept\n",
    "eqn = rf\"$y = {slope_mo:.2f}\\,( \\mathrm{{studies/mo}} )\\,x {b:+.1f}$\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# scatter & dashed fit\n",
    "ax.scatter(adj.index, y, marker='o', facecolors='none', edgecolors='black')\n",
    "ax.plot(adj.index, y_fit, linestyle='--', color='black', linewidth=1.5)\n",
    "\n",
    "# equation in bottom right\n",
    "ax.text(\n",
    "    0.98, 0.05, eqn,\n",
    "    transform=ax.transAxes,\n",
    "    ha='right', va='bottom',\n",
    "    fontsize=7\n",
    ")\n",
    "\n",
    "# legend with R^2 only\n",
    "legend_handle = Line2D([], [], linestyle='--', color='black')\n",
    "ax.legend([legend_handle], [f\"$R^2 = {r2:.2f}$\"], loc='upper left')\n",
    "\n",
    "# axes formatting\n",
    "ax.set_xlabel('Publication Month')\n",
    "ax.set_ylabel('Number of Studies')\n",
    "ax.set_title('Monthly Study Counts')\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# save at 300 dpi as an svg\n",
    "plt.savefig('monthly_study_counts_with_regression.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23934668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "df = pd.DataFrame(included_studies)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "tiers = [\"all\", 'I', 'II', 'III']\n",
    "markers = ['o', 's', '^', 'D']\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "\n",
    "monthly_by_tier = {}\n",
    "all_dates = []\n",
    "\n",
    "for tier in tiers:\n",
    "    if tier == \"all\":\n",
    "        sub = df\n",
    "    else:\n",
    "        sub = df[df['LLM-tier'].apply(lambda d: d['Tier']==tier)]\n",
    "    mc = (\n",
    "        sub.assign(year_month=lambda x: x['Date'].dt.to_period('M').dt.to_timestamp())\n",
    "           .groupby('year_month').size()\n",
    "           .sort_index()\n",
    "    )\n",
    "    # redistribute January\n",
    "    # the reason we must do this is that many studies that don't have a specific month are grouped into January\n",
    "    adj = mc.copy()\n",
    "    for ts, val in mc.items():\n",
    "        if ts.month==1 and val>0:\n",
    "            adj.loc[ts] = 0\n",
    "            spread = val/12.0\n",
    "            adj.loc[adj.index.year==ts.year] += spread\n",
    "\n",
    "    monthly_by_tier[tier] = adj\n",
    "    all_dates.extend(adj.index)\n",
    "\n",
    "# compute global x-limits\n",
    "x_min = min(all_dates)\n",
    "x_max = max(all_dates)\n",
    "global_max = max(adj.max() for adj in monthly_by_tier.values())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharey=True)\n",
    "\n",
    "for ax, tier, mk, ls in zip(axes.flat, tiers, markers, linestyles):\n",
    "    adj = monthly_by_tier[tier]\n",
    "    dates = adj.index\n",
    "    nums  = mdates.date2num(dates)\n",
    "    y     = adj.values\n",
    "\n",
    "    # fit line\n",
    "    m, b = np.polyfit(nums, y, 1)\n",
    "    y_fit = m*nums + b\n",
    "    slope_mo = m * 30.4375\n",
    "    eqn = rf\"$y = {slope_mo:.2f}\\,x {b:+.1f}$\"\n",
    "\n",
    "    ax.scatter(dates, y, marker=mk, facecolors='none', edgecolors='black')\n",
    "    ax.plot(dates, y_fit, linestyle=ls, color='black', linewidth=1.2)\n",
    "\n",
    "    # set same x-limits on every subplot\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(0, global_max*1.1)\n",
    "\n",
    "    if tier == \"all\":\n",
    "        ax.set_title('All Tiers')\n",
    "    else:\n",
    "        ax.set_title(f'Tier {tier}')\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    ax.text(\n",
    "        0.95, 0.05, eqn,\n",
    "        transform=ax.transAxes,\n",
    "        ha='right', va='bottom',\n",
    "        fontsize=7\n",
    "    )\n",
    "\n",
    "'''\n",
    "for ax in axes[1]:\n",
    "    ax.set_xlabel('Publication Month')\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel('Number of Studies')\n",
    "'''\n",
    "# shared labels\n",
    "fig.supxlabel(\"Publication Month\")\n",
    "fig.supylabel(\"Number of Studies\")\n",
    "\n",
    "#fig.suptitle('Monthly Study Counts by Tier (January redistributed)', y=0.98)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "plt.savefig('monthly_study_counts_by_tier.svg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "df = pd.DataFrame(included_studies)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "tiers = ['S', 'I', 'II', 'III']\n",
    "monthly_by_tier = {}\n",
    "\n",
    "for tier in tiers:\n",
    "    sub = df[df['LLM-tier'].apply(lambda d: d['Tier'] == tier)]\n",
    "    mc = (\n",
    "        sub.assign(year_month=lambda x: x['Date'].dt.to_period('M').dt.to_timestamp())\n",
    "           .groupby('year_month').size()\n",
    "           .sort_index()\n",
    "    )\n",
    "    # redistribute January bug\n",
    "    adj = mc.copy()\n",
    "    for ts, val in mc.items():\n",
    "        if ts.month == 1 and val > 0:\n",
    "            adj.loc[ts] = 0\n",
    "            adj.loc[adj.index.year == ts.year] += val / 12.0\n",
    "    monthly_by_tier[tier] = adj\n",
    "\n",
    "# merge tier I and S due to small sample size of S\n",
    "monthly_by_tier['I'] = monthly_by_tier['I'].add(monthly_by_tier['S'], fill_value=0)\n",
    "del monthly_by_tier['S']\n",
    "tiers.remove('S')\n",
    "\n",
    "# Fit linear model per tier to extract slope & SE\n",
    "results = {}\n",
    "for tier, adj in monthly_by_tier.items():\n",
    "    x_num = mdates.date2num(adj.index)\n",
    "    X = sm.add_constant(x_num)\n",
    "    model = sm.OLS(adj.values, X).fit()\n",
    "    results[tier] = {\n",
    "        'slope': model.params[1],\n",
    "        'se': model.bse[1],\n",
    "        'n_obs': int(model.nobs)\n",
    "    }\n",
    "\n",
    "# Pairwise slope‐difference t‐tests\n",
    "comparisons = []\n",
    "for t1, t2 in combinations(tiers, 2):\n",
    "    b1, se1, n1 = results[t1]['slope'], results[t1]['se'], results[t1]['n_obs']\n",
    "    b2, se2, n2 = results[t2]['slope'], results[t2]['se'], results[t2]['n_obs']\n",
    "    t_stat = (b1 - b2) / np.sqrt(se1**2 + se2**2)\n",
    "    df_num = (se1**2 + se2**2)**2\n",
    "    df_den = (se1**4)/(n1-2) + (se2**4)/(n2-2)\n",
    "    df_w = df_num / df_den\n",
    "    p_raw = 2 * stats.t.sf(abs(t_stat), df_w)\n",
    "    comparisons.append({\n",
    "        'Comparison': f'Tier {t1} vs {t2}',\n",
    "        't_stat': t_stat,\n",
    "        'df': df_w,\n",
    "        'p_value_raw': p_raw\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparisons)\n",
    "\n",
    "# Bonferroni correction\n",
    "m = len(comp_df)\n",
    "# manual\n",
    "comp_df['p_bonf_manual'] = np.minimum(comp_df['p_value_raw'] * m, 1.0)\n",
    "# using statsmodels\n",
    "rej, p_adj, _, _ = multipletests(comp_df['p_value_raw'], alpha=0.05, method='bonferroni')\n",
    "comp_df['p_bonf']    = p_adj\n",
    "comp_df['reject_H0'] = rej\n",
    "\n",
    "# Show results\n",
    "comp_df[['Comparison', 't_stat', 'df', 'p_value_raw', 'p_bonf', 'reject_H0']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "all_answers = []\n",
    "not_found = 0\n",
    "not_applicable = 0\n",
    "\n",
    "records = []\n",
    "for st in included_studies:\n",
    "    ans = st.get('extracted_data', {}) \\\n",
    "            .get('did_the_llm_outperform_the_human?', {}) \\\n",
    "            .get('answer', '').lower()\n",
    "    tier = st.get('LLM-tier', {}).get('Tier')\n",
    "    all_answers.append(ans)\n",
    "    if (\"yes\" in ans or (\"no\" in ans and \"not\" not in ans)) and tier in {'S', 'I', 'II', 'III'} and \"pplicable\" not in ans:\n",
    "        records.append({'Tier': tier, 'Out': ans.capitalize().split()[0]})\n",
    "    elif (\"mixed\" in ans or \"partial\" in ans) and tier in {'S', 'I', 'II', 'III'}:\n",
    "        records.append({'Tier': tier, 'Out': 'Mixed/Partial'})\n",
    "    elif (\"n/a\" in ans or \"not applicable\" in ans or \"not_applicable\" in ans) and tier in {'S', 'I', 'II', 'III'}:\n",
    "        not_applicable += 1\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ── Compute summary proportions per Tier ────────────────────────────────\n",
    "summary = df.groupby(['Tier','Out']).size().unstack(fill_value=0)\n",
    "summary['Total'] = summary.sum(axis=1)\n",
    "summary['Prop_Yes'] = summary['Yes'] / summary['Total']\n",
    "\n",
    "# ── 1) Bar chart of proportion of Yes by Tier ──────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "tiers = ['I','II','III']\n",
    "props = summary.loc[tiers, 'Prop_Yes'].values\n",
    "ax.bar(tiers, props, color=\"lightgrey\", edgecolor='black')\n",
    "ax.set_xlabel('LLM Tier')\n",
    "ax.set_title('Proportion of Studies Where LLM Outperformed Humans')\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylim(0, 0.4)\n",
    "for i, prop in enumerate(props):\n",
    "    ax.text(i, prop + 0.02, f\"{prop:.1%}\", ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"proportion_yes_by_tier.svg\")\n",
    "plt.show()\n",
    "\n",
    "# ── 2) One-sided z-tests with Bonferroni correction ─────────────────────\n",
    "comparisons = [('I','II'), ('I','III'), ('II','III')]\n",
    "p_vals = []\n",
    "labels = []\n",
    "for a,b in comparisons:\n",
    "    count = np.array([int(summary.loc[a, 'Yes']), int(summary.loc[b, 'Yes'])])\n",
    "    nobs = np.array([int(summary.loc[a, 'Total']), int(summary.loc[b, 'Total'])])\n",
    "    stat, p = proportions_ztest(count, nobs, alternative='smaller')\n",
    "    p_vals.append(p)\n",
    "    labels.append(f\"{a} < {b}\")\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Comparison': labels,\n",
    "    'Raw p-value': p_vals,\n",
    "    'Adj. p-value': p_adj,\n",
    "    'Significant': rejected\n",
    "})\n",
    "print(\"\\nOne-Sided Pairwise Comparisons (Bonferroni-corrected):\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# ── 1) Bar chart of proportion of Yes by Tier ──────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "tiers = ['I','II','III']\n",
    "props = summary.loc[tiers, 'Prop_Yes'].values\n",
    "bars = ax.bar(tiers, props, color=\"lightgrey\", edgecolor='black')\n",
    "ax.set_xlabel('LLM Tier')\n",
    "ax.set_title('Proportion of Studies Where LLM Outperformed Humans')\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylim(0, 0.4)\n",
    "for i, prop in enumerate(props):\n",
    "    ax.text(i, prop + 0.02, f\"{prop:.1%}\", ha='center')\n",
    "\n",
    "# ── 2) One‐sided z‐tests with Bonferroni correction ────────────────────\n",
    "comparisons = [('I','II'), ('I','III'), ('II','III')]\n",
    "p_vals = []\n",
    "for a,b in comparisons:\n",
    "    count = np.array([summary.loc[a, 'Yes'], summary.loc[b, 'Yes']])\n",
    "    nobs  = np.array([summary.loc[a, 'Total'], summary.loc[b, 'Total']])\n",
    "    _, p = proportions_ztest(count, nobs, alternative='smaller')\n",
    "    p_vals.append(p)\n",
    "\n",
    "rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# ── 3) Draw significance bars ──────────────────────────────────────────\n",
    "def add_sig_bar(ax, x1, x2, y, h, text):\n",
    "    \"\"\"Draw a horizontal bar with vertical ticks at x1,x2 and place text above.\"\"\"\n",
    "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1.5, c='black')\n",
    "    ax.text((x1 + x2)*0.5, y + h, text, ha='center', va='bottom')\n",
    "\n",
    "# compute where to start the first bar\n",
    "y_max = props.max()\n",
    "bar_offset = 0.03\n",
    "line_height = 0.015\n",
    "\n",
    "for i, ((a,b), sig) in enumerate(zip(comparisons, rejected)):\n",
    "    x1 = tiers.index(a)\n",
    "    x2 = tiers.index(b)\n",
    "    y = y_max + bar_offset + i*(line_height*2)\n",
    "    label = '*' if sig else 'n.s.'\n",
    "    add_sig_bar(ax, x1, x2, y, line_height, label)\n",
    "\n",
    "# extend y‐limit if needed\n",
    "ax.set_ylim(0, y + line_height*2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"proportion_yes_by_tier_with_significance.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from itertools import combinations  # ← added\n",
    "\n",
    "# Prepare data per year\n",
    "records = []\n",
    "for st in included_studies:\n",
    "    date = st.get('Date')\n",
    "    if date is None:\n",
    "        continue\n",
    "    year = pd.to_datetime(date).year\n",
    "    if year == 2022:\n",
    "        continue\n",
    "\n",
    "    tier = st.get('LLM-tier', {}).get('Tier')\n",
    "    ans = st.get('extracted_data', {})\\\n",
    "            .get('did_the_llm_outperform_the_human?', {})\\\n",
    "            .get('answer', '').lower()\n",
    "    if tier == \"S\":\n",
    "        tier = \"I\"\n",
    "    if 'yes' in ans or ('no' in ans and 'not' not in ans):\n",
    "        records.append({'Year': year, 'Tier': tier, 'Out': ans.capitalize()})\n",
    "    elif 'mixed' in ans or 'partial' in ans:\n",
    "        # split mixed/partial counts equally\n",
    "        records.append({'Year': year, 'Tier': tier, 'Out': 'Yes'})\n",
    "        records.append({'Year': year, 'Tier': tier, 'Out': 'No'})\n",
    "\n",
    "df_year = pd.DataFrame(records)\n",
    "\n",
    "# ── Compute counts and proportions ───────────────────────────────────────\n",
    "counts = df_year.groupby(['Year', 'Tier', 'Out']).size().unstack(fill_value=0)\n",
    "totals = counts.sum(axis=1)\n",
    "prop_yes = (counts['Yes'] / totals).unstack(level='Tier').loc[:, ['I','II','III']]\n",
    "\n",
    "# ── Total sample size per year ──────────────────────────────────────────\n",
    "year_totals = df_year.groupby('Year').size()\n",
    "\n",
    "# ── Plot grouped bars by year with black/grey/hatched styling ──────────\n",
    "years = prop_yes.index.values\n",
    "x = np.arange(len(years))\n",
    "width = 0.25\n",
    "\n",
    "styles = [\n",
    "    {'color': 'black',      'hatch': None},   # Tier I\n",
    "    {'color': 'lightgrey',  'hatch': None},   # Tier II\n",
    "    {'color': 'white',      'hatch': '///'}   # Tier III\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i, tier in enumerate(['I','II','III']):\n",
    "    ax.bar(\n",
    "        x + i*width,\n",
    "        prop_yes[tier],\n",
    "        width,\n",
    "        label=f'Tier {tier}',\n",
    "        color=styles[i]['color'],\n",
    "        hatch=styles[i]['hatch'],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "# annotate sample size above each cluster\n",
    "for i, year in enumerate(years):\n",
    "    ax.text(\n",
    "        x[i] + width,\n",
    "        max(prop_yes.loc[year]) + 0.02,\n",
    "        f\"n={year_totals.loc[year]}\",\n",
    "        ha='center', va='bottom', fontsize=8\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Publication Year')\n",
    "ax.set_title('Yearly LLM Outperformance by Tier')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(years)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0, 1.15))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"yearly_outperformance_by_tier.svg\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# One-sided z-tests within each year\n",
    "comparisons = [('I','II'), ('I','III'), ('II','III')]\n",
    "print(\"One-sided proportion tests (Tier A < Tier B) per Year:\\n\")\n",
    "for year in years:\n",
    "    y_counts = counts.loc[year]\n",
    "    yes_counts = y_counts['Yes']\n",
    "    total_counts = y_counts.sum(axis=1)\n",
    "\n",
    "    p_vals = []\n",
    "    labels = []\n",
    "    for a, b in comparisons:\n",
    "        count = np.array([yes_counts[a], yes_counts[b]])\n",
    "        nobs = np.array([total_counts[a], total_counts[b]])\n",
    "        stat, p = proportions_ztest(count, nobs, alternative='smaller')\n",
    "        p_vals.append(p)\n",
    "        labels.append(f\"{a} < {b}\")\n",
    "\n",
    "    # Bonferroni correction\n",
    "    rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "    print(f\"Year {year}:\")\n",
    "    for lbl, raw_p, adj_p, sig in zip(labels, p_vals, p_adj, rejected):\n",
    "        star = \"✓\" if sig else \"✗\"\n",
    "        print(f\"  {lbl}: raw p={raw_p:.3f}, adj p={adj_p:.3f} {star}\")\n",
    "    print()\n",
    "\n",
    "# Inter-year comparisons for each Tier\n",
    "print(\"Inter-year proportion tests (two-sided) for each Tier:\\n\")\n",
    "for tier in ['I', 'II', 'III']:\n",
    "    # Extract the Yes counts and totals for this tier across years\n",
    "    yes_counts_t = counts.xs(tier, level='Tier')['Yes']\n",
    "    total_counts_t = counts.xs(tier, level='Tier').sum(axis=1)\n",
    "\n",
    "    # All unique year-pairs\n",
    "    year_pairs = list(combinations(years, 2))\n",
    "    p_vals = []\n",
    "    labels = []\n",
    "\n",
    "    for y1, y2 in year_pairs:\n",
    "        count = np.array([yes_counts_t[y1], yes_counts_t[y2]])\n",
    "        nobs  = np.array([total_counts_t[y1], total_counts_t[y2]])\n",
    "        stat, p = proportions_ztest(count, nobs, alternative='two-sided')\n",
    "        p_vals.append(p)\n",
    "        labels.append(f\"{y1} vs {y2}\")\n",
    "\n",
    "    # Bonferroni correction across this tier’s comparisons\n",
    "    rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "    print(f\"Tier {tier}:\")\n",
    "    for lbl, raw_p, adj_p, sig in zip(labels, p_vals, p_adj, rejected):\n",
    "        star = \"✓\" if sig else \"✗\"\n",
    "        print(f\"  {lbl}: raw p={raw_p:.3f}, adj p={adj_p:.3f} {star}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec525d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def classify_evaluator(role: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify evaluator into one of 10 broad categories\n",
    "    (Attending, MD Unspecified, Resident, Fellow, Medical Student,\n",
    "    Student (Other), Researcher/Academic, Non-Physician Clinician,\n",
    "    Patient/Public, Other).\n",
    "    \"\"\"\n",
    "\n",
    "    role = role.strip().lower()\n",
    "\n",
    "    # --- Attending ---\n",
    "    attending_roles = {\"attending\", \"attending (fellowship-trained)\"}\n",
    "\n",
    "    # --- MD unspecified ---\n",
    "    md_unspecified = {\"md (unspecified)\", \"physician (md/do unspecified)\"}\n",
    "\n",
    "    # --- Resident ---\n",
    "    resident_roles = {\"resident\"}\n",
    "\n",
    "    # --- Fellow ---\n",
    "    fellow_roles = {\"fellow\"}\n",
    "\n",
    "    # --- Medical Student ---\n",
    "    med_student_roles = {\"medical student\", \"sub-intern\"}\n",
    "\n",
    "    # --- Student (Other) ---\n",
    "    student_other_roles = {\n",
    "        \"student (graduate, not medical)\",\n",
    "        \"student (undergraduate)\",\n",
    "        \"student (other)\",\n",
    "        \"intern\",           # often lumped here in high-level grouping\n",
    "        \"other trainee\"\n",
    "    }\n",
    "\n",
    "    # --- Researcher / Academic ---\n",
    "    academic_roles = {\n",
    "        \"researcher / scientist\", \"faculty / educator\", \"other academic\"\n",
    "    }\n",
    "\n",
    "    # --- Non-Physician Clinician ---\n",
    "    clinician_roles = {\n",
    "        \"nurse\", \"advanced practice provider (np/pa)\",\n",
    "        \"therapist / rehabilitation / mental health\",\n",
    "        \"pharmacist\", \"technician / emt\", \"other clinical provider\"\n",
    "    }\n",
    "\n",
    "    # --- Patient / Public ---\n",
    "    patient_public_roles = {\n",
    "        \"lay public / participant\",\n",
    "        \"caregiver / family member\",\n",
    "        \"non-healthcare domain expert\"\n",
    "    }\n",
    "\n",
    "    # --- Other ---\n",
    "    other_roles = {\n",
    "        \"reviewer / panelist\", \"other support / admin\", \"language services\",\n",
    "        \"information specialist\", \"medical coder\", \"other\", \"other (public)\"\n",
    "    }\n",
    "\n",
    "    # Classification logic\n",
    "    if role in attending_roles:\n",
    "        return \"Attending\"\n",
    "    elif role in md_unspecified:\n",
    "        return \"MD (Unspecified)\"\n",
    "    elif role in resident_roles:\n",
    "        return \"Resident\"\n",
    "    elif role in fellow_roles:\n",
    "        return \"Fellow\"\n",
    "    elif role in med_student_roles:\n",
    "        return \"Medical Student\"\n",
    "    elif role in student_other_roles:\n",
    "        return \"Student (Other)\"\n",
    "    elif role in academic_roles:\n",
    "        return \"Researcher / Academic\"\n",
    "    elif role in clinician_roles:\n",
    "        return \"Non-Physician Clinician\"\n",
    "    elif role in patient_public_roles:\n",
    "        return \"Patient / Public\"\n",
    "    elif role in other_roles:\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "\n",
    "all_others = []\n",
    "buckets = []\n",
    "found = 0\n",
    "for st in included_studies:\n",
    "    prev_len = len(buckets)\n",
    "    for raw in st.get('processed_data', {}).get('human_evaluators', []):\n",
    "        category = classify_evaluator(raw)\n",
    "        if category == 'Other':\n",
    "            all_others.append(raw)\n",
    "        if category != \"Unsure\":\n",
    "            buckets.append(category)\n",
    "    \n",
    "    if len(buckets) > prev_len:\n",
    "        found += 1\n",
    "\n",
    "counts = pd.Series(buckets).value_counts().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.barh(counts.index, counts.values, color='lightgrey', edgecolor='black')\n",
    "ax.set_xlabel('Number of Evaluator Entries')\n",
    "ax.set_title('Human Evaluator Composition with MD (Unspecified) Category')\n",
    "\n",
    "for y, v in enumerate(counts.values):\n",
    "    ax.text(v + 50, y, f\"{v}, {v/len(buckets)*100:.1f}%\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"human_evaluator_composition.svg\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "md_roles = [\n",
    "    \"Attending\",\n",
    "    \"Attending (Fellowship-trained)\",\n",
    "    \"Fellow\",\n",
    "    \"Intern\",\n",
    "    \"MD (Unspecified)\",\n",
    "    \"Physician (MD/DO unspecified)\",\n",
    "    \"Resident\",\n",
    "]\n",
    "\n",
    "# get number that compared to a medical doctor period\n",
    "md_count = sum(1 for cat in buckets if cat in md_roles)\n",
    "#print(md_count, md_count / len(buckets))\n",
    "print(\"Number of MD comparisons:\", md_count)\n",
    "print(\"Percentage of MD comparisons: {:.1f}%\".format(md_count / len(buckets) * 100))\n",
    "\n",
    "# now, get the % of each MD category that makes up the total MD comparisons, also print the absolute number\n",
    "md_categories = ['Attending', 'MD (Unspecified)', 'Resident', 'Fellow']\n",
    "md_counts = counts.loc[md_categories]\n",
    "md_total = md_counts.sum()\n",
    "for cat in md_categories:\n",
    "    cnt = md_counts.loc[cat]\n",
    "    print(f\"{cat}: {cnt} ({cnt/md_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c17967",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_roles = [\n",
    "    \"Attending\",\n",
    "    \"Attending (Fellowship-trained)\",\n",
    "    \"Fellow\",\n",
    "    \"Intern\",\n",
    "    \"MD (Unspecified)\",\n",
    "    \"Physician (MD/DO unspecified)\",\n",
    "    \"Resident\",\n",
    "]\n",
    "\n",
    "general_public = [\n",
    "    \"Caregiver / Family Member\",\n",
    "    \"Lay Public / Participant\",\n",
    "    \"Other (Public)\",\n",
    "    \"Patient\"\n",
    "]\n",
    "\n",
    "set_of_comparators = general_public\n",
    "\n",
    "set_of_comparators = [item.lower().strip() for item in set_of_comparators]\n",
    "\n",
    "all_counts = {} # The number of comparisons found for each category\n",
    "counts = {} # The number of outcomes found for each category (some comparisons do not have outcomes)\n",
    "\n",
    "outcomes_found = 0\n",
    "for st in included_studies:\n",
    "    ans_raw = st.get('extracted_data', {}) \\\n",
    "        .get('did_the_llm_outperform_the_human?', {}) \\\n",
    "        .get('answer', '').lower()\n",
    "    \n",
    "    if ans_raw not in (\"\", \"n/a\", \"not applicable\", \"not_applicable\", \"unsure\"):\n",
    "        outcomes_found += 1\n",
    "        comparators = [item.lower().strip() for item in st[\"processed_data\"][\"human_evaluators\"]]\n",
    "\n",
    "        if any(role in set_of_comparators for role in comparators):\n",
    "            for role in comparators:\n",
    "                if role in set_of_comparators:\n",
    "                    counts[role] = counts.get(role, 0) + 1\n",
    "        else:\n",
    "            counts[\"other\"] = counts.get(\"other\", 0) + 1\n",
    "    \n",
    "    for item in st[\"processed_data\"][\"human_evaluators\"]:\n",
    "        if item.lower().strip() in set_of_comparators:\n",
    "            all_counts[item] = all_counts.get(item, 0) + 1\n",
    "        else:\n",
    "            all_counts[\"other\"] = all_counts.get(\"other\", 0) + 1\n",
    "\n",
    "print(f\"Found outcomes for {outcomes_found} studies.\")\n",
    "print(all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ea3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_found = 0\n",
    "outcomes = []\n",
    "for st in included_studies:\n",
    "    ans_raw = st.get('extracted_data', {}) \\\n",
    "        .get('did_the_llm_outperform_the_human?', {}) \\\n",
    "        .get('answer', '').lower()\n",
    "    \n",
    "    if ans_raw not in (\"\", \"n/a\", \"not applicable\", \"not_applicable\", \"unsure\"):\n",
    "        comparators = [item.lower().strip() for item in st[\"processed_data\"][\"human_evaluators\"]]\n",
    "        results_found += 1\n",
    "    \n",
    "    if 'mixed' in ans_raw or 'partial' in ans_raw:\n",
    "        outcomes.append(\"mixed\")\n",
    "    elif 'yes' in ans_raw:\n",
    "        outcomes.append(\"yes\")\n",
    "    elif 'no' in ans_raw and \"not\" not in ans_raw:\n",
    "        outcomes.append(\"no\")\n",
    "    #else: # uncomment this to show the full set of raw answers, which is: {unsure, no, yes, mixed, not applicable, not_applicable, n/a}\n",
    "    #    outcomes.append(ans_raw)\n",
    "\n",
    "# get counts of each\n",
    "results = {}\n",
    "for item in outcomes:\n",
    "    results[item] = results.get(item, 0) + 1\n",
    "\n",
    "# print results, both absolute and percentage\n",
    "total = sum(results.values())\n",
    "for key, val in results.items():\n",
    "    print(f\"{key}: {val} ({val/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e13114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import itertools\n",
    "\n",
    "# Collect outcome data across included tiers\n",
    "records = []\n",
    "for st in included_studies:\n",
    "    tier = st.get('LLM-tier', {}).get('Tier')\n",
    "    if tier not in {'I', 'II', 'III'}:\n",
    "        continue\n",
    "\n",
    "    ans_raw = st.get('extracted_data', {}) \\\n",
    "                .get('did_the_llm_outperform_the_human?', {}) \\\n",
    "                .get('answer', '').lower()\n",
    "\n",
    "    if 'mixed' in ans_raw or 'partial' in ans_raw:\n",
    "        outs = ['Yes', 'No']\n",
    "    elif 'yes' in ans_raw:\n",
    "        outs = ['Yes']\n",
    "    elif 'no' in ans_raw and 'not' not in ans_raw:\n",
    "        outs = ['No']\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for cat in st.get('processed_data', {}).get('human_evaluators', []):\n",
    "        if cat == \"Attending (Fellowship-trained)\":\n",
    "            cat = \"Attending\"\n",
    "        if cat == \"Intern\":\n",
    "            cat = \"Resident\"\n",
    "        if cat == \"Sub-intern\":\n",
    "            cat = \"Medical Student\"\n",
    "        if (\n",
    "            cat == \"Advanced Practice Provider (NP/PA)\" or\n",
    "            cat == \"Nurse\" or\n",
    "            cat == \"Pharmacist\" or\n",
    "            cat == \"Technician / EMT\" or\n",
    "            cat == \"Therapist / Rehabilitation / Mental Health\" or\n",
    "            cat == \"Other Clinical Provider\"\n",
    "        ):\n",
    "            cat = \"Non-physician Clinician\"\n",
    "\n",
    "        for out in outs:\n",
    "            records.append({'Category': cat, 'Out': out})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ── 2) Compute proportions of “Yes” per category ─────────────────────────\n",
    "summary = df.groupby(['Category', 'Out']).size().unstack(fill_value=0)\n",
    "summary['Total'] = summary.sum(axis=1)\n",
    "summary['Prop_Yes'] = summary['Yes'] / summary['Total']\n",
    "\n",
    "# order categories\n",
    "order = [\n",
    "    'Non-physician Clinician',\n",
    "    'Attending',\n",
    "    \"Resident\",\n",
    "    \"Medical Student\",\n",
    "    'MD (Unspecified)',\n",
    "]\n",
    "summary = summary.loc[order]\n",
    "\n",
    "# ── helper to draw significance bars ─────────────────────────────────────\n",
    "def add_sig_bar(ax, x1, x2, y, h, text):\n",
    "    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1.5, c='black')\n",
    "    ax.text((x1 + x2) / 2, y + h, text, ha='center', va='bottom')\n",
    "\n",
    "# ── 3) Plot bar chart ────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "bars = ax.bar(summary.index, summary['Prop_Yes'],\n",
    "              color='lightgrey', edgecolor='black')\n",
    "\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1))\n",
    "ax.set_ylim(0, 0.6)\n",
    "\n",
    "# annotate percentages and sample sizes\n",
    "for i, (cat, row) in enumerate(summary.iterrows()):\n",
    "    prop  = row['Prop_Yes']\n",
    "    total = int(row['Total'])\n",
    "    ax.text(i, prop + 0.02, f\"{prop:.0%}\", ha='center')\n",
    "    ax.text(i, 0.75,      f\"n={total}\",      ha='center',\n",
    "            va='bottom', fontsize=8)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Pairwise z‐tests & FDR‐BH correction\n",
    "categories = summary.index.tolist()\n",
    "pairs      = list(itertools.permutations(categories, 2))\n",
    "\n",
    "p_vals = []\n",
    "labels = []\n",
    "for a, b in pairs:\n",
    "    count = [summary.loc[a, 'Yes'], summary.loc[b, 'Yes']]\n",
    "    nobs  = [summary.loc[a, 'Total'], summary.loc[b, 'Total']]\n",
    "    _, p = proportions_ztest(count, nobs, alternative='larger')\n",
    "    p_vals.append(p)\n",
    "    labels.append(f\"{a} > {b}\")\n",
    "\n",
    "# use FDR-BH to match your significance table\n",
    "rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_gbs')\n",
    "\n",
    "results   = pd.DataFrame({\n",
    "    'Comparison':   labels,\n",
    "    'Adj. p-value': p_adj,\n",
    "    'Significant':  rejected\n",
    "})\n",
    "sig_comps = results.loc[results['Significant'], 'Comparison'].tolist()\n",
    "\n",
    "# Draw significance bars for the significant comparisons\n",
    "y_max      = summary['Prop_Yes'].max()\n",
    "bar_offset = 0.05\n",
    "line_h     = 0.015\n",
    "\n",
    "for i, comp in enumerate(sig_comps):\n",
    "    a, b = comp.split(' > ')\n",
    "    x1 = categories.index(b)\n",
    "    x2 = categories.index(a)\n",
    "    y  = y_max + bar_offset + i * (line_h * 2)\n",
    "    add_sig_bar(ax, x1, x2, y, line_h, '*')\n",
    "\n",
    "# extend y‐limit to fit all bars\n",
    "ax.set_ylim(0, y + line_h * 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"llm_outperformance_by_evaluator_with_significance.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e536d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "categories = summary.index.tolist()\n",
    "\n",
    "# Prepare pairwise tests for all ordered pairs\n",
    "pairs = list(itertools.permutations(categories, 2))\n",
    "p_vals = []\n",
    "labels = []\n",
    "\n",
    "for a, b in pairs:\n",
    "    count = [int(summary.loc[a, 'Yes']), int(summary.loc[b, 'Yes'])]\n",
    "    nobs = [int(summary.loc[a, 'Total']), int(summary.loc[b, 'Total'])]\n",
    "    stat, p = proportions_ztest(count, nobs, alternative='larger')\n",
    "    p_vals.append(p)\n",
    "    labels.append(f\"{a} > {b}\")\n",
    "\n",
    "# Apply correction\n",
    "rejected, p_adj, _, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# Compile and display results\n",
    "results = pd.DataFrame({\n",
    "    'Comparison': labels,\n",
    "    'Raw p-value': p_vals,\n",
    "    'Adj. p-value': p_adj,\n",
    "    'Significant': rejected\n",
    "})\n",
    "print(results.query(\"Significant\").to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa26ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all results where the raw p-value is less than 0.05\n",
    "print(\"\\nAll comparisons with raw p-value < 0.05:\\n\")\n",
    "print(results.query(\"`Raw p-value` < 0.05\").to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc2ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
