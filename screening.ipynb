{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc4e4a",
   "metadata": {},
   "source": [
    "# Human Screening\n",
    "In this section we compute the human agreement in the inclusion/exclusion phase, as measured by Cohen's k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen1_fn = \"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Screener Group 1.csv\"\n",
    "screen2_fn = \"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Screener Group 2.csv\"\n",
    "tiebreak_fn = \"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Tiebreaks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both screener data\n",
    "# data will be a list of dict_keys(['Title', 'Abstract', 'Include?', 'Screener Name', 'Comments', ''])\n",
    "with open(screen1_fn, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    screen1_data = [row for row in reader]\n",
    "with open(screen2_fn, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    screen2_data = [row for row in reader]\n",
    "with open(tiebreak_fn, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    tiebreak_data = [row for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all studies are screened by both screeners\n",
    "screen1_titles = {row[\"Title\"] for row in screen1_data if row[\"Title\"]}\n",
    "screen2_titles = {row[\"Title\"] for row in screen2_data if row[\"Title\"]}\n",
    "intersection = screen1_titles & screen2_titles\n",
    "\n",
    "assert len(intersection) == 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the decisions into a neater structure\n",
    "decisions = {}\n",
    "for row in screen1_data:\n",
    "    if row[\"Title\"] in intersection:\n",
    "        decisions[row[\"Title\"]] = {\"Screener 1\": row[\"Include?\"],\n",
    "                                   \"Screener 2\": None}\n",
    "        decisions[row[\"Title\"]][\"Abstract\"] = row[\"Abstract\"]\n",
    "        \n",
    "for row in screen2_data:\n",
    "    if row[\"Title\"] in intersection:\n",
    "        decisions[row[\"Title\"]][\"Screener 2\"] = row[\"Include?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute agreement and add tiebreaker decisions\n",
    "for key in decisions:\n",
    "    if decisions[key][\"Screener 1\"] == decisions[key][\"Screener 2\"]:\n",
    "        decisions[key][\"agree?\"] = True\n",
    "    else:\n",
    "        decisions[key][\"agree?\"] = False\n",
    "        for row in tiebreak_data:\n",
    "            if row[\"Title\"] == key:\n",
    "                decisions[key][\"tiebreaker\"] = row[\"Final Decision\"]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute kappa score\n",
    "screener1 = [decision[\"Screener 1\"] for decision in decisions.values()]\n",
    "screener2 = [decision[\"Screener 2\"] for decision in decisions.values()]\n",
    "\n",
    "# compute kappa\n",
    "pairs = list(zip(screener1, screener2))\n",
    "n = len(pairs)\n",
    "\n",
    "obs_kappa = cohen_kappa_score(\n",
    "    [h for h, _ in pairs],\n",
    "    [l for _, l in pairs]\n",
    ")\n",
    "\n",
    "B = 50000\n",
    "boot_kappas = []\n",
    "for _ in range(B):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    k = cohen_kappa_score(h_samp, l_samp)\n",
    "    boot_kappas.append(k)\n",
    "\n",
    "# derive the 95% percentile CI\n",
    "alpha = 0.05\n",
    "lower, upper = np.percentile(boot_kappas, [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "print(f\"Observed kappa: {obs_kappa:.3f}\")\n",
    "print(f\"Bootstrap 95% CI: {lower:.3f} - {upper:.3f}\")\n",
    "\n",
    "percent_agree = sum([decision[\"agree?\"] for decision in decisions.values()]) / len(decisions) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85673d",
   "metadata": {},
   "source": [
    "# LLM Screening\n",
    "Here, we prepare a batch job to screen all deduplicated studies for inclusion/exclusion via GPT-4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448542ff",
   "metadata": {},
   "source": [
    "## Prepare Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Prompts/screening_instructions.txt\", \"r\") as f:\n",
    "    INSTRUCTIONS = f.read()\n",
    "\n",
    "studies = []\n",
    "with open(\"deduped_and_processed_studies.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        study = json.loads(line)\n",
    "        studies.append(study)\n",
    "\n",
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"high\"}, \n",
    "                      \"instructions\": INSTRUCTIONS,\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "\n",
    "out_jsonl = []\n",
    "for idx, row in tqdm(enumerate(studies), total=len(studies)):\n",
    "    out = copy.deepcopy(template)\n",
    "    out[\"custom_id\"] = str(idx)\n",
    "    out[\"body\"][\"input\"] = f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\"\n",
    "    out_jsonl.append(out)\n",
    "\n",
    "with open(\"embase_pubmed_scopus_batch-GPT-5r-high.jsonl\", \"w\") as f:\n",
    "    for entry in out_jsonl:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f74ba",
   "metadata": {},
   "source": [
    "## Process Batch Job Results\n",
    "Now, we need to process the result since the batch job only returns the responses to our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original batch\n",
    "original_messages = {}\n",
    "with open(\"embase_pubmed_scopus_batch-GPT-5r-high.jsonl\", \"r\") as f:\n",
    "    batch = [json.loads(line) for line in f]\n",
    "\n",
    "for entry in batch:\n",
    "    original_messages[entry[\"custom_id\"]] = entry[\"body\"][\"input\"]\n",
    "\n",
    "responses = []\n",
    "with open(\"Batch Responses/GPT-5r-high-screening-output.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        response = json.loads(line)\n",
    "        custom_id = response[\"custom_id\"]\n",
    "        if custom_id in original_messages:\n",
    "            response[\"original_message\"] = original_messages[custom_id]\n",
    "            responses.append(response)\n",
    "\n",
    "studies = []\n",
    "with open(\"deduped_and_processed_studies.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        study = json.loads(line)\n",
    "        studies.append(study)\n",
    "\n",
    "failures = []\n",
    "overlap = 0\n",
    "for i in tqdm(range(0, len(studies))):\n",
    "    for response in responses:\n",
    "        try:\n",
    "            if str(studies[i][\"Abstract\"]) in response[\"original_message\"] and str(studies[i][\"Title\"]) in response[\"original_message\"]:\n",
    "                result = json.loads(response[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"].replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "\n",
    "                studies[i][\"Comments\"] = result[\"Comments\"]\n",
    "                studies[i][\"Include?\"] = result[\"Include?\"]\n",
    "                \n",
    "                break\n",
    "        except Exception as e:\n",
    "            failures.append((i, str(e)))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "corrected_responses = []\n",
    "# re-process failures\n",
    "for failure in tqdm(failures):\n",
    "    corrected_responses.append(client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        instructions=INSTRUCTIONS,\n",
    "        input=f\"Title: {studies[failure[0]]['Title']}\\nAbstract: {studies[failure[0]]['Abstract']}\",\n",
    "        reasoning={\"effort\":\"high\"},\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412cb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "for failure, corrected_response in zip(failures, corrected_responses):\n",
    "    try:\n",
    "        result = json.loads(corrected_response.output[1].content[0].text.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "        studies[failure[0]][\"Comments\"] = result[\"Comments\"]\n",
    "        studies[failure[0]][\"Include?\"] = result[\"Include?\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed again for index {failure[0]}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "with open(\"deduped_and_processed_studies-GPT-5r-high.jsonl\", 'w') as f:\n",
    "    for record in studies:\n",
    "        f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b4f85",
   "metadata": {},
   "source": [
    "### Join the LLM data with the whole data structure\n",
    "Here, we take the processed LLM data and merge it with the human data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = []\n",
    "with open(\"deduped_and_processed_studies-GPT-5r-high.jsonl\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        studies.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two studies disappeared in the second scrape, so we manually process them to reproduce the results prior to revision 1 (after which the new scrape was done)\n",
    "\n",
    "# Expression of Concern: Evaluating the Factors Influencing Residency Match for Surgical Specialty Applicants and Programs: Challenges and Future Directions (The American Surgeon™, (2024), (00031348241262427), 10.1177/00031348241262427)\n",
    "# A Multiagent-Driven Robotic AI Chemist Enabling Autonomous Chemical Research On Demand\n",
    "\n",
    "for title in [\n",
    "    \"Expression of Concern: Evaluating the Factors Influencing Residency Match for Surgical Specialty Applicants and Programs: Challenges and Future Directions (The American Surgeon™, (2024), (00031348241262427), 10.1177/00031348241262427)\",\n",
    "    \"A Multiagent-Driven Robotic AI Chemist Enabling Autonomous Chemical Research On Demand\"\n",
    "]:\n",
    "    abstract = decisions[title][\"Abstract\"]\n",
    "\n",
    "    result = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        instructions=INSTRUCTIONS,\n",
    "        input=f\"Title: {title}\\nAbstract: {abstract}\",\n",
    "        reasoning={\"effort\":\"high\"},\n",
    "    )\n",
    "\n",
    "    decision = json.loads(result.output[1].content[0].text.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "    studies.append({\n",
    "        \"Title\": title,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Comments\": decision[\"Comments\"],\n",
    "        \"Include?\": decision[\"Include?\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat studies to be keyed by title (more convenient this way)\n",
    "studies_by_title = {}\n",
    "for study in studies:\n",
    "    title = study[\"Title\"]\n",
    "    if title not in studies_by_title:\n",
    "        studies_by_title[title] = study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e93d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the titles changed very slightly between the first and second scrape\n",
    "# we use difflib to find the matching titles\n",
    "# there are so few mismatches that we can just manually check them to ensure they match, which they do\n",
    "\n",
    "import difflib\n",
    "\n",
    "def find_closest_title(query, titles):\n",
    "    matches = difflib.get_close_matches(query, titles, n=1, cutoff=0.9)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    # fallback: just return the best-scoring title even if < cutoff\n",
    "    best = max(titles, key=lambda t: difflib.SequenceMatcher(None, query, t).ratio())\n",
    "    return best\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# this block computes bounds on the sensitivity and specificity of the LLM screening\n",
    "human_consensus = [] # human decisions\n",
    "llm_consensus = [] # llm decisions\n",
    "\n",
    "for study in decisions: # recall that decisions is keyed by title\n",
    "    if decisions[study][\"agree?\"]:\n",
    "        human_consensus.append(decisions[study][\"Screener 1\"].lower())\n",
    "    else:\n",
    "        if \"tiebreaker\" in decisions[study]:\n",
    "            human_consensus.append(decisions[study][\"tiebreaker\"].lower())\n",
    "\n",
    "    if study not in studies_by_title:\n",
    "        # Unfortunately, on the second scrape, some titles changed extremely slightly\n",
    "        # We manually review each of these matches to ensure they are the correct matches\n",
    "        closest_title = find_closest_title(study, studies_by_title.keys())\n",
    "        print(f\"Could not find exact match for title:\\n{study}\\nClosest match is:\\n{closest_title}\\n\")\n",
    "        llm_consensus.append(studies_by_title[closest_title][\"Include?\"].lower())\n",
    "    else:\n",
    "        llm_consensus.append(studies_by_title[study][\"Include?\"].lower())\n",
    "\n",
    "pairs = list(zip(human_consensus, llm_consensus))\n",
    "n = len(pairs)\n",
    "\n",
    "obs_kappa = cohen_kappa_score(\n",
    "    [h for h, _ in pairs],\n",
    "    [l for _, l in pairs]\n",
    ") # compute the observed kappa for the human-LLM pairs\n",
    "\n",
    "# bootstrap the kappa score to get confidence intervals\n",
    "B = 50000\n",
    "boot_kappas = []\n",
    "for _ in range(B):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    k = cohen_kappa_score(h_samp, l_samp)\n",
    "    boot_kappas.append(k)\n",
    "\n",
    "alpha = 0.05\n",
    "lower, upper = np.percentile(boot_kappas, [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "print(f\"Observed kappa: {obs_kappa:.3f}\")\n",
    "print(f\"Bootstrap 95% CI: {lower:.3f} - {upper:.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "n_iters = 50000\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "for _ in range(n_iters):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    \n",
    "    cm = confusion_matrix(h_samp, l_samp, labels=[\"no\", \"yes\"])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if tp + fn else float(\"nan\")\n",
    "    specificity = tn / (tn + fp) if tn + fp else float(\"nan\")\n",
    "    \n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "# --- derive the 95% percentile CI ---\n",
    "alpha = 0.05\n",
    "sensitivity_lower, sensitivity_upper = np.percentile(sensitivities, [100*alpha/2, 100*(1-alpha/2)])\n",
    "specificity_lower, specificity_upper = np.percentile(specificities, [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "h_samp, l_samp = zip(*pairs)\n",
    "cm = confusion_matrix(h_samp, l_samp, labels=[\"no\", \"yes\"])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"Sensitivity: {tp / (tp + fn) if tp + fn else float('nan'):.3f}\")\n",
    "print(f\"Specificity: {tn / (tn + fp) if tn + fp else float('nan'):.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity_lower:.3f} - {sensitivity_upper:.3f}\")\n",
    "print(f\"Specificity: {specificity_lower:.3f} - {specificity_upper:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of disagreements between human consensus and llm consensus\n",
    "disagreements = sum(1 for h, l in pairs if h != l)\n",
    "print(f\"Number of disagreements between human consensus and LLM consensus: {disagreements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742e8ef",
   "metadata": {},
   "source": [
    "### Estimating the true inclusion bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# constants\n",
    "N_full   = 12896\n",
    "P_obs    = 4609 # LLM inclusions\n",
    "K_yes    = sum(x == \"yes\" for x, _ in pairs) # human 'yes' in the audit\n",
    "n_audit  = 500 # number of samples we used in the human audit\n",
    "\n",
    "# Jeffreys posterior for prevalence\n",
    "alpha_p, beta_p = K_yes + 0.5, n_audit - K_yes + 0.5\n",
    "\n",
    "TP = []\n",
    "FN = []\n",
    "FP = []\n",
    "TN = []\n",
    "\n",
    "P_pred = [] # estimating how many positives the LLM would predict\n",
    "true_number_of_positives = [] # list to store the estimated number of studies that are actually positive\n",
    "\n",
    "for se, sp in zip(sensitivities, specificities):\n",
    "    # draw a random prevalence from the Beta posterior\n",
    "    pi = random.betavariate(alpha_p, beta_p)\n",
    "    \n",
    "    # propagate to full corpus\n",
    "    tp = N_full * pi * se\n",
    "    fn = N_full * pi * (1 - se)\n",
    "    fp = N_full * (1 - pi) * (1 - sp)\n",
    "    tn = N_full * (1 - pi) * sp\n",
    "\n",
    "    true_number_of_positives.append(tp + fn)\n",
    "    \n",
    "    TP.append(tp)\n",
    "    FN.append(fn)\n",
    "    FP.append(fp)\n",
    "    TN.append(tn)\n",
    "\n",
    "    P_pred.append(tp + fp)\n",
    "\n",
    "# 95 % percentile intervals\n",
    "def ci(x):\n",
    "      return np.percentile(x, [2.5, 97.5]).astype(int)\n",
    "\n",
    "ci_TP, ci_FN, ci_FP, ci_TN = map(ci, (TP, FN, FP, TN))\n",
    "mean_TP, mean_FN, mean_FP, mean_TN = map(lambda x: int(np.mean(x)),\n",
    "                                         (TP, FN, FP, TN))\n",
    "\n",
    "print(f\"TP ≈ {mean_TP} (95 % CI {ci_TP[0]}-{ci_TP[1]})\")\n",
    "print(f\"FP ≈ {mean_FP} (95 % CI {ci_FP[0]}-{ci_FP[1]})\")\n",
    "print(f\"FN ≈ {mean_FN} (95 % CI {ci_FN[0]}-{ci_FN[1]})\")\n",
    "print(f\"TN ≈ {mean_TN} (95 % CI {ci_TN[0]}-{ci_TN[1]})\")\n",
    "print(f\"Total positives ≈ {int(np.mean(true_number_of_positives))} \"\n",
    "      f\"(95 % CI {ci(true_number_of_positives)[0]}-{ci(true_number_of_positives)[1]})\")\n",
    "print(f\"Predicted LLM positives ≈ {int(np.mean(P_pred))} \"\n",
    "      f\"(the model output {P_obs})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa221ec",
   "metadata": {},
   "source": [
    "# Tiering\n",
    "Here we compute human tiering scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed753792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiering1_fn = \"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Tiering Group 1.csv\"\n",
    "tiering2_fn = \"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Tiering Group 2.csv\"\n",
    "\n",
    "tiering1_data = pd.read_csv(tiering1_fn).to_dict(orient='records')\n",
    "tiering2_data = pd.read_csv(tiering2_fn).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {}\n",
    "for row1 in tiering1_data:\n",
    "    title = row1[\"Title\"]\n",
    "    for row2 in tiering2_data:\n",
    "        if row2[\"Title\"] == title:\n",
    "            if type(row1[\"Tier\"]) is not str or type(row2[\"Tier\"]) is not str:\n",
    "                continue\n",
    "\n",
    "            # initial human rejection of studies that don't fit the inclusion criteria\n",
    "            # we are quite strict here to be positive we are not including any studies that do not fit the inclusion criteria for the LLM analysis later\n",
    "            if row1[\"Tier\"] == \"X\" or row2[\"Tier\"] == \"X\":\n",
    "                continue\n",
    "            questions[title] = {\n",
    "                \"Title\": title,\n",
    "                \"Abstract\": row1[\"Abstract\"],\n",
    "                \"Tier\": [row1[\"Tier\"], row2[\"Tier\"]],\n",
    "                \"Screeners\": [row1[\"Screener Name\"], row2[\"Screener Name\"]],\n",
    "            }\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiering_decisions1 = [item[\"Tier\"][0] for item in questions.values()]\n",
    "tiering_decisions2 = [item[\"Tier\"][1] for item in questions.values()]\n",
    "\n",
    "# bootstrap kappa score\n",
    "pairs = list(zip(tiering_decisions1, tiering_decisions2))\n",
    "n = len(pairs)\n",
    "obs_kappa = cohen_kappa_score(\n",
    "    [h for h, _ in pairs],\n",
    "    [l for _, l in pairs]\n",
    ")\n",
    "\n",
    "B = 50000\n",
    "boot_kappas = []\n",
    "for _ in range(B):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    k = cohen_kappa_score(h_samp, l_samp)\n",
    "    boot_kappas.append(k)\n",
    "\n",
    "# --- derive the 95% percentile CI ---\n",
    "alpha = 0.05\n",
    "lower, upper = np.percentile(boot_kappas, [100*alpha/2, 100*(1-alpha/2)])\n",
    "print(f\"Observed kappa (tiering): {obs_kappa:.3f}\")\n",
    "print(f\"Bootstrap 95% CI (tiering): {lower:.3f} - {upper:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9c7ad",
   "metadata": {},
   "source": [
    "# LLM Tiering\n",
    "Here we generate the batch requests for the LLM tiering process and analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd30458",
   "metadata": {},
   "source": [
    "## Batch Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Prompts/tiering_prompt.txt\", \"r\") as f:\n",
    "    TIEIRNG_PROMPT = f.read()\n",
    "\n",
    "with open(\"deduped_and_processed_studies-GPT-5r-high.jsonl\", \"r\") as f:\n",
    "    studies = [json.loads(line) for line in f]\n",
    "    df = pd.DataFrame(studies)\n",
    "\n",
    "included_df = df[df[\"Include?\"] == \"yes\"]\n",
    "\n",
    "with open(\"included_studies.jsonl\", \"w\") as f:\n",
    "    for item in included_df.reset_index().to_dict(orient='records'):\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(\"included_studies.jsonl\", \"r\") as f:\n",
    "    included_studies = [json.loads(line) for line in f]\n",
    "\n",
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"high\"}, \n",
    "                      \"instructions\": TIEIRNG_PROMPT,\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "\n",
    "out_jsonl = []\n",
    "for idx, row in tqdm(enumerate(studies), total=len(studies)):\n",
    "    out = copy.deepcopy(template)\n",
    "    out[\"custom_id\"] = str(idx)\n",
    "    out[\"body\"][\"input\"] = f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\"\n",
    "    out_jsonl.append(out)\n",
    "\n",
    "with open(\"tiering_batch_requests.jsonl\", \"w\") as f:\n",
    "    for item in out_jsonl:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(\"Batch Responses/GPT-5r-high-tiering-output.jsonl\", \"r\") as f:\n",
    "    responses = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c42188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(responses):\n",
    "    try:\n",
    "        gpt5r_tier = json.loads(item[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"].replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "        included_studies[i][\"LLM-tier\"] = gpt5r_tier\n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing response {i}: {e}\")\n",
    "\n",
    "with open(\"included_studies_with_tiers-GPT-5r-high.jsonl\", \"w\") as f:\n",
    "    for item in included_studies:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68138531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign tiebreaks\n",
    "with open(\"Human Screening and Tiering Data/Nature Medicine LLM Systematic Review - Tiering Tiebreaks.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    tiebreak_data = [row for row in reader]\n",
    "\n",
    "    tiebreaks = {}\n",
    "    for item in tiebreak_data:\n",
    "        title = item[\"Title\"]\n",
    "        assert title not in tiebreaks, f\"Duplicate title found: {title}\"\n",
    "        tiebreaks[title] = item[\"Tiebreak\"]\n",
    "\n",
    "for title in questions:\n",
    "    if questions[title][\"Tier\"][0] == questions[title][\"Tier\"][1]:\n",
    "        assert title not in tiebreaks, f\"Tiebreak found for title with no disagreement: {title}\"\n",
    "    else:\n",
    "        assert title in tiebreaks, f\"No tiebreak found for title with disagreement: {title}\"\n",
    "        questions[title][\"Tiebreak\"] = tiebreaks[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf21214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same story as before, some works were removed in the second scrape\n",
    "# We manually add them here to reproduce the results prior to revision 1 (after which the new scrape was done)\n",
    "for title in tqdm([\"ChatGPT-o1 and the Pitfalls of Familiar Reasoning in Medical Ethics\",\n",
    "                \"From Answers to Insights: Unveiling the Strengths and Limitations of ChatGPT and Biomedical Knowledge Graphs\",\n",
    "                \"Leveraging artificial intelligence to detect ethical concerns in medical research: A case study\",\n",
    "                \"Evaluating the performance of Generative Pre-trained Transformer-4 (GPT-4) in standardizing radiology reports\",\n",
    "                \"Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation\",\n",
    "                \"A Novel RAG Framework with Knowledge-Enhancement for Biomedical Question Answering\",\n",
    "                \"MedGen: An Explainable Multi-Agent Architecture for Clinical Decision Support through Multisource Knowledge Fusion\",\n",
    "                \"Managing class imbalance in the training of a large language model to predict patient selection for total knee arthroplasty: Results from the Artificial intelligence to Revolutionise the patient Care pathway in Hip and knEe aRthroplastY (ARCHERY) project\",\n",
    "                \"Expert evaluation of large language models for clinical dialogue summarization\",\n",
    "                \"Automated and code-free development of a risk calculator using ChatGPT-4 for predicting diabetic retinopathy and macular edema without retinal imaging\"]):\n",
    "    # find associated abstract\n",
    "    abstract = questions[title][\"Abstract\"]\n",
    "    result = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        instructions=TIEIRNG_PROMPT,\n",
    "        input=f\"Title: {title}\\nAbstract: {abstract}\",\n",
    "        reasoning={\"effort\":\"high\"},\n",
    "    )\n",
    "\n",
    "    decision = json.loads(result.output[1].content[0].text.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "    included_studies.append({\n",
    "        \"Title\": title,\n",
    "        \"Abstract\": abstract,\n",
    "        \"LLM-tier\": decision\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the titles changed very slightly between the first and second scrape\n",
    "# we use difflib to find the matching titles\n",
    "# there are so few mismatches that we can just manually check them to ensure they match, which they do\n",
    "\n",
    "import difflib\n",
    "\n",
    "def find_closest_title(query, titles):\n",
    "    matches = difflib.get_close_matches(query, titles, n=1, cutoff=0.5)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    # fallback: just return the best-scoring title even if < cutoff\n",
    "    best = max(titles, key=lambda t: difflib.SequenceMatcher(None, query, t).ratio())\n",
    "    return best\n",
    "\n",
    "# compute LLM kappa\n",
    "ground_truth_tierings = [item[\"Tier\"][0].lower() if \"Tiebreak\" not in item else item[\"Tiebreak\"].lower() for item in questions.values()]\n",
    "llm_tierings = []\n",
    "for title in questions.keys():\n",
    "    found_study = False\n",
    "    for item in included_studies:\n",
    "        if item[\"Title\"] == title:\n",
    "            llm_tierings.append(item[\"LLM-tier\"][\"Tier\"].lower())\n",
    "            found_study = True\n",
    "            break\n",
    "    \n",
    "    if not found_study:\n",
    "        closest_title = find_closest_title(title, [item[\"Title\"] for item in included_studies])\n",
    "        print(f\"Could not find exact match for title:\\n{title}\\nClosest match is:\\n{closest_title}\\n\")\n",
    "        for item in included_studies:\n",
    "            if item[\"Title\"] == closest_title:\n",
    "                llm_tierings.append(item[\"LLM-tier\"][\"Tier\"].lower())\n",
    "                found_study = True\n",
    "                break\n",
    "\n",
    "    assert found_study, f\"Study not found for title: {title}\"\n",
    "\n",
    "pairs = list(zip(ground_truth_tierings, llm_tierings))\n",
    "n = len(pairs)\n",
    "obs_kappa = cohen_kappa_score(\n",
    "    [h for h, _ in pairs],\n",
    "    [l for _, l in pairs]\n",
    ")\n",
    "B = 50000\n",
    "boot_kappas = []\n",
    "for _ in range(B):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    k = cohen_kappa_score(h_samp, l_samp)\n",
    "    boot_kappas.append(k)\n",
    "\n",
    "# derive the 95% percentile CI\n",
    "alpha = 0.05\n",
    "lower, upper = np.percentile(boot_kappas, [100*alpha/2, 100*(1-alpha/2)])\n",
    "print(f\"Observed kappa (tiering): {obs_kappa:.3f}\")\n",
    "print(f\"Bootstrap 95% CI (tiering): {lower:.3f} - {upper:.3f}\")\n",
    "\n",
    "# compute sensitivity and specificity for each tier\n",
    "def compute_tiering_sensitivity_specificity(ground_truth, llm_predictions):\n",
    "    tiers = [\"i\", \"ii\", \"iii\"]\n",
    "    sensitivity = {}\n",
    "    specificity = {}\n",
    "    \n",
    "    for tier in tiers:\n",
    "        tp = tn = fp = fn = 0\n",
    "        for gt, pred in zip(ground_truth, llm_predictions):\n",
    "            if gt == tier and pred == tier:\n",
    "                tp += 1\n",
    "            elif gt == tier and pred != tier:\n",
    "                fn += 1\n",
    "            elif gt != tier and pred != tier:\n",
    "                tn += 1\n",
    "            elif gt != tier and pred == tier:\n",
    "                fp += 1\n",
    "        \n",
    "        sensitivity[tier] = tp / (tp + fn) if (tp + fn) else 0\n",
    "        specificity[tier] = tn / (tn + fp) if (tn + fp) else 0\n",
    "    \n",
    "    return sensitivity, specificity\n",
    "\n",
    "sensitivity, specificity = compute_tiering_sensitivity_specificity(ground_truth_tierings, llm_tierings)\n",
    "n_iters = 50000\n",
    "sensitivities = {tier: [] for tier in sensitivity.keys()}\n",
    "specificities = {tier: [] for tier in specificity.keys()}\n",
    "for _ in range(n_iters):\n",
    "    # sample with replacement by index\n",
    "    samp = [pairs[i] for i in random.choices(range(n), k=n)]\n",
    "    h_samp, l_samp = zip(*samp)\n",
    "    sens, spec = compute_tiering_sensitivity_specificity(h_samp, l_samp)\n",
    "    \n",
    "    for tier in sensitivity.keys():\n",
    "        sensitivities[tier].append(sens[tier])\n",
    "        specificities[tier].append(spec[tier])\n",
    "\n",
    "# 95% percentile CI\n",
    "alpha = 0.05\n",
    "ci_sensitivities = {tier: np.percentile(sensitivities[tier], [100*alpha/2, 100*(1-alpha/2)]) for tier in sensitivity.keys()}\n",
    "ci_specificities = {tier: np.percentile(specificities[tier], [100*alpha/2, 100*(1-alpha/2)]) for tier in specificity.keys()}\n",
    "print(\"\\nSensitivity and specificity for each tier:\")\n",
    "for tier in sensitivity.keys():\n",
    "    print(f\"Tier: {tier}\")\n",
    "    print(f\"  Sensitivity: {sensitivity[tier]:.3f} (95% CI: {ci_sensitivities[tier][0]:.3f} - {ci_sensitivities[tier][1]:.3f})\")\n",
    "    print(f\"  Specificity: {specificity[tier]:.3f} (95% CI: {ci_specificities[tier][0]:.3f} - {ci_specificities[tier][1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47947721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of disagreements between human consensus and llm consensus\n",
    "disagreements = sum(1 for h, l in pairs if h != l)\n",
    "print(f\"Number of disagreements between human consensus and LLM consensus (tiering): {disagreements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- macro (un-weighted) averages -----\n",
    "macro_sensitivity = np.mean(list(sensitivity.values()))\n",
    "macro_specificity = np.mean(list(specificity.values()))\n",
    "\n",
    "print(f\"\\nMacro-averaged sensitivity:  {macro_sensitivity:.3f}\")\n",
    "print(f\"Macro-averaged specificity:  {macro_specificity:.3f}\")\n",
    "\n",
    "\n",
    "# ----- bootstrap macro values -----\n",
    "boot_macro_sens = []\n",
    "boot_macro_spec = []\n",
    "\n",
    "for i in range(n_iters):\n",
    "    boot_macro_sens.append(\n",
    "        np.mean([sensitivities[t][i] for t in sensitivity.keys()])\n",
    "    )\n",
    "    boot_macro_spec.append(\n",
    "        np.mean([specificities[t][i] for t in specificity.keys()])\n",
    "    )\n",
    "\n",
    "# 95 % percentile CIs\n",
    "alpha = 0.05\n",
    "ci_macro_sens = np.percentile(boot_macro_sens,\n",
    "                              [100*alpha/2, 100*(1-alpha/2)])\n",
    "ci_macro_spec = np.percentile(boot_macro_spec,\n",
    "                              [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "print(f\"Macro-sens (95% CI): {macro_sensitivity:.3f} \"\n",
    "      f\"({ci_macro_sens[0]:.3f}-{ci_macro_sens[1]:.3f})\")\n",
    "print(f\"Macro-spec (95% CI): {macro_specificity:.3f} \"\n",
    "      f\"({ci_macro_spec[0]:.3f}-{ci_macro_spec[1]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of erroneous studies that were only one tier off\n",
    "tier_ordering = [\"s\", \"i\", \"ii\", \"iii\"]\n",
    "\n",
    "one_too_high = 0\n",
    "one_too_low = 0\n",
    "off_by_one = 0\n",
    "total_errors = 0\n",
    "\n",
    "for i in range(0, len(llm_tierings)):\n",
    "    if llm_tierings[i] == ground_truth_tierings[i]:\n",
    "        continue\n",
    "\n",
    "    total_errors += 1\n",
    "    llm_tier_index = tier_ordering.index(llm_tierings[i])\n",
    "    gt_tier_index = tier_ordering.index(ground_truth_tierings[i])\n",
    "\n",
    "    if abs(llm_tier_index - gt_tier_index) == 1:\n",
    "        off_by_one += 1\n",
    "\n",
    "        if llm_tier_index > gt_tier_index:\n",
    "            one_too_high += 1\n",
    "        else:\n",
    "            one_too_low += 1\n",
    "\n",
    "print(f\"Total errors: {total_errors}\")\n",
    "print(f\"Off by one tier: {off_by_one}\")\n",
    "print(f\"Percentage off by one tier: {off_by_one / total_errors * 100:.2f}%\")\n",
    "print(f\"One tier too high: {one_too_high}\")\n",
    "print(f\"One tier too low: {one_too_low}\")\n",
    "print(f\"Percentage one tier too high: {one_too_high / off_by_one * 100:.2f}%\")\n",
    "print(f\"Percentage one tier too low: {one_too_low / off_by_one * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989eaff2",
   "metadata": {},
   "source": [
    "## Posterior Estimation of True Tier Prevalence\n",
    "We want to estimate the true tier prevalences. This is a bit tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd91388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pymc as pm, pytensor.tensor as pt, arviz as az\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "with open(\"included_studies_with_tiers-GPT-5r-high.jsonl\", \"r\") as f:\n",
    "    included_studies = [json.loads(line) for line in f]\n",
    "\n",
    "# get number of studies in each tier\n",
    "tier_counts = {'i': 0, 'ii': 0, 'iii': 0}\n",
    "for item in included_studies:\n",
    "    tier = item[\"LLM-tier\"][\"Tier\"].lower()\n",
    "    if tier in tier_counts:\n",
    "        tier_counts[tier] += 1\n",
    "    if tier == \"s\":\n",
    "        tier_counts['i'] += 1  # treat 's' as 'i' for the model\n",
    "\n",
    "# confusion matrix for the audit\n",
    "cm = confusion_matrix(ground_truth_tierings, llm_tierings, labels=['i', 'ii', 'iii'])\n",
    "\n",
    "# INPUTS\n",
    "N_tot  = sum(tier_counts.values()) # total number of studies that were screened\n",
    "T_vec  = np.array([tier_counts['i'], tier_counts['ii'], tier_counts['iii']]) # observed counts of each tier in vector form\n",
    "M_audit = cm # matrix of observed counts from both human and LLM audits\n",
    "\n",
    "# dirichlet priors\n",
    "alpha = np.ones(3) # this is our prior for *each row* of the confusion matrix\n",
    "beta  = np.ones(3) # this is our prior for the prevalence of each tier, we assume uniformity for simplicity, but this doesn't affect results very much\n",
    "\n",
    "# MODEL\n",
    "with pm.Model() as model:\n",
    "    # prevalence\n",
    "    phi = pm.Dirichlet(\"phi\", a=beta)\n",
    "\n",
    "    # confusion rows\n",
    "    theta = pm.Dirichlet(\"theta\", a=alpha, shape=(3,3))\n",
    "\n",
    "    # audit likelihood (three independent rows)\n",
    "    for i in range(3):\n",
    "        pm.Multinomial(f\"audit_row_{i}\",\n",
    "                       n=M_audit[i].sum(),\n",
    "                       p=theta[i],\n",
    "                       observed=M_audit[i])\n",
    "\n",
    "    # big sweep likelihood\n",
    "    pi = pm.Deterministic(\"pi\", pt.dot(phi, theta))\n",
    "    pm.Multinomial(\"big_sweep\",\n",
    "                   n=N_tot,\n",
    "                   p=pi,\n",
    "                   observed=T_vec)\n",
    "\n",
    "    # derived true-tier counts\n",
    "    N_true = pm.Deterministic(\"N_true\", phi * N_tot)\n",
    "\n",
    "    idata = pm.sample(4000, tune=2000,\n",
    "                      chains=4, target_accept=0.9, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix in greyscale\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=['i', 'ii', 'iii'], columns=['i', 'ii', 'iii'])\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Greys',\n",
    "            xticklabels=['I', 'II', 'III'], yticklabels=['I', 'II', 'III'],\n",
    "            cbar=False, linewidths=0.5, linecolor='black', annot_kws={\"size\": 16})\n",
    "plt.xlabel('LLM Predicted Tier')\n",
    "plt.ylabel('Human Labeled Tier')\n",
    "plt.savefig('confusion_matrix.svg', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df28736",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata, var_names=[\"phi\", \"theta\", \"N_true\"], hdi_prob=0.95)\n",
    "az.plot_trace(idata, var_names=[\"phi\", \"N_true\"])\n",
    "\n",
    "# idata is the object returned by pm.sample()\n",
    "az.summary(\n",
    "    idata,\n",
    "    var_names=[\"N_true\"],   # the derived vector φ × N_tot we defined\n",
    "    hdi_prob=0.95,          # highest-density interval\n",
    "    round_to=0              # no decimals for counts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf634c82",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "Here, we create a batch request to extract various data fields of interest. Note that not all fields were used in the final analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_studies = []\n",
    "with open(\"included_studies_with_tiers-GPT-5r-high.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        included_studies.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extraction_prompt = \"\"\"\n",
    "Given a study's title and abstract, extract the following information according to this structured JSON format. Return only the JSON object, with no additional text or formatting.\n",
    "\n",
    "{\n",
    "  \"models_used\": [\"\", ...], # (e.g. \"GPT-3.5\", \"GPT-4\", \"Gemini Pro\", etc.)\n",
    "  \"specialty\": [\"\", ...], # (e.g. \"emergency medicine\", \"radiology\", \"oncology\", etc.)\n",
    "  \"subspecialty\": [], # (if applicable, can also list things like \"medical education\" here)\n",
    "  \"types_of_human_evaluators\": [\"\", ...], # (e.g. \"medical students\", \"residents\", \"fellows\", \"attendings\", etc.)\n",
    "  \"quantitative?\": \"\", # (yes, no, unsure) # if both qualitative and quantitative, use \"yes\"\n",
    "  \"sample_size\": \"\",\n",
    "  \"task_type\": [\"\", ...], # give a very brief description of the task type, e.g. \"diagnosis\", \"triage\", \"patient question answering\", etc.\n",
    "  \"geographical_region\": \"\", # (e.g. \"North America\", \"Europe\", \"Asia\", or \"unsure\")\n",
    "  \"evaluation_type(s)\": [\"\", ...], # (capability, etc.)\n",
    "  \"evaluation_metric(s)\": [\"\", ...], # (subjective? likert? etc.)?\n",
    "  \"datasets_used\": [\"\", ...], # (if applicable, e.g. \"MedQA\", \"MIMIC-III\", etc.)\n",
    "  \"did_the_llm_outperform_the_human?\": {\"answer\": \"\", \"details\": \"\"}, # (yes, no, unsure)\n",
    "  \"extremely_brief_summary_of_results\": \"\",\n",
    "  \"types_of_data_sources\": [\"\", ...], # (e.g. \"clinical notes\", \"online vignettes\", \"patient questions\", etc.)\n",
    "}\n",
    "\"\"\".strip()\n",
    "\n",
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"high\"}, \n",
    "                      \"instructions\": data_extraction_prompt,\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "batch_request = []\n",
    "\n",
    "for idx, study in enumerate(included_studies):\n",
    "    query = f\"Title: {study['Title']}\\nAbstract: {study['Abstract']}\"\n",
    "    request = copy.deepcopy(template)\n",
    "    request[\"custom_id\"] = str(idx)\n",
    "    request[\"body\"][\"input\"] = query\n",
    "    batch_request.append(request)\n",
    "\n",
    "with open(\"data-extraction_batch_requests.jsonl\", \"w\") as f:\n",
    "    for item in batch_request:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(\"data-extraction_batch_requests.jsonl\", \"w\") as f:\n",
    "    for item in batch_request:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "with open(\"Batch Responses/GPT-5r-high-data-extraction-output.jsonl\", \"r\") as f:\n",
    "    responses = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ecfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(responses):\n",
    "    try:\n",
    "        extracted_data = json.loads(item[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"].replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "    except:\n",
    "        print(item[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"].replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "    included_studies[i][\"extracted_data\"] = extracted_data\n",
    "\n",
    "# get a set of each field:\n",
    "extracted_fields = {}\n",
    "for study in included_studies:\n",
    "    for key in study[\"extracted_data\"].keys():\n",
    "        if key not in extracted_fields:\n",
    "            extracted_fields[key] = set()\n",
    "        if isinstance(study[\"extracted_data\"][key], list):\n",
    "            for item in study[\"extracted_data\"][key]:\n",
    "                extracted_fields[key].add(item)\n",
    "        elif isinstance(study[\"extracted_data\"][key], dict):\n",
    "            continue\n",
    "        else:\n",
    "            extracted_fields[key].add(study[\"extracted_data\"][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"included_studies_with_extracted_data-GPT-5r-high.jsonl\", \"w\") as f:\n",
    "    for item in included_studies:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161e31d",
   "metadata": {},
   "source": [
    "# Parsing Extracted Data\n",
    "This is where we organize the data the model extracted (This will be a long section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a72929",
   "metadata": {},
   "outputs": [],
   "source": [
    "specialty_categories = {\n",
    "  # ————— Medicine & Core Boards —————\n",
    "  \"Allergy and Immunology\": {\n",
    "    \"Allergy and Immunology\": []\n",
    "  },\n",
    "\n",
    "  \"Anesthesiology\": {\n",
    "    \"Adult Cardiac Anesthesiology\": [],\n",
    "    \"Critical Care Medicine\": [],\n",
    "    \"Health Care Administration, Leadership, and Management\": [],\n",
    "    \"Hospice and Palliative Medicine\": [],\n",
    "    \"Neurocritical Care\": [],\n",
    "    \"Pain Medicine\": [],\n",
    "    \"Pediatric Anesthesiology\": [],\n",
    "    \"Obstetric Anesthesiology\": [],\n",
    "    \"Pediatric Cardiac Anesthesiology\": [],\n",
    "    \"Regional Anesthesiology and Acute Pain Medicine\": [],\n",
    "    \"Sleep Medicine\": []\n",
    "  },\n",
    "\n",
    "  \"Dermatology\": {\n",
    "    \"Dermatopathology\": [],\n",
    "    \"Micrographic Dermatologic Surgery\": [],\n",
    "    \"Pediatric Dermatology\": [],\n",
    "    \"Cosmetic Dermatologic Surgery\": []\n",
    "  },\n",
    "\n",
    "  \"Emergency Medicine\": {\n",
    "    \"Anesthesiology Critical Care Medicine\": [],\n",
    "    \"Emergency Medical Services\": [],\n",
    "    \"Health Care Administration, Leadership, and Management\": [],\n",
    "    \"Hospice and Palliative Medicine\": [],\n",
    "    \"Internal Medicine-Critical Care Medicine\": [],\n",
    "    \"Medical Toxicology\": [],\n",
    "    \"Neurocritical Care\": [],\n",
    "    \"Pain Medicine\": [],\n",
    "    \"Pediatric Emergency Medicine\": [],\n",
    "    \"Sports Medicine\": [],\n",
    "    \"Undersea and Hyperbaric Medicine\": []\n",
    "  },\n",
    "\n",
    "  \"Family Medicine\": {\n",
    "    \"Adolescent Medicine\": [],\n",
    "    \"Geriatric Medicine\": [],\n",
    "    \"Health Care Administration, Leadership, and Management\": [],\n",
    "    \"Hospice and Palliative Medicine\": [],\n",
    "    \"Pain Medicine\": [],\n",
    "    \"Sleep Medicine\": [],\n",
    "    \"Sports Medicine\": [],\n",
    "    \"Clinical Informatics\": []\n",
    "  },\n",
    "\n",
    "  \"Internal Medicine\": {\n",
    "    \"Adolescent Medicine\": [],\n",
    "    \"Adult Congenital Heart Disease\": [],\n",
    "    \"Advanced Heart Failure and Transplant Cardiology\": [],\n",
    "    \"Cardiovascular Disease\": [],\n",
    "    \"Clinical Cardiac Electrophysiology\": [],\n",
    "    \"Critical Care Medicine\": [],\n",
    "    \"Endocrinology, Diabetes and Metabolism\": [],\n",
    "    \"Gastroenterology\": [],\n",
    "    \"Geriatric Medicine\": [],\n",
    "    \"Hematology\": [],\n",
    "    \"Hospice and Palliative Medicine\": [],\n",
    "    \"Infectious Disease\": [],\n",
    "    \"Interventional Cardiology\": [],\n",
    "    \"Medical Oncology\": [],\n",
    "    \"Nephrology\": [],\n",
    "    \"Neurocritical Care\": [],\n",
    "    \"Pulmonary Disease\": [\n",
    "      \"Pulmonary Disease and Critical Care Medicine\",\n",
    "      \"Interventional Pulmonology\"\n",
    "    ],\n",
    "    \"Rheumatology\": [],\n",
    "    \"Sleep Medicine\": [],\n",
    "    \"Sports Medicine\": [],\n",
    "    \"Transplant Hepatology\": []\n",
    "  },\n",
    "\n",
    "  \"Medical Genetics and Genomics\": {\n",
    "    \"Clinical Genetics and Genomics\": [\n",
    "      \"Molecular Genetic Pathology\"\n",
    "    ],\n",
    "    \"Clinical Biochemical Genetics\": [],\n",
    "    \"Laboratory Genetics and Genomics\": [],\n",
    "    \"Medical Biochemical Genetics\": []\n",
    "  },\n",
    "\n",
    "  \"Neurology\": {\n",
    "    \"Neurology (Adult)\": [\n",
    "      \"Clinical Neurophysiology\",\n",
    "      \"Epilepsy\",\n",
    "      \"Neuromuscular Medicine\",\n",
    "      \"Sleep Medicine\",\n",
    "      \"Pain Medicine\",\n",
    "      \"Vascular Neurology\",\n",
    "      \"Brain Injury Medicine\",\n",
    "      \"Neurocritical Care\",\n",
    "      \"Neuroimmunology and Multiple Sclerosis\",\n",
    "      \"Movement Disorders\"\n",
    "    ],\n",
    "    \"Child Neurology\": [],\n",
    "    \"Neurodevelopmental Disabilities\": []\n",
    "  },\n",
    "\n",
    "  \"Psychiatry\": {\n",
    "    \"Addiction Psychiatry\": [],\n",
    "    \"Child and Adolescent Psychiatry\": [],\n",
    "    \"Consultation-Liaison Psychiatry\": [],\n",
    "    \"Forensic Psychiatry\": [],\n",
    "    \"Geriatric Psychiatry\": [],\n",
    "    \"Brain Injury Medicine\": [],\n",
    "    \"Sleep Medicine\": [],\n",
    "    \"Pain Medicine\": []\n",
    "  },\n",
    "\n",
    "  \"Preventive Medicine\": {\n",
    "    \"Aerospace Medicine\": [],\n",
    "    \"Occupational and Environmental Medicine\": [],\n",
    "    \"Public Health and General Preventive Medicine\": [],\n",
    "    \"Addiction Medicine\": [],\n",
    "    \"Clinical Informatics\": [],\n",
    "    \"Health Care Administration, Leadership, and Management\": [],\n",
    "    \"Medical Toxicology\": [],\n",
    "    \"Undersea and Hyperbaric Medicine\": [],\n",
    "    \"Lifestyle Medicine\": []\n",
    "  },\n",
    "\n",
    "  \"Pediatrics\": {\n",
    "    \"Pediatrics (General)\": [],\n",
    "    \"Adolescent Medicine\": [],\n",
    "    \"Child Abuse Pediatrics\": [],\n",
    "    \"Developmental-Behavioral Pediatrics\": [],\n",
    "    \"Hospice and Palliative Medicine\": [],\n",
    "    \"Medical Toxicology\": [],\n",
    "    \"Neonatal-Perinatal Medicine\": [],\n",
    "    \"Pediatric Cardiology\": [],\n",
    "    \"Pediatric Critical Care Medicine\": [],\n",
    "    \"Pediatric Emergency Medicine\": [],\n",
    "    \"Pediatric Endocrinology\": [],\n",
    "    \"Pediatric Gastroenterology\": [],\n",
    "    \"Pediatric Hematology-Oncology\": [],\n",
    "    \"Pediatric Hospital Medicine\": [],\n",
    "    \"Pediatric Infectious Diseases\": [],\n",
    "    \"Pediatric Nephrology\": [],\n",
    "    \"Pediatric Pulmonology\": [],\n",
    "    \"Pediatric Rheumatology\": [],\n",
    "    \"Pediatric Transplant Hepatology\": [],\n",
    "    \"Sleep Medicine\": [],\n",
    "    \"Sports Medicine\": []\n",
    "  },\n",
    "\n",
    "  # ————— Surgery & Procedure-based Boards —————\n",
    "  \"Surgery (American Board of Surgery)\": {\n",
    "    \"General Surgery\": [\n",
    "      \"Complex General Surgical Oncology\",\n",
    "      \"Pediatric Surgery\",\n",
    "      \"Surgery of the Hand\",\n",
    "      \"Surgical Critical Care\"\n",
    "    ],\n",
    "    \"Vascular Surgery\": [\n",
    "      \"Integrated Vascular Surgery\"\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"Thoracic Surgery\": {\n",
    "    \"Thoracic and Cardiac Surgery\": [\n",
    "      \"Integrated Thoracic Surgery\"\n",
    "    ],\n",
    "    \"Congenital Cardiac Surgery\": []\n",
    "  },\n",
    "\n",
    "  \"Colon and Rectal Surgery\": {\n",
    "    \"Colon and Rectal Surgery\": []\n",
    "  },\n",
    "\n",
    "  \"Orthopaedic Surgery\": {\n",
    "    \"Orthopaedic Sports Medicine\": [],\n",
    "    \"Surgery of the Hand\": [],\n",
    "    \"Adult Reconstructive Orthopaedic Surgery\": [],\n",
    "    \"Foot and Ankle Orthopaedic Surgery\": [],\n",
    "    \"Orthopaedic Surgery of the Spine\": [],\n",
    "    \"Orthopaedic Trauma\": [],\n",
    "    \"Musculoskeletal Oncology\": []\n",
    "  },\n",
    "\n",
    "  \"Plastic Surgery\": {\n",
    "    \"Plastic Surgery\": [\n",
    "      \"Craniofacial Surgery\",\n",
    "      \"Microsurgery\"\n",
    "    ],\n",
    "    \"Plastic Surgery within the Head and Neck\": [],\n",
    "    \"Surgery of the Hand\": []\n",
    "  },\n",
    "\n",
    "  \"Urology\": {\n",
    "    \"Urology\": [],\n",
    "    \"Pediatric Urology\": [],\n",
    "    \"Female Pelvic Medicine and Reconstructive Surgery\": []\n",
    "  },\n",
    "\n",
    "  \"Obstetrics and Gynecology\": {\n",
    "    \"Obstetrics and Gynecology\": [],\n",
    "    \"Complex Family Planning\": [],\n",
    "    \"Critical Care Medicine\": [],\n",
    "    \"Gynecologic Oncology\": [],\n",
    "    \"Maternal-Fetal Medicine\": [],\n",
    "    \"Reproductive Endocrinology and Infertility\": [],\n",
    "    \"Urogynecology and Reconstructive Pelvic Surgery\": []\n",
    "  },\n",
    "\n",
    "  \"Otolaryngology - Head and Neck Surgery\": {\n",
    "    \"Complex Pediatric Otolaryngology\": [],\n",
    "    \"Neurotology\": [],\n",
    "    \"Plastic Surgery within the Head and Neck\": [],\n",
    "    \"Sleep Medicine\": [],\n",
    "    \"Facial Plastic Surgery\": [],\n",
    "    \"Rhinology/Nasal and Sinus Care\": [],\n",
    "    \"Laryngology and Voice\": []\n",
    "  },\n",
    "\n",
    "  \"Ophthalmology\": {\n",
    "    \"Neuro-Ophthalmology\": [],\n",
    "    \"Oculofacial Plastic/Ophthalmic Plastic and Reconstructive Surgery\": [],\n",
    "    \"Ophthalmic Pathology\": [],\n",
    "    \"Pediatric Ophthalmology\": [],\n",
    "    \"Uveitis and Ocular Immunology\": []\n",
    "  },\n",
    "\n",
    "  # ————— Imaging, Pathology & Related —————\n",
    "  \"Radiology\": {\n",
    "    \"Diagnostic Radiology\": [\n",
    "      \"Abdominal Imaging Radiology\",\n",
    "      \"Cardiothoracic Radiology\",\n",
    "      \"Musculoskeletal Imaging Radiology\",\n",
    "      \"Neuroradiology\",\n",
    "      \"Nuclear Radiology\",\n",
    "      \"Pediatric Radiology\",\n",
    "      \"Pain Medicine\"\n",
    "    ],\n",
    "    \"Interventional Radiology and Diagnostic Radiology\": [\n",
    "      \"Interventional Radiology (Integrated)\"\n",
    "    ],\n",
    "    \"Radiation Oncology\": [],\n",
    "    \"Medical Physics\": [\n",
    "      \"Diagnostic Medical Physics\",\n",
    "      \"Nuclear Medical Physics\",\n",
    "      \"Therapeutic Medical Physics\"\n",
    "    ],\n",
    "    \"Endovascular Surgical Neuroradiology\": []\n",
    "  },\n",
    "\n",
    "  \"Nuclear Medicine\": {\n",
    "    \"Nuclear Medicine\": []\n",
    "  },\n",
    "\n",
    "  \"Pathology\": {\n",
    "    \"Pathology - Anatomic/Clinical (AP/CP)\": [],\n",
    "    \"Pathology - Anatomic\": [],\n",
    "    \"Pathology - Clinical\": [],\n",
    "    \"Blood Banking/Transfusion Medicine\": [],\n",
    "    \"Clinical Informatics\": [],\n",
    "    \"Cytopathology\": [],\n",
    "    \"Dermatopathology\": [],\n",
    "    \"Hematopathology\": [],\n",
    "    \"Neuropathology\": [],\n",
    "    \"Pathology - Chemical\": [],\n",
    "    \"Pathology - Forensic\": [],\n",
    "    \"Pathology - Medical Microbiology\": [],\n",
    "    \"Pathology - Molecular Genetic\": [],\n",
    "    \"Pathology - Pediatric\": [],\n",
    "    \"Selective Pathology\": [],\n",
    "    \"Medical Microbiology Pathology\": []\n",
    "  },\n",
    "\n",
    "  # ————— PM&R and Overlapping Procedural/Functional Areas —————\n",
    "  \"Physical Medicine and Rehabilitation\": {\n",
    "    \"Brain Injury Medicine\": [],\n",
    "    \"Neuromuscular Medicine\": [],\n",
    "    \"Pain Medicine\": [],\n",
    "    \"Pediatric Rehabilitation Medicine\": [],\n",
    "    \"Spinal Cord Injury Medicine\": [],\n",
    "    \"Sports Medicine\": []\n",
    "  },\n",
    "\n",
    "  # ————— Additional Boards / Categories —————\n",
    "  \"Neurological Surgery\": {\n",
    "    \"Neurological Surgery\": [],\n",
    "    \"Neurocritical Care\": [],\n",
    "    \"Endovascular Surgical Neuroradiology\": []\n",
    "  },\n",
    "\n",
    "  \"Psychiatry and Neurology (Shared/Multi-board subs)\": {\n",
    "    \"Multi-board Subspecialties\": [\n",
    "      \"Brain Injury Medicine\",\n",
    "      \"Clinical Informatics\",\n",
    "      \"Neurocritical Care\",\n",
    "      \"Pain Medicine\",\n",
    "      \"Sleep Medicine\"\n",
    "    ]\n",
    "  },\n",
    "  \n",
    "  \"Osteopathic Neuromusculoskeletal Medicine\": {\n",
    "    \"Osteopathic Neuromusculoskeletal Medicine\": []\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "task_categories = ['Knowledge Retrieval & Clinical QA',\n",
    " 'Diagnostic Reasoning & Disease Detection',\n",
    " 'Prognosis & Risk Stratification',\n",
    " 'Therapeutic Decision Support & Treatment Planning',\n",
    " 'Clinical Management & Workflow Guidance',\n",
    " 'Patient-Facing Communication & Education',\n",
    " 'Psychological & Mental Health Support',\n",
    " 'Medical Imaging & Signal Interpretation',\n",
    " 'Clinical Documentation, Reporting & Summarization',\n",
    " 'Information Extraction, Coding & Classification',\n",
    " 'Education, Assessment & Simulation',\n",
    " 'Translation & Simplification',\n",
    " 'Administrative & Operational Support',\n",
    " 'Ethical, Safety & Quality Oversight',\n",
    " 'Other']\n",
    "\n",
    "evaluation_categories = ['Accuracy & Core Capability',\n",
    " 'Factuality and Groundedness',\n",
    " 'Calibration & Uncertainty Quantification',\n",
    " 'Alignment & Guideline Concordance',\n",
    " 'Safety & Risk/Harm Assessment',\n",
    " 'Reliability and Robustness',\n",
    " 'Generalizability & External Validity',\n",
    " 'Bias & Fairness',\n",
    " 'Human Expert Evaluation & Comparison',\n",
    " 'Accessibility & Language Coverage',\n",
    " 'Explainability & Transparency',\n",
    " 'Patient/End-User Outcomes & Satisfaction',\n",
    " 'User Experience, Usability & Trust',\n",
    " 'Education & Training Impact',\n",
    " 'Efficiency & Workflow Impact',\n",
    " 'Implementation & Feasibility',\n",
    " 'Regulatory & Ethical Compliance',\n",
    " 'Benchmarking & Head-to-Head Evaluations',\n",
    " 'Content Quality & Readability',\n",
    " 'Other']\n",
    "\n",
    "\n",
    "metric_categories = ['Task-level Accuracy & Performance',\n",
    " 'Safety & Harm Assessment',\n",
    " 'Clinical Appropriateness & Guideline Concordance',\n",
    " 'Reliability, Consistency & Reproducibility',\n",
    " 'Inter-Rater Agreement & Concordance',\n",
    " 'Statistical Significance & Effect Size',\n",
    " 'Diagnostic/Screening Operating Characteristics',\n",
    " 'Readability & Linguistic Complexity',\n",
    " 'Empathy, Bedside Manner & Emotional Support',\n",
    " 'Comprehensiveness & Coverage',\n",
    " 'Clarity, Readability & Communication Quality',\n",
    " 'User/Clinician Preference & Acceptability',\n",
    " 'Usability & Workflow Efficiency',\n",
    " 'Calibration & Confidence',\n",
    " 'Faithfulness & Fact-Checking',\n",
    " 'Bias & Fairness',\n",
    " 'Similarity & Semantic Overlap',\n",
    " 'Patient Education Readiness',\n",
    " 'Cognitive/Knowledge Outcomes',\n",
    " 'Resource Utilization & Cost Efficiency',\n",
    " 'Other']\n",
    "\n",
    "dataset_categories = [\n",
    " 'Patient-Facing Q&A & FAQs',\n",
    " 'Clinician Board & Self-Assessment Questions',\n",
    " 'Clinical Vignettes & Case Reports',\n",
    " 'Real-World Electronic Health Records',\n",
    " 'Synthetic / Simulated Clinical Data',\n",
    " 'Dialogue & Chat Corpora',\n",
    " 'Imaging Data & Reports',\n",
    " 'Physiological Signals & Wearables',\n",
    " 'Laboratory & Pathology Data',\n",
    " 'Genomics & -Omics',\n",
    " 'Pharmacology & Medication Knowledge',\n",
    " 'Clinical Guidelines & Consensus Statements',\n",
    " 'Knowledge Graphs / Ontologies / Terminologies',\n",
    " 'Survey Instruments & Psychometric Scales',\n",
    " 'Educational & Reference Texts',\n",
    " 'Imaging Challenges & Benchmarks',\n",
    " 'Research Literature & Abstract Corpora',\n",
    " 'Clinical Trial Eligibility & Structured Cohorts',\n",
    " 'Social-Media & Search-Query Data',\n",
    " 'Regulatory & Administrative Documents',\n",
    "\n",
    " 'Unstructured Clinical Notes',\n",
    " 'Structured EHR Tables & Event Logs',\n",
    " 'Operative, Procedure & Anesthesia Reports',\n",
    " 'Emergency/Prehospital & Triage Notes',\n",
    " 'Patient Portal & Secure Messages',\n",
    " 'Telemedicine & Call-Center Transcripts',\n",
    " 'Audio Dictations & Speech Corpora',\n",
    " 'Digital Pathology & Whole-Slide Images',\n",
    " 'ICU/Bedside Monitoring Waveforms & Time-Series',\n",
    " 'Device & Equipment Telemetry',\n",
    " 'Claims, Billing & Utilization Data',\n",
    " 'Terminology & Code-Mapping Corpora (ICD/CPT/RxNorm mappings)',\n",
    "\n",
    " 'Other',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1f5ee",
   "metadata": {},
   "source": [
    "## Create the batch job to classify free-text fields into the above structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ff161",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_classes = [\n",
    "    # Physicians\n",
    "    \"Attending (Fellowship-trained)\",\n",
    "    \"Attending\",\n",
    "    \"Fellow\",\n",
    "    \"Resident\",\n",
    "    \"Intern\",\n",
    "    \"Sub-intern\",\n",
    "    \"Medical Student\",\n",
    "    \"Student (Undergraduate)\",\n",
    "    \"Student (Other)\",\n",
    "    \"MD (Unspecified)\",\n",
    "    \"DO (Unspecified)\",\n",
    "    \"Other Trainee\",\n",
    "\n",
    "    # Non-Physician Clinical Providers\n",
    "    \"Nurse\",\n",
    "    \"Advanced Practice Provider (NP/PA)\",\n",
    "    \"Pharmacist\",\n",
    "    \"Therapist / Rehabilitation / Mental Health\",\n",
    "    \"Technician / EMT\",\n",
    "    \"Other Clinical Provider\",\n",
    "    \"Non-Physician Clinician\",\n",
    "\n",
    "    # Researchers and Academics\n",
    "    \"Faculty / Educator\",\n",
    "    \"Reviewer / Panelist\",\n",
    "    \"Researcher / Scientist\",\n",
    "    \"Student (Post-doc)\"\n",
    "    \"Student (Graduate, not medical)\",\n",
    "    \"Other Academic\",\n",
    "\n",
    "    # Patients, Caregivers, and Public\n",
    "    \"Patient\",\n",
    "    \"Caregiver / Family Member\",\n",
    "    \"Lay Public / Participant\",\n",
    "    \"Other (Public)\",\n",
    "\n",
    "    # Healthcare Support and Administration\n",
    "    \"Language Services\",\n",
    "    \"Medical Coder\",\n",
    "    \"Program Administration\",\n",
    "    \"Information Specialist\",\n",
    "    \"Other Support / Admin\",\n",
    "\n",
    "    # Non-Healthcare Domain Experts\n",
    "    \"Non-healthcare Domain Expert\",\n",
    "\n",
    "    # last resort if unsure\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "\n",
    "specialty_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the study's medical specialties and subspecialties according to the following JSON format.\\n\\n\"\n",
    "    f\"{json.dumps(specialty_categories, indent=2)}\\n\\n\"\n",
    "    \"Output a list of lists representing the specialties and subspecialties. For example, if a study is in both Neuroradiology and Cardiology, the output should be:\\n\\n\"\n",
    "    '[[\"Radiology\", \"Diagnostic Radiology\", \"Neuroradiology\"], [\"Internal Medicine\", \"Cardiovascular Disease\"]]\\n\\n'\n",
    "    \"Notice that each specialty must traverse the hierarchy from top to bottom. If a study is only in a top-level specialty with no subspecialty, \"\n",
    "    \"It can be represented as a single-item list, e.g. [\\\"Dermatology\\\"]. If a study is not clearly in any specialty, return an empty list.\\n\\n\"\n",
    "    \n",
    "    \"Output only the JSON object, with no additional text or formatting.\"\n",
    ")\n",
    "\n",
    "task_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the tasks the study evaluates according to the following categories.\\n\\n\"\n",
    "    f\"{json.dumps(task_categories, indent=2)}\\n\\n\"\n",
    "    \"Output a list of the most relevant categories, listed by order of relevance (most relevant first). \"\n",
    "    \"The list should have the MINIMAL number of elements necessary to describe the task. Be selective and specific. \"\n",
    "    \"If none apply, return \\\"Other\\\".\\n\\n\"\n",
    "    \"Output only the JSON array, with no additional text or formatting.\"\n",
    ")\n",
    "\n",
    "evaluation_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the evaluation types the study uses according to the following categories.\\n\\n\"\n",
    "    f\"{json.dumps(evaluation_categories, indent=2)}\\n\\n\"\n",
    "    \"Output a list of the most relevant categories. If none apply, return an empty list.\\n\\n\"\n",
    "    \"Output only the JSON array, with no additional text or formatting.\"\n",
    ")\n",
    "\n",
    "metric_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the evaluation metrics the study uses according to the following categories.\\n\\n\"\n",
    "    f\"{json.dumps(metric_categories, indent=2)}\\n\\n\"\n",
    "    \"Output a list of the most relevant categories. If none apply, return an empty list.\\n\\n\"\n",
    "    \"Output only the JSON array, with no additional text or formatting.\"\n",
    ")\n",
    "\n",
    "dataset_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the datasets the study uses according to the following categories.\\n\\n\"\n",
    "    f\"{json.dumps(dataset_categories, indent=2)}\\n\\n\"\n",
    "    \"Output a list of the most relevant categories. If none apply, return an empty list. Additionally, classify whether the dataset is open-access, proprietary, or unsure. \"\n",
    "    \"Here is an example output:\\n\\n\"\n",
    "    '[{\"category\": \"Real-World Electronic Health Records\", \"access\": \"proprietary\", \"parsed_name_or_description\": \"MIMIC III\"}, '\n",
    "    '{\"category\": \"Clinical Vignettes & Case Reports\", \"access\": \"open-access\", \"parsed_name_or_description\": \"NEJM case of the month\"}...]\\n\\n'\n",
    "    \"\\nOutput only the JSON array, with no additional text or formatting. \"\n",
    "    \"The JSON should be structured as the example, with each unique dataset represented as an object with 'category', 'access', and 'parsed_name_or_description' fields. \"\n",
    "    \"Assign at MOST one category PER DATASET mentioned in the abstract (if multiple datasets are mentioned, categorize them all. \"\n",
    "    \"If no datasets are mentioned, return an empty list for categories).\"\n",
    ")\n",
    "\n",
    "model_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the LLM models the study uses according to the following classes.\\n\\n\"\n",
    "    '\"open-source\", \"proprietary\", \"fine-tuned\", \"custom\", \"unsure\"\\n\\n'\n",
    "    \"Output a list of the most relevant categories. If none apply, return an empty list.\\n\\n\"\n",
    "    \"Output only the JSON array, with no additional text or formatting.\"\n",
    ")\n",
    "\n",
    "evaluator_prompt = (\n",
    "    \"Given a study's title, abstract, and some already extracted data, classify the types of human evaluators the study uses according to the following classes.\\n\\n\"\n",
    "    f\"{json.dumps(expert_classes, indent=2)}\\n\\n\"\n",
    "    \"Output a list of the most relevant categories. Use the minimal number of categories (and most descriptive) to describe all experts in the study. If none apply, return an empty list.\\n\\n\"\n",
    "    \"Output only the JSON array, with no additional text or formatting.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f305126",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_studies = []\n",
    "with open(\"included_studies_with_extracted_data-GPT-5r-high.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        included_studies.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ff7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"minimal\"}, # this is mostly a classification task, so minimal effort should suffice\n",
    "                      \"instructions\": \"\",\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "\n",
    "out_jsonl = []\n",
    "\n",
    "for idx, row in tqdm(enumerate(included_studies), total=len(included_studies)):\n",
    "    #for prompt, prompt_name in zip([specialty_prompt, task_prompt, evaluation_prompt, metric_prompt, dataset_prompt, evaluator_prompt], [\"specialty\", \"task\", \"evaluation\", \"metric\", \"dataset\", \"evaluator\"]):\n",
    "    for prompt, prompt_name in zip([specialty_prompt, task_prompt, evaluation_prompt, metric_prompt, dataset_prompt], [\"specialty\", \"task\", \"evaluation\", \"metric\", \"dataset\"]):\n",
    "        out = copy.deepcopy(template)\n",
    "        out[\"custom_id\"] = str(idx) + \"-\" + prompt_name\n",
    "        out[\"body\"][\"instructions\"] = prompt\n",
    "        out[\"body\"][\"input\"] = f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\\nExtracted data: {json.dumps(row['extracted_data'], indent=2)}\"\n",
    "        out_jsonl.append(out)\n",
    "\n",
    "with open(\"extracted_data_parsing.jsonl\", \"w\") as f:\n",
    "    for entry in out_jsonl:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"minimal\"}, # this is mostly a classification task, so minimal effort should suffice\n",
    "                      \"instructions\": \"\",\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "\n",
    "out_jsonl = []\n",
    "\n",
    "for idx, row in tqdm(enumerate(included_studies), total=len(included_studies)):\n",
    "    for prompt, prompt_name in zip([task_prompt], [\"task_specific\"]):\n",
    "        out = copy.deepcopy(template)\n",
    "        out[\"custom_id\"] = str(idx) + \"-\" + prompt_name\n",
    "        out[\"body\"][\"instructions\"] = prompt\n",
    "        out[\"body\"][\"input\"] = f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\\nExtracted data: {json.dumps(row['extracted_data'], indent=2)}\"\n",
    "        out_jsonl.append(out)\n",
    "\n",
    "with open(\"extracted_data_parsing_tasks.jsonl\", \"w\") as f:\n",
    "    for entry in out_jsonl:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a35cff",
   "metadata": {},
   "source": [
    "## Grouping the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into model names list\n",
    "models = [m.strip() for m in extracted_fields[\"models_used\"] if m.strip()]\n",
    "\n",
    "# Define category structure\n",
    "categories = {\n",
    "    'A1_ClosedSource_GeneralLLM': [],\n",
    "    'A2_OpenSource_BigLLM': [],\n",
    "    'B1_TinyOrDistilled': [],\n",
    "    'B2_Midsize_OpenSource': [],\n",
    "    'C1_BiomedicalClinical': [],\n",
    "    'C2_OtherDomain': [],\n",
    "    'D1_FineTuned_ClinicalChat': [],\n",
    "    'D2_FineTuned_ResearchPilot': [],\n",
    "    'E1_RAG': [],\n",
    "    'E2_Agentic': [],\n",
    "    'F1_VisionLanguage': [],\n",
    "    'F2_AudioLanguage': [],\n",
    "    'F3_GeneralMLLM': [],\n",
    "    'F4_VisionDxBackbone': [],\n",
    "    'G_VisionOnly': [],\n",
    "    'H_Embeddings': [],\n",
    "    'I_Encoders': [],\n",
    "    'J1_TreeEnsemble': [],\n",
    "    'J2_LinearProb': [],\n",
    "    'J3_Ngram': [],\n",
    "    'K_SpeechAssistants': [],\n",
    "    'L_OtherNeuralHybrid': [],\n",
    "    'Uncategorized': []\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def has_any(name, keywords):\n",
    "    return any(k.lower() in name.lower() for k in keywords)\n",
    "\n",
    "def classify(name):\n",
    "    n = name.lower()\n",
    "    # J categories first for traditional ML\n",
    "    if has_any(n, ['random forest', 'xgboost', 'gradient boosting', 'lightgbm']):\n",
    "        return 'J1_TreeEnsemble'\n",
    "    if has_any(n, ['logistic regression', 'support vector machine', 'svm']):\n",
    "        return 'J2_LinearProb'\n",
    "    if has_any(n, ['n-gram', 'bag-of-words']):\n",
    "        return 'J3_Ngram'\n",
    "    \n",
    "    # Vision-only\n",
    "    if has_any(n, ['u-net', 'yolov', 'densenet', 'vgg', 'resnet', 'vision transformer', 'cnn', 'dcnn', 'segmentation model', 'yolo', 'transformer-based vision model']):\n",
    "        return 'G_VisionOnly'\n",
    "    \n",
    "    # Embeddings\n",
    "    if has_any(n, ['embedding', 'sbert', 'sentence-bert', 'openai embeddings', 'text-embedding']):\n",
    "        return 'H_Embeddings'\n",
    "    \n",
    "    # Encoders\n",
    "    if has_any(n, ['bert', 'roberta', 'deberta', 'longformer', 'albert', 'bart', 'electra', 'word2vec', 'fasttext', 'gpt-2']):\n",
    "        return 'I_Encoders'\n",
    "    \n",
    "    # Audio language\n",
    "    if has_any(n, ['whisper', 'audiopalm']):\n",
    "        return 'F2_AudioLanguage'\n",
    "    \n",
    "    # Multimodal - vision language\n",
    "    if has_any(n, ['gpt-4v', 'vision', 'blip', 'flamingo', 'llava', 'multimodal', 'imagebind', 'skin', 'm4cxr', 'cxlava', 'cxr', 'llmseg', 'mlmm']):\n",
    "        return 'F1_VisionLanguage'\n",
    "    \n",
    "    # General MLLM\n",
    "    if has_any(n, ['gpt-4o', 'grok', 'gemini pro vision', 'gemini pro-v', '4o1', 'mlmm']):\n",
    "        return 'F3_GeneralMLLM'\n",
    "    \n",
    "    # Vision backbone\n",
    "    if has_any(n, ['u-net +', '+ vgg', '+ inception', 'yolo', 'densenet', 'kera-cxr', 'boneview', 'medflamingo']):\n",
    "        return 'F4_VisionDxBackbone'\n",
    "    \n",
    "    # RAG\n",
    "    if has_any(n, ['rag', 'self-biorag', 'retrieval-augmented']):\n",
    "        return 'E1_RAG'\n",
    "    \n",
    "    # Agentic\n",
    "    if has_any(n, ['agent', 'multi-agent', 'agents']):\n",
    "        return 'E2_Agentic'\n",
    "    \n",
    "    # Biomedical\n",
    "    if has_any(n, ['med', 'clinical', 'bio', 'onc', 'derm', 'rad', 'gastro', 'surg', 'cardio', 'neuro', 'oph', 'peds', 'tcml', 'tcm', 'hpt', 'pharm', 'hiv', 'covid']):\n",
    "        return 'C1_BiomedicalClinical'\n",
    "    \n",
    "    # Other domain specialized\n",
    "    if has_any(n, ['legal', 'finance', 'code', 'chem', 'econ']):\n",
    "        return 'C2_OtherDomain'\n",
    "    \n",
    "    # Fine-tuned clinical chat\n",
    "    if has_any(n, ['gpt', 'chat', 'bot', 'assistant', 'ai', 'copilot']) and has_any(n, ['derm', 'skin', 'drug', 'doctor', 'clinic', 'health', 'rare', 'radiolog', 'surg', 'prostate', 'epilep', 'cardio', 'ecg', 'rad', 'onc', 'peds', 'eau', 'gastro', 'otolaryng']):\n",
    "        return 'D1_FineTuned_ClinicalChat'\n",
    "    \n",
    "    # Fine-tuned research pilot/general\n",
    "    if has_any(n, ['custom', 'fine-tuned', 'pilot', 'research', 'tool']):\n",
    "        return 'D2_FineTuned_ResearchPilot'\n",
    "    \n",
    "    # Closed source General LLM\n",
    "    closed_brands = ['gpt', 'chatgpt', 'claude', 'gemini', 'bard', 'grok', 'copilot', 'bing', 'pi', 'perplexity', 'youchat', 'alexa', 'google assistant', 'my ai']\n",
    "    if has_any(n, closed_brands):\n",
    "        return 'A1_ClosedSource_GeneralLLM'\n",
    "    \n",
    "    # Open-source Large\n",
    "    if has_any(n, ['llama', 'mixtral', 'falcon', 'mistral', 'qwen', 'deepseek', 'wizardlm', 'vicuna', 'gemma', 'yi', 'baichuan', 'bloom', 'reka', 'orca']):\n",
    "        # size\n",
    "        if has_any(n, ['7b', '8b', '9b', '3b', '4b', '5b', '6b']):\n",
    "            return 'B2_Midsize_OpenSource'\n",
    "        else:\n",
    "            return 'A2_OpenSource_BigLLM'\n",
    "    \n",
    "    # Tiny / distilled\n",
    "    if has_any(n, ['tiny', 'mini', 'distil', 'phi', 'orca_mini', 'o1-mini', 'bc-slm', 'slm', 'small']):\n",
    "        return 'B1_TinyOrDistilled'\n",
    "    \n",
    "    # Speech assistants\n",
    "    if has_any(n, ['alexa', 'google assistant', 'bing chatbot', 'bing ai', 'bing chat', 'google voice']):\n",
    "        return 'K_SpeechAssistants'\n",
    "    \n",
    "    # Other neural / hybrid\n",
    "    if has_any(n, ['lstm', 'cnn', 'clip', 'fft', 'transformer']):\n",
    "        return 'L_OtherNeuralHybrid'\n",
    "    \n",
    "    return 'Uncategorized'\n",
    "\n",
    "# Classify all models\n",
    "for m in models:\n",
    "    cat = classify(m)\n",
    "    categories[cat].append(m)\n",
    "\n",
    "# For brevity, remove empty categories\n",
    "categories = {k: v for k, v in categories.items() if v}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43145d81",
   "metadata": {},
   "source": [
    "## Model Brand Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Comprehensive brand patterns\n",
    "# - Put specific/versioned patterns *before* broad brand buckets to avoid\n",
    "#   false captures. Hyphens/underscores/slashes in model ids are common,\n",
    "#   so patterns are liberal to handle \"org/Model-3.1-8B-Instruct\" shapes.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "brand_patterns = [\n",
    "    # ─── OpenAI family (specific before generic) ────────────────────────────\n",
    "    # 4o / Omni / o-series (o1/o3) appear in many OpenAI model ids.\n",
    "    ('GPT-4o',          re.compile(r'\\b(?:gpt|chat[\\s\\-]?gpt)?[\\s\\-]?(?:4[o0]|4-?omni)\\b|\\b(?:o1|o3)(?:[\\-\\w]*)\\b', re.I)),\n",
    "    ('GPT-4.1',         re.compile(r'\\b(?:gpt|chat[\\s\\-]?gpt)[\\s\\-]?4\\.1(?:[\\s\\-]?(?:mini|nano))?\\b', re.I)),\n",
    "    ('ChatGPT-4',       re.compile(r'Chat[\\s\\-]?GPT.*\\b4(?:\\.0|v)?\\b', re.I)),\n",
    "    ('ChatGPT-3.5',     re.compile(r'Chat[\\s\\-]?GPT.*\\b3\\.5\\b', re.I)),\n",
    "    ('GPT-4',           re.compile(r'^\\s*GPT[\\s\\-]?4(?![a-zA-Z])', re.I)),\n",
    "    ('GPT-3.5',         re.compile(r'^\\s*GPT[\\s\\-]?3\\.5(?![a-zA-Z0-9])', re.I)),\n",
    "\n",
    "    # ─── Anthropic Claude (version-aware) ───────────────────────────────────\n",
    "    # Matches forms like:\n",
    "    #   claude-3-5-sonnet-20240620, claude-3-opus, claude-3-haiku, claude-2.1\n",
    "    ('Claude 3.5',      re.compile(r'\\bclaude[\\-\\w]*3(?:[.\\-]5)[\\-\\w]*\\b', re.I)),\n",
    "    ('Claude 3',        re.compile(r'\\bclaude[\\-\\w]*3[\\-\\w]*(?:opus|sonnet|haiku)?\\b', re.I)),\n",
    "    ('Claude 2',        re.compile(r'\\bclaude[\\-\\w]*2(?:\\.\\d+)?[\\-\\w]*(?:instant)?\\b', re.I)),\n",
    "    ('Claude (unspecified)', re.compile(r'\\bClaude\\b', re.I)),\n",
    "\n",
    "    # ─── Google Gemini (version-aware) ──────────────────────────────────────\n",
    "    # Matches:\n",
    "    #   gemini-1.5-pro|flash|ultra|vision, gemini-1.0-pro|ultra, gemini-nano\n",
    "    ('Gemini 1.5',      re.compile(r'\\bgemini[\\s\\-]?1\\.5[\\-\\w]*\\b', re.I)),\n",
    "    ('Gemini 1.0',      re.compile(r'\\bgemini[\\s\\-]?1\\.0[\\-\\w]*\\b', re.I)),\n",
    "    ('Gemini Nano',     re.compile(r'\\bgemini[\\s\\-]?nano\\b', re.I)),\n",
    "    ('Gemini',          re.compile(r'\\bGemini\\b', re.I)),\n",
    "\n",
    "    # ─── Meta LLaMA (version-aware + subfamilies) ───────────────────────────\n",
    "    # Matches:\n",
    "    #   meta-llama/Llama-3.1-8B-Instruct, Llama-3-70B, Llama 2, Code Llama, Llama Guard\n",
    "    ('Code Llama',      re.compile(r'\\bcode[\\s\\-]?llama\\b', re.I)),\n",
    "    ('Llama Guard',     re.compile(r'\\bllama[\\s\\-]?guard\\b', re.I)),\n",
    "    ('LLaMA 3.x',       re.compile(r'\\b(?:meta[\\s\\-]?)?llama[\\s\\-]?3(?:\\.\\d+)?\\b', re.I)),\n",
    "    ('LLaMA 2',         re.compile(r'\\b(?:meta[\\s\\-]?)?llama[\\s\\-]?2\\b', re.I)),\n",
    "    ('LLaMA',           re.compile(r'\\b(?:meta[\\s\\-]?)?llama\\b', re.I)),\n",
    "\n",
    "    # ─── Mistral AI ─────────────────────────────────────────────────────────\n",
    "    ('Mixtral',         re.compile(r'\\bmixtral\\b', re.I)),\n",
    "    ('Mistral',         re.compile(r'\\bmistral(?:ai)?\\b', re.I)),\n",
    "\n",
    "    # ─── Alibaba Qwen ───────────────────────────────────────────────────────\n",
    "    # Matches:\n",
    "    #   Qwen2.5-72B-Instruct, Qwen2-VL, Qwen1.5, Qwen-7B-Chat, CodeQwen\n",
    "    ('Qwen',            re.compile(r'\\bqwen(?:[\\-\\s]?(?:1\\.5|2(?:\\.5)?|vl|coder|audio|vision)?[\\w\\-]*)?\\b', re.I)),\n",
    "\n",
    "    # ─── Microsoft Phi ──────────────────────────────────────────────────────\n",
    "    # Matches: Phi-3, Phi-3.5, phi-3-mini-4k-instruct\n",
    "    ('Phi',             re.compile(r'\\bphi[\\s\\-]?\\d', re.I)),\n",
    "\n",
    "    # ─── Google Gemma ───────────────────────────────────────────────────────\n",
    "    # Matches: gemma-2b-it, gemma-7b, codegemma\n",
    "    ('CodeGemma',       re.compile(r'\\bcode[\\s\\-]?gemma\\b', re.I)),\n",
    "    ('Gemma',           re.compile(r'\\bgemma(?:[\\-\\s]?\\d)?\\b', re.I)),\n",
    "\n",
    "    # ─── Databricks ─────────────────────────────────────────────────────────\n",
    "    ('DBRX',            re.compile(r'\\bdbrx\\b', re.I)),\n",
    "\n",
    "    # ─── DeepSeek (incl. Coder) ─────────────────────────────────────────────\n",
    "    ('DeepSeek',        re.compile(r'\\bdeepseek(?:[\\-\\s]?(?:coder|v\\d(?:\\.\\d)?))?\\b', re.I)),\n",
    "\n",
    "    # ─── ZhipuAI GLM / ChatGLM ──────────────────────────────────────────────\n",
    "    ('GLM / ChatGLM',   re.compile(r'\\b(?:chat)?glm[\\w\\-\\.\\s]*\\b', re.I)),\n",
    "\n",
    "    # ─── 01.AI Yi ───────────────────────────────────────────────────────────\n",
    "    # Guard with digit to avoid \"yi\" false matches.\n",
    "    ('Yi',              re.compile(r'(?<![a-z])yi[\\-\\s]?\\d', re.I)),\n",
    "\n",
    "    # ─── Baichuan ───────────────────────────────────────────────────────────\n",
    "    ('Baichuan',        re.compile(r'\\bbaichuan\\b', re.I)),\n",
    "\n",
    "    # ─── xAI Grok ───────────────────────────────────────────────────────────\n",
    "    ('Grok',            re.compile(r'(?<![a-z])grok[\\-\\s]?\\d', re.I)),\n",
    "\n",
    "    # ─── LLaVA (VLM) ────────────────────────────────────────────────────────\n",
    "    ('LLaVA',           re.compile(r'\\bllava\\b', re.I)),\n",
    "\n",
    "    # ─── MosaicML / Databricks MPT ──────────────────────────────────────────\n",
    "    ('MPT',             re.compile(r'\\bmpt[\\-\\s]?\\d', re.I)),\n",
    "\n",
    "    # ─── EleutherAI & community families ────────────────────────────────────\n",
    "    ('GPT-NeoX',        re.compile(r'\\bgpt[\\-\\s]?neox\\b', re.I)),\n",
    "    ('GPT-J',           re.compile(r'\\bgpt[\\-\\s]?j\\b', re.I)),\n",
    "    ('Pythia',          re.compile(r'\\bpythia[\\-\\s]?\\d', re.I)),\n",
    "    ('RedPajama',       re.compile(r'\\bredpajama\\b', re.I)),\n",
    "\n",
    "    # ─── AI21 Labs ──────────────────────────────────────────────────────────\n",
    "    ('Jamba (AI21)',    re.compile(r'\\bjamba\\b', re.I)),\n",
    "    ('Jurassic (AI21)', re.compile(r'\\bjurassic\\b', re.I)),\n",
    "\n",
    "    # ─── Cohere ─────────────────────────────────────────────────────────────\n",
    "    # Matches: command, command-r, command-r+, command-light\n",
    "    ('Cohere Command',  re.compile(r'\\b(?:cohere[\\-\\s]?)?(?:command(?:[\\s\\-]?(?:r\\+?|light)?)|c4ai\\-command)\\b', re.I)),\n",
    "\n",
    "    # ─── NVIDIA ─────────────────────────────────────────────────────────────\n",
    "    ('Nemotron (NVIDIA)', re.compile(r'\\bnemotron\\b', re.I)),\n",
    "\n",
    "    # ─── TII UAE ────────────────────────────────────────────────────────────\n",
    "    ('Falcon',          re.compile(r'\\bfalcon[\\-\\s]?\\d', re.I)),\n",
    "\n",
    "    # ─── LMSYS & popular fine-tunes ─────────────────────────────────────────\n",
    "    ('Vicuna',          re.compile(r'\\bvicuna\\b', re.I)),\n",
    "    ('WizardLM',        re.compile(r'\\bwizard(?:lm|coder)\\b', re.I)),\n",
    "    ('Zephyr',          re.compile(r'\\bzephyr\\b', re.I)),\n",
    "    ('Nous Hermes',     re.compile(r'\\b(?:nous[\\-\\s]?)?hermes\\b|openhermes', re.I)),\n",
    "\n",
    "    # ─── Reka AI ────────────────────────────────────────────────────────────\n",
    "    ('Reka',            re.compile(r'\\breka\\b', re.I)),\n",
    "\n",
    "    # ─── Perplexity (API model ids like pplx-70b-online) ───────────────────\n",
    "    ('Perplexity (pplx)', re.compile(r'\\b(?:perplexity|pplx)[\\w\\-]*\\b', re.I)),\n",
    "\n",
    "    # ─── Alibaba Tongyi (other than Qwen) ───────────────────────────────────\n",
    "    ('Tongyi',          re.compile(r'\\btongyi\\b', re.I)),\n",
    "\n",
    "    # ─── OpenBMB MiniCPM ────────────────────────────────────────────────────\n",
    "    ('MiniCPM',         re.compile(r'\\bminicpm\\b', re.I)),\n",
    "\n",
    "    # ─── BigScience BLOOM ───────────────────────────────────────────────────\n",
    "    ('BLOOM',           re.compile(r'\\bbloomz?\\b', re.I)),\n",
    "\n",
    "    # ─── RWKV ───────────────────────────────────────────────────────────────\n",
    "    ('RWKV',            re.compile(r'\\brwkv\\b', re.I)),\n",
    "\n",
    "    # ─── Google Bard (legacy) ───────────────────────────────────────────────\n",
    "    ('Bard',            re.compile(r'\\bBard\\b', re.I)),\n",
    "\n",
    "    # ─── Assistants / bots (broad) ──────────────────────────────────────────\n",
    "    ('Bing Chat',       re.compile(r'\\bBing\\b', re.I)),\n",
    "    ('Pi',              re.compile(r'\\bPi\\b', re.I)),\n",
    "    ('Alexa',           re.compile(r'Alexa', re.I)),\n",
    "    ('Google Assistant',re.compile(r'Google Assistant', re.I)),\n",
    "\n",
    "    # ─── OpenAI catch‑all ───────────────────────────────────────────────────\n",
    "    ('OpenAI (other)',  re.compile(r'\\bOpenAI\\b', re.I)),\n",
    "\n",
    "    # ─── Broad buckets LAST ─────────────────────────────────────────────────\n",
    "    ('ChatGPT-unspecified',\n",
    "                        re.compile(r'\\bChat[\\s\\-]?GPT\\b', re.I)),\n",
    "    ('GPT-unspecified', re.compile(r'^\\s*GPT\\b(?![\\s\\-]?(?:4|3\\.5))', re.I)),\n",
    "]\n",
    "\n",
    "def categorize_brands(model_list):\n",
    "    \"\"\"\n",
    "    Return an OrderedDict mapping brand -> [matching original names] for the\n",
    "    provided iterable of model names.\n",
    "\n",
    "    Notes:\n",
    "      - First-match wins (ordering in `brand_patterns` matters).\n",
    "      - Buckets include one key per brand label; 'Other' collects leftovers.\n",
    "    \"\"\"\n",
    "    # Ensure unique keys appear once in the buckets, preserving *final* order\n",
    "    # from brand_patterns (duplicated labels would otherwise be overwritten).\n",
    "    seen = set()\n",
    "    ordered_keys = []\n",
    "    for brand, _ in brand_patterns:\n",
    "        if brand not in seen:\n",
    "            ordered_keys.append(brand)\n",
    "            seen.add(brand)\n",
    "\n",
    "    buckets = OrderedDict((brand, []) for brand in ordered_keys)\n",
    "    buckets['Other'] = []\n",
    "\n",
    "    for name in model_list:\n",
    "        for brand, pattern in brand_patterns:\n",
    "            if pattern.search(name):\n",
    "                buckets[brand].append(name)\n",
    "                break\n",
    "        else:\n",
    "            buckets['Other'].append(name)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "def categorize_brand(model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the brand key that best matches this single model name.\n",
    "    If no pattern matches, returns 'Other'.\n",
    "    \"\"\"\n",
    "    for brand, pattern in brand_patterns:\n",
    "        if pattern.search(model_name):\n",
    "            return brand\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5db5e",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcabedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Batch Responses/GPT-5r-minimal-data-classification-output.jsonl\", \"r\") as f:\n",
    "    responses = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up skeleton for processed data\n",
    "for study in included_studies:\n",
    "    processed_data = {\"models_types\": [],\n",
    "        \"model_categories\": [],\n",
    "        \"specialties\": [],\n",
    "        \"task_types\": [],\n",
    "        \"evaluation_types\": [],\n",
    "        \"evaluation_metrics\": [],\n",
    "        \"human_evaluators\": [],\n",
    "        \"dataset_types\": [],\n",
    "    }\n",
    "\n",
    "    study[\"processed_data\"] = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_requests = []\n",
    "with open(\"extracted_data_parsing.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        original_requests.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25446b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-5r-minimal had formatting issues with 16 out of 23045 responses (0.07% error rate!!), so we manually fixed the formatting (not content) of these:\n",
    "\n",
    "corrected_failures = {\n",
    "    569: '{\"categories\": [\"Knowledge Graphs / Ontologies / Terminologies\", \"Pharmacology & Medication Knowledge\", \"Educational & Reference Texts\"], \"access\": \"proprietary\"}',\n",
    "    6094: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Educational & Reference Texts\"], \"access\": \"unsure\"}',\n",
    "    7419: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Educational & Reference Texts\"], \"access\": \"proprietary\"}',\n",
    "    7779: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Patient-Facing Q&A & FAQs\", \"Research Literature & Abstract Corpora\", \"Educational & Reference Texts\"], \"access\": \"open-access\"}',\n",
    "    8929: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Clinical Vignettes & Case Reports\", \"Educational & Reference Texts\"], \"access\": \"proprietary\"}',\n",
    "    9309: '{\"categories\": [\"Imaging Data & Reports\", \"Real-World Electronic Health Records\"], \"access\": \"unsure\"}',\n",
    "    9645: '[[\"Radiology\", \"Diagnostic Radiology\"], [\"Pathology\", \"Pathology - Anatomic/Clinical (AP/CP)\"]]',\n",
    "    9704: '{\"categories\": [\"Clinical Vignettes & Case Reports\", \"Educational & Reference Texts\"], \"access\": \"unsure\"}',\n",
    "    11924: '{\"categories\": [\"Real-World Electronic Health Records\", \"Research Literature & Abstract Corpora\", \"Clinical Trial Eligibility & Structured Cohorts\"], \"access\": \"unsure\"}',\n",
    "    13669: '{\"categories\": [\"Synthetic / Simulated Clinical Data\", \"Dialogue & Chat Corpora\", \"Survey Instruments & Psychometric Scales\", \"Clinical Vignettes & Case Reports\", \"Social-Media & Search-Query Data\"], \"access\": \"unsure\"}',\n",
    "    15124: '{\"categories\": [\"Clinical Vignettes & Case Reports\", \"Real-World Electronic Health Records\"], \"access\": \"unsure\"}',\n",
    "    16789: '{\"categories\": [\"Patient-Facing Q&A & FAQs\", \"Dialogue & Chat Corpora\", \"Social-Media & Search-Query Data\"], \"access\": \"unsure\"}',\n",
    "    18244: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Educational & Reference Texts\", \"Research Literature & Abstract Corpora\"], \"access\": \"unsure\"}',\n",
    "    19504: '{\"categories\": [\"Clinician Board & Self-Assessment Questions\", \"Educational & Reference Texts\"], \"access\": \"open-access\"}',\n",
    "    20834: '{\"categories\": [\"Research Literature & Abstract Corpora\", \"Clinical Guidelines & Consensus Statements\"], \"access\": \"open-access\"}',\n",
    "    22994: '{\"categories\": [\"Research Literature & Abstract Corpora\", \"Imaging Data & Reports\", \"Educational & Reference Texts\", \"Clinical Vignettes & Case Reports\", \"Real-World Electronic Health Records\"], \"access\": \"open-access\"}'\n",
    "\n",
    "}\n",
    "\n",
    "for i, response in tqdm(enumerate(responses)):\n",
    "    try:\n",
    "        try:\n",
    "            data = json.loads(response[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"])\n",
    "        except:\n",
    "            data = json.loads(corrected_failures[i])\n",
    "\n",
    "        data_type = response[\"custom_id\"].split(\"-\")[1]\n",
    "\n",
    "        # find original study title/abstract to match from the batch request\n",
    "        assert original_requests[i][\"custom_id\"] == response[\"custom_id\"], \"Mismatched request/response!\"\n",
    "        original_study = original_requests[i]\n",
    "\n",
    "        # find matching study (by title/abstract)\n",
    "        # this isn't the most efficient thing on the planet but computers are fast :)\n",
    "        for study in included_studies:\n",
    "            found = False\n",
    "\n",
    "            title = study.get(\"Title\")\n",
    "            abstract = study.get(\"Abstract\")\n",
    "\n",
    "            if title and abstract and title in original_study[\"body\"][\"input\"]:\n",
    "                if isinstance(abstract, str):\n",
    "                    if abstract in original_study[\"body\"][\"input\"]:\n",
    "                        found = True\n",
    "                        break\n",
    "                else: # sometimes there is no abstract, so we handle this case\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        assert found, \"Could not find matching study for response!\"\n",
    "\n",
    "        if data_type == \"specialty\":\n",
    "            study[\"processed_data\"][\"specialties\"] = data\n",
    "        elif data_type == \"task\":\n",
    "            study[\"processed_data\"][\"task_types\"] = data\n",
    "        elif data_type == \"evaluation\":\n",
    "            study[\"processed_data\"][\"evaluation_types\"] = data\n",
    "        elif data_type == \"metric\":\n",
    "            study[\"processed_data\"][\"evaluation_metrics\"] = data\n",
    "        elif data_type == \"dataset\":\n",
    "            study[\"processed_data\"][\"dataset_types\"] = data\n",
    "        else:\n",
    "            print(f\"Unknown data type: {data_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process response {i}: {e}\")\n",
    "        failures.append((i, str(e), response))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d832ef",
   "metadata": {},
   "source": [
    "## This part is extra since we initially did not put the evaluator type in the prompt list, so we run/parse this separately\n",
    "(Note that this does not change methodology at all since each classification task is an individual batch request anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/responses\",\n",
    "              \"body\": {\"model\": \"gpt-5\", \n",
    "                      \"reasoning\": {\"effort\": \"minimal\"}, # this is mostly a classification task, so minimal effort should suffice\n",
    "                      \"instructions\": \"\",\n",
    "                      \"input\": \"\",\n",
    "              }\n",
    "            }\n",
    "\n",
    "out_jsonl = []\n",
    "\n",
    "for idx, row in tqdm(enumerate(included_studies), total=len(included_studies)):\n",
    "    for prompt, prompt_name in zip([evaluator_prompt], [\"evaluator\"]):\n",
    "        out = copy.deepcopy(template)\n",
    "        out[\"custom_id\"] = str(idx) + \"-\" + prompt_name\n",
    "        out[\"body\"][\"instructions\"] = prompt\n",
    "        out[\"body\"][\"input\"] = f\"Title: {row['Title']}\\nAbstract: {row['Abstract']}\\nExtracted data: {json.dumps(row['extracted_data'], indent=2)}\"\n",
    "        out_jsonl.append(out)\n",
    "\n",
    "with open(\"extracted_data_parsing_evaluator_only.jsonl\", \"w\") as f:\n",
    "    for entry in out_jsonl:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_requests = []\n",
    "with open(\"extracted_data_parsing_evaluator_only.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        original_requests.append(json.loads(line))\n",
    "\n",
    "with open(\"Batch Responses/GPT-5r-minimal-expert-classification-output.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        original_requests.append(json.loads(line))\n",
    "\n",
    "responses = [json.loads(line) for line in open(\"Batch Responses/GPT-5r-minimal-expert-classification-output.jsonl\", \"r\")]\n",
    "failures = []\n",
    "\n",
    "for i, response in tqdm(enumerate(responses)):\n",
    "    try:\n",
    "        try:\n",
    "            data = json.loads(response[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"])\n",
    "        except:\n",
    "            data = json.loads(corrected_failures[i])\n",
    "\n",
    "        data_type = response[\"custom_id\"].split(\"-\")[1]\n",
    "\n",
    "        # find original study title/abstract to match from the batch request\n",
    "        assert original_requests[i][\"custom_id\"] == response[\"custom_id\"], \"Mismatched request/response!\"\n",
    "        original_study = original_requests[i]\n",
    "\n",
    "        # find matching study (by title/abstract)\n",
    "        # this isn't the most efficient thing on the planet but computers are fast :)\n",
    "        for study in included_studies:\n",
    "            found = False\n",
    "\n",
    "            title = study.get(\"Title\")\n",
    "            abstract = study.get(\"Abstract\")\n",
    "\n",
    "            if title and abstract and title in original_study[\"body\"][\"input\"]:\n",
    "                if isinstance(abstract, str):\n",
    "                    if abstract in original_study[\"body\"][\"input\"]:\n",
    "                        found = True\n",
    "                        break\n",
    "                else: # sometimes there is no abstract, so we handle this case\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        assert found, \"Could not find matching study for response!\"\n",
    "\n",
    "        if data_type == \"specialty\":\n",
    "            study[\"processed_data\"][\"specialties\"] = data\n",
    "        elif data_type == \"task\":\n",
    "            study[\"processed_data\"][\"task_types\"] = data\n",
    "        elif data_type == \"evaluation\":\n",
    "            study[\"processed_data\"][\"evaluation_types\"] = data\n",
    "        elif data_type == \"metric\":\n",
    "            study[\"processed_data\"][\"evaluation_metrics\"] = data\n",
    "        elif data_type == \"dataset\":\n",
    "            study[\"processed_data\"][\"dataset_types\"] = data\n",
    "        elif data_type == \"evaluator\":\n",
    "            study[\"processed_data\"][\"human_evaluators\"] = data\n",
    "        else:\n",
    "            print(f\"Unknown data type: {data_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process response {i}: {e}\")\n",
    "        failures.append((i, str(e), response))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d85120",
   "metadata": {},
   "source": [
    "# Finally putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_data_parsing_dataset.jsonl\", \"r\") as f:\n",
    "    original_requests = [json.loads(line) for line in f]\n",
    "\n",
    "responses = []\n",
    "with open(\"Batch Responses/GPT-5r-minimal-dataset-reclassification_output.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        responses.append(json.loads(line))\n",
    "\n",
    "failures = []\n",
    "\n",
    "for i, response in tqdm(enumerate(responses)):\n",
    "    try:\n",
    "        try:\n",
    "            data = json.loads(response[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"])\n",
    "        except:\n",
    "            data = json.loads(corrected_failures[i])\n",
    "\n",
    "        data_type = response[\"custom_id\"].split(\"-\")[1]\n",
    "\n",
    "        # find original study title/abstract to match from the batch request\n",
    "        assert original_requests[i][\"custom_id\"] == response[\"custom_id\"], \"Mismatched request/response!\"\n",
    "        original_study = original_requests[i]\n",
    "\n",
    "        # find matching study (by title/abstract)\n",
    "        # this isn't the most efficient thing on the planet but computers are fast :)\n",
    "        for study in included_studies:\n",
    "            found = False\n",
    "\n",
    "            title = study.get(\"Title\")\n",
    "            abstract = study.get(\"Abstract\")\n",
    "\n",
    "            if title and abstract and title in original_study[\"body\"][\"input\"]:\n",
    "                if isinstance(abstract, str):\n",
    "                    if abstract in original_study[\"body\"][\"input\"]:\n",
    "                        found = True\n",
    "                        break\n",
    "                else: # sometimes there is no abstract, so we handle this case\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        assert found, \"Could not find matching study for response!\"\n",
    "\n",
    "        if data_type == \"specialty\":\n",
    "            study[\"processed_data\"][\"specialties\"] = data\n",
    "        elif data_type == \"task\":\n",
    "            study[\"processed_data\"][\"task_types\"] = data\n",
    "        elif data_type == \"evaluation\":\n",
    "            study[\"processed_data\"][\"evaluation_types\"] = data\n",
    "        elif data_type == \"metric\":\n",
    "            study[\"processed_data\"][\"evaluation_metrics\"] = data\n",
    "        elif data_type == \"dataset_specific\":\n",
    "            study[\"processed_data\"][\"dataset_types\"] = data\n",
    "        elif data_type == \"evaluator\":\n",
    "            study[\"processed_data\"][\"human_evaluators\"] = data\n",
    "        else:\n",
    "            print(f\"Unknown data type: {data_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process response {i}: {e}\")\n",
    "        failures.append((i, str(e), response))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_data_parsing_tasks.jsonl\", \"r\") as f:\n",
    "    original_requests = [json.loads(line) for line in f]\n",
    "\n",
    "responses = []\n",
    "with open(\"Batch Responses/GPT-5r-minimal-task-specific-output.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        responses.append(json.loads(line))\n",
    "\n",
    "failures = []\n",
    "\n",
    "for i, response in tqdm(enumerate(responses)):\n",
    "    try:\n",
    "        try:\n",
    "            data = json.loads(response[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"])\n",
    "        except:\n",
    "            data = json.loads(corrected_failures[i])\n",
    "\n",
    "        data_type = response[\"custom_id\"].split(\"-\")[1]\n",
    "\n",
    "        # find original study title/abstract to match from the batch request\n",
    "        assert original_requests[i][\"custom_id\"] == response[\"custom_id\"], \"Mismatched request/response!\"\n",
    "        original_study = original_requests[i]\n",
    "\n",
    "        # find matching study (by title/abstract)\n",
    "        # this isn't the most efficient thing on the planet but computers are fast :)\n",
    "        for study in included_studies:\n",
    "            found = False\n",
    "\n",
    "            title = study.get(\"Title\")\n",
    "            abstract = study.get(\"Abstract\")\n",
    "\n",
    "            if title and abstract and title in original_study[\"body\"][\"input\"]:\n",
    "                if isinstance(abstract, str):\n",
    "                    if abstract in original_study[\"body\"][\"input\"]:\n",
    "                        found = True\n",
    "                        break\n",
    "                else: # sometimes there is no abstract, so we handle this case\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "        assert found, \"Could not find matching study for response!\"\n",
    "\n",
    "        if data_type == \"specialty\":\n",
    "            study[\"processed_data\"][\"specialties\"] = data\n",
    "        elif data_type == \"task_specific\":\n",
    "            study[\"processed_data\"][\"task_types\"] = data\n",
    "        elif data_type == \"evaluation\":\n",
    "            study[\"processed_data\"][\"evaluation_types\"] = data\n",
    "        elif data_type == \"metric\":\n",
    "            study[\"processed_data\"][\"evaluation_metrics\"] = data\n",
    "        elif data_type == \"dataset_specific\":\n",
    "            study[\"processed_data\"][\"dataset_types\"] = data\n",
    "        elif data_type == \"evaluator\":\n",
    "            study[\"processed_data\"][\"human_evaluators\"] = data\n",
    "        else:\n",
    "            print(f\"Unknown data type: {data_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process response {i}: {e}\")\n",
    "        failures.append((i, str(e), response))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f06905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for study in included_studies:\n",
    "    # find the category for the model\n",
    "    processed_data = {\"models_types\": [],\n",
    "        \"model_categories\": []}\n",
    "    for model in study[\"extracted_data\"][\"models_used\"]:\n",
    "        if model == \"\":\n",
    "            continue\n",
    "        found = False\n",
    "        for key, value in categories.items():\n",
    "            if model in value:\n",
    "                processed_data[\"models_types\"].append(key)\n",
    "                processed_data[\"model_categories\"].append(categorize_brand(model))\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            raise ValueError(f\"Model '{model}' not found in any category.\")\n",
    "    \n",
    "    study[\"processed_data\"].update(processed_data)\n",
    "\n",
    "with open(\"final_processed_studies.jsonl\", \"w\") as f:\n",
    "    for study in included_studies:\n",
    "        f.write(json.dumps(study) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
